[
    {
        "id": "http://arxiv.org/abs/0705.2363v1",
        "title": "Lasso type classifiers with a reject option",
        "summary": "  We consider the problem of binary classification where one can, for a\nparticular cost, choose not to classify an observation. We present a simple\nproof for the oracle inequality for the excess risk of structural risk\nminimizers using a lasso type penalty.\n",
        "published": "2007-05-16T14:23:17Z",
        "pdf_link": "http://arxiv.org/pdf/0705.2363v1"
    },
    {
        "id": "http://arxiv.org/abs/0706.3499v1",
        "title": "Metric Embedding for Nearest Neighbor Classification",
        "summary": "  The distance metric plays an important role in nearest neighbor (NN)\nclassification. Usually the Euclidean distance metric is assumed or a\nMahalanobis distance metric is optimized to improve the NN performance. In this\npaper, we study the problem of embedding arbitrary metric spaces into a\nEuclidean space with the goal to improve the accuracy of the NN classifier. We\npropose a solution by appealing to the framework of regularization in a\nreproducing kernel Hilbert space and prove a representer-like theorem for NN\nclassification. The embedding function is then determined by solving a\nsemidefinite program which has an interesting connection to the soft-margin\nlinear binary support vector machine classifier. Although the main focus of\nthis paper is to present a general, theoretical framework for metric embedding\nin a NN setting, we demonstrate the performance of the proposed method on some\nbenchmark datasets and show that it performs better than the Mahalanobis metric\nlearning algorithm in terms of leave-one-out and generalization errors.\n",
        "published": "2007-06-24T06:50:24Z",
        "pdf_link": "http://arxiv.org/pdf/0706.3499v1"
    },
    {
        "id": "http://arxiv.org/abs/0707.3536v1",
        "title": "Degenerating families of dendrograms",
        "summary": "  Dendrograms used in data analysis are ultrametric spaces, hence objects of\nnonarchimedean geometry. It is known that there exist $p$-adic representation\nof dendrograms. Completed by a point at infinity, they can be viewed as\nsubtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.\nThe implications are that certain moduli spaces known in algebraic geometry are\n$p$-adic parameter spaces of (families of) dendrograms, and stochastic\nclassification can also be handled within this framework. At the end, we\ncalculate the topology of the hidden part of a dendrogram.\n",
        "published": "2007-07-24T12:45:39Z",
        "pdf_link": "http://arxiv.org/pdf/0707.3536v1"
    },
    {
        "id": "http://arxiv.org/abs/0707.4072v1",
        "title": "Families of dendrograms",
        "summary": "  A conceptual framework for cluster analysis from the viewpoint of p-adic\ngeometry is introduced by describing the space of all dendrograms for n\ndatapoints and relating it to the moduli space of p-adic Riemannian spheres\nwith punctures using a method recently applied by Murtagh (2004b). This method\nembeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the\np-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.\nAfter explaining the definitions, the concept of classifiers is discussed in\nthe context of moduli spaces, and upper bounds for the number of hidden\nvertices in dendrograms are given.\n",
        "published": "2007-07-27T09:37:28Z",
        "pdf_link": "http://arxiv.org/pdf/0707.4072v1"
    },
    {
        "id": "http://arxiv.org/abs/0709.2760v3",
        "title": "Supervised Machine Learning with a Novel Kernel Density Estimator",
        "summary": "  In recent years, kernel density estimation has been exploited by computer\nscientists to model machine learning problems. The kernel density estimation\nbased approaches are of interest due to the low time complexity of either O(n)\nor O(n*log(n)) for constructing a classifier, where n is the number of sampling\ninstances. Concerning design of kernel density estimators, one essential issue\nis how fast the pointwise mean square error (MSE) and/or the integrated mean\nsquare error (IMSE) diminish as the number of sampling instances increases. In\nthis article, it is shown that with the proposed kernel function it is feasible\nto make the pointwise MSE of the density estimator converge at O(n^-2/3)\nregardless of the dimension of the vector space, provided that the probability\ndensity function at the point of interest meets certain conditions.\n",
        "published": "2007-09-18T06:45:30Z",
        "pdf_link": "http://arxiv.org/pdf/0709.2760v3"
    },
    {
        "id": "http://arxiv.org/abs/0709.2936v1",
        "title": "Bayesian Classification and Regression with High Dimensional Features",
        "summary": "  This thesis responds to the challenges of using a large number, such as\nthousands, of features in regression and classification problems.\n  There are two situations where such high dimensional features arise. One is\nwhen high dimensional measurements are available, for example, gene expression\ndata produced by microarray techniques. For computational or other reasons,\npeople may select only a small subset of features when modelling such data, by\nlooking at how relevant the features are to predicting the response, based on\nsome measure such as correlation with the response in the training data.\nAlthough it is used very commonly, this procedure will make the response appear\nmore predictable than it actually is. In Chapter 2, we propose a Bayesian\nmethod to avoid this selection bias, with application to naive Bayes models and\nmixture models.\n  High dimensional features also arise when we consider high-order\ninteractions. The number of parameters will increase exponentially with the\norder considered. In Chapter 3, we propose a method for compressing a group of\nparameters into a single one, by exploiting the fact that many predictor\nvariables derived from high-order interactions have the same values for all the\ntraining cases. The number of compressed parameters may have converged before\nconsidering the highest possible order. We apply this compression method to\nlogistic sequence prediction models and logistic classification models.\n  We use both simulated data and real data to test our methods in both\nchapters.\n",
        "published": "2007-09-18T23:56:17Z",
        "pdf_link": "http://arxiv.org/pdf/0709.2936v1"
    },
    {
        "id": "http://arxiv.org/abs/0709.2989v1",
        "title": "Simulated Annealing: Rigorous finite-time guarantees for optimization on\n  continuous domains",
        "summary": "  Simulated annealing is a popular method for approaching the solution of a\nglobal optimization problem. Existing results on its performance apply to\ndiscrete combinatorial optimization where the optimization variables can assume\nonly a finite set of possible values. We introduce a new general formulation of\nsimulated annealing which allows one to guarantee finite-time performance in\nthe optimization of functions of continuous variables. The results hold\nuniversally for any optimization problem on a bounded domain and establish a\nconnection between simulated annealing and up-to-date theory of convergence of\nMarkov chain Monte Carlo methods on continuous domains. This work is inspired\nby the concept of finite-time learning with known accuracy and confidence\ndeveloped in statistical learning theory.\n",
        "published": "2007-09-19T10:56:13Z",
        "pdf_link": "http://arxiv.org/pdf/0709.2989v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.0845v3",
        "title": "The nested Chinese restaurant process and Bayesian nonparametric\n  inference of topic hierarchies",
        "summary": "  We present the nested Chinese restaurant process (nCRP), a stochastic process\nwhich assigns probability distributions to infinitely-deep,\ninfinitely-branching trees. We show how this stochastic process can be used as\na prior distribution in a Bayesian nonparametric model of document collections.\nSpecifically, we present an application to information retrieval in which\ndocuments are modeled as paths down a random tree, and the preferential\nattachment dynamics of the nCRP leads to clustering of documents according to\nsharing of topics at multiple levels of abstraction. Given a corpus of\ndocuments, a posterior inference algorithm finds an approximation to a\nposterior distribution over trees, topics and allocations of words to levels of\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\nfrom several journals. This model exemplifies a recent trend in statistical\nmachine learning--the use of Bayesian nonparametric methods to infer\ndistributions on flexible data structures.\n",
        "published": "2007-10-03T17:32:21Z",
        "pdf_link": "http://arxiv.org/pdf/0710.0845v3"
    },
    {
        "id": "http://arxiv.org/abs/0710.3183v1",
        "title": "Probabilistic coherence and proper scoring rules",
        "summary": "  We provide self-contained proof of a theorem relating probabilistic coherence\nof forecasts to their non-domination by rival forecasts with respect to any\nproper scoring rule. The theorem appears to be new but is closely related to\nresults achieved by other investigators.\n",
        "published": "2007-10-16T21:16:29Z",
        "pdf_link": "http://arxiv.org/pdf/0710.3183v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.3742v1",
        "title": "Bayesian Online Changepoint Detection",
        "summary": "  Changepoints are abrupt variations in the generative parameters of a data\nsequence. Online detection of changepoints is useful in modelling and\nprediction of time series in application areas such as finance, biometrics, and\nrobotics. While frequentist methods have yielded online filtering and\nprediction techniques, most Bayesian papers have focused on the retrospective\nsegmentation problem. Here we examine the case where the model parameters\nbefore and after the changepoint are independent and we derive an online\nalgorithm for exact inference of the most recent changepoint. We compute the\nprobability distribution of the length of the current ``run,'' or time since\nthe last changepoint, using a simple message-passing algorithm. Our\nimplementation is highly modular so that the algorithm may be applied to a\nvariety of types of data. We illustrate this modularity by demonstrating the\nalgorithm on three different real-world data sets.\n",
        "published": "2007-10-19T17:18:30Z",
        "pdf_link": "http://arxiv.org/pdf/0710.3742v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.2434v1",
        "title": "Variable importance in binary regression trees and forests",
        "summary": "  We characterize and study variable importance (VIMP) and pairwise variable\nassociations in binary regression trees. A key component involves the node mean\nsquared error for a quantity we refer to as a maximal subtree. The theory\nnaturally extends from single trees to ensembles of trees and applies to\nmethods like random forests. This is useful because while importance values\nfrom random forests are used to screen variables, for example they are used to\nfilter high throughput genomic data in Bioinformatics, very little theory\nexists about their properties.\n",
        "published": "2007-11-15T15:09:41Z",
        "pdf_link": "http://arxiv.org/pdf/0711.2434v1"
    },
    {
        "id": "http://arxiv.org/abs/0712.0248v1",
        "title": "Pac-Bayesian Supervised Classification: The Thermodynamics of\n  Statistical Learning",
        "summary": "  This monograph deals with adaptive supervised classification, using tools\nborrowed from statistical mechanics and information theory, stemming from the\nPACBayesian approach pioneered by David McAllester and applied to a conception\nof statistical learning theory forged by Vladimir Vapnik. Using convex analysis\non the set of posterior probability measures, we show how to get local measures\nof the complexity of the classification model involving the relative entropy of\nposterior distributions with respect to Gibbs posterior measures. We then\ndiscuss relative bounds, comparing the generalization error of two\nclassification rules, showing how the margin assumption of Mammen and Tsybakov\ncan be replaced with some empirical measure of the covariance structure of the\nclassification model.We show how to associate to any posterior distribution an\neffective temperature relating it to the Gibbs prior distribution with the same\nlevel of expected error rate, and how to estimate this effective temperature\nfrom data, resulting in an estimator whose expected error rate converges\naccording to the best possible power of the sample size adaptively under any\nmargin and parametric complexity assumptions. We describe and study an\nalternative selection scheme based on relative bounds between estimators, and\npresent a two step localization technique which can handle the selection of a\nparametric model from a family of those. We show how to extend systematically\nall the results obtained in the inductive setting to transductive learning, and\nuse this to improve Vapnik's generalization bounds, extending them to the case\nwhen the sample is made of independent non-identically distributed pairs of\npatterns and labels. Finally we review briefly the construction of Support\nVector Machines and show how to derive generalization bounds for them,\nmeasuring the complexity either through the number of support vectors or\nthrough the value of the transductive or inductive margin.\n",
        "published": "2007-12-03T13:49:36Z",
        "pdf_link": "http://arxiv.org/pdf/0712.0248v1"
    },
    {
        "id": "http://arxiv.org/abs/0802.2906v2",
        "title": "Classification Constrained Dimensionality Reduction",
        "summary": "  Dimensionality reduction is a topic of recent interest. In this paper, we\npresent the classification constrained dimensionality reduction (CCDR)\nalgorithm to account for label information. The algorithm can account for\nmultiple classes as well as the semi-supervised setting. We present an\nout-of-sample expressions for both labeled and unlabeled data. For unlabeled\ndata, we introduce a method of embedding a new point as preprocessing to a\nclassifier. For labeled data, we introduce a method that improves the embedding\nduring the training phase using the out-of-sample extension. We investigate\nclassification performance using the CCDR algorithm on hyper-spectral satellite\nimagery data. We demonstrate the performance gain for both local and global\nclassifiers and demonstrate a 10% improvement of the $k$-nearest neighbors\nalgorithm performance. We present a connection between intrinsic dimension\nestimation and the optimal embedding dimension obtained using the CCDR\nalgorithm.\n",
        "published": "2008-02-20T18:26:31Z",
        "pdf_link": "http://arxiv.org/pdf/0802.2906v2"
    },
    {
        "id": "http://arxiv.org/abs/0803.1628v1",
        "title": "Component models for large networks",
        "summary": "  Being among the easiest ways to find meaningful structure from discrete data,\nLatent Dirichlet Allocation (LDA) and related component models have been\napplied widely. They are simple, computationally fast and scalable,\ninterpretable, and admit nonparametric priors. In the currently popular field\nof network modeling, relatively little work has taken uncertainty of data\nseriously in the Bayesian sense, and component models have been introduced to\nthe field only recently, by treating each node as a bag of out-going links. We\nintroduce an alternative, interaction component model for communities (ICMc),\nwhere the whole network is a bag of links, stemming from different components.\nThe former finds both disassortative and assortative structure, while the\nalternative assumes assortativity and finds community-like structures like the\nearlier methods motivated by physics. With Dirichlet Process priors and an\nefficient implementation the models are highly scalable, as demonstrated with a\nsocial network from the Last.fm web site, with 670,000 nodes and 1.89 million\nlinks.\n",
        "published": "2008-03-11T18:38:52Z",
        "pdf_link": "http://arxiv.org/pdf/0803.1628v1"
    },
    {
        "id": "http://arxiv.org/abs/0804.1026v1",
        "title": "Testing for Homogeneity with Kernel Fisher Discriminant Analysis",
        "summary": "  We propose to investigate test statistics for testing homogeneity in\nreproducing kernel Hilbert spaces. Asymptotic null distributions under null\nhypothesis are derived, and consistency against fixed and local alternatives is\nassessed. Finally, experimental evidence of the performance of the proposed\napproach on both artificial data and a speaker verification task is provided.\n",
        "published": "2008-04-07T13:46:27Z",
        "pdf_link": "http://arxiv.org/pdf/0804.1026v1"
    },
    {
        "id": "http://arxiv.org/abs/0804.1325v1",
        "title": "On the underestimation of model uncertainty by Bayesian K-nearest\n  neighbors",
        "summary": "  When using the K-nearest neighbors method, one often ignores uncertainty in\nthe choice of K. To account for such uncertainty, Holmes and Adams (2002)\nproposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN\n(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain\nMonte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams\n(2002) focused on the performance of BKNN in terms of misclassification error\nbut did not assess its ability to quantify uncertainty. We present some\nevidence to show that BKNN still significantly underestimates model\nuncertainty.\n",
        "published": "2008-04-08T16:58:11Z",
        "pdf_link": "http://arxiv.org/pdf/0804.1325v1"
    },
    {
        "id": "http://arxiv.org/abs/0804.2848v1",
        "title": "Information Preserving Component Analysis: Data Projections for Flow\n  Cytometry Analysis",
        "summary": "  Flow cytometry is often used to characterize the malignant cells in leukemia\nand lymphoma patients, traced to the level of the individual cell. Typically,\nflow cytometric data analysis is performed through a series of 2-dimensional\nprojections onto the axes of the data set. Through the years, clinicians have\ndetermined combinations of different fluorescent markers which generate\nrelatively known expression patterns for specific subtypes of leukemia and\nlymphoma -- cancers of the hematopoietic system. By only viewing a series of\n2-dimensional projections, the high-dimensional nature of the data is rarely\nexploited. In this paper we present a means of determining a low-dimensional\nprojection which maintains the high-dimensional relationships (i.e.\ninformation) between differing oncological data sets. By using machine learning\ntechniques, we allow clinicians to visualize data in a low dimension defined by\na linear combination of all of the available markers, rather than just 2 at a\ntime. This provides an aid in diagnosing similar forms of cancer, as well as a\nmeans for variable selection in exploratory flow cytometric research. We refer\nto our method as Information Preserving Component Analysis (IPCA).\n",
        "published": "2008-04-17T16:25:48Z",
        "pdf_link": "http://arxiv.org/pdf/0804.2848v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.2646v1",
        "title": "Manifold Learning: The Price of Normalization",
        "summary": "  We analyze the performance of a class of manifold-learning algorithms that\nfind their output by minimizing a quadratic form under some normalization\nconstraints. This class consists of Locally Linear Embedding (LLE), Laplacian\nEigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and\nDiffusion maps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite sample case and\nthe limit case are analyzed. We show that there are simple manifolds in which\nthe necessary conditions are violated, and hence the algorithms cannot recover\nthe underlying manifolds. Finally, we present numerical results that\ndemonstrate our claims.\n",
        "published": "2008-06-16T19:54:49Z",
        "pdf_link": "http://arxiv.org/pdf/0806.2646v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.2669v1",
        "title": "Local Procrustes for Manifold Embedding: A Measure of Embedding Quality\n  and Embedding Algorithms",
        "summary": "  We present the Procrustes measure, a novel measure based on Procrustes\nrotation that enables quantitative comparison of the output of manifold-based\nembedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum\net al, 2000)). The measure also serves as a natural tool when choosing\ndimension-reduction parameters. We also present two novel dimension-reduction\ntechniques that attempt to minimize the suggested measure, and compare the\nresults of these techniques to the results of existing algorithms. Finally, we\nsuggest a simple iterative method that can be used to improve the output of\nexisting algorithms.\n",
        "published": "2008-06-16T20:41:57Z",
        "pdf_link": "http://arxiv.org/pdf/0806.2669v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.2831v1",
        "title": "Supervised functional classification: A theoretical remark and some\n  comparisons",
        "summary": "  The problem of supervised classification (or discrimination) with functional\ndata is considered, with a special interest on the popular k-nearest neighbors\n(k-NN) classifier. First, relying on a recent result by Cerou and Guyader\n(2006), we prove the consistency of the k-NN classifier for functional data\nwhose distribution belongs to a broad family of Gaussian processes with\ntriangular covariance functions. Second, on a more practical side, we check the\nbehavior of the k-NN method when compared with a few other functional\nclassifiers. This is carried out through a small simulation study and the\nanalysis of several real functional data sets. While no global \"uniform\" winner\nemerges from such comparisons, the overall performance of the k-NN method,\ntogether with its sound intuitive motivation and relative simplicity, suggests\nthat it could represent a reasonable benchmark for the classification problem\nwith functional data.\n",
        "published": "2008-06-17T16:20:35Z",
        "pdf_link": "http://arxiv.org/pdf/0806.2831v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.4115v4",
        "title": "High-dimensional additive modeling",
        "summary": "  We propose a new sparsity-smoothness penalty for high-dimensional generalized\nadditive models. The combination of sparsity and smoothness is crucial for\nmathematical theory as well as performance for finite-sample data. We present a\ncomputationally efficient algorithm, with provable numerical convergence\nproperties, for optimizing the penalized likelihood. Furthermore, we provide\noracle results which yield asymptotic optimality of our estimator for high\ndimensional but sparse additive models. Finally, an adaptive version of our\nsparsity-smoothness penalized approach yields large additional performance\ngains.\n",
        "published": "2008-06-25T14:54:21Z",
        "pdf_link": "http://arxiv.org/pdf/0806.4115v4"
    },
    {
        "id": "http://arxiv.org/abs/0808.0780v1",
        "title": "LLE with low-dimensional neighborhood representation",
        "summary": "  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing\ntechnique, widely used due to its computational simplicity and intuitive\napproach. LLE first linearly reconstructs each input point from its nearest\nneighbors and then preserves these neighborhood relations in the\nlow-dimensional embedding. We show that the reconstruction weights computed by\nLLE capture the high-dimensional structure of the neighborhoods, and not the\nlow-dimensional manifold structure. Consequently, the weight vectors are highly\nsensitive to noise. Moreover, this causes LLE to converge to a linear\nprojection of the input, as opposed to its non-linear embedding goal. To\novercome both of these problems, we propose to compute the weight vectors using\na low-dimensional neighborhood representation. We prove theoretically that this\nstraightforward and computationally simple modification of LLE reduces LLE's\nsensitivity to noise. This modification also removes the need for\nregularization when the number of neighbors is larger than the dimension of the\ninput. We present numerical examples demonstrating both the perturbation and\nlinear projection problems, and the improved outputs using the low-dimensional\nneighborhood representation.\n",
        "published": "2008-08-06T06:25:52Z",
        "pdf_link": "http://arxiv.org/pdf/0808.0780v1"
    },
    {
        "id": "http://arxiv.org/abs/0808.2241v1",
        "title": "Persistent Clustering and a Theorem of J. Kleinberg",
        "summary": "  We construct a framework for studying clustering algorithms, which includes\ntwo key ideas: persistence and functoriality. The first encodes the idea that\nthe output of a clustering scheme should carry a multiresolution structure, the\nsecond the idea that one should be able to compare the results of clustering\nalgorithms as one varies the data set, for example by adding points or by\napplying functions to it. We show that within this framework, one can prove a\ntheorem analogous to one of J. Kleinberg, in which one obtains an existence and\nuniqueness theorem instead of a non-existence result. We explore further\nproperties of this unique scheme, stability and convergence are established.\n",
        "published": "2008-08-16T08:16:11Z",
        "pdf_link": "http://arxiv.org/pdf/0808.2241v1"
    },
    {
        "id": "http://arxiv.org/abs/0808.2337v1",
        "title": "Decomposable Principal Component Analysis",
        "summary": "  We consider principal component analysis (PCA) in decomposable Gaussian\ngraphical models. We exploit the prior information in these models in order to\ndistribute its computation. For this purpose, we reformulate the problem in the\nsparse inverse covariance (concentration) domain and solve the global\neigenvalue problem using a sequence of local eigenvalue problems in each of the\ncliques of the decomposable graph. We demonstrate the application of our\nmethodology in the context of decentralized anomaly detection in the Abilene\nbackbone network. Based on the topology of the network, we propose an\napproximate statistical graphical model and distribute the computation of PCA.\n",
        "published": "2008-08-18T19:07:06Z",
        "pdf_link": "http://arxiv.org/pdf/0808.2337v1"
    },
    {
        "id": "http://arxiv.org/abs/0810.0901v2",
        "title": "Large Scale Variational Inference and Experimental Design for Sparse\n  Generalized Linear Models",
        "summary": "  Many problems of low-level computer vision and image processing, such as\ndenoising, deconvolution, tomographic reconstruction or super-resolution, can\nbe addressed by maximizing the posterior distribution of a sparse linear model\n(SLM). We show how higher-order Bayesian decision-making problems, such as\noptimizing image acquisition in magnetic resonance scanners, can be addressed\nby querying the SLM posterior covariance, unrelated to the density's mode. We\npropose a scalable algorithmic framework, with which SLM posteriors over full,\nhigh-resolution images can be approximated for the first time, solving a\nvariational optimization problem which is convex iff posterior mode finding is\nconvex. These methods successfully drive the optimization of sampling\ntrajectories for real-world magnetic resonance imaging through Bayesian\nexperimental design, which has not been attempted before. Our methodology\nprovides new insight into similarities and differences between sparse\nreconstruction and approximate Bayesian inference, and has important\nimplications for compressive sensing of real-world images.\n",
        "published": "2008-10-06T07:59:45Z",
        "pdf_link": "http://arxiv.org/pdf/0810.0901v2"
    },
    {
        "id": "http://arxiv.org/abs/0810.4553v1",
        "title": "Online Coordinate Boosting",
        "summary": "  We present a new online boosting algorithm for adapting the weights of a\nboosted classifier, which yields a closer approximation to Freund and\nSchapire's AdaBoost algorithm than previous online boosting algorithms. We also\ncontribute a new way of deriving the online algorithm that ties together\nprevious online boosting work. We assume that the weak hypotheses were selected\nbeforehand, and only their weights are updated during online boosting. The\nupdate rule is derived by minimizing AdaBoost's loss when viewed in an\nincremental form. The equations show that optimization is computationally\nexpensive. However, a fast online approximation is possible. We compare\napproximation error to batch AdaBoost on synthetic datasets and generalization\nerror on face datasets and the MNIST dataset.\n",
        "published": "2008-10-24T21:05:16Z",
        "pdf_link": "http://arxiv.org/pdf/0810.4553v1"
    },
    {
        "id": "http://arxiv.org/abs/0810.5117v1",
        "title": "A non-negative expansion for small Jensen-Shannon Divergences",
        "summary": "  In this report, we derive a non-negative series expansion for the\nJensen-Shannon divergence (JSD) between two probability distributions. This\nseries expansion is shown to be useful for numerical calculations of the JSD,\nwhen the probability distributions are nearly equal, and for which,\nconsequently, small numerical errors dominate evaluation.\n",
        "published": "2008-10-28T19:42:15Z",
        "pdf_link": "http://arxiv.org/pdf/0810.5117v1"
    },
    {
        "id": "http://arxiv.org/abs/0812.1615v1",
        "title": "Missing Data using Decision Forest and Computational Intelligence",
        "summary": "  Autoencoder neural network is implemented to estimate the missing data.\nGenetic algorithm is implemented for network optimization and estimating the\nmissing data. Missing data is treated as Missing At Random mechanism by\nimplementing maximum likelihood algorithm. The network performance is\ndetermined by calculating the mean square error of the network prediction. The\nnetwork is further optimized by implementing Decision Forest. The impact of\nmissing data is then investigated and decision forrests are found to improve\nthe results.\n",
        "published": "2008-12-09T04:33:38Z",
        "pdf_link": "http://arxiv.org/pdf/0812.1615v1"
    },
    {
        "id": "http://arxiv.org/abs/0812.1949v1",
        "title": "Prediction with Restricted Resources and Finite Automata",
        "summary": "  We obtain an index of the complexity of a random sequence by allowing the\nrole of the measure in classical probability theory to be played by a function\nwe call the generating mechanism. Typically, this generating mechanism will be\na finite automata. We generate a set of biased sequences by applying a finite\nstate automata with a specified number, $m$, of states to the set of all binary\nsequences. Thus we can index the complexity of our random sequence by the\nnumber of states of the automata. We detail optimal algorithms to predict\nsequences generated in this way.\n",
        "published": "2008-12-10T16:22:26Z",
        "pdf_link": "http://arxiv.org/pdf/0812.1949v1"
    },
    {
        "id": "http://arxiv.org/abs/0901.0026v1",
        "title": "On the Geometry of Discrete Exponential Families with Application to\n  Exponential Random Graph Models",
        "summary": "  There has been an explosion of interest in statistical models for analyzing\nnetwork data, and considerable interest in the class of exponential random\ngraph (ERG) models, especially in connection with difficulties in computing\nmaximum likelihood estimates. The issues associated with these difficulties\nrelate to the broader structure of discrete exponential families. This paper\nre-examines the issues in two parts. First we consider the closure of\n$k$-dimensional exponential families of distribution with discrete base measure\nand polyhedral convex support $\\mathrm{P}$. We show that the normal fan of\n$\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving\nthe statistical and geometric properties of the corresponding extended\nexponential families. We discuss its relevance to maximum likelihood\nestimation, both from a theoretical and computational standpoint. Second, we\napply our results to the analysis of ERG models. In particular, by means of a\ndetailed example, we provide some characterization of the properties of ERG\nmodels, and, in particular, of certain behaviors of ERG models known as\ndegeneracy.\n",
        "published": "2008-12-30T23:09:18Z",
        "pdf_link": "http://arxiv.org/pdf/0901.0026v1"
    },
    {
        "id": "http://arxiv.org/abs/0902.0600v5",
        "title": "Reconstruction of Epsilon-Machines in Predictive Frameworks and\n  Decisional States",
        "summary": "  This article introduces both a new algorithm for reconstructing\nepsilon-machines from data, as well as the decisional states. These are defined\nas the internal states of a system that lead to the same decision, based on a\nuser-provided utility or pay-off function. The utility function encodes some a\npriori knowledge external to the system, it quantifies how bad it is to make\nmistakes. The intrinsic underlying structure of the system is modeled by an\nepsilon-machine and its causal states. The decisional states form a partition\nof the lower-level causal states that is defined according to the higher-level\nuser's knowledge. In a complex systems perspective, the decisional states are\nthus the \"emerging\" patterns corresponding to the utility function. The\ntransitions between these decisional states correspond to events that lead to a\nchange of decision. The new REMAPF algorithm estimates both the epsilon-machine\nand the decisional states from data. Application examples are given for hidden\nmodel reconstruction, cellular automata filtering, and edge detection in\nimages.\n",
        "published": "2009-02-03T20:48:24Z",
        "pdf_link": "http://arxiv.org/pdf/0902.0600v5"
    },
    {
        "id": "http://arxiv.org/abs/0902.3347v1",
        "title": "Lanczos Approximations for the Speedup of Kernel Partial Least Squares\n  Regression",
        "summary": "  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is\nquadratic in the number of examples. However, the necessity of obtaining\nsensitivity measures as degrees of freedom for model selection or confidence\nintervals for more detailed analysis requires cubic runtime, and thus\nconstitutes a computational bottleneck in real-world data analysis. We propose\na novel algorithm for KPLS which not only computes (a) the fit, but also (b)\nits approximate degrees of freedom and (c) error bars in quadratic runtime. The\nalgorithm exploits a close connection between Kernel PLS and the Lanczos\nalgorithm for approximating the eigenvalues of symmetric matrices, and uses\nthis approximation to compute the trace of powers of the kernel matrix in\nquadratic runtime.\n",
        "published": "2009-02-19T11:28:20Z",
        "pdf_link": "http://arxiv.org/pdf/0902.3347v1"
    },
    {
        "id": "http://arxiv.org/abs/0902.3453v1",
        "title": "Escaping the curse of dimensionality with a tree-based regressor",
        "summary": "  We present the first tree-based regressor whose convergence rate depends only\non the intrinsic dimension of the data, namely its Assouad dimension. The\nregressor uses the RPtree partitioning procedure, a simple randomized variant\nof k-d trees.\n",
        "published": "2009-02-19T20:50:53Z",
        "pdf_link": "http://arxiv.org/pdf/0902.3453v1"
    },
    {
        "id": "http://arxiv.org/abs/0903.0649v1",
        "title": "The Nonparanormal: Semiparametric Estimation of High Dimensional\n  Undirected Graphs",
        "summary": "  Recent methods for estimating sparse undirected graphs for real-valued data\nin high dimensional problems rely heavily on the assumption of normality. We\nshow how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high\ndimensional inference. Just as additive models extend linear models by\nreplacing linear functions with a set of one-dimensional smooth functions, the\nnonparanormal extends the normal by transforming the variables by smooth\nfunctions. We derive a method for estimating the nonparanormal, study the\nmethod's theoretical properties, and show that it works well in many examples.\n",
        "published": "2009-03-03T22:55:18Z",
        "pdf_link": "http://arxiv.org/pdf/0903.0649v1"
    },
    {
        "id": "http://arxiv.org/abs/0904.0838v2",
        "title": "Finding Exogenous Variables in Data with Many More Variables than\n  Observations",
        "summary": "  Many statistical methods have been proposed to estimate causal models in\nclassical situations with fewer variables than observations (p<n, p: the number\nof variables and n: the number of observations). However, modern datasets\nincluding gene expression data need high-dimensional causal modeling in\nchallenging situations with orders of magnitude more variables than\nobservations (p>>n). In this paper, we propose a method to find exogenous\nvariables in a linear non-Gaussian causal model, which requires much smaller\nsample sizes than conventional methods and works even when p>>n. The key idea\nis to identify which variables are exogenous based on non-Gaussianity instead\nof estimating the entire structure of the model. Exogenous variables work as\ntriggers that activate a causal chain in the model, and their identification\nleads to more efficient experimental designs and better understanding of the\ncausal mechanism. We present experiments with artificial data and real-world\ngene expression data to evaluate the method.\n",
        "published": "2009-04-06T03:36:01Z",
        "pdf_link": "http://arxiv.org/pdf/0904.0838v2"
    },
    {
        "id": "http://arxiv.org/abs/0904.3523v3",
        "title": "Structured Variable Selection with Sparsity-Inducing Norms",
        "summary": "  We consider the empirical risk minimization problem for linear supervised\nlearning, with regularization by structured sparsity-inducing norms. These are\ndefined as sums of Euclidean norms on certain subsets of variables, extending\nthe usual $\\ell_1$-norm and the group $\\ell_1$-norm by allowing the subsets to\noverlap. This leads to a specific set of allowed nonzero patterns for the\nsolutions of such problems. We first explore the relationship between the\ngroups defining the norm and the resulting nonzero patterns, providing both\nforward and backward algorithms to go back and forth from groups to patterns.\nThis allows the design of norms adapted to specific prior knowledge expressed\nin terms of nonzero patterns. We also present an efficient active set\nalgorithm, and analyze the consistency of variable selection for least-squares\nlinear regression in low and high-dimensional settings.\n",
        "published": "2009-04-22T18:38:07Z",
        "pdf_link": "http://arxiv.org/pdf/0904.3523v3"
    },
    {
        "id": "http://arxiv.org/abs/0905.1540v1",
        "title": "Supplementary material for Markov equivalence for ancestral graphs",
        "summary": "  We prove that the criterion for Markov equivalence provided by Zhao et al.\n(2005) may involve a set of features of a graph that is exponential in the\nnumber of vertices.\n",
        "published": "2009-05-11T04:37:02Z",
        "pdf_link": "http://arxiv.org/pdf/0905.1540v1"
    },
    {
        "id": "http://arxiv.org/abs/0905.2138v1",
        "title": "A more robust boosting algorithm",
        "summary": "  We present a new boosting algorithm, motivated by the large margins theory\nfor boosting. We give experimental evidence that the new algorithm is\nsignificantly more robust against label noise than existing boosting algorithm.\n",
        "published": "2009-05-13T16:04:02Z",
        "pdf_link": "http://arxiv.org/pdf/0905.2138v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.3590v1",
        "title": "Forest Garrote",
        "summary": "  Variable selection for high-dimensional linear models has received a lot of\nattention lately, mostly in the context of l1-regularization. Part of the\nattraction is the variable selection effect: parsimonious models are obtained,\nwhich are very suitable for interpretation. In terms of predictive power,\nhowever, these regularized linear models are often slightly inferior to machine\nlearning procedures like tree ensembles. Tree ensembles, on the other hand,\nlack usually a formal way of variable selection and are difficult to visualize.\nA Garrote-style convex penalty for trees ensembles, in particular Random\nForests, is proposed. The penalty selects functional groups of nodes in the\ntrees. These could be as simple as monotone functions of individual predictor\nvariables. This yields a parsimonious function fit, which lends itself easily\nto visualization and interpretation. The predictive power is maintained at\nleast at the same level as the original tree ensemble. A key feature of the\nmethod is that, once a tree ensemble is fitted, no further tuning parameter\nneeds to be selected. The empirical performance is demonstrated on a wide array\nof datasets.\n",
        "published": "2009-06-19T07:29:34Z",
        "pdf_link": "http://arxiv.org/pdf/0906.3590v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.4258v1",
        "title": "The Feature Importance Ranking Measure",
        "summary": "  Most accurate predictions are typically obtained by learning machines with\ncomplex feature spaces (as e.g. induced by kernels). Unfortunately, such\ndecision rules are hardly accessible to humans and cannot easily be used to\ngain insights about the application domain. Therefore, one often resorts to\nlinear models in combination with variable selection, thereby sacrificing some\npredictive power for presumptive interpretability. Here, we introduce the\nFeature Importance Ranking Measure (FIRM), which by retrospective analysis of\narbitrary learning machines allows to achieve both excellent predictive\nperformance and superior interpretation. In contrast to standard raw feature\nweighting, FIRM takes the underlying correlation structure of the features into\naccount. Thereby, it is able to discover the most relevant features, even if\ntheir appearance in the training data is entirely prevented by noise. The\ndesirable properties of FIRM are investigated analytically and illustrated in\nsimulations.\n",
        "published": "2009-06-23T13:45:10Z",
        "pdf_link": "http://arxiv.org/pdf/0906.4258v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.4391v1",
        "title": "KNIFE: Kernel Iterative Feature Extraction",
        "summary": "  Selecting important features in non-linear or kernel spaces is a difficult\nchallenge in both classification and regression problems. When many of the\nfeatures are irrelevant, kernel methods such as the support vector machine and\nkernel ridge regression can sometimes perform poorly. We propose weighting the\nfeatures within a kernel with a sparse set of weights that are estimated in\nconjunction with the original classification or regression problem. The\niterative algorithm, KNIFE, alternates between finding the coefficients of the\noriginal problem and finding the feature weights through kernel linearization.\nIn addition, a slight modification of KNIFE yields an efficient algorithm for\nfinding feature regularization paths, or the paths of each feature's weight.\nSimulation results demonstrate the utility of KNIFE for both kernel regression\nand support vector machines with a variety of kernels. Feature path\nrealizations also reveal important non-linear correlations among features that\nprove useful in determining a subset of significant variables. Results on vowel\nrecognition data, Parkinson's disease data, and microarray data are also given.\n",
        "published": "2009-06-24T02:17:36Z",
        "pdf_link": "http://arxiv.org/pdf/0906.4391v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.0781v1",
        "title": "Bayesian Agglomerative Clustering with Coalescents",
        "summary": "  We introduce a new Bayesian model for hierarchical clustering based on a\nprior over trees called Kingman's coalescent. We develop novel greedy and\nsequential Monte Carlo inferences which operate in a bottom-up agglomerative\nfashion. We show experimentally the superiority of our algorithms over others,\nand demonstrate our approach in document clustering and phylolinguistics.\n",
        "published": "2009-07-04T18:24:06Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0781v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.1013v1",
        "title": "Visualizing Topics with Multi-Word Expressions",
        "summary": "  We describe a new method for visualizing topics, the distributions over terms\nthat are automatically extracted from large text corpora using latent variable\nmodels. Our method finds significant $n$-grams related to a topic, which are\nthen used to help understand and interpret the underlying distribution.\nCompared with the usual visualization, which simply lists the most probable\ntopical terms, the multi-word expressions provide a better intuitive impression\nfor what a topic is \"about.\" Our approach is based on a language model of\narbitrary length expressions, for which we develop a new methodology based on\nnested permutation tests to find significant phrases. We show that this method\noutperforms the more standard use of $\\chi^2$ and likelihood ratio tests. We\nillustrate the topic presentations on corpora of scientific abstracts and news\narticles.\n",
        "published": "2009-07-06T15:29:00Z",
        "pdf_link": "http://arxiv.org/pdf/0907.1013v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.2337v2",
        "title": "Sparsistent Estimation of Time-Varying Discrete Markov Random Fields",
        "summary": "  Network models have been popular for modeling and representing complex\nrelationships and dependencies between observed variables. When data comes from\na dynamic stochastic process, a single static network model cannot adequately\ncapture transient dependencies, such as, gene regulatory dependencies\nthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed a\nmethod based on kernel-smoothing l1-penalized logistic regression for\nestimating time-varying networks from nodal observations collected from a\ntime-series of observational data. In this paper, we establish conditions under\nwhich the proposed method consistently recovers the structure of a time-varying\nnetwork. This work complements previous empirical findings by providing sound\ntheoretical guarantees for the proposed estimation procedure. For completeness,\nwe include numerical simulations in the paper.\n",
        "published": "2009-07-14T12:09:33Z",
        "pdf_link": "http://arxiv.org/pdf/0907.2337v2"
    },
    {
        "id": "http://arxiv.org/abs/0907.3740v1",
        "title": "Empirical Bernstein Bounds and Sample Variance Penalization",
        "summary": "  We give improved constants for data dependent and variance sensitive\nconfidence bounds, called empirical Bernstein bounds, and extend these\ninequalities to hold uniformly over classes of functionswhose growth function\nis polynomial in the sample size n. The bounds lead us to consider sample\nvariance penalization, a novel learning method which takes into account the\nempirical variance of the loss function. We give conditions under which sample\nvariance penalization is effective. In particular, we present a bound on the\nexcess risk incurred by the method. Using this, we argue that there are\nsituations in which the excess risk of our method is of order 1/n, while the\nexcess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some\nexperimental results, which confirm the theory. Finally, we discuss the\npotential application of our results to sample compression schemes.\n",
        "published": "2009-07-21T20:21:38Z",
        "pdf_link": "http://arxiv.org/pdf/0907.3740v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.4643v2",
        "title": "Mean-Field Theory of Meta-Learning",
        "summary": "  We discuss here the mean-field theory for a cellular automata model of\nmeta-learning. The meta-learning is the process of combining outcomes of\nindividual learning procedures in order to determine the final decision with\nhigher accuracy than any single learning method. Our method is constructed from\nan ensemble of interacting, learning agents, that acquire and process incoming\ninformation using various types, or different versions of machine learning\nalgorithms. The abstract learning space, where all agents are located, is\nconstructed here using a fully connected model that couples all agents with\nrandom strength values. The cellular automata network simulates the higher\nlevel integration of information acquired from the independent learning trials.\nThe final classification of incoming input data is therefore defined as the\nstationary state of the meta-learning system using simple majority rule, yet\nthe minority clusters that share opposite classification outcome can be\nobserved in the system. Therefore, the probability of selecting proper class\nfor a given input data, can be estimated even without the prior knowledge of\nits affiliation. The fuzzy logic can be easily introduced into the system, even\nif learning agents are build from simple binary classification machine learning\nalgorithms by calculating the percentage of agreeing agents.\n",
        "published": "2009-07-27T14:43:22Z",
        "pdf_link": "http://arxiv.org/pdf/0907.4643v2"
    },
    {
        "id": "http://arxiv.org/abs/0907.5494v1",
        "title": "How the initialization affects the stability of the k-means algorithm",
        "summary": "  We investigate the role of the initialization for the stability of the\nk-means clustering algorithm. As opposed to other papers, we consider the\nactual k-means algorithm and do not ignore its property of getting stuck in\nlocal optima. We are interested in the actual clustering, not only in the costs\nof the solution. We analyze when different initializations lead to the same\nlocal optimum, and when they lead to different local optima. This enables us to\nprove that it is reasonable to select the number of clusters based on stability\nscores.\n",
        "published": "2009-07-31T09:19:49Z",
        "pdf_link": "http://arxiv.org/pdf/0907.5494v1"
    },
    {
        "id": "http://arxiv.org/abs/0908.2284v1",
        "title": "Classification by Set Cover: The Prototype Vector Machine",
        "summary": "  We introduce a new nearest-prototype classifier, the prototype vector machine\n(PVM). It arises from a combinatorial optimization problem which we cast as a\nvariant of the set cover problem. We propose two algorithms for approximating\nits solution. The PVM selects a relatively small number of representative\npoints which can then be used for classification. It contains 1-NN as a special\ncase. The method is compatible with any dissimilarity measure, making it\namenable to situations in which the data are not embedded in an underlying\nfeature space or in which using a non-Euclidean metric is desirable. Indeed, we\ndemonstrate on the much studied ZIP code data how the PVM can reap the benefits\nof a problem-specific metric. In this example, the PVM outperforms the highly\nsuccessful 1-NN with tangent distance, and does so retaining fewer than half of\nthe data points. This example highlights the strengths of the PVM in yielding a\nlow-error, highly interpretable model. Additionally, we apply the PVM to a\nprotein classification problem in which a kernel-based distance is used.\n",
        "published": "2009-08-17T05:10:22Z",
        "pdf_link": "http://arxiv.org/pdf/0908.2284v1"
    },
    {
        "id": "http://arxiv.org/abs/0908.2579v2",
        "title": "Convex Multiview Fisher Discriminant Analysis",
        "summary": "  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.\nA rewritten version will be posted in the future.\n",
        "published": "2009-08-18T14:35:03Z",
        "pdf_link": "http://arxiv.org/pdf/0908.2579v2"
    },
    {
        "id": "http://arxiv.org/abs/0908.3321v1",
        "title": "Relative Expected Improvement in Kriging Based Optimization",
        "summary": "  We propose an extension of the concept of Expected Improvement criterion\ncommonly used in Kriging based optimization. We extend it for more complex\nKriging models, e.g. models using derivatives. The target field of application\nare CFD problems, where objective function are extremely expensive to evaluate,\nbut the theory can be also used in other fields.\n",
        "published": "2009-08-23T17:45:07Z",
        "pdf_link": "http://arxiv.org/pdf/0908.3321v1"
    },
    {
        "id": "http://arxiv.org/abs/0908.3817v2",
        "title": "Learning Bayesian Networks with the bnlearn R Package",
        "summary": "  bnlearn is an R package which includes several algorithms for learning the\nstructure of Bayesian networks with either discrete or continuous variables.\nBoth constraint-based and score-based algorithms are implemented, and can use\nthe functionality provided by the snow package to improve their performance via\nparallel computing. Several network scores and conditional independence\nalgorithms are available for both the learning algorithms and independent use.\nAdvanced plotting options are provided by the Rgraphviz package.\n",
        "published": "2009-08-26T13:36:18Z",
        "pdf_link": "http://arxiv.org/pdf/0908.3817v2"
    },
    {
        "id": "http://arxiv.org/abs/0909.0991v1",
        "title": "Kernels for Measures Defined on the Gram Matrix of their Support",
        "summary": "  We present in this work a new family of kernels to compare positive measures\non arbitrary spaces $\\Xcal$ endowed with a positive kernel $\\kappa$, which\ntranslates naturally into kernels between histograms or clouds of points. We\nfirst cover the case where $\\Xcal$ is Euclidian, and focus on kernels which\ntake into account the variance matrix of the mixture of two measures to compute\ntheir similarity. The kernels we define are semigroup kernels in the sense that\nthey only use the sum of two measures to compare them, and spectral in the\nsense that they only use the eigenspectrum of the variance matrix of this\nmixture. We show that such a family of kernels has close bonds with the laplace\ntransforms of nonnegative-valued functions defined on the cone of positive\nsemidefinite matrices, and we present some closed formulas that can be derived\nas special cases of such integral expressions. By focusing further on functions\nwhich are invariant to the addition of a null eigenvalue to the spectrum of the\nvariance matrix, we can define kernels between atomic measures on arbitrary\nspaces $\\Xcal$ endowed with a kernel $\\kappa$ by using directly the eigenvalues\nof the centered Gram matrix of the joined support of the compared measures. We\nprovide explicit formulas suited for applications and present preliminary\nexperiments to illustrate the interest of the approach.\n",
        "published": "2009-09-07T15:26:47Z",
        "pdf_link": "http://arxiv.org/pdf/0909.0991v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.1440v1",
        "title": "Structured Sparse Principal Component Analysis",
        "summary": "  We present an extension of sparse PCA, or sparse dictionary learning, where\nthe sparsity patterns of all dictionary elements are structured and constrained\nto belong to a prespecified set of shapes. This \\emph{structured sparse PCA} is\nbased on a structured regularization recently introduced by [1]. While\nclassical sparse priors only deal with \\textit{cardinality}, the regularization\nwe use encodes higher-order information about the data. We propose an efficient\nand simple optimization procedure to solve this problem. Experiments with two\npractical tasks, face recognition and the study of the dynamics of a protein\ncomplex, demonstrate the benefits of the proposed structured approach over\nunstructured approaches.\n",
        "published": "2009-09-08T13:42:35Z",
        "pdf_link": "http://arxiv.org/pdf/0909.1440v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.4386v1",
        "title": "Telling cause from effect based on high-dimensional observations",
        "summary": "  We describe a method for inferring linear causal relations among\nmulti-dimensional variables. The idea is to use an asymmetry between the\ndistributions of cause and effect that occurs if both the covariance matrix of\nthe cause and the structure matrix mapping cause to the effect are\nindependently chosen. The method works for both stochastic and deterministic\ncausal relations, provided that the dimensionality is sufficiently high (in\nsome experiments, 5 was enough). It is applicable to Gaussian as well as\nnon-Gaussian data.\n",
        "published": "2009-09-24T08:54:15Z",
        "pdf_link": "http://arxiv.org/pdf/0909.4386v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.4395v1",
        "title": "Initialization Free Graph Based Clustering",
        "summary": "  This paper proposes an original approach to cluster multi-component data\nsets, including an estimation of the number of clusters. From the construction\nof a minimal spanning tree with Prim's algorithm, and the assumption that the\nvertices are approximately distributed according to a Poisson distribution, the\nnumber of clusters is estimated by thresholding the Prim's trajectory. The\ncorresponding cluster centroids are then computed in order to initialize the\ngeneralized Lloyd's algorithm, also known as $K$-means, which allows to\ncircumvent initialization problems. Some results are derived for evaluating the\nfalse positive rate of our cluster detection algorithm, with the help of\napproximations relevant in Euclidean spaces. Metrics used for measuring\nsimilarity between multi-dimensional data points are based on symmetrical\ndivergences. The use of these informational divergences together with the\nproposed method leads to better results, compared to other clustering methods\nfor the problem of astrophysical data processing. Some applications of this\nmethod in the multi/hyper-spectral imagery domain to a satellite view of Paris\nand to an image of the Mars planet are also presented. In order to demonstrate\nthe usefulness of divergences in our problem, the method with informational\ndivergence as similarity measure is compared with the same method using\nclassical metrics. In the astrophysics application, we also compare the method\nwith the spectral clustering algorithms.\n",
        "published": "2009-09-24T09:35:10Z",
        "pdf_link": "http://arxiv.org/pdf/0909.4395v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.5194v2",
        "title": "Dirichlet Process Mixtures of Generalized Linear Models",
        "summary": "  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),\na new method of nonparametric regression that accommodates continuous and\ncategorical inputs, and responses that can be modeled by a generalized linear\nmodel. We prove conditions for the asymptotic unbiasedness of the DP-GLM\nregression mean function estimate. We also give examples for when those\nconditions hold, including models for compactly supported continuous\ndistributions and a model with continuous covariates and categorical response.\nWe empirically analyze the properties of the DP-GLM and why it provides better\nresults than existing Dirichlet process mixture regression models. We evaluate\nDP-GLM on several data sets, comparing it to modern methods of nonparametric\nregression like CART, Bayesian trees and Gaussian processes. Compared to\nexisting techniques, the DP-GLM provides a single model (and corresponding\ninference algorithms) that performs well in many regression settings.\n",
        "published": "2009-09-28T20:04:28Z",
        "pdf_link": "http://arxiv.org/pdf/0909.5194v2"
    },
    {
        "id": "http://arxiv.org/abs/0909.5422v1",
        "title": "Laplacian Support Vector Machines Trained in the Primal",
        "summary": "  In the last few years, due to the growing ubiquity of unlabeled data, much\neffort has been spent by the machine learning community to develop better\nunderstanding and improve the quality of classifiers exploiting unlabeled data.\nFollowing the manifold regularization approach, Laplacian Support Vector\nMachines (LapSVMs) have shown the state of the art performance in\nsemi--supervised classification. In this paper we present two strategies to\nsolve the primal LapSVM problem, in order to overcome some issues of the\noriginal dual formulation. Whereas training a LapSVM in the dual requires two\nsteps, using the primal form allows us to collapse training to a single step.\nMoreover, the computational complexity of the training algorithm is reduced\nfrom O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the\ncombined number of labeled and unlabeled examples. We speed up training by\nusing an early stopping strategy based on the prediction on unlabeled data or,\nif available, on labeled validation examples. This allows the algorithm to\nquickly compute approximate solutions with roughly the same classification\naccuracy as the optimal ones, considerably reducing the training time. Due to\nits simplicity, training LapSVM in the primal can be the starting point for\nadditional enhancements of the original LapSVM formulation, such as those for\ndealing with large datasets. We present an extensive experimental evaluation on\nreal world data showing the benefits of the proposed approach.\n",
        "published": "2009-09-29T19:54:05Z",
        "pdf_link": "http://arxiv.org/pdf/0909.5422v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.0115v1",
        "title": "Expectation Propagation on the Maximum of Correlated Normal Variables",
        "summary": "  Many inference problems involving questions of optimality ask for the maximum\nor the minimum of a finite set of unknown quantities. This technical report\nderives the first two posterior moments of the maximum of two correlated\nGaussian variables and the first two posterior moments of the two generating\nvariables (corresponding to Gaussian approximations minimizing relative\nentropy). It is shown how this can be used to build a heuristic approximation\nto the maximum relationship over a finite set of Gaussian variables, allowing\napproximate inference by Expectation Propagation on such quantities.\n",
        "published": "2009-10-01T09:12:38Z",
        "pdf_link": "http://arxiv.org/pdf/0910.0115v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.1013v1",
        "title": "Functional learning through kernels",
        "summary": "  This paper reviews the functional aspects of statistical learning theory. The\nmain point under consideration is the nature of the hypothesis set when no\nprior information is available but data. Within this framework we first discuss\nabout the hypothesis set: it is a vectorial space, it is a set of pointwise\ndefined functions, and the evaluation functional on this set is a continuous\nmapping. Based on these principles an original theory is developed generalizing\nthe notion of reproduction kernel Hilbert space to non hilbertian sets. Then it\nis shown that the hypothesis set of any learning machine has to be a\ngeneralized reproducing set. Therefore, thanks to a general \"representer\ntheorem\", the solution of the learning problem is still a linear combination of\na kernel. Furthermore, a way to design these kernels is given. To illustrate\nthis framework some examples of such reproducing sets and kernels are given.\n",
        "published": "2009-10-06T14:19:08Z",
        "pdf_link": "http://arxiv.org/pdf/0910.1013v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.4135v1",
        "title": "Sparsification and feature selection by compressive linear regression",
        "summary": "  The Minimum Description Length (MDL) principle states that the optimal model\nfor a given data set is that which compresses it best. Due to practial\nlimitations the model can be restricted to a class such as linear regression\nmodels, which we address in this study. As in other formulations such as the\nLASSO and forward step-wise regression we are interested in sparsifying the\nfeature set while preserving generalization ability. We derive a\nwell-principled set of codes for both parameters and error residuals along with\nsmooth approximations to lengths of these codes as to allow gradient descent\noptimization of description length, and go on to show that sparsification and\nfeature selection using our approach is faster than the LASSO on several\ndatasets from the UCI and StatLib repositories, with favorable generalization\naccuracy, while being fully automatic, requiring neither cross-validation nor\ntuning of regularization hyper-parameters, allowing even for a nonlinear\nexpansion of the feature set followed by sparsification.\n",
        "published": "2009-10-21T16:27:18Z",
        "pdf_link": "http://arxiv.org/pdf/0910.4135v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.5561v1",
        "title": "Distinguishing Cause and Effect via Second Order Exponential Models",
        "summary": "  We propose a method to infer causal structures containing both discrete and\ncontinuous variables. The idea is to select causal hypotheses for which the\nconditional density of every variable, given its causes, becomes smooth. We\ndefine a family of smooth densities and conditional densities by second order\nexponential models, i.e., by maximizing conditional entropy subject to first\nand second statistical moments. If some of the variables take only values in\nproper subsets of R^n, these conditionals can induce different families of\njoint distributions even for Markov-equivalent graphs.\n  We consider the case of one binary and one real-valued variable where the\nmethod can distinguish between cause and effect. Using this example, we\ndescribe that sometimes a causal hypothesis must be rejected because\nP(effect|cause) and P(cause) share algorithmic information (which is untypical\nif they are chosen independently). This way, our method is in the same spirit\nas faithfulness-based causal inference because it also rejects non-generic\nmutual adjustments among DAG-parameters.\n",
        "published": "2009-10-29T08:45:03Z",
        "pdf_link": "http://arxiv.org/pdf/0910.5561v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.0280v1",
        "title": "Causal Inference on Discrete Data using Additive Noise Models",
        "summary": "  Inferring the causal structure of a set of random variables from a finite\nsample of the joint distribution is an important problem in science. Recently,\nmethods using additive noise models have been suggested to approach the case of\ncontinuous variables. In many situations, however, the variables of interest\nare discrete or even have only finitely many states. In this work we extend the\nnotion of additive noise models to these cases. We prove that whenever the\njoint distribution $\\prob^{(X,Y)}$ admits such a model in one direction, e.g.\n$Y=f(X)+N, N \\independent X$, it does not admit the reversed model\n$X=g(Y)+\\tilde N, \\tilde N \\independent Y$ as long as the model is chosen in a\ngeneric way. Based on these deliberations we propose an efficient new algorithm\nthat is able to distinguish between cause and effect for a finite sample of\ndiscrete variables. In an extensive experimental study we show that this\nalgorithm works both on synthetic and real data sets.\n",
        "published": "2009-11-02T12:00:33Z",
        "pdf_link": "http://arxiv.org/pdf/0911.0280v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.4397v1",
        "title": "How slow is slow? SFA detects signals that are slower than the driving\n  force",
        "summary": "  Slow feature analysis (SFA) is a method for extracting slowly varying driving\nforces from quickly varying nonstationary time series. We show here that it is\npossible for SFA to detect a component which is even slower than the driving\nforce itself (e.g. the envelope of a modulated sine wave). It is shown that it\ndepends on circumstances like the embedding dimension, the time series\npredictability, or the base frequency, whether the driving force itself or a\nslower subcomponent is detected. We observe a phase transition from one regime\nto the other and it is the purpose of this work to quantify the influence of\nvarious parameters on this phase transition. We conclude that what is percieved\nas slow by SFA varies and that a more or less fast switching from one regime to\nthe other occurs, perhaps showing some similarity to human perception.\n",
        "published": "2009-11-23T14:00:50Z",
        "pdf_link": "http://arxiv.org/pdf/0911.4397v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.5107v1",
        "title": "Sparse Convolved Multiple Output Gaussian Processes",
        "summary": "  Recently there has been an increasing interest in methods that deal with\nmultiple outputs. This has been motivated partly by frameworks like multitask\nlearning, multisensor networks or structured output data. From a Gaussian\nprocesses perspective, the problem reduces to specifying an appropriate\ncovariance function that, whilst being positive semi-definite, captures the\ndependencies between all the data points and across all the outputs. One\napproach to account for non-trivial correlations between outputs employs\nconvolution processes. Under a latent function interpretation of the\nconvolution transform we establish dependencies between output variables. The\nmain drawbacks of this approach are the associated computational and storage\ndemands. In this paper we address these issues. We present different sparse\napproximations for dependent output Gaussian processes constructed through the\nconvolution formalism. We exploit the conditional independencies present\nnaturally in the model. This leads to a form of the covariance similar in\nspirit to the so called PITC and FITC approximations for a single output. We\nshow experimental results with synthetic and real data, in particular, we show\nresults in pollution prediction, school exams score prediction and gene\nexpression data.\n",
        "published": "2009-11-26T16:01:20Z",
        "pdf_link": "http://arxiv.org/pdf/0911.5107v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.5367v2",
        "title": "Positive Definite Kernels in Machine Learning",
        "summary": "  This survey is an introduction to positive definite kernels and the set of\nmethods they have inspired in the machine learning literature, namely kernel\nmethods. We first discuss some properties of positive definite kernels as well\nas reproducing kernel Hibert spaces, the natural extension of the set of\nfunctions $\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$ associated with a kernel $k$ defined\non a space $\\mathcal{X}$. We discuss at length the construction of kernel\nfunctions that take advantage of well-known statistical models. We provide an\noverview of numerous data-analysis methods which take advantage of reproducing\nkernel Hilbert spaces and discuss the idea of combining several kernels to\nimprove the performance on certain tasks. We also provide a short cookbook of\ndifferent kernels which are particularly useful for certain data-types such as\nimages, graphs or speech segments.\n",
        "published": "2009-11-28T01:42:04Z",
        "pdf_link": "http://arxiv.org/pdf/0911.5367v2"
    },
    {
        "id": "http://arxiv.org/abs/1001.0160v2",
        "title": "Learning the Structure of Deep Sparse Graphical Models",
        "summary": "  Deep belief networks are a powerful way to model complex probability\ndistributions. However, learning the structure of a belief network,\nparticularly one with hidden units, is difficult. The Indian buffet process has\nbeen used as a nonparametric Bayesian prior on the directed structure of a\nbelief network with a single infinitely wide hidden layer. In this paper, we\nintroduce the cascading Indian buffet process (CIBP), which provides a\nnonparametric prior on the structure of a layered, directed belief network that\nis unbounded in both depth and width, yet allows tractable inference. We use\nthe CIBP prior with the nonlinear Gaussian belief network so each unit can\nadditionally vary its behavior between discrete and continuous representations.\nWe provide Markov chain Monte Carlo algorithms for inference in these belief\nnetworks and explore the structures learned on several image data sets.\n",
        "published": "2009-12-31T19:47:34Z",
        "pdf_link": "http://arxiv.org/pdf/1001.0160v2"
    },
    {
        "id": "http://arxiv.org/abs/1001.1557v2",
        "title": "Forest Density Estimation",
        "summary": "  We study graph estimation and density estimation in high dimensions, using a\nfamily of density estimators based on forest structured undirected graphical\nmodels. For density estimation, we do not assume the true distribution\ncorresponds to a forest; rather, we form kernel density estimates of the\nbivariate and univariate marginals, and apply Kruskal's algorithm to estimate\nthe optimal forest on held out data. We prove an oracle inequality on the\nexcess risk of the resulting estimator relative to the risk of the best forest.\nFor graph estimation, we consider the problem of estimating forests with\nrestricted tree sizes. We prove that finding a maximum weight spanning forest\nwith restricted tree size is NP-hard, and develop an approximation algorithm\nfor this problem. Viewing the tree size as a complexity parameter, we then\nselect a forest using data splitting, and prove bounds on excess risk and\nstructure selection consistency of the procedure. Experiments with simulated\ndata and microarray data indicate that the methods are a practical alternative\nto Gaussian graphical models.\n",
        "published": "2010-01-10T20:39:25Z",
        "pdf_link": "http://arxiv.org/pdf/1001.1557v2"
    },
    {
        "id": "http://arxiv.org/abs/1002.1994v3",
        "title": "Probabilistic Recovery of Multiple Subspaces in Point Clouds by\n  Geometric lp Minimization",
        "summary": "  We assume data independently sampled from a mixture distribution on the unit\nball of the D-dimensional Euclidean space with K+1 components: the first\ncomponent is a uniform distribution on that ball representing outliers and the\nother K components are uniform distributions along K d-dimensional linear\nsubspaces restricted to that ball. We study both the simultaneous recovery of\nall K underlying subspaces and the recovery of the best l0 subspace (i.e., with\nlargest number of points) by minimizing the lp-averaged distances of data\npoints from d-dimensional subspaces of the D-dimensional space. Unlike other lp\nminimization problems, this minimization is non-convex for all p>0 and thus\nrequires different methods for its analysis. We show that if 0<p <= 1, then\nboth all underlying subspaces and the best l0 subspace can be precisely\nrecovered by lp minimization with overwhelming probability. This result extends\nto additive homoscedastic uniform noise around the subspaces (i.e., uniform\ndistribution in a strip around them) and near recovery with an error\nproportional to the noise level. On the other hand, if K>1 and p>1, then we\nshow that both all underlying subspaces and the best l0 subspace cannot be\nrecovered and even nearly recovered. Further relaxations are also discussed. We\nuse the results of this paper for partially justifying recent effective\nalgorithms for modeling data by mixtures of multiple subspaces as well as for\ndiscussing the effect of using variants of lp minimizations in RANSAC-type\nstrategies for single subspace recovery.\n",
        "published": "2010-02-09T22:30:14Z",
        "pdf_link": "http://arxiv.org/pdf/1002.1994v3"
    },
    {
        "id": "http://arxiv.org/abs/1002.3684v1",
        "title": "Robust Independent Component Analysis by Iterative Maximization of the\n  Kurtosis Contrast with Algebraic Optimal Step Size",
        "summary": "  Independent component analysis (ICA) aims at decomposing an observed random\nvector into statistically independent variables. Deflation-based\nimplementations, such as the popular one-unit FastICA algorithm and its\nvariants, extract the independent components one after another. A novel method\nfor deflationary ICA, referred to as RobustICA, is put forward in this paper.\nThis simple technique consists of performing exact line search optimization of\nthe kurtosis contrast function. The step size leading to the global maximum of\nthe contrast along the search direction is found among the roots of a\nfourth-degree polynomial. This polynomial rooting can be performed\nalgebraically, and thus at low cost, at each iteration. Among other practical\nbenefits, RobustICA can avoid prewhitening and deals with real- and\ncomplex-valued mixtures of possibly noncircular sources alike. The absence of\nprewhitening improves asymptotic performance. The algorithm is robust to local\nextrema and shows a very high convergence speed in terms of the computational\ncost required to reach a given source extraction quality, particularly for\nshort data records. These features are demonstrated by a comparative numerical\nanalysis on synthetic data. RobustICA's capabilities in processing real-world\ndata involving noncircular complex strongly super-Gaussian sources are\nillustrated by the biomedical problem of atrial activity (AA) extraction in\natrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an\nalternative ICA-based technique.\n",
        "published": "2010-02-19T08:33:43Z",
        "pdf_link": "http://arxiv.org/pdf/1002.3684v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.0078v1",
        "title": "Security Analysis of Online Centroid Anomaly Detection",
        "summary": "  Security issues are crucial in a number of machine learning applications,\nespecially in scenarios dealing with human activity rather than natural\nphenomena (e.g., information ranking, spam detection, malware detection, etc.).\nIt is to be expected in such cases that learning algorithms will have to deal\nwith manipulated data aimed at hampering decision making. Although some\nprevious work addressed the handling of malicious data in the context of\nsupervised learning, very little is known about the behavior of anomaly\ndetection methods in such scenarios. In this contribution we analyze the\nperformance of a particular method -- online centroid anomaly detection -- in\nthe presence of adversarial noise. Our analysis addresses the following\nsecurity-related issues: formalization of learning and attack processes,\nderivation of an optimal attack, analysis of its efficiency and constraints. We\nderive bounds on the effectiveness of a poisoning attack against centroid\nanomaly under different conditions: bounded and unbounded percentage of\ntraffic, and bounded false positive rate. Our bounds show that whereas a\npoisoning attack can be effectively staged in the unconstrained case, it can be\nmade arbitrarily difficult (a strict upper bound on the attacker's gain) if\nexternal constraints are properly used. Our experimental evaluation carried out\non real HTTP and exploit traces confirms the tightness of our theoretical\nbounds and practicality of our protection mechanisms.\n",
        "published": "2010-02-27T08:35:51Z",
        "pdf_link": "http://arxiv.org/pdf/1003.0078v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.0783v1",
        "title": "Supervised Topic Models",
        "summary": "  We introduce supervised latent Dirichlet allocation (sLDA), a statistical\nmodel of labelled documents. The model accommodates a variety of response\ntypes. We derive an approximate maximum-likelihood procedure for parameter\nestimation, which relies on variational methods to handle intractable posterior\nexpectations. Prediction problems motivate this research: we use the fitted\nmodel to predict response values for new documents. We test sLDA on two\nreal-world problems: movie ratings predicted from reviews, and the political\ntone of amendments in the U.S. Senate based on the amendment text. We\nillustrate the benefits of sLDA versus modern regularized regression, as well\nas versus an unsupervised LDA analysis followed by a separate regression.\n",
        "published": "2010-03-03T11:36:56Z",
        "pdf_link": "http://arxiv.org/pdf/1003.0783v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.2245v1",
        "title": "Optimal Allocation Strategies for the Dark Pool Problem",
        "summary": "  We study the problem of allocating stocks to dark pools. We propose and\nanalyze an optimal approach for allocations, if continuous-valued allocations\nare allowed. We also propose a modification for the case when only\ninteger-valued allocations are possible. We extend the previous work on this\nproblem to adversarial scenarios, while also improving on their results in the\niid setup. The resulting algorithms are efficient, and perform well in\nsimulations under stochastic and adversarial inputs.\n",
        "published": "2010-03-11T03:05:59Z",
        "pdf_link": "http://arxiv.org/pdf/1003.2245v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.3570v1",
        "title": "Linear Time Feature Selection for Regularized Least-Squares",
        "summary": "  We propose a novel algorithm for greedy forward feature selection for\nregularized least-squares (RLS) regression and classification, also known as\nthe least-squares support vector machine or ridge regression. The algorithm,\nwhich we call greedy RLS, starts from the empty feature set, and on each\niteration adds the feature whose addition provides the best leave-one-out\ncross-validation performance. Our method is considerably faster than the\npreviously proposed ones, since its time complexity is linear in the number of\ntraining examples, the number of features in the original data set, and the\ndesired size of the set of selected features. Therefore, as a side effect we\nobtain a new training algorithm for learning sparse linear RLS predictors which\ncan be used for large scale learning. This speed is possible due to matrix\ncalculus based short-cuts for leave-one-out and feature addition. We\nexperimentally demonstrate the scalability of our algorithm and its ability to\nfind good quality feature sets.\n",
        "published": "2010-03-18T12:39:34Z",
        "pdf_link": "http://arxiv.org/pdf/1003.3570v1"
    },
    {
        "id": "http://arxiv.org/abs/1005.0928v2",
        "title": "Training linear ranking SVMs in linearithmic time using red-black trees",
        "summary": "  We introduce an efficient method for training the linear ranking support\nvector machine. The method combines cutting plane optimization with red-black\ntree based approach to subgradient calculations, and has O(m*s+m*log(m)) time\ncomplexity, where m is the number of training examples, and s the average\nnumber of non-zero features per example. Best previously known training\nalgorithms achieve the same efficiency only for restricted special cases,\nwhereas the proposed approach allows any real valued utility scores in the\ntraining data. Experiments demonstrate the superior scalability of the proposed\napproach, when compared to the fastest existing RankSVM implementations.\n",
        "published": "2010-05-06T08:38:24Z",
        "pdf_link": "http://arxiv.org/pdf/1005.0928v2"
    },
    {
        "id": "http://arxiv.org/abs/1005.1440v1",
        "title": "Improving the Johnson-Lindenstrauss Lemma",
        "summary": "  The Johnson-Lindenstrauss Lemma allows for the projection of $n$ points in\n$p-$dimensional Euclidean space onto a $k-$dimensional Euclidean space, with $k\n\\ge \\frac{24\\ln \\emph{n}}{3\\epsilon^2-2\\epsilon^3}$, so that the pairwise\ndistances are preserved within a factor of $1\\pm\\epsilon$. Here, working\ndirectly with the distributions of the random distances rather than resorting\nto the moment generating function technique, an improvement on the lower bound\nfor $k$ is obtained. The additional reduction in dimension when compared to\nbounds found in the literature, is at least $13\\%$, and, in some cases, up to\n$30\\%$ additional reduction is achieved. Using the moment generating function\ntechnique, we further provide a lower bound for $k$ using pairwise $L_2$\ndistances in the space of points to be projected and pairwise $L_1$ distances\nin the space of the projected points. Comparison with the results obtained in\nthe literature shows that the bound presented here provides an additional\n$36-40\\%$ reduction.\n",
        "published": "2010-05-10T03:05:23Z",
        "pdf_link": "http://arxiv.org/pdf/1005.1440v1"
    },
    {
        "id": "http://arxiv.org/abs/1005.1593v2",
        "title": "Refinements of Universal Approximation Results for Deep Belief Networks\n  and Restricted Boltzmann Machines",
        "summary": "  We improve recently published results about resources of Restricted Boltzmann\nMachines (RBM) and Deep Belief Networks (DBN) required to make them Universal\nApproximators. We show that any distribution p on the set of binary vectors of\nlength n can be arbitrarily well approximated by an RBM with k-1 hidden units,\nwhere k is the minimal number of pairs of binary vectors differing in only one\nentry such that their union contains the support set of p. In important cases\nthis number is half of the cardinality of the support set of p. We construct a\nDBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of\napproximating any distribution on {0,1}^n arbitrarily well. This confirms a\nconjecture presented by Le Roux and Bengio 2010.\n",
        "published": "2010-05-10T15:38:54Z",
        "pdf_link": "http://arxiv.org/pdf/1005.1593v2"
    },
    {
        "id": "http://arxiv.org/abs/1006.3316v1",
        "title": "Stability Approach to Regularization Selection (StARS) for High\n  Dimensional Graphical Models",
        "summary": "  A challenging problem in estimating high-dimensional graphical models is to\nchoose the regularization parameter in a data-dependent way. The standard\ntechniques include $K$-fold cross-validation ($K$-CV), Akaike information\ncriterion (AIC), and Bayesian information criterion (BIC). Though these methods\nwork well for low-dimensional problems, they are not suitable in high\ndimensional settings. In this paper, we present StARS: a new stability-based\nmethod for choosing the regularization parameter in high dimensional inference\nfor undirected graphs. The method has a clear interpretation: we use the least\namount of regularization that simultaneously makes a graph sparse and\nreplicable under random sampling. This interpretation requires essentially no\nconditions. Under mild conditions, we show that StARS is partially sparsistent\nin terms of graph estimation: i.e. with high probability, all the true edges\nwill be included in the selected model even when the graph size diverges with\nthe sample size. Empirically, the performance of StARS is compared with the\nstate-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on\nboth synthetic data and a real microarray dataset. StARS outperforms all these\ncompeting procedures.\n",
        "published": "2010-06-16T20:07:20Z",
        "pdf_link": "http://arxiv.org/pdf/1006.3316v1"
    },
    {
        "id": "http://arxiv.org/abs/1006.3640v2",
        "title": "Gaussian Mixture Modeling with Gaussian Process Latent Variable Models",
        "summary": "  Density modeling is notoriously difficult for high dimensional data. One\napproach to the problem is to search for a lower dimensional manifold which\ncaptures the main characteristics of the data. Recently, the Gaussian Process\nLatent Variable Model (GPLVM) has successfully been used to find low\ndimensional manifolds in a variety of complex data. The GPLVM consists of a set\nof points in a low dimensional latent space, and a stochastic map to the\nobserved space. We show how it can be interpreted as a density model in the\nobserved space. However, the GPLVM is not trained as a density model and\ntherefore yields bad density estimates. We propose a new training strategy and\nobtain improved generalisation performance and better density estimates in\ncomparative evaluations on several benchmark data sets.\n",
        "published": "2010-06-18T08:55:28Z",
        "pdf_link": "http://arxiv.org/pdf/1006.3640v2"
    },
    {
        "id": "http://arxiv.org/abs/1007.0832v1",
        "title": "Euclidean Distances, soft and spectral Clustering on Weighted Graphs",
        "summary": "  We define a class of Euclidean distances on weighted graphs, enabling to\nperform thermodynamic soft graph clustering. The class can be constructed form\nthe \"raw coordinates\" encountered in spectral clustering, and can be extended\nby means of higher-dimensional embeddings (Schoenberg transformations).\nGeographical flow data, properly conditioned, illustrate the procedure as well\nas visualization aspects.\n",
        "published": "2010-07-06T08:36:21Z",
        "pdf_link": "http://arxiv.org/pdf/1007.0832v1"
    },
    {
        "id": "http://arxiv.org/abs/1007.1075v1",
        "title": "Clustering Stability: An Overview",
        "summary": "  A popular method for selecting the number of clusters is based on stability\narguments: one chooses the number of clusters such that the corresponding\nclustering results are \"most stable\". In recent years, a series of papers has\nanalyzed the behavior of this method from a theoretical point of view. However,\nthe results are very technical and difficult to interpret for non-experts. In\nthis paper we give a high-level overview about the existing literature on\nclustering stability. In addition to presenting the results in a slightly\ninformal but accessible way, we relate them to each other and discuss their\ndifferent implications.\n",
        "published": "2010-07-07T08:31:17Z",
        "pdf_link": "http://arxiv.org/pdf/1007.1075v1"
    },
    {
        "id": "http://arxiv.org/abs/1007.2450v1",
        "title": "Directional Statistics on Permutations",
        "summary": "  Distributions over permutations arise in applications ranging from\nmulti-object tracking to ranking of instances. The difficulty of dealing with\nthese distributions is caused by the size of their domain, which is factorial\nin the number of considered entities ($n!$). It makes the direct definition of\na multinomial distribution over permutation space impractical for all but a\nvery small $n$. In this work we propose an embedding of all $n!$ permutations\nfor a given $n$ in a surface of a hypersphere defined in\n$\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to\ndefine continuous distributions over a hypersphere with all the benefits of\ndirectional statistics. We provide polynomial time projections between the\ncontinuous hypersphere representation and the $n!$-element permutation space.\nThe framework provides a way to use continuous directional probability\ndensities and the methods developed thereof for establishing densities over\npermutations. As a demonstration of the benefits of the framework we derive an\ninference procedure for a state-space model over permutations. We demonstrate\nthe approach with applications.\n",
        "published": "2010-07-14T22:59:19Z",
        "pdf_link": "http://arxiv.org/pdf/1007.2450v1"
    },
    {
        "id": "http://arxiv.org/abs/1007.3098v3",
        "title": "Reduced Rank Vector Generalized Linear Models for Feature Extraction",
        "summary": "  Supervised linear feature extraction can be achieved by fitting a reduced\nrank multivariate model. This paper studies rank penalized and rank constrained\nvector generalized linear models. From the perspective of thresholding rules,\nwe build a framework for fitting singular value penalized models and use it for\nfeature extraction. Through solving the rank constraint form of the problem, we\npropose progressive feature space reduction for fast computation in high\ndimensions with little performance loss. A novel projective cross-validation is\nproposed for parameter tuning in such nonconvex setups. Real data applications\nare given to show the power of the methodology in supervised dimension\nreduction and feature extraction.\n",
        "published": "2010-07-19T09:30:19Z",
        "pdf_link": "http://arxiv.org/pdf/1007.3098v3"
    },
    {
        "id": "http://arxiv.org/abs/1007.4062v1",
        "title": "Support Vector Machines for Additive Models: Consistency and Robustness",
        "summary": "  Support vector machines (SVMs) are special kernel based methods and belong to\nthe most successful learning methods since more than a decade. SVMs can\ninformally be described as a kind of regularized M-estimators for functions and\nhave demonstrated their usefulness in many complicated real-life problems.\nDuring the last years a great part of the statistical research on SVMs has\nconcentrated on the question how to design SVMs such that they are universally\nconsistent and statistically robust for nonparametric classification or\nnonparametric regression purposes. In many applications, some qualitative prior\nknowledge of the distribution P or of the unknown function f to be estimated is\npresent or the prediction function with a good interpretability is desired,\nsuch that a semiparametric model or an additive model is of interest.\n  In this paper we mainly address the question how to design SVMs by choosing\nthe reproducing kernel Hilbert space (RKHS) or its corresponding kernel to\nobtain consistent and statistically robust estimators in additive models. We\ngive an explicit construction of kernels - and thus of their RKHSs - which\nleads in combination with a Lipschitz continuous loss function to consistent\nand statistically robust SMVs for additive models. Examples are quantile\nregression based on the pinball loss function, regression based on the\nepsilon-insensitive loss function, and classification based on the hinge loss\nfunction.\n",
        "published": "2010-07-23T07:57:04Z",
        "pdf_link": "http://arxiv.org/pdf/1007.4062v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.2908v1",
        "title": "A unifying view for performance measures in multi-class prediction",
        "summary": "  In the last few years, many different performance measures have been\nintroduced to overcome the weakness of the most natural metric, the Accuracy.\nAmong them, Matthews Correlation Coefficient has recently gained popularity\namong researchers not only in machine learning but also in several application\nfields such as bioinformatics. Nonetheless, further novel functions are being\nproposed in literature. We show that Confusion Entropy, a recently introduced\nclassifier performance measure for multi-class problems, has a strong\n(monotone) relation with the multi-class generalization of a classical metric,\nthe Matthews Correlation Coefficient. Computational evidence in support of the\nclaim is provided, together with an outline of the theoretical explanation.\n",
        "published": "2010-08-17T14:47:31Z",
        "pdf_link": "http://arxiv.org/pdf/1008.2908v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.3951v3",
        "title": "A Simple CW-SSIM Kernel-based Nearest Neighbor Method for Handwritten\n  Digit Classification",
        "summary": "  We propose a simple kernel based nearest neighbor approach for handwritten\ndigit classification. The \"distance\" here is actually a kernel defining the\nsimilarity between two images. We carefully study the effects of different\nnumber of neighbors and weight schemes and report the results. With only a few\nnearest neighbors (or most similar images) to vote, the test set error rate on\nMNIST database could reach about 1.5%-2.0%, which is very close to many\nadvanced models.\n",
        "published": "2010-08-24T02:47:00Z",
        "pdf_link": "http://arxiv.org/pdf/1008.3951v3"
    },
    {
        "id": "http://arxiv.org/abs/1008.3952v1",
        "title": "Kernel induced random survival forests",
        "summary": "  Kernel Induced Random Survival Forests (KIRSF) is a statistical learning\nalgorithm which aims to improve prediction accuracy for survival data. As in\nRandom Survival Forests (RSF), Cumulative Hazard Function is predicted for each\nindividual in the test set. Prediction error is estimated using Harrell's\nconcordance index (C index) [Harrell et al. (1982)]. The C-index can be\ninterpreted as a misclassification probability and does not depend on a single\nfixed time for evaluation. The C-index also specifically accounts for\ncensoring. By utilizing kernel functions, KIRSF achieves better results than\nRSF in many situations. In this report, we show how to incorporate kernel\nfunctions into RSF. We test the performance of KIRSF and compare our method to\nRSF. We find that the KIRSF's performance is better than RSF in many occasions.\n",
        "published": "2010-08-24T02:54:03Z",
        "pdf_link": "http://arxiv.org/pdf/1008.3952v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.4988v1",
        "title": "Sparse Group Restricted Boltzmann Machines",
        "summary": "  Since learning is typically very slow in Boltzmann machines, there is a need\nto restrict connections within hidden layers. However, the resulting states of\nhidden units exhibit statistical dependencies. Based on this observation, we\npropose using $l_1/l_2$ regularization upon the activation possibilities of\nhidden units in restricted Boltzmann machines to capture the loacal\ndependencies among hidden units. This regularization not only encourages hidden\nunits of many groups to be inactive given observed data but also makes hidden\nunits within a group compete with each other for modeling observed data. Thus,\nthe $l_1/l_2$ regularization on RBMs yields sparsity at both the group and the\nhidden unit levels. We call RBMs trained with the regularizer \\emph{sparse\ngroup} RBMs. The proposed sparse group RBMs are applied to three tasks:\nmodeling patches of natural images, modeling handwritten digits and pretaining\na deep networks for a classification task. Furthermore, we illustrate the\nregularizer can also be applied to deep Boltzmann machines, which lead to\nsparse group deep Boltzmann machines. When adapted to the MNIST data set, a\ntwo-layer sparse group Boltzmann machine achieves an error rate of $0.84\\%$,\nwhich is, to our knowledge, the best published result on the\npermutation-invariant version of the MNIST task.\n",
        "published": "2010-08-30T02:44:19Z",
        "pdf_link": "http://arxiv.org/pdf/1008.4988v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.5211v1",
        "title": "Union Support Recovery in Multi-task Learning",
        "summary": "  We sharply characterize the performance of different penalization schemes for\nthe problem of selecting the relevant variables in the multi-task setting.\nPrevious work focuses on the regression problem where conditions on the design\nmatrix complicate the analysis. A clearer and simpler picture emerges by\nstudying the Normal means model. This model, often used in the field of\nstatistics, is a simplified model that provides a laboratory for studying\ncomplex procedures.\n",
        "published": "2010-08-31T03:49:26Z",
        "pdf_link": "http://arxiv.org/pdf/1008.5211v1"
    },
    {
        "id": "http://arxiv.org/abs/1009.2139v4",
        "title": "Proximal Methods for Hierarchical Sparse Coding",
        "summary": "  Sparse coding consists in representing signals as sparse linear combinations\nof atoms selected from a dictionary. We consider an extension of this framework\nwhere the atoms are further assumed to be embedded in a tree. This is achieved\nusing a recently introduced tree-structured sparse regularization norm, which\nhas proven useful in several applications. This norm leads to regularized\nproblems that are difficult to optimize, and we propose in this paper efficient\nalgorithms for solving them. More precisely, we show that the proximal operator\nassociated with this norm is computable exactly via a dual approach that can be\nviewed as the composition of elementary proximal operators. Our procedure has a\ncomplexity linear, or close to linear, in the number of atoms, and allows the\nuse of accelerated gradient techniques to solve the tree-structured sparse\napproximation problem at the same computational cost as traditional ones using\nthe L1-norm. Our method is efficient and scales gracefully to millions of\nvariables, which we illustrate in two types of applications: first, we consider\nfixed hierarchical dictionaries of wavelets to denoise natural images. Then, we\napply our optimization tools in the context of dictionary learning, where\nlearned dictionary elements naturally organize in a prespecified arborescent\nstructure, leading to a better performance in reconstruction of natural image\npatches. When applied to text documents, our method learns hierarchies of\ntopics, thus providing a competitive alternative to probabilistic topic models.\n",
        "published": "2010-09-11T05:46:55Z",
        "pdf_link": "http://arxiv.org/pdf/1009.2139v4"
    },
    {
        "id": "http://arxiv.org/abs/1009.2718v1",
        "title": "Calibrated Surrogate Losses for Classification with Label-Dependent\n  Costs",
        "summary": "  We present surrogate regret bounds for arbitrary surrogate losses in the\ncontext of binary classification with label-dependent costs. Such bounds relate\na classifier's risk, assessed with respect to a surrogate loss, to its\ncost-sensitive classification risk. Two approaches to surrogate regret bounds\nare developed. The first is a direct generalization of Bartlett et al. [2006],\nwho focus on margin-based losses and cost-insensitive classification, while the\nsecond adopts the framework of Steinwart [2007] based on calibration functions.\nNontrivial surrogate regret bounds are shown to exist precisely when the\nsurrogate loss satisfies a \"calibration\" condition that is easily verified for\nmany common losses. We apply this theory to the class of uneven margin losses,\nand characterize when these losses are properly calibrated. The uneven hinge,\nsquared error, exponential, and sigmoid losses are then treated in detail.\n",
        "published": "2010-09-14T17:02:42Z",
        "pdf_link": "http://arxiv.org/pdf/1009.2718v1"
    },
    {
        "id": "http://arxiv.org/abs/1009.3890v1",
        "title": "Fast Sparse Decomposition by Iterative Detection-Estimation",
        "summary": "  Finding sparse solutions of underdetermined systems of linear equations is a\nfundamental problem in signal processing and statistics which has become a\nsubject of interest in recent years. In general, these systems have infinitely\nmany solutions. However, it may be shown that sufficiently sparse solutions may\nbe identified uniquely. In other words, the corresponding linear transformation\nwill be invertible if we restrict its domain to sufficiently sparse vectors.\nThis property may be used, for example, to solve the underdetermined Blind\nSource Separation (BSS) problem, or to find sparse representation of a signal\nin an `overcomplete' dictionary of primitive elements (i.e., the so-called\natomic decomposition). The main drawback of current methods of finding sparse\nsolutions is their computational complexity. In this paper, we will show that\nby detecting `active' components of the (potential) solution, i.e., those\ncomponents having a considerable value, a framework for fast solution of the\nproblem may be devised. The idea leads to a family of algorithms, called\n`Iterative Detection-Estimation (IDE)', which converge to the solution by\nsuccessive detection and estimation of its active part. Comparing the\nperformance of IDE(s) with one of the most successful method to date, which is\nbased on Linear Programming (LP), an improvement in speed of about two to three\norders of magnitude is observed.\n",
        "published": "2010-09-20T17:12:37Z",
        "pdf_link": "http://arxiv.org/pdf/1009.3890v1"
    },
    {
        "id": "http://arxiv.org/abs/1009.5358v2",
        "title": "Task-Driven Dictionary Learning",
        "summary": "  Modeling data with linear combinations of a few elements from a learned\ndictionary has been the focus of much recent research in machine learning,\nneuroscience and signal processing. For signals such as natural images that\nadmit such sparse representations, it is now well established that these models\nare well suited to restoration tasks. In this context, learning the dictionary\namounts to solving a large-scale matrix factorization problem, which can be\ndone efficiently with classical optimization tools. The same approach has also\nbeen used for learning features from data for other purposes, e.g., image\nclassification, but tuning the dictionary in a supervised way for these tasks\nhas proven to be more difficult. In this paper, we present a general\nformulation for supervised dictionary learning adapted to a wide variety of\ntasks, and present an efficient algorithm for solving the corresponding\noptimization problem. Experiments on handwritten digit classification, digital\nart identification, nonlinear inverse image problems, and compressed sensing\ndemonstrate that our approach is effective in large-scale settings, and is well\nsuited to supervised and semi-supervised classification, as well as regression\ntasks for data that admit sparse representations.\n",
        "published": "2010-09-27T19:06:49Z",
        "pdf_link": "http://arxiv.org/pdf/1009.5358v2"
    },
    {
        "id": "http://arxiv.org/abs/1009.5736v4",
        "title": "Kernel Bayes' rule",
        "summary": "  A nonparametric kernel-based method for realizing Bayes' rule is proposed,\nbased on representations of probabilities in reproducing kernel Hilbert spaces.\nProbabilities are uniquely characterized by the mean of the canonical map to\nthe RKHS. The prior and conditional probabilities are expressed in terms of\nRKHS functions of an empirical sample: no explicit parametric model is needed\nfor these quantities. The posterior is likewise an RKHS mean of a weighted\nsample. The estimator for the expectation of a function of the posterior is\nderived, and rates of consistency are shown. Some representative applications\nof the kernel Bayes' rule are presented, including Baysian computation without\nlikelihood and filtering with a nonparametric state-space model.\n",
        "published": "2010-09-29T01:51:49Z",
        "pdf_link": "http://arxiv.org/pdf/1009.5736v4"
    },
    {
        "id": "http://arxiv.org/abs/1010.0535v3",
        "title": "Asymptotic Normality of Support Vector Machine Variants and Other\n  Regularized Kernel Methods",
        "summary": "  In nonparametric classification and regression problems, regularized kernel\nmethods, in particular support vector machines, attract much attention in\ntheoretical and in applied statistics. In an abstract sense, regularized kernel\nmethods (simply called SVMs here) can be seen as regularized M-estimators for a\nparameter in a (typically infinite dimensional) reproducing kernel Hilbert\nspace. For smooth loss functions, it is shown that the difference between the\nestimator, i.e.\\ the empirical SVM, and the theoretical SVM is asymptotically\nnormal with rate $\\sqrt{n}$. That is, the standardized difference converges\nweakly to a Gaussian process in the reproducing kernel Hilbert space. As common\nin real applications, the choice of the regularization parameter may depend on\nthe data. The proof is done by an application of the functional delta-method\nand by showing that the SVM-functional is suitably Hadamard-differentiable.\n",
        "published": "2010-10-04T10:46:32Z",
        "pdf_link": "http://arxiv.org/pdf/1010.0535v3"
    },
    {
        "id": "http://arxiv.org/abs/1010.0556v2",
        "title": "Regularizers for Structured Sparsity",
        "summary": "  We study the problem of learning a sparse linear regression vector under\nadditional conditions on the structure of its sparsity pattern. This problem is\nrelevant in machine learning, statistics and signal processing. It is well\nknown that a linear regression can benefit from knowledge that the underlying\nregression vector is sparse. The combinatorial problem of selecting the nonzero\ncomponents of this vector can be \"relaxed\" by regularizing the squared error\nwith a convex penalty function like the $\\ell_1$ norm. However, in many\napplications, additional conditions on the structure of the regression vector\nand its sparsity pattern are available. Incorporating this information into the\nlearning method may lead to a significant decrease of the estimation error. In\nthis paper, we present a family of convex penalty functions, which encode prior\nknowledge on the structure of the vector formed by the absolute values of the\nregression coefficients. This family subsumes the $\\ell_1$ norm and is flexible\nenough to include different models of sparsity patterns, which are of practical\nand theoretical importance. We establish the basic properties of these penalty\nfunctions and discuss some examples where they can be computed explicitly.\nMoreover, we present a convergent optimization algorithm for solving\nregularized least squares with these penalty functions. Numerical simulations\nhighlight the benefit of structured sparsity and the advantage offered by our\napproach over the Lasso method and other related methods.\n",
        "published": "2010-10-04T12:04:44Z",
        "pdf_link": "http://arxiv.org/pdf/1010.0556v2"
    },
    {
        "id": "http://arxiv.org/abs/1010.0772v1",
        "title": "A bagging SVM to learn from positive and unlabeled examples",
        "summary": "  We consider the problem of learning a binary classifier from a training set\nof positive and unlabeled examples, both in the inductive and in the\ntransductive setting. This problem, often referred to as \\emph{PU learning},\ndiffers from the standard supervised classification problem by the lack of\nnegative examples in the training set. It corresponds to an ubiquitous\nsituation in many applications such as information retrieval or gene ranking,\nwhen we have identified a set of data of interest sharing a particular\nproperty, and we wish to automatically retrieve additional data sharing the\nsame property among a large and easily available pool of unlabeled data. We\npropose a conceptually simple method, akin to bagging, to approach both\ninductive and transductive PU learning problems, by converting them into series\nof supervised binary classification problems discriminating the known positive\nexamples from random subsamples of the unlabeled set. We empirically\ndemonstrate the relevance of the method on simulated and real data, where it\nperforms at least as well as existing methods while being faster.\n",
        "published": "2010-10-05T06:03:09Z",
        "pdf_link": "http://arxiv.org/pdf/1010.0772v1"
    },
    {
        "id": "http://arxiv.org/abs/1010.2770v1",
        "title": "Online Multiple Kernel Learning for Structured Prediction",
        "summary": "  Despite the recent progress towards efficient multiple kernel learning (MKL),\nthe structured output case remains an open research front. Current approaches\ninvolve repeatedly solving a batch learning problem, which makes them\ninadequate for large scale scenarios. We propose a new family of online\nproximal algorithms for MKL (as well as for group-lasso and variants thereof),\nwhich overcomes that drawback. We show regret, convergence, and generalization\nbounds for the proposed method. Experiments on handwriting recognition and\ndependency parsing testify for the successfulness of the approach.\n",
        "published": "2010-10-13T20:48:30Z",
        "pdf_link": "http://arxiv.org/pdf/1010.2770v1"
    },
    {
        "id": "http://arxiv.org/abs/1010.3320v2",
        "title": "Exact block-wise optimization in group lasso and sparse group lasso for\n  linear regression",
        "summary": "  The group lasso is a penalized regression method, used in regression problems\nwhere the covariates are partitioned into groups to promote sparsity at the\ngroup level. Existing methods for finding the group lasso estimator either use\ngradient projection methods to update the entire coefficient vector\nsimultaneously at each step, or update one group of coefficients at a time\nusing an inexact line search to approximate the optimal value for the group of\ncoefficients when all other groups' coefficients are fixed. We present a new\nmethod of computation for the group lasso in the linear regression case, the\nSingle Line Search (SLS) algorithm, which operates by computing the exact\noptimal value for each group (when all other coefficients are fixed) with one\nunivariate line search. We perform simulations demonstrating that the SLS\nalgorithm is often more efficient than existing computational methods. We also\nextend the SLS algorithm to the sparse group lasso problem via the Signed\nSingle Line Search (SSLS) algorithm, and give theoretical results to support\nboth algorithms.\n",
        "published": "2010-10-16T05:15:50Z",
        "pdf_link": "http://arxiv.org/pdf/1010.3320v2"
    },
    {
        "id": "http://arxiv.org/abs/1010.4236v1",
        "title": "Maximum Likelihood Joint Tracking and Association in a Strong Clutter\n  without Combinatorial Complexity",
        "summary": "  We have developed an efficient algorithm for the maximum likelihood joint\ntracking and association problem in a strong clutter for GMTI data. By using an\niterative procedure of the dynamic logic process \"from vague-to-crisp,\" the new\ntracker overcomes combinatorial complexity of tracking in highly-cluttered\nscenarios and results in a significant improvement in signal-to-clutter ratio.\n",
        "published": "2010-10-20T16:03:40Z",
        "pdf_link": "http://arxiv.org/pdf/1010.4236v1"
    },
    {
        "id": "http://arxiv.org/abs/1010.4945v1",
        "title": "f-divergence estimation and two-sample homogeneity test under\n  semiparametric density-ratio models",
        "summary": "  A density ratio is defined by the ratio of two probability densities. We\nstudy the inference problem of density ratios and apply a semi-parametric\ndensity-ratio estimator to the two-sample homogeneity test. In the proposed\ntest procedure, the f-divergence between two probability densities is estimated\nusing a density-ratio estimator. The f-divergence estimator is then exploited\nfor the two-sample homogeneity test. We derive the optimal estimator of\nf-divergence in the sense of the asymptotic variance, and then investigate the\nrelation between the proposed test procedure and the existing score test based\non empirical likelihood estimator. Through numerical studies, we illustrate the\nadequacy of the asymptotic theory for finite-sample inference.\n",
        "published": "2010-10-24T09:11:40Z",
        "pdf_link": "http://arxiv.org/pdf/1010.4945v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.0096v1",
        "title": "Concentration inequalities of the cross-validation estimator for\n  Empirical Risk Minimiser",
        "summary": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for empirical risk\nminimizers. In the general setting, we prove sanity-check bounds in the spirit\nof \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case\nerror of this estimate is not much worse that of training error estimate}\n\\textquotedblright . General loss functions and class of predictors with finite\nVC-dimension are considered. We closely follow the formalism introduced by\n\\cite{DUD03} to cover a large variety of cross-validation procedures including\nleave-one-out cross-validation, $k$% -fold cross-validation, hold-out\ncross-validation (or split sample), and the leave-$\\upsilon$-out\ncross-validation.\n  In particular, we focus on proving the consistency of the various\ncross-validation procedures. We point out the interest of each cross-validation\nprocedure in terms of rate of convergence. An estimation curve with transition\nphases depending on the cross-validation procedure and not only on the\npercentage of observations in the test sample gives a simple rule on how to\nchoose the cross-validation. An interesting consequence is that the size of the\ntest sample is not required to grow to infinity for the consistency of the\ncross-validation procedure.\n",
        "published": "2010-10-30T17:48:04Z",
        "pdf_link": "http://arxiv.org/pdf/1011.0096v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.1026v1",
        "title": "The Lasso under Heteroscedasticity",
        "summary": "  The performance of the Lasso is well understood under the assumptions of the\nstandard linear model with homoscedastic noise. However, in several\napplications, the standard model does not describe the important features of\nthe data. This paper examines how the Lasso performs on a non-standard model\nthat is motivated by medical imaging applications. In these applications, the\nvariance of the noise scales linearly with the expectation of the observation.\nLike all heteroscedastic models, the noise terms in this Poisson-like model are\n\\textit{not} independent of the design matrix.\n  More specifically, this paper studies the sign consistency of the Lasso under\na sparse Poisson-like model. In addition to studying sufficient conditions for\nthe sign consistency of the Lasso estimate, this paper also gives necessary\nconditions for sign consistency. Both sets of conditions are comparable to\nresults for the homoscedastic model, showing that when a measure of the signal\nto noise ratio is large, the Lasso performs well on both Poisson-like data and\nhomoscedastic data.\n  Simulations reveal that the Lasso performs equally well in terms of model\nselection performance on both Poisson-like data and homoscedastic data (with\nproperly scaled noise variance), across a range of parameterizations. Taken as\na whole, these results suggest that the Lasso is robust to the Poisson-like\nheteroscedastic noise.\n",
        "published": "2010-11-03T22:26:10Z",
        "pdf_link": "http://arxiv.org/pdf/1011.1026v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.4088v1",
        "title": "An Introduction to Conditional Random Fields",
        "summary": "  Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.\n",
        "published": "2010-11-17T22:14:50Z",
        "pdf_link": "http://arxiv.org/pdf/1011.4088v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.5133v1",
        "title": "Concentration inequalities of the cross-validation estimate for stable\n  predictors",
        "summary": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for stable predictors in\nthe context of risk assessment. The notion of stability has been first\nintroduced by \\cite{DEWA79} and extended by \\cite{KEA95}, \\cite{BE01} and\n\\cite{KUNIY02} to characterize class of predictors with infinite VC dimension.\nIn particular, this covers $k$-nearest neighbors rules, bayesian algorithm\n(\\cite{KEA95}), boosting,... General loss functions and class of predictors are\nconsidered. We use the formalism introduced by \\cite{DUD03} to cover a large\nvariety of cross-validation procedures including leave-one-out\ncross-validation, $k$-fold cross-validation, hold-out cross-validation (or\nsplit sample), and the leave-$\\upsilon$-out cross-validation.\n  In particular, we give a simple rule on how to choose the cross-validation,\ndepending on the stability of the class of predictors. In the special case of\nuniform stability, an interesting consequence is that the number of elements in\nthe test set is not required to grow to infinity for the consistency of the\ncross-validation procedure. In this special case, the particular interest of\nleave-one-out cross-validation is emphasized.\n",
        "published": "2010-11-23T15:31:57Z",
        "pdf_link": "http://arxiv.org/pdf/1011.5133v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.5142v1",
        "title": "Estimating Subagging by cross-validation",
        "summary": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for subagged estimators,\nboth for classification and regressor. General loss functions and class of\npredictors with both finite and infinite VC-dimension are considered. We\nslightly generalize the formalism introduced by \\cite{DUD03} to cover a large\nvariety of cross-validation procedures including leave-one-out\ncross-validation, $k$-fold cross-validation, hold-out cross-validation (or\nsplit sample), and the leave-$\\upsilon$-out cross-validation.\n  \\bigskip\n  \\noindent An interesting consequence is that the probability upper bound is\nbounded by the minimum of a Hoeffding-type bound and a Vapnik-type bounds, and\nthus is smaller than 1 even for small learning set. Finally, we give a simple\nrule on how to subbag the predictor. \\bigskip\n",
        "published": "2010-11-23T16:12:33Z",
        "pdf_link": "http://arxiv.org/pdf/1011.5142v1"
    },
    {
        "id": "http://arxiv.org/abs/1101.0316v1",
        "title": "Bistatic SAR ATR",
        "summary": "  With the present revival of interest in bistatic radar systems, research in\nthat area has gained momentum. Given some of the strategic advantages for a\nbistatic configuration, and tech- nological advances in the past few years,\nlarge-scale implementation of the bistatic systems is a scope for the near\nfuture. If the bistatic systems are to replace the monostatic systems (at least\npar- tially), then all the existing usages of a monostatic system should be\nmanageable in a bistatic system. A detailed investigation of the possibilities\nof an automatic target recognition (ATR) facil- ity in a bistatic radar system\nis presented. Because of the lack of data, experiments were carried out on\nsimulated data. Still, the results are positive and make a positive case for\nthe introduction of the bistatic configuration. First, it was found that,\ncontrary to the popular expectation that the bistatic ATR performance might be\nsubstantially worse than the monostatic ATR performance, the bistatic ATR\nperformed fairly well (though not better than the monostatic ATR). Second, the\nATR per- formance does not deteriorate substantially with increasing bistatic\nangle. Last, the polarimetric data from bistatic scattering were found to have\ndistinct information, contrary to expert opinions. Along with these results,\nsuggestions were also made about how to stabilise the bistatic-ATR per-\nformance with changing bistatic angle. Finally, a new fast and robust ATR\nalgorithm (developed in the present work) has been presented.\n",
        "published": "2011-01-01T04:36:35Z",
        "pdf_link": "http://arxiv.org/pdf/1101.0316v1"
    },
    {
        "id": "http://arxiv.org/abs/1101.0317v1",
        "title": "Generation of SAR Image for Real-life Objects using General Purpose EM\n  Simulators",
        "summary": "  In the applications related to airborne radars, simulation has always played\nan important role. This is mainly because of the two fold reason of the\nunavailability of desired data and the difficulty associated with the\ncollection of data under controlled environment. A simple example will be\nregarding the collection of pure multipolar radar data. Even after phenomenal\ndevelopment in the field of radar hardware design and signal processing, till\nnow the collection of pure multipolar data is a challenge for the radar system\ndesigners. Till very recently, the power of computer simulation of radar signal\nreturn was available to a very selected few. This was because of the heavy cost\nassociated with some of the main line electro magnetic (EM) simulators for\nradar signal simulation, and secondly because many such EM simulators are for\nrestricted marketting. However, because of the fast progress made in the field\nof EM simulation, many of the current generic EM simulators can be used to\nsimulate radar returns from realistic targets. The current article expounds the\nsteps towards generating a synthetic aperture radar (SAR) image database of\nground targets, using a eneric EM g simulator. It also demonstrates by the help\nof some example images, the quality of the SAR mage generated i using a general\npurpose EM simulator.\n",
        "published": "2011-01-01T04:42:07Z",
        "pdf_link": "http://arxiv.org/pdf/1101.0317v1"
    },
    {
        "id": "http://arxiv.org/abs/1101.0673v1",
        "title": "Autoregressive Kernels For Time Series",
        "summary": "  We propose in this work a new family of kernels for variable-length time\nseries. Our work builds upon the vector autoregressive (VAR) model for\nmultivariate stochastic processes: given a multivariate time series x, we\nconsider the likelihood function p_{\\theta}(x) of different parameters \\theta\nin the VAR model as features to describe x. To compare two time series x and\nx', we form the product of their features p_{\\theta}(x) p_{\\theta}(x') which is\nintegrated out w.r.t \\theta using a matrix normal-inverse Wishart prior. Among\nother properties, this kernel can be easily computed when the dimension d of\nthe time series is much larger than the lengths of the considered time series x\nand x'. It can also be generalized to time series taking values in arbitrary\nstate spaces, as long as the state space itself is endowed with a kernel\n\\kappa. In that case, the kernel between x and x' is a a function of the Gram\nmatrices produced by \\kappa on observations and subsequences of observations\nenumerated in x and x'. We describe a computationally efficient implementation\nof this generalization that uses low-rank matrix factorization techniques.\nThese kernels are compared to other known kernels using a set of benchmark\nclassification tasks carried out with support vector machines.\n",
        "published": "2011-01-04T08:44:14Z",
        "pdf_link": "http://arxiv.org/pdf/1101.0673v1"
    },
    {
        "id": "http://arxiv.org/abs/1101.2489v3",
        "title": "DirectLiNGAM: A direct method for learning a linear non-Gaussian\n  structural equation model",
        "summary": "  Structural equation models and Bayesian networks have been widely used to\nanalyze causal relations between continuous variables. In such frameworks,\nlinear acyclic models are typically used to model the data-generating process\nof variables. Recently, it was shown that use of non-Gaussianity identifies the\nfull structure of a linear acyclic model, i.e., a causal ordering of variables\nand their connection strengths, without using any prior knowledge on the\nnetwork structure, which is not the case with conventional methods. However,\nexisting estimation methods are based on iterative search algorithms and may\nnot converge to a correct solution in a finite number of steps. In this paper,\nwe propose a new direct method to estimate a causal ordering and connection\nstrengths based on non-Gaussianity.\n  In contrast to the previous methods, our algorithm requires no algorithmic\nparameters and is guaranteed to converge to the right solution within a small\nfixed number of steps if the data strictly follows the model.\n",
        "published": "2011-01-13T03:58:47Z",
        "pdf_link": "http://arxiv.org/pdf/1101.2489v3"
    },
    {
        "id": "http://arxiv.org/abs/1101.5435v1",
        "title": "An Analysis of the Convergence of Graph Laplacians",
        "summary": "  Existing approaches to analyzing the asymptotics of graph Laplacians\ntypically assume a well-behaved kernel function with smoothness assumptions. We\nremove the smoothness assumption and generalize the analysis of graph\nLaplacians to include previously unstudied graphs including kNN graphs. We also\nintroduce a kernel-free framework to analyze graph constructions with shrinking\nneighborhoods in general and apply it to analyze locally linear embedding\n(LLE). We also describe how for a given limiting Laplacian operator desirable\nproperties such as a convergent spectrum and sparseness can be achieved\nchoosing the appropriate graph construction.\n",
        "published": "2011-01-28T03:32:01Z",
        "pdf_link": "http://arxiv.org/pdf/1101.5435v1"
    },
    {
        "id": "http://arxiv.org/abs/1102.0844v1",
        "title": "A convex model for non-negative matrix factorization and dimensionality\n  reduction on physical space",
        "summary": "  A collaborative convex framework for factoring a data matrix $X$ into a\nnon-negative product $AS$, with a sparse coefficient matrix $S$, is proposed.\nWe restrict the columns of the dictionary matrix $A$ to coincide with certain\ncolumns of the data matrix $X$, thereby guaranteeing a physically meaningful\ndictionary and dimensionality reduction. We use $l_{1,\\infty}$ regularization\nto select the dictionary from the data and show this leads to an exact convex\nrelaxation of $l_0$ in the case of distinct noise free data. We also show how\nto relax the restriction-to-$X$ constraint by initializing an alternating\nminimization approach with the solution of the convex model, obtaining a\ndictionary close to but not necessarily in $X$. We focus on applications of the\nproposed framework to hyperspectral endmember and abundances identification and\nalso show an application to blind source separation of NMR data.\n",
        "published": "2011-02-04T07:16:10Z",
        "pdf_link": "http://arxiv.org/pdf/1102.0844v1"
    },
    {
        "id": "http://arxiv.org/abs/1102.1204v2",
        "title": "Large Scale Correlation Screening",
        "summary": "  This paper treats the problem of screening for variables with high\ncorrelations in high dimensional data in which there can be many fewer samples\nthan variables. We focus on threshold-based correlation screening methods for\nthree related applications: screening for variables with large correlations\nwithin a single treatment (autocorrelation screening); screening for variables\nwith large cross-correlations over two treatments (cross-correlation\nscreening); screening for variables that have persistently large\nauto-correlations over two treatments (persistent-correlation screening). The\nnovelty of correlation screening is that it identifies a smaller number of\nvariables which are highly correlated with others, as compared to identifying a\nnumber of correlation parameters. Correlation screening suffers from a phase\ntransition phenomenon: as the correlation threshold decreases the number of\ndiscoveries increases abruptly. We obtain asymptotic expressions for the mean\nnumber of discoveries and the phase transition thresholds as a function of the\nnumber of samples, the number of variables, and the joint sample distribution.\nWe also show that under a weak dependency condition the number of discoveries\nis dominated by a Poisson random variable giving an asymptotic expression for\nthe false positive rate. The correlation screening approach bears tremendous\ndividends in terms of the type and strength of the asymptotic results that can\nbe obtained. It also overcomes some of the major hurdles faced by existing\nmethods in the literature as correlation screening is naturally scalable to\nhigh dimension. Numerical results strongly validate the theory that is\npresented in this paper. We illustrate the application of the correlation\nscreening methodology on a large scale gene-expression dataset, revealing a few\ninfluential variables that exhibit a significant amount of correlation over\nmultiple treatments.\n",
        "published": "2011-02-06T21:35:37Z",
        "pdf_link": "http://arxiv.org/pdf/1102.1204v2"
    },
    {
        "id": "http://arxiv.org/abs/1102.1492v4",
        "title": "On Nonparametric Guidance for Learning Autoencoder Representations",
        "summary": "  Unsupervised discovery of latent representations, in addition to being useful\nfor density modeling, visualisation and exploratory data analysis, is also\nincreasingly important for learning features relevant to discriminative tasks.\nAutoencoders, in particular, have proven to be an effective way to learn latent\ncodes that reflect meaningful variations in data. A continuing challenge,\nhowever, is guiding an autoencoder toward representations that are useful for\nparticular tasks. A complementary challenge is to find codes that are invariant\nto irrelevant transformations of the data. The most common way of introducing\nsuch problem-specific guidance in autoencoders has been through the\nincorporation of a parametric component that ties the latent representation to\nthe label information. In this work, we argue that a preferable approach relies\ninstead on a nonparametric guidance mechanism. Conceptually, it ensures that\nthere exists a function that can predict the label information, without\nexplicitly instantiating that function. The superiority of this guidance\nmechanism is confirmed on two datasets. In particular, this approach is able to\nincorporate invariance information (lighting, elevation, etc.) from the small\nNORB object recognition dataset and yields state-of-the-art performance for a\nsingle layer, non-convolutional network.\n",
        "published": "2011-02-08T02:33:30Z",
        "pdf_link": "http://arxiv.org/pdf/1102.1492v4"
    },
    {
        "id": "http://arxiv.org/abs/1102.4548v2",
        "title": "Predictive Active Set Selection Methods for Gaussian Processes",
        "summary": "  We propose an active set selection framework for Gaussian process\nclassification for cases when the dataset is large enough to render its\ninference prohibitive. Our scheme consists of a two step alternating procedure\nof active set update rules and hyperparameter optimization based upon marginal\nlikelihood maximization. The active set update rules rely on the ability of the\npredictive distributions of a Gaussian process classifier to estimate the\nrelative contribution of a datapoint when being either included or removed from\nthe model. This means that we can use it to include points with potentially\nhigh impact to the classifier decision process while removing those that are\nless relevant. We introduce two active set rules based on different criteria,\nthe first one prefers a model with interpretable active set parameters whereas\nthe second puts computational complexity first, thus a model with active set\nparameters that directly control its complexity. We also provide both\ntheoretical and empirical support for our active set selection strategy being a\ngood approximation of a full Gaussian process classifier. Our extensive\nexperiments show that our approach can compete with state-of-the-art\nclassification techniques with reasonable time complexity. Source code publicly\navailable at http://cogsys.imm.dtu.dk/passgp.\n",
        "published": "2011-02-22T16:33:24Z",
        "pdf_link": "http://arxiv.org/pdf/1102.4548v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.0431v2",
        "title": "Fast Convergence Rate of Multiple Kernel Learning with Elastic-net\n  Regularization",
        "summary": "  We investigate the learning rate of multiple kernel leaning (MKL) with\nelastic-net regularization, which consists of an $\\ell_1$-regularizer for\ninducing the sparsity and an $\\ell_2$-regularizer for controlling the\nsmoothness. We focus on a sparse setting where the total number of kernels is\nlarge but the number of non-zero components of the ground truth is relatively\nsmall, and prove that elastic-net MKL achieves the minimax learning rate on the\n$\\ell_2$-mixed-norm ball. Our bound is sharper than the convergence rates ever\nshown, and has a property that the smoother the truth is, the faster the\nconvergence rate is.\n",
        "published": "2011-03-02T13:59:51Z",
        "pdf_link": "http://arxiv.org/pdf/1103.0431v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.0790v1",
        "title": "The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning",
        "summary": "  We derive an upper bound on the local Rademacher complexity of $\\ell_p$-norm\nmultiple kernel learning, which yields a tighter excess risk bound than global\napproaches. Previous local approaches aimed at analyzed the case $p=1$ only\nwhile our analysis covers all cases $1\\leq p\\leq\\infty$, assuming the different\nfeature mappings corresponding to the different kernels to be uncorrelated. We\nalso show a lower bound that shows that the bound is tight, and derive\nconsequences regarding excess loss, namely fast convergence rates of the order\n$O(n^{-\\frac{\\alpha}{1+\\alpha}})$, where $\\alpha$ is the minimum eigenvalue\ndecay rate of the individual kernels.\n",
        "published": "2011-03-03T21:29:00Z",
        "pdf_link": "http://arxiv.org/pdf/1103.0790v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.0897v3",
        "title": "Multiple Kernel Learning: A Unifying Probabilistic Viewpoint",
        "summary": "  We present a probabilistic viewpoint to multiple kernel learning unifying\nwell-known regularised risk approaches and recent advances in approximate\nBayesian inference relaxations. The framework proposes a general objective\nfunction suitable for regression, robust regression and classification that is\nlower bound of the marginal likelihood and contains many regularised risk\napproaches as special cases. Furthermore, we derive an efficient and provably\nconvergent optimisation algorithm.\n",
        "published": "2011-03-04T13:32:28Z",
        "pdf_link": "http://arxiv.org/pdf/1103.0897v3"
    },
    {
        "id": "http://arxiv.org/abs/1103.4023v1",
        "title": "Additive Kernels for Gaussian Process Modeling",
        "summary": "  Gaussian Process (GP) models are often used as mathematical approximations of\ncomputationally expensive experiments. Provided that its kernel is suitably\nchosen and that enough data is available to obtain a reasonable fit of the\nsimulator, a GP model can beneficially be used for tasks such as prediction,\noptimization, or Monte-Carlo-based quantification of uncertainty. However, the\nformer conditions become unrealistic when using classical GPs as the dimension\nof input increases. One popular alternative is then to turn to Generalized\nAdditive Models (GAMs), relying on the assumption that the simulator's response\ncan approximately be decomposed as a sum of univariate functions. If such an\napproach has been successfully applied in approximation, it is nevertheless not\ncompletely compatible with the GP framework and its versatile applications. The\nambition of the present work is to give an insight into the use of GPs for\nadditive models by integrating additivity within the kernel, and proposing a\nparsimonious numerical method for data-driven parameter estimation. The first\npart of this article deals with the kernels naturally associated to additive\nprocesses and the properties of the GP models based on such kernels. The second\npart is dedicated to a numerical procedure based on relaxation for additive\nkernel parameter estimation. Finally, the efficiency of the proposed method is\nillustrated and compared to other approaches on Sobol's g-function.\n",
        "published": "2011-03-21T14:16:55Z",
        "pdf_link": "http://arxiv.org/pdf/1103.4023v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.4614v2",
        "title": "Theoretical Properties of the Overlapping Groups Lasso",
        "summary": "  We present two sets of theoretical results on the grouped lasso with overlap\nof Jacob, Obozinski and Vert (2009) in the linear regression setting. This\nmethod allows for joint selection of predictors in sparse regression, allowing\nfor complex structured sparsity over the predictors encoded as a set of groups.\nThis flexible framework suggests that arbitrarily complex structures can be\nencoded with an intricate set of groups. Our results show that this strategy\nresults in unexpected theoretical consequences for the procedure. In\nparticular, we give two sets of results: (1) finite sample bounds on prediction\nand estimation, and (2) asymptotic distribution and selection. Both sets of\nresults give insight into the consequences of choosing an increasingly complex\nset of groups for the procedure, as well as what happens when the set of groups\ncannot recover the true sparsity pattern. Additionally, these results\ndemonstrate the differences and similarities between the the grouped lasso\nprocedure with and without overlapping groups. Our analysis shows the set of\ngroups must be chosen with caution - an overly complex set of groups will\ndamage the analysis.\n",
        "published": "2011-03-23T20:01:04Z",
        "pdf_link": "http://arxiv.org/pdf/1103.4614v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.4789v3",
        "title": "The Discrete Infinite Logistic Normal Distribution",
        "summary": "  We present the discrete infinite logistic normal distribution (DILN), a\nBayesian nonparametric prior for mixed membership models. DILN is a\ngeneralization of the hierarchical Dirichlet process (HDP) that models\ncorrelation structure between the weights of the atoms at the group level. We\nderive a representation of DILN as a normalized collection of gamma-distributed\nrandom variables, and study its statistical properties. We consider\napplications to topic modeling and derive a variational inference algorithm for\napproximate posterior inference. We study the empirical performance of the DILN\ntopic model on four corpora, comparing performance with the HDP and the\ncorrelated topic model (CTM). To deal with large-scale data sets, we also\ndevelop an online inference algorithm for DILN and compare with online HDP and\nonline LDA on the Nature magazine, which contains approximately 350,000\narticles.\n",
        "published": "2011-03-24T15:31:47Z",
        "pdf_link": "http://arxiv.org/pdf/1103.4789v3"
    },
    {
        "id": "http://arxiv.org/abs/1103.4998v1",
        "title": "Sufficient Component Analysis for Supervised Dimension Reduction",
        "summary": "  The purpose of sufficient dimension reduction (SDR) is to find the\nlow-dimensional subspace of input features that is sufficient for predicting\noutput values. In this paper, we propose a novel distribution-free SDR method\ncalled sufficient component analysis (SCA), which is computationally more\nefficient than existing methods. In our method, a solution is computed by\niteratively performing dependence estimation and maximization: Dependence\nestimation is analytically carried out by recently-proposed least-squares\nmutual information (LSMI), and dependence maximization is also analytically\ncarried out by utilizing the Epanechnikov kernel. Through large-scale\nexperiments on real-world image classification and audio tagging problems, the\nproposed method is shown to compare favorably with existing dimension reduction\napproaches.\n",
        "published": "2011-03-25T15:35:16Z",
        "pdf_link": "http://arxiv.org/pdf/1103.4998v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.5201v2",
        "title": "Sharp Convergence Rate and Support Consistency of Multiple Kernel\n  Learning with Sparse and Dense Regularization",
        "summary": "  We theoretically investigate the convergence rate and support consistency\n(i.e., correctly identifying the subset of non-zero coefficients in the large\nsample limit) of multiple kernel learning (MKL). We focus on MKL with block-l1\nregularization (inducing sparse kernel combination), block-l2 regularization\n(inducing uniform kernel combination), and elastic-net regularization\n(including both block-l1 and block-l2 regularization). For the case where the\ntrue kernel combination is sparse, we show a sharper convergence rate of the\nblock-l1 and elastic-net MKL methods than the existing rate for block-l1 MKL.\nWe further show that elastic-net MKL requires a milder condition for being\nconsistent than block-l1 MKL. For the case where the optimal kernel combination\nis not exactly sparse, we prove that elastic-net MKL can achieve a faster\nconvergence rate than the block-l1 and block-l2 MKL methods by carefully\ncontrolling the balance between the block-l1and block-l2 regularizers. Thus,\nour theoretical results overall suggest the use of elastic-net regularization\nin MKL.\n",
        "published": "2011-03-27T11:41:21Z",
        "pdf_link": "http://arxiv.org/pdf/1103.5201v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.5202v2",
        "title": "Fast Learning Rate of lp-MKL and its Minimax Optimality",
        "summary": "  In this paper, we give a new sharp generalization bound of lp-MKL which is a\ngeneralized framework of multiple kernel learning (MKL) and imposes\nlp-mixed-norm regularization instead of l1-mixed-norm regularization. We\nutilize localization techniques to obtain the sharp learning rate. The bound is\ncharacterized by the decay rate of the eigenvalues of the associated kernels. A\nlarger decay rate gives a faster convergence rate. Furthermore, we give the\nminimax learning rate on the ball characterized by lp-mixed-norm in the product\nspace. Then we show that our derived learning rate of lp-MKL achieves the\nminimax optimal rate on the lp-mixed-norm ball.\n",
        "published": "2011-03-27T11:49:44Z",
        "pdf_link": "http://arxiv.org/pdf/1103.5202v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.5537v2",
        "title": "Least-Squares Independence Regression for Non-Linear Causal Inference\n  under Non-Gaussian Noise",
        "summary": "  The discovery of non-linear causal relationship under additive non-Gaussian\nnoise models has attracted considerable attention recently because of their\nhigh flexibility. In this paper, we propose a novel causal inference algorithm\ncalled least-squares independence regression (LSIR). LSIR learns the additive\nnoise model through the minimization of an estimator of the squared-loss mutual\ninformation between inputs and residuals. A notable advantage of LSIR over\nexisting approaches is that tuning parameters such as the kernel width and the\nregularization parameter can be naturally optimized by cross-validation,\nallowing us to avoid overfitting in a data-dependent fashion. Through\nexperiments with real-world datasets, we show that LSIR compares favorably with\na state-of-the-art causal inference method.\n",
        "published": "2011-03-29T03:15:32Z",
        "pdf_link": "http://arxiv.org/pdf/1103.5537v2"
    },
    {
        "id": "http://arxiv.org/abs/1103.6119v1",
        "title": "Auto-associative models, nonlinear Principal component analysis,\n  manifolds and projection pursuit",
        "summary": "  In this paper, auto-associative models are proposed as candidates to the\ngeneralization of Principal Component Analysis. We show that these models are\ndedicated to the approximation of the dataset by a manifold. Here, the word\n\"manifold\" refers to the topology properties of the structure. The\napproximating manifold is built by a projection pursuit algorithm. At each step\nof the algorithm, the dimension of the manifold is incremented. Some\ntheoretical properties are provided. In particular, we can show that, at each\nstep of the algorithm, the mean residuals norm is not increased. Moreover, it\nis also established that the algorithm converges in a finite number of steps.\nSome particular auto-associative models are exhibited and compared to the\nclassical PCA and some neural networks models. Implementation aspects are\ndiscussed. We show that, in numerous cases, no optimization procedure is\nrequired. Some illustrations on simulated and real data are presented.\n",
        "published": "2011-03-31T08:03:54Z",
        "pdf_link": "http://arxiv.org/pdf/1103.6119v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.0455v1",
        "title": "Robust Nonparametric Regression via Sparsity Control with Application to\n  Load Curve Data Cleansing",
        "summary": "  Nonparametric methods are widely applicable to statistical inference\nproblems, since they rely on a few modeling assumptions. In this context, the\nfresh look advocated here permeates benefits from variable selection and\ncompressive sampling, to robustify nonparametric regression against outliers -\nthat is, data markedly deviating from the postulated models. A variational\ncounterpart to least-trimmed squares regression is shown closely related to an\nL0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector\nexplicitly modeling the outliers. This connection suggests efficient solvers\nbased on convex relaxation, which lead naturally to a variational M-type\nestimator equivalent to the least-absolute shrinkage and selection operator\n(Lasso). Outliers are identified by judiciously tuning regularization\nparameters, which amounts to controlling the sparsity of the outlier vector\nalong the whole robustification path of Lasso solutions. Reduced bias and\nenhanced generalization capability are attractive features of an improved\nestimator obtained after replacing the L0-(pseudo)norm with a nonconvex\nsurrogate. The novel robust spline-based smoother is adopted to cleanse load\ncurve data, a key task aiding operational decisions in the envisioned smart\ngrid system. Computer simulations and tests on real load curve data corroborate\nthe effectiveness of the novel sparsity-controlling robust estimators.\n",
        "published": "2011-04-04T02:30:25Z",
        "pdf_link": "http://arxiv.org/pdf/1104.0455v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.1992v1",
        "title": "Unified Treatment of Hidden Markov Switching Models",
        "summary": "  Many real-world problems encountered in several disciplines deal with the\nmodeling of time-series containing different underlying dynamical regimes, for\nwhich probabilistic approaches are very often employed. In this paper we\ndescribe several such approaches in the common framework of graphical models.\nWe give a unified overview of models previously introduced in the literature,\nwhich is simpler and more comprehensive than previous descriptions and enables\nus to highlight commonalities and differences among models that were not\nobserved in the past. In addition, we present several new models and inference\nroutines, which are naturally derived within this unified viewpoint.\n",
        "published": "2011-04-11T16:45:47Z",
        "pdf_link": "http://arxiv.org/pdf/1104.1992v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.5341v2",
        "title": "Joint estimation of linear non-Gaussian acyclic models",
        "summary": "  A linear non-Gaussian structural equation model called LiNGAM is an\nidentifiable model for exploratory causal analysis. Previous methods estimate a\ncausal ordering of variables and their connection strengths based on a single\ndataset. However, in many application domains, data are obtained under\ndifferent conditions, that is, multiple datasets are obtained rather than a\nsingle dataset. In this paper, we present a new method to jointly estimate\nmultiple LiNGAMs under the assumption that the models share a causal ordering\nbut may have different connection strengths and differently distributed\nvariables. In simulations, the new method estimates the models more accurately\nthan estimating them separately.\n",
        "published": "2011-04-28T10:02:17Z",
        "pdf_link": "http://arxiv.org/pdf/1104.5341v2"
    },
    {
        "id": "http://arxiv.org/abs/1105.0363v2",
        "title": "Multi-scale Mining of fMRI data with Hierarchical Structured Sparsity",
        "summary": "  Inverse inference, or \"brain reading\", is a recent paradigm for analyzing\nfunctional magnetic resonance imaging (fMRI) data, based on pattern recognition\nand statistical learning. By predicting some cognitive variables related to\nbrain activation maps, this approach aims at decoding brain activity. Inverse\ninference takes into account the multivariate information between voxels and is\ncurrently the only way to assess how precisely some cognitive information is\nencoded by the activity of neural populations within the whole brain. However,\nit relies on a prediction function that is plagued by the curse of\ndimensionality, since there are far more features than samples, i.e., more\nvoxels than fMRI volumes. To address this problem, different methods have been\nproposed, such as, among others, univariate feature selection, feature\nagglomeration and regularization techniques. In this paper, we consider a\nsparse hierarchical structured regularization. Specifically, the penalization\nwe use is constructed from a tree that is obtained by spatially-constrained\nagglomerative clustering. This approach encodes the spatial structure of the\ndata at different scales into the regularization, which makes the overall\nprediction procedure more robust to inter-subject variability. The\nregularization used induces the selection of spatially coherent predictive\nbrain regions simultaneously at different scales. We test our algorithm on real\ndata acquired to study the mental representation of objects, and we show that\nthe proposed algorithm not only delineates meaningful brain regions but yields\nas well better prediction accuracy than reference methods.\n",
        "published": "2011-05-02T15:39:50Z",
        "pdf_link": "http://arxiv.org/pdf/1105.0363v2"
    },
    {
        "id": "http://arxiv.org/abs/1105.0875v2",
        "title": "A Risk Comparison of Ordinary Least Squares vs Ridge Regression",
        "summary": "  We compare the risk of ridge regression to a simple variant of ordinary least\nsquares, in which one simply projects the data onto a finite dimensional\nsubspace (as specified by a Principal Component Analysis) and then performs an\nordinary (un-regularized) least squares regression in this subspace. This note\nshows that the risk of this ordinary least squares method is within a constant\nfactor (namely 4) of the risk of ridge regression.\n",
        "published": "2011-05-04T17:25:52Z",
        "pdf_link": "http://arxiv.org/pdf/1105.0875v2"
    },
    {
        "id": "http://arxiv.org/abs/1105.2493v6",
        "title": "Closed-form EM for Sparse Coding and its Application to Source\n  Separation",
        "summary": "  We define and discuss the first sparse coding algorithm based on closed-form\nEM updates and continuous latent variables. The underlying generative model\nconsists of a standard `spike-and-slab' prior and a Gaussian noise model.\nClosed-form solutions for E- and M-step equations are derived by generalizing\nprobabilistic PCA. The resulting EM algorithm can take all modes of a\npotentially multi-modal posterior into account. The computational cost of the\nalgorithm scales exponentially with the number of hidden dimensions. However,\nwith current computational resources, it is still possible to efficiently learn\nmodel parameters for medium-scale problems. Thus the model can be applied to\nthe typical range of source separation tasks. In numerical experiments on\nartificial data we verify likelihood maximization and show that the derived\nalgorithm recovers the sparse directions of standard sparse coding\ndistributions. On source separation benchmarks comprised of realistic data we\nshow that the algorithm is competitive with other recent methods.\n",
        "published": "2011-05-12T14:50:09Z",
        "pdf_link": "http://arxiv.org/pdf/1105.2493v6"
    },
    {
        "id": "http://arxiv.org/abs/1105.3361v2",
        "title": "Independent screening for single-index hazard rate models with\n  ultra-high dimensional features",
        "summary": "  In data sets with many more features than observations, independent screening\nbased on all univariate regression models leads to a computationally convenient\nvariable selection method. Recent efforts have shown that in the case of\ngeneralized linear models, independent screening may suffice to capture all\nrelevant features with high probability, even in ultra-high dimension. It is\nunclear whether this formal sure screening property is attainable when the\nresponse is a right-censored survival time. We propose a computationally very\nefficient independent screening method for survival data which can be viewed as\nthe natural survival equivalent of correlation screening. We state conditions\nunder which the method admits the sure screening property within a general\nclass of single-index hazard rate models with ultra-high dimensional features.\nAn iterative variant is also described which combines screening with penalized\nregression in order to handle more complex feature covariance structures. The\nmethods are evaluated through simulation studies and through application to a\nreal gene expression dataset.\n",
        "published": "2011-05-17T13:05:11Z",
        "pdf_link": "http://arxiv.org/pdf/1105.3361v2"
    },
    {
        "id": "http://arxiv.org/abs/1105.4871v1",
        "title": "Minimax Policies for Combinatorial Prediction Games",
        "summary": "  We address the online linear optimization problem when the actions of the\nforecaster are represented by binary vectors. Our goal is to understand the\nmagnitude of the minimax regret for the worst possible set of actions. We study\nthe problem under three different assumptions for the feedback: full\ninformation, and the partial information models of the so-called \"semi-bandit\",\nand \"bandit\" problems. We consider both $L_\\infty$-, and $L_2$-type of\nrestrictions for the losses assigned by the adversary.\n  We formulate a general strategy using Bregman projections on top of a\npotential-based gradient descent, which generalizes the ones studied in the\nseries of papers Gyorgy et al. (2007), Dani et al. (2008), Abernethy et al.\n(2008), Cesa-Bianchi and Lugosi (2009), Helmbold and Warmuth (2009), Koolen et\nal. (2010), Uchiya et al. (2010), Kale et al. (2010) and Audibert and Bubeck\n(2010). We provide simple proofs that recover most of the previous results. We\npropose new upper bounds for the semi-bandit game. Moreover we derive lower\nbounds for all three feedback assumptions. With the only exception of the\nbandit game, the upper and lower bounds are tight, up to a constant factor.\nFinally, we answer a question asked by Koolen et al. (2010) by showing that the\nexponentially weighted average forecaster is suboptimal against $L_{\\infty}$\nadversaries.\n",
        "published": "2011-05-24T19:53:21Z",
        "pdf_link": "http://arxiv.org/pdf/1105.4871v1"
    },
    {
        "id": "http://arxiv.org/abs/1105.5669v3",
        "title": "PAC learnability under non-atomic measures: a problem by Vidyasagar",
        "summary": "  In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAC\nlearnability of a concept class $\\mathscr C$ under the family of all non-atomic\n(diffuse) measures on the domain $\\Omega$. The uniform Glivenko--Cantelli\nproperty with respect to non-atomic measures is no longer a necessary\ncondition, and consistent learnability cannot in general be expected. Our\ncriterion is stated in terms of a combinatorial parameter $\\VC({\\mathscr\nC}\\,{\\mathrm{mod}}\\,\\omega_1)$ which we call the VC dimension of $\\mathscr C$\nmodulo countable sets. The new parameter is obtained by \"thickening up\" single\npoints in the definition of VC dimension to uncountable \"clusters\".\nEquivalently, $\\VC(\\mathscr C\\modd\\omega_1)\\leq d$ if and only if every\ncountable subclass of $\\mathscr C$ has VC dimension $\\leq d$ outside a\ncountable subset of $\\Omega$. The new parameter can be also expressed as the\nclassical VC dimension of $\\mathscr C$ calculated on a suitable subset of a\ncompactification of $\\Omega$. We do not make any measurability assumptions on\n$\\mathscr C$, assuming instead the validity of Martin's Axiom (MA). Similar\nresults are obtained for function learning in terms of fat-shattering dimension\nmodulo countable sets, but, just like in the classical distribution-free case,\nthe finiteness of this parameter is sufficient but not necessary for PAC\nlearnability under non-atomic measures.\n",
        "published": "2011-05-27T23:31:15Z",
        "pdf_link": "http://arxiv.org/pdf/1105.5669v3"
    },
    {
        "id": "http://arxiv.org/abs/1106.0474v1",
        "title": "Restricted Collapsed Draw: Accurate Sampling for Hierarchical Chinese\n  Restaurant Process Hidden Markov Models",
        "summary": "  We propose a restricted collapsed draw (RCD) sampler, a general Markov chain\nMonte Carlo sampler of simultaneous draws from a hierarchical Chinese\nrestaurant process (HCRP) with restriction. Models that require simultaneous\ndraws from a hierarchical Dirichlet process with restriction, such as infinite\nHidden markov models (iHMM), were difficult to enjoy benefits of \\markerg{the}\nHCRP due to combinatorial explosion in calculating distributions of coupled\ndraws. By constructing a proposal of seating arrangements (partitioning) and\nstochastically accepts the proposal by the Metropolis-Hastings algorithm, the\nRCD sampler makes accurate sampling for complex combination of draws while\nretaining efficiency of HCRP representation. Based on the RCD sampler, we\ndeveloped a series of sophisticated sampling algorithms for iHMMs, including\nblocked Gibbs sampling, beam sampling, and split-merge sampling, that\noutperformed conventional iHMM samplers in experiments\n",
        "published": "2011-06-02T17:08:36Z",
        "pdf_link": "http://arxiv.org/pdf/1106.0474v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.0565v2",
        "title": "Multi-stage Convex Relaxation for Feature Selection",
        "summary": "  A number of recent work studied the effectiveness of feature selection using\nLasso. It is known that under the restricted isometry properties (RIP), Lasso\ndoes not generally lead to the exact recovery of the set of nonzero\ncoefficients, due to the looseness of convex relaxation. This paper considers\nthe feature selection property of nonconvex regularization, where the solution\nis given by a multi-stage convex relaxation scheme. Under appropriate\nconditions, we show that the local solution obtained by this procedure recovers\nthe set of nonzero coefficients without suffering from the bias of Lasso\nrelaxation, which complements parameter estimation results of this procedure.\n",
        "published": "2011-06-03T04:49:53Z",
        "pdf_link": "http://arxiv.org/pdf/1106.0565v2"
    },
    {
        "id": "http://arxiv.org/abs/1106.0762v1",
        "title": "Causal Network Inference via Group Sparse Regularization",
        "summary": "  This paper addresses the problem of inferring sparse causal networks modeled\nby multivariate auto-regressive (MAR) processes. Conditions are derived under\nwhich the Group Lasso (gLasso) procedure consistently estimates sparse network\nstructure. The key condition involves a \"false connection score.\" In\nparticular, we show that consistent recovery is possible even when the number\nof observations of the network is far less than the number of parameters\ndescribing the network, provided that the false connection score is less than\none. The false connection score is also demonstrated to be a useful metric of\nrecovery in non-asymptotic regimes. The conditions suggest a modified gLasso\nprocedure which tends to improve the false connection score and reduce the\nchances of reversing the direction of causal influence. Computational\nexperiments and a real network based electrocorticogram (ECoG) simulation study\ndemonstrate the effectiveness of the approach.\n",
        "published": "2011-06-03T20:27:39Z",
        "pdf_link": "http://arxiv.org/pdf/1106.0762v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.2474v1",
        "title": "Source Separation and Clustering of Phase-Locked Subspaces: Derivations\n  and Proofs",
        "summary": "  Due to space limitations, our submission \"Source Separation and Clustering of\nPhase-Locked Subspaces\", accepted for publication on the IEEE Transactions on\nNeural Networks in 2011, presented some results without proof. Those proofs are\nprovided in this paper.\n",
        "published": "2011-06-13T15:34:38Z",
        "pdf_link": "http://arxiv.org/pdf/1106.2474v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.2494v2",
        "title": "Pitman-Yor Diffusion Trees",
        "summary": "  We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical\nclustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which\nremoves the restriction to binary branching structure. The generative process\nis described and shown to result in an exchangeable distribution over data\npoints. We prove some theoretical properties of the model and then present two\ninference methods: a collapsed MCMC sampler which allows us to model\nuncertainty over tree structures, and a computationally efficient greedy\nBayesian EM search algorithm. Both algorithms use message passing on the tree\nstructure. The utility of the model and algorithms is demonstrated on synthetic\nand real world data, both continuous and binary.\n",
        "published": "2011-06-13T17:23:50Z",
        "pdf_link": "http://arxiv.org/pdf/1106.2494v2"
    },
    {
        "id": "http://arxiv.org/abs/1106.3571v2",
        "title": "ANOVA kernels and RKHS of zero mean functions for model-based\n  sensitivity analysis",
        "summary": "  Given a reproducing kernel Hilbert space H of real-valued functions and a\nsuitable measure mu over the source space D (subset of R), we decompose H as\nthe sum of a subspace of centered functions for mu and its orthogonal in H.\nThis decomposition leads to a special case of ANOVA kernels, for which the\nfunctional ANOVA representation of the best predictor can be elegantly derived,\neither in an interpolation or regularization framework. The proposed kernels\nappear to be particularly convenient for analyzing the e ffect of each (group\nof) variable(s) and computing sensitivity indices without recursivity.\n",
        "published": "2011-06-17T20:20:32Z",
        "pdf_link": "http://arxiv.org/pdf/1106.3571v2"
    },
    {
        "id": "http://arxiv.org/abs/1106.4198v1",
        "title": "Online algorithms for Nonnegative Matrix Factorization with the\n  Itakura-Saito divergence",
        "summary": "  Nonnegative matrix factorization (NMF) is now a common tool for audio source\nseparation. When learning NMF on large audio databases, one major drawback is\nthat the complexity in time is O(FKN) when updating the dictionary (where (F;N)\nis the dimension of the input power spectrograms, and K the number of basis\nspectra), thus forbidding its application on signals longer than an hour. We\nprovide an online algorithm with a complexity of O(FK) in time and memory for\nupdates in the dictionary. We show on audio simulations that the online\napproach is faster for short audio signals and allows to analyze audio signals\nof several hours.\n",
        "published": "2011-06-21T13:34:06Z",
        "pdf_link": "http://arxiv.org/pdf/1106.4198v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.5175v1",
        "title": "Sparse Inverse Covariance Estimation via an Adaptive Gradient-Based\n  Method",
        "summary": "  We study the problem of estimating from data, a sparse approximation to the\ninverse covariance matrix. Estimating a sparsity constrained inverse covariance\nmatrix is a key component in Gaussian graphical model learning, but one that is\nnumerically very challenging. We address this challenge by developing a new\nadaptive gradient-based method that carefully combines gradient information\nwith an adaptive step-scaling strategy, which results in a scalable, highly\ncompetitive method. Our algorithm, like its predecessors, maximizes an\n$\\ell_1$-norm penalized log-likelihood and has the same per iteration\narithmetic complexity as the best methods in its class. Our experiments reveal\nthat our approach outperforms state-of-the-art competitors, often significantly\nso, for large problems.\n",
        "published": "2011-06-25T21:38:55Z",
        "pdf_link": "http://arxiv.org/pdf/1106.5175v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.0521v3",
        "title": "On a Rapid Simulation of the Dirichlet Process",
        "summary": "  We describe a simple and efficient procedure for approximating the L\\'evy\nmeasure of a $\\text{Gamma}(\\alpha,1)$ random variable. We use this\napproximation to derive a finite sum-representation that converges almost\nsurely to Ferguson's representation of the Dirichlet process based on arrivals\nof a homogeneous Poisson process. We compare the efficiency of our\napproximation to several other well known approximations of the Dirichlet\nprocess and demonstrate a substantial improvement.\n",
        "published": "2011-07-04T04:09:03Z",
        "pdf_link": "http://arxiv.org/pdf/1107.0521v3"
    },
    {
        "id": "http://arxiv.org/abs/1107.4340v1",
        "title": "Spectral approximations in machine learning",
        "summary": "  In many areas of machine learning, it becomes necessary to find the\neigenvector decompositions of large matrices. We discuss two methods for\nreducing the computational burden of spectral decompositions: the more\nvenerable Nystom extension and a newly introduced algorithm based on random\nprojections. Previous work has centered on the ability to reconstruct the\noriginal matrix. We argue that a more interesting and relevant comparison is\ntheir relative performance in clustering and classification tasks using the\napproximate eigenvectors as features. We demonstrate that performance is task\nspecific and depends on the rank of the approximation.\n",
        "published": "2011-07-21T18:44:51Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4340v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.4506v2",
        "title": "Robustness of Anytime Bandit Policies",
        "summary": "  This paper studies the deviations of the regret in a stochastic multi-armed\nbandit problem. When the total number of plays n is known beforehand by the\nagent, Audibert et al. (2009) exhibit a policy such that with probability at\nleast 1-1/n, the regret of the policy is of order log(n). They have also shown\nthat such a property is not shared by the popular ucb1 policy of Auer et al.\n(2002). This work first answers an open question: it extends this negative\nresult to any anytime policy. The second contribution of this paper is to\ndesign anytime robust policies for specific multi-armed bandit problems in\nwhich some restrictions are put on the set of possible distributions of the\ndifferent arms.\n",
        "published": "2011-07-22T12:55:34Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4506v2"
    },
    {
        "id": "http://arxiv.org/abs/1108.0453v1",
        "title": "On the Evaluation Criterions for the Active Learning Processes",
        "summary": "  In many data mining applications collection of sufficiently large datasets is\nthe most time consuming and expensive. On the other hand, industrial methods of\ndata collection create huge databases, and make difficult direct applications\nof the advanced machine learning algorithms. To address the above problems, we\nconsider active learning (AL), which may be very efficient either for the\nexperimental design or for the data filtering. In this paper we demonstrate\nusing the online evaluation opportunity provided by the AL Challenge that quite\ncompetitive results may be produced using a small percentage of the available\ndata. Also, we present several alternative criteria, which may be useful for\nthe evaluation of the active learning processes. The author of this paper\nattended special presentation in Barcelona, where results of the WCCI 2010 AL\nChallenge were discussed.\n",
        "published": "2011-08-02T00:23:20Z",
        "pdf_link": "http://arxiv.org/pdf/1108.0453v1"
    },
    {
        "id": "http://arxiv.org/abs/1108.1483v2",
        "title": "Algebraic Geometric Comparison of Probability Distributions",
        "summary": "  We propose a novel algebraic framework for treating probability distributions\nrepresented by their cumulants such as the mean and covariance matrix. As an\nexample, we consider the unsupervised learning problem of finding the subspace\non which several probability distributions agree. Instead of minimizing an\nobjective function involving the estimated cumulants, we show that by treating\nthe cumulants as elements of the polynomial ring we can directly solve the\nproblem, at a lower computational cost and with higher accuracy. Moreover, the\nalgebraic viewpoint on probability distributions allows us to invoke the theory\nof Algebraic Geometry, which we demonstrate in a compact proof for an\nidentifiability criterion.\n",
        "published": "2011-08-06T14:02:44Z",
        "pdf_link": "http://arxiv.org/pdf/1108.1483v2"
    },
    {
        "id": "http://arxiv.org/abs/1108.2228v3",
        "title": "A consistent adjacency spectral embedding for stochastic blockmodel\n  graphs",
        "summary": "  We present a method to estimate block membership of nodes in a random graph\ngenerated by a stochastic blockmodel. We use an embedding procedure motivated\nby the random dot product graph model, a particular example of the latent\nposition model. The embedding associates each node with a vector; these vectors\nare clustered via minimization of a square error criterion. We prove that this\nmethod is consistent for assigning nodes to blocks, as only a negligible number\nof nodes will be mis-assigned. We prove consistency of the method for directed\nand undirected graphs. The consistent block assignment makes possible\nconsistent parameter estimation for a stochastic blockmodel. We extend the\nresult in the setting where the number of blocks grows slowly with the number\nof nodes. Our method is also computationally feasible even for very large\ngraphs. We compare our method to Laplacian spectral clustering through analysis\nof simulated data and a graph derived from Wikipedia documents.\n",
        "published": "2011-08-10T17:34:52Z",
        "pdf_link": "http://arxiv.org/pdf/1108.2228v3"
    },
    {
        "id": "http://arxiv.org/abs/1108.4324v3",
        "title": "Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real\n  and Complex Linear Models",
        "summary": "  In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been\nused to model sparsity-inducing priors that realize a class of concave penalty\nfunctions for the regression task in real-valued signal models. Motivated by\nthe relative scarcity of formal tools for SBL in complex-valued models, this\npaper proposes a GSM model - the Bessel K model - that induces concave penalty\nfunctions for the estimation of complex sparse signals. The properties of the\nBessel K model are analyzed when it is applied to Type I and Type II\nestimation. This analysis reveals that, by tuning the parameters of the mixing\npdf different penalty functions are invoked depending on the estimation type\nused, the value of the noise variance, and whether real or complex signals are\nestimated. Using the Bessel K model, we derive a sparse estimator based on a\nmodification of the expectation-maximization algorithm formulated for Type II\nestimation. The estimator includes as a special instance the algorithms\nproposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical results\nshow the superiority of the proposed estimator over these state-of-the-art\nestimators in terms of convergence speed, sparseness, reconstruction error, and\nrobustness in low and medium signal-to-noise ratio regimes.\n",
        "published": "2011-08-22T14:12:11Z",
        "pdf_link": "http://arxiv.org/pdf/1108.4324v3"
    },
    {
        "id": "http://arxiv.org/abs/1108.4988v2",
        "title": "A General Theory of Concave Regularization for High Dimensional Sparse\n  Estimation Problems",
        "summary": "  Concave regularization methods provide natural procedures for sparse\nrecovery. However, they are difficult to analyze in the high dimensional\nsetting. Only recently a few sparse recovery results have been established for\nsome specific local solutions obtained via specialized numerical procedures.\nStill, the fundamental relationship between these solutions such as whether\nthey are identical or their relationship to the global minimizer of the\nunderlying nonconvex formulation is unknown. The current paper fills this\nconceptual gap by presenting a general theoretical framework showing that under\nappropriate conditions, the global solution of nonconvex regularization leads\nto desirable recovery performance; moreover, under suitable conditions, the\nglobal solution corresponds to the unique sparse local solution, which can be\nobtained via different numerical procedures. Under this unified framework, we\npresent an overview of existing results and discuss their connections. The\nunified view of this work leads to a more satisfactory treatment of concave\nhigh dimensional sparse estimation procedures, and serves as guideline for\ndeveloping further numerical procedures for concave regularization.\n",
        "published": "2011-08-25T01:48:58Z",
        "pdf_link": "http://arxiv.org/pdf/1108.4988v2"
    },
    {
        "id": "http://arxiv.org/abs/1109.0730v1",
        "title": "Variable Selection in High Dimensions with Random Designs and Orthogonal\n  Matching Pursuit",
        "summary": "  The performance of Orthogonal Matching Pursuit (OMP) for variable selection\nis analyzed for random designs. When contrasted with the deterministic case,\nsince the performance is here measured after averaging over the distribution of\nthe design matrix, one can have far less stringent sparsity constraints on the\ncoefficient vector. We demonstrate that for exact sparse vectors, the\nperformance of the OMP is similar to known results on the Lasso algorithm\n[\\textit{IEEE Trans. Inform. Theory} \\textbf{55} (2009) 2183--2202]. Moreover,\nvariable selection under a more relaxed sparsity assumption on the coefficient\nvector, whereby one has only control on the $\\ell_1$ norm of the smaller\ncoefficients, is also analyzed. As a consequence of these results, we also show\nthat the coefficient estimate satisfies strong oracle type inequalities.\n",
        "published": "2011-09-04T17:03:00Z",
        "pdf_link": "http://arxiv.org/pdf/1109.0730v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.0887v7",
        "title": "Learning Nonlinear Functions Using Regularized Greedy Forest",
        "summary": "  We consider the problem of learning a forest of nonlinear decision rules with\ngeneral loss functions. The standard methods employ boosted decision trees such\nas Adaboost for exponential loss and Friedman's gradient boosting for general\nloss. In contrast to these traditional boosting algorithms that treat a tree\nlearner as a black box, the method we propose directly learns decision forests\nvia fully-corrective regularized greedy search using the underlying forest\nstructure. Our method achieves higher accuracy and smaller models than gradient\nboosting (and Adaboost with exponential loss) on many datasets.\n",
        "published": "2011-09-05T13:17:08Z",
        "pdf_link": "http://arxiv.org/pdf/1109.0887v7"
    },
    {
        "id": "http://arxiv.org/abs/1109.2553v2",
        "title": "Nominal Association Vector and Matrix",
        "summary": "  When response variables are nominal and populations are cross-classified with\nrespect to multiple polytomies, questions often arise about the degree of\nassociation of the responses with explanatory variables. When populations are\nknown, we introduce a nominal association vector and matrix to evaluate the\ndependence of a response variable with an explanatory variable. These measures\nprovide detailed evaluations of nominal associations at both local and global\nlevels. We also define a general class of global association measures which\nembraces the well known association measure by Goodman-Kruskal (1954). The\nproposed association matrix also gives rise to the expected generalized\nconfusion matrix in classification. The hierarchy of equivalence relations\ndefined by the association vector and matrix are also shown.\n",
        "published": "2011-09-12T18:12:09Z",
        "pdf_link": "http://arxiv.org/pdf/1109.2553v2"
    },
    {
        "id": "http://arxiv.org/abs/1109.4389v1",
        "title": "Mixtures of conditional Gaussian scale mixtures applied to multiscale\n  image representations",
        "summary": "  We present a probabilistic model for natural images which is based on\nGaussian scale mixtures and a simple multiscale representation. In contrast to\nthe dominant approach to modeling whole images focusing on Markov random\nfields, we formulate our model in terms of a directed graphical model. We show\nthat it is able to generate images with interesting higher-order correlations\nwhen trained on natural images or samples from an occlusion based model. More\nimportantly, the directed model enables us to perform a principled evaluation.\nWhile it is easy to generate visually appealing images, we demonstrate that our\nmodel also yields the best performance reported to date when evaluated with\nrespect to the cross-entropy rate, a measure tightly linked to the average\nlog-likelihood.\n",
        "published": "2011-09-20T18:39:31Z",
        "pdf_link": "http://arxiv.org/pdf/1109.4389v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.6804v1",
        "title": "Comparing Probabilistic Models for Melodic Sequences",
        "summary": "  Modelling the real world complexity of music is a challenge for machine\nlearning. We address the task of modeling melodic sequences from the same music\ngenre. We perform a comparative analysis of two probabilistic models; a\nDirichlet Variable Length Markov Model (Dirichlet-VMM) and a Time Convolutional\nRestricted Boltzmann Machine (TC-RBM). We show that the TC-RBM learns\ndescriptive music features, such as underlying chords and typical melody\ntransitions and dynamics. We assess the models for future prediction and\ncompare their performance to a VMM, which is the current state of the art in\nmelody generation. We show that both models perform significantly better than\nthe VMM, with the Dirichlet-VMM marginally outperforming the TC-RBM. Finally,\nwe evaluate the short order statistics of the models, using the\nKullback-Leibler divergence between test sequences and model samples, and show\nthat our proposed methods match the statistics of the music genre significantly\nbetter than the VMM.\n",
        "published": "2011-09-30T12:17:50Z",
        "pdf_link": "http://arxiv.org/pdf/1109.6804v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.3204v1",
        "title": "Bayesian Group Factor Analysis",
        "summary": "  We introduce a factor analysis model that summarizes the dependencies between\nobserved variable groups, instead of dependencies between individual variables\nas standard factor analysis does. A group may correspond to one view of the\nsame set of objects, one of many data sets tied by co-occurrence, or a set of\nalternative variables collected from statistics tables to measure one property\nof interest. We show that by assuming group-wise sparse factors, active in a\nsubset of the sets, the variation can be decomposed into factors explaining\nrelationships between the sets and factors explaining away set-specific\nvariation. We formulate the assumptions in a Bayesian model which provides the\nfactors, and apply the model to two data analysis tasks, in neuroimaging and\nchemical systems biology.\n",
        "published": "2011-10-14T13:26:09Z",
        "pdf_link": "http://arxiv.org/pdf/1110.3204v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.4300v1",
        "title": "k-NN Regression Adapts to Local Intrinsic Dimension",
        "summary": "  Many nonparametric regressors were recently shown to converge at rates that\ndepend only on the intrinsic dimension of data. These regressors thus escape\nthe curse of dimension when high-dimensional data has low intrinsic dimension\n(e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic\ndimension. In particular our rates are local to a query x and depend only on\nthe way masses of balls centered at x vary with radius.\n  Furthermore, we show a simple way to choose k = k(x) locally at any x so as\nto nearly achieve the minimax rate at x in terms of the unknown intrinsic\ndimension in the vicinity of x. We also establish that the minimax rate does\nnot depend on a particular choice of metric space or distribution, but rather\nthat this minimax rate holds for any metric space and doubling measure.\n",
        "published": "2011-10-19T14:49:04Z",
        "pdf_link": "http://arxiv.org/pdf/1110.4300v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.4347v3",
        "title": "Is the k-NN classifier in high dimensions affected by the curse of\n  dimensionality?",
        "summary": "  There is an increasing body of evidence suggesting that exact nearest\nneighbour search in high-dimensional spaces is affected by the curse of\ndimensionality at a fundamental level. Does it necessarily mean that the same\nis true for k nearest neighbours based learning algorithms such as the k-NN\nclassifier? We analyse this question at a number of levels and show that the\nanswer is different at each of them. As our first main observation, we show the\nconsistency of a k approximate nearest neighbour classifier. However, the\nperformance of the classifier in very high dimensions is provably unstable. As\nour second main observation, we point out that the existing model for\nstatistical learning is oblivious of dimension of the domain and so every\nlearning problem admits a universally consistent deterministic reduction to the\none-dimensional case by means of a Borel isomorphism.\n",
        "published": "2011-10-19T18:25:27Z",
        "pdf_link": "http://arxiv.org/pdf/1110.4347v3"
    },
    {
        "id": "http://arxiv.org/abs/1110.4531v4",
        "title": "Regression for sets of polynomial equations",
        "summary": "  We propose a method called ideal regression for approximating an arbitrary\nsystem of polynomial equations by a system of a particular type. Using\ntechniques from approximate computational algebraic geometry, we show how we\ncan solve ideal regression directly without resorting to numerical\noptimization. Ideal regression is useful whenever the solution to a learning\nproblem can be described by a system of polynomial equations. As an example, we\ndemonstrate how to formulate Stationary Subspace Analysis (SSA), a source\nseparation problem, in terms of ideal regression, which also yields a\nconsistent estimator for SSA. We then compare this estimator in simulations\nwith previous optimization-based approaches for SSA.\n",
        "published": "2011-10-20T13:51:16Z",
        "pdf_link": "http://arxiv.org/pdf/1110.4531v4"
    },
    {
        "id": "http://arxiv.org/abs/1110.5238v1",
        "title": "Multiple Gaussian Process Models",
        "summary": "  We consider a Gaussian process formulation of the multiple kernel learning\nproblem. The goal is to select the convex combination of kernel matrices that\nbest explains the data and by doing so improve the generalisation on unseen\ndata. Sparsity in the kernel weights is obtained by adopting a hierarchical\nBayesian approach: Gaussian process priors are imposed over the latent\nfunctions and generalised inverse Gaussians on their associated weights. This\nconstruction is equivalent to imposing a product of heavy-tailed process priors\nover function space. A variational inference algorithm is derived for\nregression and binary classification.\n",
        "published": "2011-10-24T14:01:26Z",
        "pdf_link": "http://arxiv.org/pdf/1110.5238v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.5508v1",
        "title": "A Flexible, Scalable and Efficient Algorithmic Framework for Primal\n  Graphical Lasso",
        "summary": "  We propose a scalable, efficient and statistically motivated computational\nframework for Graphical Lasso (Friedman et al., 2007b) - a covariance\nregularization framework that has received significant attention in the\nstatistics community over the past few years. Existing algorithms have trouble\nin scaling to dimensions larger than a thousand. Our proposal significantly\nenhances the state-of-the-art for such moderate sized problems and gracefully\nscales to larger problems where other algorithms become practically infeasible.\nThis requires a few key new ideas. We operate on the primal problem and use a\nsubtle variation of block-coordinate-methods which drastically reduces the\ncomputational complexity by orders of magnitude. We provide rigorous\ntheoretical guarantees on the convergence and complexity of our algorithm and\ndemonstrate the effectiveness of our proposal via experiments. We believe that\nour framework extends the applicability of Graphical Lasso to large-scale\nmodern applications like bioinformatics, collaborative filtering and social\nnetworks, among others.\n",
        "published": "2011-10-25T14:07:40Z",
        "pdf_link": "http://arxiv.org/pdf/1110.5508v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.5847v1",
        "title": "Structural Similarity and Distance in Learning",
        "summary": "  We propose a novel method of introducing structure into existing machine\nlearning techniques by developing structure-based similarity and distance\nmeasures. To learn structural information, low-dimensional structure of the\ndata is captured by solving a non-linear, low-rank representation problem. We\nshow that this low-rank representation can be kernelized, has a closed-form\nsolution, allows for separation of independent manifolds, and is robust to\nnoise. From this representation, similarity between observations based on\nnon-linear structure is computed and can be incorporated into existing feature\ntransformations, dimensionality reduction techniques, and machine learning\nmethods. Experimental results on both synthetic and real data sets show\nperformance improvements for clustering, and anomaly detection through the use\nof structural similarity.\n",
        "published": "2011-10-26T17:21:18Z",
        "pdf_link": "http://arxiv.org/pdf/1110.5847v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.6416v1",
        "title": "Adaptive Hedge",
        "summary": "  Most methods for decision-theoretic online learning are based on the Hedge\nalgorithm, which takes a parameter called the learning rate. In most previous\nanalyses the learning rate was carefully tuned to obtain optimal worst-case\nperformance, leading to suboptimal performance on easy instances, for example\nwhen there exists an action that is significantly better than all others. We\npropose a new way of setting the learning rate, which adapts to the difficulty\nof the learning problem: in the worst case our procedure still guarantees\noptimal performance, but on easy instances it achieves much smaller regret. In\nparticular, our adaptive method achieves constant regret in a probabilistic\nsetting, when there exists an action that on average obtains strictly smaller\nloss than all other actions. We also provide a simulation study comparing our\napproach to existing methods.\n",
        "published": "2011-10-28T18:09:50Z",
        "pdf_link": "http://arxiv.org/pdf/1110.6416v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.1915v1",
        "title": "The theory and application of penalized methods or Reproducing Kernel\n  Hilbert Spaces made easy",
        "summary": "  The popular cubic smoothing spline estimate of a regression function arises\nas the minimizer of the penalized sum of squares $\\sum_j(Y_j - {\\mu}(t_j))^2 +\n{\\lambda}\\int_a^b [{\\mu}\"(t)]^2 dt$, where the data are $t_j,Y_j$, $j=1,...,\nn$. The minimization is taken over an infinite-dimensional function space, the\nspace of all functions with square integrable second derivatives. But the\ncalculations can be carried out in a finite-dimensional space. The reduction\nfrom minimizing over an infinite dimensional space to minimizing over a finite\ndimensional space occurs for more general objective functions: the data may be\nrelated to the function ${\\mu}$ in another way, the sum of squares may be\nreplaced by a more suitable expression, or the penalty, $\\int_a^b [{\\mu}\"(t)]^2\ndt$, might take a different form. This paper reviews the Reproducing Kernel\nHilbert Space structure that provides a finite-dimensional solution for a\ngeneral minimization problem. Particular attention is paid to penalties based\non linear differential operators. In this case, one can sometimes easily\ncalculate the minimizer explicitly, using Green's functions.\n",
        "published": "2011-11-08T14:21:17Z",
        "pdf_link": "http://arxiv.org/pdf/1111.1915v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3404v1",
        "title": "Estimated VC dimension for risk bounds",
        "summary": "  Vapnik-Chervonenkis (VC) dimension is a fundamental measure of the\ngeneralization capacity of learning algorithms. However, apart from a few\nspecial cases, it is hard or impossible to calculate analytically. Vapnik et\nal. [10] proposed a technique for estimating the VC dimension empirically.\nWhile their approach behaves well in simulations, it could not be used to bound\nthe generalization risk of classifiers, because there were no bounds for the\nestimation error of the VC dimension itself. We rectify this omission,\nproviding high probability concentration results for the proposed estimator and\nderiving corresponding generalization bounds.\n",
        "published": "2011-11-15T01:44:12Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3404v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3781v1",
        "title": "Fast Learning Rate of Non-Sparse Multiple Kernel Learning and Optimal\n  Regularization Strategies",
        "summary": "  In this paper, we give a new generalization error bound of Multiple Kernel\nLearning (MKL) for a general class of regularizations, and discuss what kind of\nregularization gives a favorable predictive accuracy. Our main target in this\npaper is dense type regularizations including \\ellp-MKL. According to the\nrecent numerical experiments, the sparse regularization does not necessarily\nshow a good performance compared with dense type regularizations. Motivated by\nthis fact, this paper gives a general theoretical tool to derive fast learning\nrates of MKL that is applicable to arbitrary mixed-norm-type regularizations in\na unifying manner. This enables us to compare the generalization performances\nof various types of regularizations. As a consequence, we observe that the\nhomogeneity of the complexities of candidate reproducing kernel Hilbert spaces\n(RKHSs) affects which regularization strategy (\\ell1 or dense) is preferred. In\nfact, in homogeneous complexity settings where the complexities of all RKHSs\nare evenly same, \\ell1-regularization is optimal among all isotropic norms. On\nthe other hand, in inhomogeneous complexity settings, dense type\nregularizations can show better learning rate than sparse \\ell1-regularization.\nWe also show that our learning rate achieves the minimax lower bound in\nhomogeneous complexity settings.\n",
        "published": "2011-11-16T12:35:01Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3781v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.5948v1",
        "title": "On l_1 Mean and Variance Filtering",
        "summary": "  This paper addresses the problem of segmenting a time-series with respect to\nchanges in the mean value or in the variance. The first case is when the time\ndata is modeled as a sequence of independent and normal distributed random\nvariables with unknown, possibly changing, mean value but fixed variance. The\nmain assumption is that the mean value is piecewise constant in time, and the\ntask is to estimate the change times and the mean values within the segments.\nThe second case is when the mean value is constant, but the variance can\nchange. The assumption is that the variance is piecewise constant in time, and\nwe want to estimate change times and the variance values within the segments.\nTo find solutions to these problems, we will study an l_1 regularized maximum\nlikelihood method, related to the fused lasso method and l_1 trend filtering,\nwhere the parameters to be estimated are free to vary at each sample. To\npenalize variations in the estimated parameters, the l_1-norm of the time\ndifference of the parameters is used as a regularization term. This idea is\nclosely related to total variation denoising. The main contribution is that a\nconvex formulation of this variance estimation problem, where the\nparametrization is based on the inverse of the variance, can be formulated as a\ncertain l_1 mean estimation problem. This implies that results and methods for\nmean estimation can be applied to the challenging problem of variance\nsegmentation/estimation.\n",
        "published": "2011-11-25T11:22:23Z",
        "pdf_link": "http://arxiv.org/pdf/1111.5948v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.6160v1",
        "title": "Optimal exponential bounds on the accuracy of classification",
        "summary": "  We consider a standard binary classification problem. The performance of any\nbinary classifier based on the training data is characterized by the excess\nrisk. We study Bahadur's type exponential bounds on the minimax accuracy\nconfidence function based on the excess risk. We study how this quantity\ndepends on the complexity of the class of distributions characterized by\nexponents of entropies of the class of regression functions or of the class of\nBayes classifiers corresponding to the distributions from the class. We also\nstudy its dependence on margin parameters of the classification problem.\n",
        "published": "2011-11-26T13:43:40Z",
        "pdf_link": "http://arxiv.org/pdf/1111.6160v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.6233v1",
        "title": "Additive Covariance Kernels for High-Dimensional Gaussian Process\n  Modeling",
        "summary": "  Gaussian process models -also called Kriging models- are often used as\nmathematical approximations of expensive experiments. However, the number of\nobservation required for building an emulator becomes unrealistic when using\nclassical covariance kernels when the dimension of input increases. In oder to\nget round the curse of dimensionality, a popular approach is to consider\nsimplified models such as additive models. The ambition of the present work is\nto give an insight into covariance kernels that are well suited for building\nadditive Kriging models and to describe some properties of the resulting\nmodels.\n",
        "published": "2011-11-27T08:05:17Z",
        "pdf_link": "http://arxiv.org/pdf/1111.6233v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.6254v1",
        "title": "Fast, Linear Time, m-Adic Hierarchical Clustering for Search and\n  Retrieval using the Baire Metric, with linkages to Generalized Ultrametrics,\n  Hashing, Formal Concept Analysis, and Precision of Data Measurement",
        "summary": "  We describe many vantage points on the Baire metric and its use in clustering\ndata, or its use in preprocessing and structuring data in order to support\nsearch and retrieval operations. In some cases, we proceed directly to clusters\nand do not directly determine the distances. We show how a hierarchical\nclustering can be read directly from one pass through the data. We offer\ninsights also on practical implications of precision of data measurement. As a\nmechanism for treating multidimensional data, including very high dimensional\ndata, we use random projections.\n",
        "published": "2011-11-27T12:59:32Z",
        "pdf_link": "http://arxiv.org/pdf/1111.6254v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.6832v2",
        "title": "Gaussian Probabilities and Expectation Propagation",
        "summary": "  While Gaussian probability densities are omnipresent in applied mathematics,\nGaussian cumulative probabilities are hard to calculate in any but the\nunivariate case. We study the utility of Expectation Propagation (EP) as an\napproximate integration method for this problem. For rectangular integration\nregions, the approximation is highly accurate. We also extend the derivations\nto the more general case of polyhedral integration regions. However, we find\nthat in this polyhedral case, EP's answer, though often accurate, can be almost\narbitrarily wrong. We consider these unexpected results empirically and\ntheoretically, both for the problem of Gaussian probabilities and for EP more\ngenerally. These results elucidate an interesting and non-obvious feature of EP\nnot yet studied in detail.\n",
        "published": "2011-11-29T14:59:12Z",
        "pdf_link": "http://arxiv.org/pdf/1111.6832v2"
    },
    {
        "id": "http://arxiv.org/abs/1112.0611v1",
        "title": "Information-Maximization Clustering based on Squared-Loss Mutual\n  Information",
        "summary": "  Information-maximization clustering learns a probabilistic classifier in an\nunsupervised manner so that mutual information between feature vectors and\ncluster assignments is maximized. A notable advantage of this approach is that\nit only involves continuous optimization of model parameters, which is\nsubstantially easier to solve than discrete optimization of cluster\nassignments. However, existing methods still involve non-convex optimization\nproblems, and therefore finding a good local optimal solution is not\nstraightforward in practice. In this paper, we propose an alternative\ninformation-maximization clustering method based on a squared-loss variant of\nmutual information. This novel approach gives a clustering solution\nanalytically in a computationally efficient way via kernel eigenvalue\ndecomposition. Furthermore, we provide a practical model selection procedure\nthat allows us to objectively optimize tuning parameters included in the kernel\nfunction. Through experiments, we demonstrate the usefulness of the proposed\napproach.\n",
        "published": "2011-12-03T00:27:50Z",
        "pdf_link": "http://arxiv.org/pdf/1112.0611v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.2288v1",
        "title": "Asynchronous Stochastic Approximation with Differential Inclusions",
        "summary": "  The asymptotic pseudo-trajectory approach to stochastic approximation of\nBenaim, Hofbauer and Sorin is extended for asynchronous stochastic\napproximations with a set-valued mean field. The asynchronicity of the process\nis incorporated into the mean field to produce convergence results which remain\nsimilar to those of an equivalent synchronous process. In addition, this allows\nmany of the restrictive assumptions previously associated with asynchronous\nstochastic approximation to be removed. The framework is extended for a coupled\nasynchronous stochastic approximation process with set-valued mean fields.\nTwo-timescales arguments are used here in a similar manner to the original work\nin this area by Borkar. The applicability of this approach is demonstrated\nthrough learning in a Markov decision process.\n",
        "published": "2011-12-10T16:49:43Z",
        "pdf_link": "http://arxiv.org/pdf/1112.2288v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.2289v1",
        "title": "Convergent Expectation Propagation in Linear Models with Spike-and-slab\n  Priors",
        "summary": "  Exact inference in the linear regression model with spike and slab priors is\noften intractable. Expectation propagation (EP) can be used for approximate\ninference. However, the regular sequential form of EP (R-EP) may fail to\nconverge in this model when the size of the training set is very small. As an\nalternative, we propose a provably convergent EP algorithm (PC-EP). PC-EP is\nproved to minimize an energy function which, under some constraints, is bounded\nfrom below and whose stationary points coincide with the solution of R-EP.\nExperiments with synthetic data indicate that when R-EP does not converge, the\napproximation generated by PC-EP is often better. By contrast, when R-EP\nconverges, both methods perform similarly.\n",
        "published": "2011-12-10T17:04:01Z",
        "pdf_link": "http://arxiv.org/pdf/1112.2289v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.2319v1",
        "title": "Graph Construction for Learning with Unbalanced Data",
        "summary": "  Unbalanced data arises in many learning tasks such as clustering of\nmulti-class data, hierarchical divisive clustering and semisupervised learning.\nGraph-based approaches are popular tools for these problems. Graph construction\nis an important aspect of graph-based learning. We show that graph-based\nalgorithms can fail for unbalanced data for many popular graphs such as k-NN,\n\\epsilon-neighborhood and full-RBF graphs. We propose a novel graph\nconstruction technique that encodes global statistical information into node\ndegrees through a ranking scheme. The rank of a data sample is an estimate of\nits p-value and is proportional to the total number of data samples with\nsmaller density. This ranking scheme serves as a surrogate for density; can be\nreliably estimated; and indicates whether a data sample is close to\nvalleys/modes. This rank-modulated degree(RMD) scheme is able to significantly\nsparsify the graph near valleys and provides an adaptive way to cope with\nunbalanced data. We then theoretically justify our method through limit cut\nanalysis. Unsupervised and semi-supervised experiments on synthetic and real\ndata sets demonstrate the superiority of our method.\n",
        "published": "2011-12-11T04:25:29Z",
        "pdf_link": "http://arxiv.org/pdf/1112.2319v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.3699v8",
        "title": "Ensemble Models with Trees and Rules",
        "summary": "  In this article, we have proposed several approaches for post processing a\nlarge ensemble of prediction models or rules. The results from our simulations\nshow that the post processing methods we have considered here are promising. We\nhave used the techniques developed here for estimation of quantitative traits\nfrom markers, on the benchmark \"Bostob Housing\"data set and in some\nsimulations. In most cases, the produced models had better prediction\nperformance than, for example, the ones produced by the random forest or the\nrulefit algorithms.\n",
        "published": "2011-12-16T01:14:48Z",
        "pdf_link": "http://arxiv.org/pdf/1112.3699v8"
    },
    {
        "id": "http://arxiv.org/abs/1112.3827v1",
        "title": "Regret lower bounds and extended Upper Confidence Bounds policies in\n  stochastic multi-armed bandit problem",
        "summary": "  This paper is devoted to regret lower bounds in the classical model of\nstochastic multi-armed bandit. A well-known result of Lai and Robbins, which\nhas then been extended by Burnetas and Katehakis, has established the presence\nof a logarithmic bound for all consistent policies. We relax the notion of\nconsistence, and exhibit a generalisation of the logarithmic bound. We also\nshow the non existence of logarithmic bound in the general case of Hannan\nconsistency. To get these results, we study variants of popular Upper\nConfidence Bounds (ucb) policies. As a by-product, we prove that it is\nimpossible to design an adaptive policy that would select the best of two\nalgorithms by taking advantage of the properties of the environment.\n",
        "published": "2011-12-16T14:38:04Z",
        "pdf_link": "http://arxiv.org/pdf/1112.3827v1"
    },
    {
        "id": "http://arxiv.org/abs/1201.3302v2",
        "title": "A General Framework of Dual Certificate Analysis for Structured Sparse\n  Recovery Problems",
        "summary": "  This paper develops a general theoretical framework to analyze structured\nsparse recovery problems using the notation of dual certificate. Although\ncertain aspects of the dual certificate idea have already been used in some\nprevious work, due to the lack of a general and coherent theory, the analysis\nhas so far only been carried out in limited scopes for specific problems. In\nthis context the current paper makes two contributions. First, we introduce a\ngeneral definition of dual certificate, which we then use to develop a unified\ntheory of sparse recovery analysis for convex programming. Second, we present a\nclass of structured sparsity regularization called structured Lasso for which\ncalculations can be readily performed under our theoretical framework. This new\ntheory includes many seemingly loosely related previous work as special cases;\nit also implies new results that improve existing ones even for standard\nformulations such as L1 regularization.\n",
        "published": "2012-01-16T16:10:42Z",
        "pdf_link": "http://arxiv.org/pdf/1201.3302v2"
    },
    {
        "id": "http://arxiv.org/abs/1201.6082v1",
        "title": "A robust and sparse K-means clustering algorithm",
        "summary": "  In many situations where the interest lies in identifying clusters one might\nexpect that not all available variables carry information about these groups.\nFurthermore, data quality (e.g. outliers or missing entries) might present a\nserious and sometimes hard-to-assess problem for large and complex datasets. In\nthis paper we show that a small proportion of atypical observations might have\nserious adverse effects on the solutions found by the sparse clustering\nalgorithm of Witten and Tibshirani (2010). We propose a robustification of\ntheir sparse K-means algorithm based on the trimmed K-means algorithm of\nCuesta-Albertos et al. (1997) Our proposal is also able to handle datasets with\nmissing values. We illustrate the use of our method on microarray data for\ncancer patients where we are able to identify strong biological clusters with a\nmuch reduced number of genes. Our simulation studies show that, when there are\noutliers in the data, our robust sparse K-means algorithm performs better than\nother competing methods both in terms of the selection of features and also the\nidentified clusters. This robust sparse K-means algorithm is implemented in the\nR package RSKC which is publicly available from the CRAN repository.\n",
        "published": "2012-01-29T21:17:02Z",
        "pdf_link": "http://arxiv.org/pdf/1201.6082v1"
    },
    {
        "id": "http://arxiv.org/abs/1202.0825v1",
        "title": "Multi-view predictive partitioning in high dimensions",
        "summary": "  Many modern data mining applications are concerned with the analysis of\ndatasets in which the observations are described by paired high-dimensional\nvectorial representations or \"views\". Some typical examples can be found in web\nmining and genomics applications. In this article we present an algorithm for\ndata clustering with multiple views, Multi-View Predictive Partitioning (MVPP),\nwhich relies on a novel criterion of predictive similarity between data points.\nWe assume that, within each cluster, the dependence between multivariate views\ncan be modelled by using a two-block partial least squares (TB-PLS) regression\nmodel, which performs dimensionality reduction and is particularly suitable for\nhigh-dimensional settings. The proposed MVPP algorithm partitions the data such\nthat the within-cluster predictive ability between views is maximised. The\nproposed objective function depends on a measure of predictive influence of\npoints under the TB-PLS model which has been derived as an extension of the\nPRESS statistic commonly used in ordinary least squares regression. Using\nsimulated data, we compare the performance of MVPP to that of competing\nmulti-view clustering methods which rely upon geometric structures of points,\nbut ignore the predictive relationship between the two views. State-of-art\nresults are obtained on benchmark web mining datasets.\n",
        "published": "2012-02-02T18:22:46Z",
        "pdf_link": "http://arxiv.org/pdf/1202.0825v1"
    },
    {
        "id": "http://arxiv.org/abs/1202.1787v1",
        "title": "Greedy Learning of Markov Network Structure",
        "summary": "  We propose a new yet natural algorithm for learning the graph structure of\ngeneral discrete graphical models (a.k.a. Markov random fields) from samples.\nOur algorithm finds the neighborhood of a node by sequentially adding nodes\nthat produce the largest reduction in empirical conditional entropy; it is\ngreedy in the sense that the choice of addition is based only on the reduction\nachieved at that iteration. Its sequential nature gives it a lower\ncomputational complexity as compared to other existing comparison-based\ntechniques, all of which involve exhaustive searches over every node set of a\ncertain size. Our main result characterizes the sample complexity of this\nprocedure, as a function of node degrees, graph size and girth in factor-graph\nrepresentation. We subsequently specialize this result to the case of Ising\nmodels, where we provide a simple transparent characterization of sample\ncomplexity as a function of model and graph parameters.\n  For tree graphs, our algorithm is the same as the classical Chow-Liu\nalgorithm, and in that sense can be considered the extension of the same to\ngraphs with cycles.\n",
        "published": "2012-02-08T18:18:46Z",
        "pdf_link": "http://arxiv.org/pdf/1202.1787v1"
    },
    {
        "id": "http://arxiv.org/abs/1202.2169v3",
        "title": "High Dimensional Semiparametric Gaussian Copula Graphical Models",
        "summary": "  In this paper, we propose a semiparametric approach, named nonparanormal\nskeptic, for efficiently and robustly estimating high dimensional undirected\ngraphical models. To achieve modeling flexibility, we consider Gaussian Copula\ngraphical models (or the nonparanormal) as proposed by Liu et al. (2009). To\nachieve estimation robustness, we exploit nonparametric rank-based correlation\ncoefficient estimators, including Spearman's rho and Kendall's tau. In high\ndimensional settings, we prove that the nonparanormal skeptic achieves the\noptimal parametric rate of convergence in both graph and parameter estimation.\nThis celebrating result suggests that the Gaussian copula graphical models can\nbe used as a safe replacement of the popular Gaussian graphical models, even\nwhen the data are truly Gaussian. Besides theoretical analysis, we also conduct\nthorough numerical simulations to compare different estimators for their graph\nrecovery performance under both ideal and noisy settings. The proposed methods\nare then applied on a large-scale genomic dataset to illustrate their empirical\nusefulness. The R language software package huge implementing the proposed\nmethods is available on the Comprehensive R Archive Network: http://cran.\nr-project.org/.\n",
        "published": "2012-02-10T03:06:07Z",
        "pdf_link": "http://arxiv.org/pdf/1202.2169v3"
    },
    {
        "id": "http://arxiv.org/abs/1202.2476v1",
        "title": "Regularized Tensor Factorizations and Higher-Order Principal Components\n  Analysis",
        "summary": "  High-dimensional tensors or multi-way data are becoming prevalent in areas\nsuch as biomedical imaging, chemometrics, networking and bibliometrics.\nTraditional approaches to finding lower dimensional representations of tensor\ndata include flattening the data and applying matrix factorizations such as\nprincipal components analysis (PCA) or employing tensor decompositions such as\nthe CANDECOMP / PARAFAC (CP) and Tucker decompositions. The former can lose\nimportant structure in the data, while the latter Higher-Order PCA (HOPCA)\nmethods can be problematic in high-dimensions with many irrelevant features. We\nintroduce frameworks for sparse tensor factorizations or Sparse HOPCA based on\nheuristic algorithmic approaches and by solving penalized optimization problems\nrelated to the CP decomposition. Extensions of these approaches lead to methods\nfor general regularized tensor factorizations, multi-way Functional HOPCA and\ngeneralizations of HOPCA for structured data. We illustrate the utility of our\nmethods for dimension reduction, feature selection, and signal recovery on\nsimulated data and multi-dimensional microarrays and functional MRIs.\n",
        "published": "2012-02-11T21:22:41Z",
        "pdf_link": "http://arxiv.org/pdf/1202.2476v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.0117v3",
        "title": "Learning a Common Substructure of Multiple Graphical Gaussian Models",
        "summary": "  Properties of data are frequently seen to vary depending on the sampled\nsituations, which usually changes along a time evolution or owing to\nenvironmental effects. One way to analyze such data is to find invariances, or\nrepresentative features kept constant over changes. The aim of this paper is to\nidentify one such feature, namely interactions or dependencies among variables\nthat are common across multiple datasets collected under different conditions.\nTo that end, we propose a common substructure learning (CSSL) framework based\non a graphical Gaussian model. We further present a simple learning algorithm\nbased on the Dual Augmented Lagrangian and the Alternating Direction Method of\nMultipliers. We confirm the performance of CSSL over other existing techniques\nin finding unchanging dependency structures in multiple datasets through\nnumerical simulations on synthetic data and through a real world application to\nanomaly detection in automobile sensors.\n",
        "published": "2012-03-01T08:34:30Z",
        "pdf_link": "http://arxiv.org/pdf/1203.0117v3"
    },
    {
        "id": "http://arxiv.org/abs/1203.1065v1",
        "title": "Subspace clustering of high-dimensional data: a predictive approach",
        "summary": "  In several application domains, high-dimensional observations are collected\nand then analysed in search for naturally occurring data clusters which might\nprovide further insights about the nature of the problem. In this paper we\ndescribe a new approach for partitioning such high-dimensional data. Our\nassumption is that, within each cluster, the data can be approximated well by a\nlinear subspace estimated by means of a principal component analysis (PCA). The\nproposed algorithm, Predictive Subspace Clustering (PSC) partitions the data\ninto clusters while simultaneously estimating cluster-wise PCA parameters. The\nalgorithm minimises an objective function that depends upon a new measure of\ninfluence for PCA models. A penalised version of the algorithm is also\ndescribed for carrying our simultaneous subspace clustering and variable\nselection. The convergence of PSC is discussed in detail, and extensive\nsimulation results and comparisons to competing methods are presented. The\ncomparative performance of PSC has been assessed on six real gene expression\ndata sets for which PSC often provides state-of-art results.\n",
        "published": "2012-03-05T22:10:02Z",
        "pdf_link": "http://arxiv.org/pdf/1203.1065v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.1828v1",
        "title": "An ADMM Algorithm for a Class of Total Variation Regularized Estimation\n  Problems",
        "summary": "  We present an alternating augmented Lagrangian method for convex optimization\nproblems where the cost function is the sum of two terms, one that is separable\nin the variable blocks, and a second that is separable in the difference\nbetween consecutive variable blocks. Examples of such problems include Fused\nLasso estimation, total variation denoising, and multi-period portfolio\noptimization with transaction costs. In each iteration of our method, the first\nstep involves separately optimizing over each variable block, which can be\ncarried out in parallel. The second step is not separable in the variables, but\ncan be carried out very efficiently. We apply the algorithm to segmentation of\ndata based on changes inmean (l_1 mean filtering) or changes in variance (l_1\nvariance filtering). In a numerical example, we show that our implementation is\naround 10000 times faster compared with the generic optimization solver SDPT3.\n",
        "published": "2012-03-08T15:34:08Z",
        "pdf_link": "http://arxiv.org/pdf/1203.1828v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.4354v1",
        "title": "Asymptotic Confidence Sets for General Nonparametric Regression and\n  Classification by Regularized Kernel Methods",
        "summary": "  Regularized kernel methods such as, e.g., support vector machines and\nleast-squares support vector regression constitute an important class of\nstandard learning algorithms in machine learning. Theoretical investigations\nconcerning asymptotic properties have manly focused on rates of convergence\nduring the last years but there are only very few and limited (asymptotic)\nresults on statistical inference so far. As this is a serious limitation for\ntheir use in mathematical statistics, the goal of the article is to fill this\ngap. Based on asymptotic normality of many of these methods, the article\nderives a strongly consistent estimator for the unknown covariance matrix of\nthe limiting normal distribution. In this way, we obtain asymptotically correct\nconfidence sets for $\\psi(f_{P,\\lambda_0})$ where $f_{P,\\lambda_0}$ denotes the\nminimizer of the regularized risk in the reproducing kernel Hilbert space $H$\nand $\\psi:H\\rightarrow\\mathds{R}^m$ is any Hadamard-differentiable functional.\nApplications include (multivariate) pointwise confidence sets for values of\n$f_{P,\\lambda_0}$ and confidence sets for gradients, integrals, and norms.\n",
        "published": "2012-03-20T09:28:49Z",
        "pdf_link": "http://arxiv.org/pdf/1203.4354v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5647v1",
        "title": "Polynomial expansion of the binary classification function",
        "summary": "  This paper describes a novel method to approximate the polynomial\ncoefficients of regression functions, with particular interest on\nmulti-dimensional classification. The derivation is simple, and offers a fast,\nrobust classification technique that is resistant to over-fitting.\n",
        "published": "2012-03-26T12:43:19Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5647v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.6345v2",
        "title": "Empirical Normalization for Quadratic Discriminant Analysis and\n  Classifying Cancer Subtypes",
        "summary": "  We introduce a new discriminant analysis method (Empirical Discriminant\nAnalysis or EDA) for binary classification in machine learning. Given a dataset\nof feature vectors, this method defines an empirical feature map transforming\nthe training and test data into new data with components having Gaussian\nempirical distributions. This map is an empirical version of the Gaussian\ncopula used in probability and mathematical finance. The purpose is to form a\nfeature mapped dataset as close as possible to Gaussian, after which standard\nquadratic discriminants can be used for classification. We discuss this method\nin general, and apply it to some datasets in computational biology.\n",
        "published": "2012-03-28T19:24:35Z",
        "pdf_link": "http://arxiv.org/pdf/1203.6345v2"
    },
    {
        "id": "http://arxiv.org/abs/1204.0656v1",
        "title": "Application of Bayesian Hierarchical Prior Modeling to Sparse Channel\n  Estimation",
        "summary": "  Existing methods for sparse channel estimation typically provide an estimate\ncomputed as the solution maximizing an objective function defined as the sum of\nthe log-likelihood function and a penalization term proportional to the l1-norm\nof the parameter of interest. However, other penalization terms have proven to\nhave strong sparsity-inducing properties. In this work, we design\npilot-assisted channel estimators for OFDM wireless receivers within the\nframework of sparse Bayesian learning by defining hierarchical Bayesian prior\nmodels that lead to sparsity-inducing penalization terms. The estimators result\nas an application of the variational message-passing algorithm on the factor\ngraph representing the signal model extended with the hierarchical prior\nmodels. Numerical results demonstrate the superior performance of our channel\nestimators as compared to traditional and state-of-the-art sparse methods.\n",
        "published": "2012-04-03T11:12:52Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0656v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.1795v1",
        "title": "Estimation of causal orders in a linear non-Gaussian acyclic model: a\n  method robust against latent confounders",
        "summary": "  We consider to learn a causal ordering of variables in a linear non-Gaussian\nacyclic model called LiNGAM. Several existing methods have been shown to\nconsistently estimate a causal ordering assuming that all the model assumptions\nare correct. But, the estimation results could be distorted if some assumptions\nactually are violated. In this paper, we propose a new algorithm for learning\ncausal orders that is robust against one typical violation of the model\nassumptions: latent confounders. We demonstrate the effectiveness of our method\nusing artificial data.\n",
        "published": "2012-04-09T05:29:07Z",
        "pdf_link": "http://arxiv.org/pdf/1204.1795v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.2049v1",
        "title": "Coherence Functions with Applications in Large-Margin Classification\n  Methods",
        "summary": "  Support vector machines (SVMs) naturally embody sparseness due to their use\nof hinge loss functions. However, SVMs can not directly estimate conditional\nclass probabilities. In this paper we propose and study a family of coherence\nfunctions, which are convex and differentiable, as surrogates of the hinge\nfunction. The coherence function is derived by using the maximum-entropy\nprinciple and is characterized by a temperature parameter. It bridges the hinge\nfunction and the logit function in logistic regression. The limit of the\ncoherence function at zero temperature corresponds to the hinge function, and\nthe limit of the minimizer of its expected error is the minimizer of the\nexpected error of the hinge loss. We refer to the use of the coherence function\nin large-margin classification as C-learning, and we present efficient\ncoordinate descent algorithms for the training of regularized ${\\cal\nC}$-learning models.\n",
        "published": "2012-04-10T05:38:58Z",
        "pdf_link": "http://arxiv.org/pdf/1204.2049v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.3573v2",
        "title": "Learning Sets with Separating Kernels",
        "summary": "  We consider the problem of learning a set from random samples. We show how\nrelevant geometric and topological properties of a set can be studied\nanalytically using concepts from the theory of reproducing kernel Hilbert\nspaces. A new kind of reproducing kernel, that we call separating kernel, plays\na crucial role in our study and is analyzed in detail. We prove a new analytic\ncharacterization of the support of a distribution, that naturally leads to a\nfamily of provably consistent regularized learning algorithms and we discuss\nthe stability of these methods with respect to random sampling. Numerical\nexperiments show that the approach is competitive, and often better, than other\nstate of the art techniques.\n",
        "published": "2012-04-16T17:09:24Z",
        "pdf_link": "http://arxiv.org/pdf/1204.3573v2"
    },
    {
        "id": "http://arxiv.org/abs/1204.3942v1",
        "title": "Regularized Partial Least Squares with an Application to NMR\n  Spectroscopy",
        "summary": "  High-dimensional data common in genomics, proteomics, and chemometrics often\ncontains complicated correlation structures. Recently, partial least squares\n(PLS) and Sparse PLS methods have gained attention in these areas as dimension\nreduction techniques in the context of supervised data analysis. We introduce a\nframework for Regularized PLS by solving a relaxation of the SIMPLS\noptimization problem with penalties on the PLS loadings vectors. Our approach\nenjoys many advantages including flexibility, general penalties, easy\ninterpretation of results, and fast computation in high-dimensional settings.\nWe also outline extensions of our methods leading to novel methods for\nNon-negative PLS and Generalized PLS, an adaption of PLS for structured data.\nWe demonstrate the utility of our methods through simulations and a case study\non proton Nuclear Magnetic Resonance (NMR) spectroscopy data.\n",
        "published": "2012-04-17T23:38:29Z",
        "pdf_link": "http://arxiv.org/pdf/1204.3942v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.3965v1",
        "title": "Semi-Supervised learning with Density-Ratio Estimation",
        "summary": "  In this paper, we study statistical properties of semi-supervised learning,\nwhich is considered as an important problem in the community of machine\nlearning. In the standard supervised learning, only the labeled data is\nobserved. The classification and regression problems are formalized as the\nsupervised learning. In semi-supervised learning, unlabeled data is also\nobtained in addition to labeled data. Hence, exploiting unlabeled data is\nimportant to improve the prediction accuracy in semi-supervised learning. This\nproblems is regarded as a semiparametric estimation problem with missing data.\nUnder the the discriminative probabilistic models, it had been considered that\nthe unlabeled data is useless to improve the estimation accuracy. Recently, it\nwas revealed that the weighted estimator using the unlabeled data achieves\nbetter prediction accuracy in comparison to the learning method using only\nlabeled data, especially when the discriminative probabilistic model is\nmisspecified. That is, the improvement under the semiparametric model with\nmissing data is possible, when the semiparametric model is misspecified. In\nthis paper, we apply the density-ratio estimator to obtain the weight function\nin the semi-supervised learning. The benefit of our approach is that the\nproposed estimator does not require well-specified probabilistic models for the\nprobability of the unlabeled data. Based on the statistical asymptotic theory,\nwe prove that the estimation accuracy of our method outperforms the supervised\nlearning using only labeled data. Some numerical experiments present the\nusefulness of our methods.\n",
        "published": "2012-04-18T03:06:40Z",
        "pdf_link": "http://arxiv.org/pdf/1204.3965v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.4243v1",
        "title": "EP-GIG Priors and Applications in Bayesian Sparse Learning",
        "summary": "  In this paper we propose a novel framework for the construction of\nsparsity-inducing priors. In particular, we define such priors as a mixture of\nexponential power distributions with a generalized inverse Gaussian density\n(EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the\nspecial cases include Gaussian scale mixtures and Laplace scale mixtures.\nFurthermore, Laplace scale mixtures can subserve a Bayesian framework for\nsparse learning with nonconvex penalization. The densities of EP-GIG can be\nexplicitly expressed. Moreover, the corresponding posterior distribution also\nfollows a generalized inverse Gaussian distribution. These properties lead us\nto EM algorithms for Bayesian sparse learning. We show that these algorithms\nbear an interesting resemblance to iteratively re-weighted $\\ell_2$ or $\\ell_1$\nmethods. In addition, we present two extensions for grouped variable selection\nand logistic regression.\n",
        "published": "2012-04-19T02:59:03Z",
        "pdf_link": "http://arxiv.org/pdf/1204.4243v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.4708v1",
        "title": "Efficient hierarchical clustering for continuous data",
        "summary": "  We present an new sequential Monte Carlo sampler for coalescent based\nBayesian hierarchical clustering. Our model is appropriate for modeling\nnon-i.i.d. data and offers a substantial reduction of computational cost when\ncompared to the original sampler without resorting to approximations. We also\npropose a quadratic complexity approximation that in practice shows almost no\nloss in performance compared to its counterpart. We show that as a byproduct of\nour formulation, we obtain a greedy algorithm that exhibits performance\nimprovement over other greedy algorithms, particularly in small data sets. In\norder to exploit the correlation structure of the data, we describe how to\nincorporate Gaussian process priors in the model as a flexible way to model\nnon-i.i.d. data. Results on artificial and real data show significant\nimprovements over closely related approaches.\n",
        "published": "2012-04-20T19:19:05Z",
        "pdf_link": "http://arxiv.org/pdf/1204.4708v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.5540v3",
        "title": "Learning Loosely Connected Markov Random Fields",
        "summary": "  We consider the structure learning problem for graphical models that we call\nloosely connected Markov random fields, in which the number of short paths\nbetween any pair of nodes is small, and present a new conditional independence\ntest based algorithm for learning the underlying graph structure. The novel\nmaximization step in our algorithm ensures that the true edges are detected\ncorrectly even when there are short cycles in the graph. The number of samples\nrequired by our algorithm is C*log p, where p is the size of the graph and the\nconstant C depends on the parameters of the model. We show that several\npreviously studied models are examples of loosely connected Markov random\nfields, and our algorithm achieves the same or lower computational complexity\nthan the previously designed algorithms for individual cases. We also get new\nresults for more general graphical models, in particular, our algorithm learns\ngeneral Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with\nrunning time O(np^5).\n",
        "published": "2012-04-25T02:39:40Z",
        "pdf_link": "http://arxiv.org/pdf/1204.5540v3"
    },
    {
        "id": "http://arxiv.org/abs/1205.1406v2",
        "title": "Graph Prediction in a Low-Rank and Autoregressive Setting",
        "summary": "  We study the problem of prediction for evolving graph data. We formulate the\nproblem as the minimization of a convex objective encouraging sparsity and\nlow-rank of the solution, that reflect natural graph properties. The convex\nformulation allows to obtain oracle inequalities and efficient solvers. We\nprovide empirical results for our algorithm and comparison with competing\nmethods, and point out two open questions related to compressed sensing and\nalgebra of low-rank and sparse matrices.\n",
        "published": "2012-05-07T14:24:24Z",
        "pdf_link": "http://arxiv.org/pdf/1205.1406v2"
    },
    {
        "id": "http://arxiv.org/abs/1205.3234v5",
        "title": "Asymptotic Accuracy of Bayes Estimation for Latent Variables with\n  Redundancy",
        "summary": "  Hierarchical parametric models consisting of observable and latent variables\nare widely used for unsupervised learning tasks. For example, a mixture model\nis a representative hierarchical model for clustering. From the statistical\npoint of view, the models can be regular or singular due to the distribution of\ndata. In the regular case, the models have the identifiability; there is\none-to-one relation between a probability density function for the model\nexpression and the parameter. The Fisher information matrix is positive\ndefinite, and the estimation accuracy of both observable and latent variables\nhas been studied. In the singular case, on the other hand, the models are not\nidentifiable and the Fisher matrix is not positive definite. Conventional\nstatistical analysis based on the inverse Fisher matrix is not applicable.\nRecently, an algebraic geometrical analysis has been developed and is used to\nelucidate the Bayes estimation of observable variables. The present paper\napplies this analysis to latent-variable estimation and determines its\ntheoretical performance. Our results clarify behavior of the convergence of the\nposterior distribution. It is found that the posterior of the\nobservable-variable estimation can be different from the one in the\nlatent-variable estimation. Because of the difference, the Markov chain Monte\nCarlo method based on the parameter and the latent variable cannot construct\nthe desired posterior distribution.\n",
        "published": "2012-05-15T01:53:26Z",
        "pdf_link": "http://arxiv.org/pdf/1205.3234v5"
    },
    {
        "id": "http://arxiv.org/abs/1207.3399v2",
        "title": "Scaling of Model Approximation Errors and Expected Entropy Distances",
        "summary": "  We compute the expected value of the Kullback-Leibler divergence to various\nfundamental statistical models with respect to canonical priors on the\nprobability simplex. We obtain closed formulas for the expected model\napproximation errors, depending on the dimension of the models and the\ncardinalities of their sample spaces. For the uniform prior, the expected\ndivergence from any model containing the uniform distribution is bounded by a\nconstant $1-\\gamma$, and for the models that we consider, this bound is\napproached if the state space is very large and the models' dimension does not\ngrow too fast. For Dirichlet priors the expected divergence is bounded in a\nsimilar way, if the concentration parameters take reasonable values. These\nresults serve as reference values for more complicated statistical models.\n",
        "published": "2012-07-14T07:10:19Z",
        "pdf_link": "http://arxiv.org/pdf/1207.3399v2"
    },
    {
        "id": "http://arxiv.org/abs/1207.3649v1",
        "title": "Nested Expectation Propagation for Gaussian Process Classification with\n  a Multinomial Probit Likelihood",
        "summary": "  We consider probabilistic multinomial probit classification using Gaussian\nprocess (GP) priors. The challenges with the multiclass GP classification are\nthe integration over the non-Gaussian posterior distribution, and the increase\nof the number of unknown latent variables as the number of target classes\ngrows. Expectation propagation (EP) has proven to be a very accurate method for\napproximate inference but the existing EP approaches for the multinomial probit\nGP classification rely on numerical quadratures or independence assumptions\nbetween the latent values from different classes to facilitate the\ncomputations. In this paper, we propose a novel nested EP approach which does\nnot require numerical quadratures, and approximates accurately all\nbetween-class posterior dependencies of the latent values, but still scales\nlinearly in the number of classes. The predictive accuracy of the nested EP\napproach is compared to Laplace, variational Bayes, and Markov chain Monte\nCarlo (MCMC) approximations with various benchmark data sets. In the\nexperiments nested EP was the most consistent method with respect to MCMC\nsampling, but the differences between the compared methods were small if only\nthe classification accuracy is concerned.\n",
        "published": "2012-07-16T12:29:22Z",
        "pdf_link": "http://arxiv.org/pdf/1207.3649v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.4674v1",
        "title": "Models of Disease Spectra",
        "summary": "  Case vs control comparisons have been the classical approach to the study of\nneurological diseases. However, most patients will not fall cleanly into either\ngroup. Instead, clinicians will typically find patients that cannot be\nclassified as having clearly progressed into the disease state. For those\nsubjects, very little can be said about their brain function on the basis of\nanalyses of group differences. To describe the intermediate brain function\nrequires models that interpolate between the disease states. We have chosen\nGaussian Processes (GP) regression to obtain a continuous spectrum of brain\nactivation and to extract the unknown disease progression profile. Our models\nincorporate spatial distribution of measures of activation, e.g. the\ncorrelation of an fMRI trace with an input stimulus, and so constitute\nultra-high multi-variate GP regressors. We applied GPs to model fMRI image\nphenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACE\netc. scores, and obtained predictions at non-observed MMSE/ACE values. The\noverall model confirmed the known reduction in the spatial extent of activity\nin response to reading versus false-font stimulation. The predictive\nuncertainty indicated the worsening confidence intervals at behavioural scores\ndistance from those used for GP training. Thus, the model indicated the type of\npatient (what behavioural score) that would need to included in the training\ndata to improve models predictions.\n",
        "published": "2012-07-19T14:06:36Z",
        "pdf_link": "http://arxiv.org/pdf/1207.4674v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.6684v2",
        "title": "Group Iterative Spectrum Thresholding for Super-Resolution Sparse\n  Spectral Selection",
        "summary": "  Recently, sparsity-based algorithms are proposed for super-resolution\nspectrum estimation. However, to achieve adequately high resolution in\nreal-world signal analysis, the dictionary atoms have to be close to each other\nin frequency, thereby resulting in a coherent design. The popular convex\ncompressed sensing methods break down in presence of high coherence and large\nnoise. We propose a new regularization approach to handle model collinearity\nand obtain parsimonious frequency selection simultaneously. It takes advantage\nof the pairing structure of sine and cosine atoms in the frequency dictionary.\nA probabilistic spectrum screening is also developed for fast computation in\nhigh dimensions. A data-resampling version of high-dimensional Bayesian\nInformation Criterion is used to determine the regularization parameters.\nExperiments show the efficacy and efficiency of the proposed algorithms in\nchallenging situations with small sample size, high frequency resolution, and\nlow signal-to-noise ratio.\n",
        "published": "2012-07-28T07:01:09Z",
        "pdf_link": "http://arxiv.org/pdf/1207.6684v2"
    },
    {
        "id": "http://arxiv.org/abs/1208.4183v1",
        "title": "Learning LiNGAM based on data with more variables than observations",
        "summary": "  A very important topic in systems biology is developing statistical methods\nthat automatically find causal relations in gene regulatory networks with no\nprior knowledge of causal connectivity. Many methods have been developed for\ntime series data. However, discovery methods based on steady-state data are\noften necessary and preferable since obtaining time series data can be more\nexpensive and/or infeasible for many biological systems. A conventional\napproach is causal Bayesian networks. However, estimation of Bayesian networks\nis ill-posed. In many cases it cannot uniquely identify the underlying causal\nnetwork and only gives a large class of equivalent causal networks that cannot\nbe distinguished between based on the data distribution. We propose a new\ndiscovery algorithm for uniquely identifying the underlying causal network of\ngenes. To the best of our knowledge, the proposed method is the first algorithm\nfor learning gene networks based on a fully identifiable causal model called\nLiNGAM. We here compare our algorithm with competing algorithms using\nartificially-generated data, although it is definitely better to test it based\non real microarray gene expression data.\n",
        "published": "2012-08-21T03:46:08Z",
        "pdf_link": "http://arxiv.org/pdf/1208.4183v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.4411v1",
        "title": "A non-parametric mixture model for topic modeling over time",
        "summary": "  A single, stationary topic model such as latent Dirichlet allocation is\ninappropriate for modeling corpora that span long time periods, as the\npopularity of topics is likely to change over time. A number of models that\nincorporate time have been proposed, but in general they either exhibit limited\nforms of temporal variation, or require computationally expensive inference\nmethods. In this paper we propose non-parametric Topics over Time (npTOT), a\nmodel for time-varying topics that allows an unbounded number of topics and\nexible distribution over the temporal variations in those topics' popularity.\nWe develop a collapsed Gibbs sampler for the proposed model and compare against\nexisting models on synthetic and real document sets.\n",
        "published": "2012-08-22T02:02:40Z",
        "pdf_link": "http://arxiv.org/pdf/1208.4411v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.0016v2",
        "title": "On the convergence of maximum variance unfolding",
        "summary": "  Maximum Variance Unfolding is one of the main methods for (nonlinear)\ndimensionality reduction. We study its large sample limit, providing specific\nrates of convergence under standard assumptions. We find that it is consistent\nwhen the underlying submanifold is isometric to a convex subset, and we provide\nsome simple examples where it fails to be consistent.\n",
        "published": "2012-08-31T20:58:15Z",
        "pdf_link": "http://arxiv.org/pdf/1209.0016v2"
    },
    {
        "id": "http://arxiv.org/abs/1209.0367v4",
        "title": "Seeded Graph Matching",
        "summary": "  Given two graphs, the graph matching problem is to align the two vertex sets\nso as to minimize the number of adjacency disagreements between the two graphs.\nThe seeded graph matching problem is the graph matching problem when we are\nfirst given a partial alignment that we are tasked with completing. In this\npaper, we modify the state-of-the-art approximate graph matching algorithm\n\"FAQ\" of Vogelstein et al. (2015) to make it a fast approximate seeded graph\nmatching algorithm, adapt its applicability to include graphs with differently\nsized vertex sets, and extend the algorithm so as to provide, for each\nindividual vertex, a nomination list of likely matches. We demonstrate the\neffectiveness of our algorithm via simulation and real data experiments;\nindeed, knowledge of even a few seeds can be extremely effective when our\nseeded graph matching algorithm is used to recover a naturally existing\nalignment that is only partially observed.\n",
        "published": "2012-09-03T14:45:53Z",
        "pdf_link": "http://arxiv.org/pdf/1209.0367v4"
    },
    {
        "id": "http://arxiv.org/abs/1209.1996v1",
        "title": "A Bayesian Boosting Model",
        "summary": "  We offer a novel view of AdaBoost in a statistical setting. We propose a\nBayesian model for binary classification in which label noise is modeled\nhierarchically. Using variational inference to optimize a dynamic evidence\nlower bound, we derive a new boosting-like algorithm called VIBoost. We show\nits close connections to AdaBoost and give experimental results from four\ndatasets.\n",
        "published": "2012-09-10T13:57:37Z",
        "pdf_link": "http://arxiv.org/pdf/1209.1996v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.3230v1",
        "title": "Link Prediction in Graphs with Autoregressive Features",
        "summary": "  In the paper, we consider the problem of link prediction in time-evolving\ngraphs. We assume that certain graph features, such as the node degree, follow\na vector autoregressive (VAR) model and we propose to use this information to\nimprove the accuracy of prediction. Our strategy involves a joint optimization\nprocedure over the space of adjacency matrices and VAR matrices which takes\ninto account both sparsity and low rank properties of the matrices. Oracle\ninequalities are derived and illustrate the trade-offs in the choice of\nsmoothing parameters when modeling the joint effect of sparsity and low rank\nproperty. The estimate is computed efficiently using proximal methods through a\ngeneralized forward-backward agorithm.\n",
        "published": "2012-09-14T15:27:45Z",
        "pdf_link": "http://arxiv.org/pdf/1209.3230v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.3431v2",
        "title": "Recovering Block-structured Activations Using Compressive Measurements",
        "summary": "  We consider the problems of detection and localization of a contiguous block\nof weak activation in a large matrix, from a small number of noisy, possibly\nadaptive, compressive (linear) measurements. This is closely related to the\nproblem of compressed sensing, where the task is to estimate a sparse vector\nusing a small number of linear measurements. Contrary to results in compressed\nsensing, where it has been shown that neither adaptivity nor contiguous\nstructure help much, we show that for reliable localization the magnitude of\nthe weakest signals is strongly influenced by both structure and the ability to\nchoose measurements adaptively while for detection neither adaptivity nor\nstructure reduce the requirement on the magnitude of the signal. We\ncharacterize the precise tradeoffs between the various problem parameters, the\nsignal strength and the number of measurements required to reliably detect and\nlocalize the block of activation. The sufficient conditions are complemented\nwith information theoretic lower bounds.\n",
        "published": "2012-09-15T20:06:33Z",
        "pdf_link": "http://arxiv.org/pdf/1209.3431v2"
    },
    {
        "id": "http://arxiv.org/abs/1209.4120v2",
        "title": "Scaling Multidimensional Inference for Structured Gaussian Processes",
        "summary": "  Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N,\nmaking it intractable for large N. Many algorithms for improving GP scaling\napproximate the covariance with lower rank matrices. Other work has exploited\nstructure inherent in particular covariance functions, including GPs with\nimplied Markov structure, and equispaced inputs (both enable O(N) runtime).\nHowever, these GP advances have not been extended to the multidimensional input\nsetting, despite the preponderance of multidimensional applications. This paper\nintroduces and tests novel extensions of structured GPs to multidimensional\ninputs. We present new methods for additive GPs, showing a novel connection\nbetween the classic backfitting method and the Bayesian framework. To achieve\noptimal accuracy-complexity tradeoff, we extend this model with a novel variant\nof projection pursuit regression. Our primary result -- projection pursuit\nGaussian Process Regression -- shows orders of magnitude speedup while\npreserving high accuracy. The natural second and third steps include\nnon-Gaussian observations and higher dimensional equispaced grid methods. We\nintroduce novel techniques to address both of these necessary directions. We\nthoroughly illustrate the power of these three advances on several datasets,\nachieving close performance to the naive Full GP at orders of magnitude less\ncost.\n",
        "published": "2012-09-18T23:03:01Z",
        "pdf_link": "http://arxiv.org/pdf/1209.4120v2"
    },
    {
        "id": "http://arxiv.org/abs/1209.4360v4",
        "title": "Variational Inference in Nonconjugate Models",
        "summary": "  Mean-field variational methods are widely used for approximate posterior\ninference in many probabilistic models. In a typical application, mean-field\nmethods approximately compute the posterior with a coordinate-ascent\noptimization algorithm. When the model is conditionally conjugate, the\ncoordinate updates are easily derived and in closed form. However, many models\nof interest---like the correlated topic model and Bayesian logistic\nregression---are nonconjuate. In these models, mean-field methods cannot be\ndirectly applied and practitioners have had to develop variational algorithms\non a case-by-case basis. In this paper, we develop two generic methods for\nnonconjugate models, Laplace variational inference and delta method variational\ninference. Our methods have several advantages: they allow for easily derived\nvariational algorithms with a wide class of nonconjugate models; they extend\nand unify some of the existing algorithms that have been derived for specific\nmodels; and they work well on real-world datasets. We studied our methods on\nthe correlated topic model, Bayesian logistic regression, and hierarchical\nBayesian logistic regression.\n",
        "published": "2012-09-19T20:05:44Z",
        "pdf_link": "http://arxiv.org/pdf/1209.4360v4"
    },
    {
        "id": "http://arxiv.org/abs/1209.5375v2",
        "title": "Improving accuracy and power with transfer learning using a\n  meta-analytic database",
        "summary": "  Typical cohorts in brain imaging studies are not large enough for systematic\ntesting of all the information contained in the images. To build testable\nworking hypotheses, investigators thus rely on analysis of previous work,\nsometimes formalized in a so-called meta-analysis. In brain imaging, this\napproach underlies the specification of regions of interest (ROIs) that are\nusually selected on the basis of the coordinates of previously detected\neffects. In this paper, we propose to use a database of images, rather than\ncoordinates, and frame the problem as transfer learning: learning a\ndiscriminant model on a reference task to apply it to a different but related\nnew task. To facilitate statistical analysis of small cohorts, we use a sparse\ndiscriminant model that selects predictive voxels on the reference task and\nthus provides a principled procedure to define ROIs. The benefits of our\napproach are twofold. First it uses the reference database for prediction, i.e.\nto provide potential biomarkers in a clinical setting. Second it increases\nstatistical power on the new task. We demonstrate on a set of 18 pairs of\nfunctional MRI experimental conditions that our approach gives good prediction.\nIn addition, on a specific transfer situation involving different scanners at\ndifferent locations, we show that voxel selection based on transfer learning\nleads to higher detection power on small cohorts.\n",
        "published": "2012-09-24T19:38:46Z",
        "pdf_link": "http://arxiv.org/pdf/1209.5375v2"
    },
    {
        "id": "http://arxiv.org/abs/1210.0805v2",
        "title": "Robust PCA and subspace tracking from incomplete observations using\n  L0-surrogates",
        "summary": "  Many applications in data analysis rely on the decomposition of a data matrix\ninto a low-rank and a sparse component. Existing methods that tackle this task\nuse the nuclear norm and L1-cost functions as convex relaxations of the rank\nconstraint and the sparsity measure, respectively, or employ thresholding\ntechniques. We propose a method that allows for reconstructing and tracking a\nsubspace of upper-bounded dimension from incomplete and corrupted observations.\nIt does not require any a priori information about the number of outliers. The\ncore of our algorithm is an intrinsic Conjugate Gradient method on the set of\northogonal projection matrices, the so-called Grassmannian. Non-convex sparsity\nmeasures are used for outlier detection, which leads to improved performance in\nterms of robustly recovering and tracking the low-rank matrix. In particular,\nour approach can cope with more outliers and with an underlying matrix of\nhigher rank than other state-of-the-art methods.\n",
        "published": "2012-10-02T15:26:15Z",
        "pdf_link": "http://arxiv.org/pdf/1210.0805v2"
    },
    {
        "id": "http://arxiv.org/abs/1210.3335v3",
        "title": "Improved Graph Clustering",
        "summary": "  Graph clustering involves the task of dividing nodes into clusters, so that\nthe edge density is higher within clusters as opposed to across clusters. A\nnatural, classic and popular statistical setting for evaluating solutions to\nthis problem is the stochastic block model, also referred to as the planted\npartition model.\n  In this paper we present a new algorithm--a convexified version of Maximum\nLikelihood--for graph clustering. We show that, in the classic stochastic block\nmodel setting, it outperforms existing methods by polynomial factors when the\ncluster size is allowed to have general scalings. In fact, it is within\nlogarithmic factors of known lower bounds for spectral methods, and there is\nevidence suggesting that no polynomial time algorithm would do significantly\nbetter.\n  We then show that this guarantee carries over to a more general extension of\nthe stochastic block model. Our method can handle the settings of semi-random\ngraphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliated\nnodes, partially observed graphs and planted clique/coloring etc. In\nparticular, our results provide the best exact recovery guarantees to date for\nthe planted partition, planted k-disjoint-cliques and planted noisy coloring\nmodels with general cluster sizes; in other settings, we match the best\nexisting results up to logarithmic factors.\n",
        "published": "2012-10-11T19:27:28Z",
        "pdf_link": "http://arxiv.org/pdf/1210.3335v3"
    },
    {
        "id": "http://arxiv.org/abs/1210.5345v1",
        "title": "Adaptive Stratified Sampling for Monte-Carlo integration of\n  Differentiable functions",
        "summary": "  We consider the problem of adaptive stratified sampling for Monte Carlo\nintegration of a differentiable function given a finite number of evaluations\nto the function. We construct a sampling scheme that samples more often in\nregions where the function oscillates more, while allocating the samples such\nthat they are well spread on the domain (this notion shares similitude with low\ndiscrepancy). We prove that the estimate returned by the algorithm is almost\nsimilarly accurate as the estimate that an optimal oracle strategy (that would\nknow the variations of the function everywhere) would return, and provide a\nfinite-sample analysis.\n",
        "published": "2012-10-19T09:03:24Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5345v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.5806v1",
        "title": "Multi-Stage Multi-Task Feature Learning",
        "summary": "  Multi-task sparse feature learning aims to improve the generalization\nperformance by exploiting the shared features among tasks. It has been\nsuccessfully applied to many applications including computer vision and\nbiomedical informatics. Most of the existing multi-task sparse feature learning\nalgorithms are formulated as a convex sparse regularization problem, which is\nusually suboptimal, due to its looseness for approximating an $\\ell_0$-type\nregularizer. In this paper, we propose a non-convex formulation for multi-task\nsparse feature learning based on a novel non-convex regularizer. To solve the\nnon-convex optimization problem, we propose a Multi-Stage Multi-Task Feature\nLearning (MSMTFL) algorithm; we also provide intuitive interpretations,\ndetailed convergence and reproducibility analysis for the proposed algorithm.\nMoreover, we present a detailed theoretical analysis showing that MSMTFL\nachieves a better parameter estimation error bound than the convex formulation.\nEmpirical studies on both synthetic and real-world data sets demonstrate the\neffectiveness of MSMTFL in comparison with the state of the art multi-task\nsparse feature learning algorithms.\n",
        "published": "2012-10-22T05:41:29Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5806v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.7665v2",
        "title": "Graph Estimation From Multi-attribute Data",
        "summary": "  Many real world network problems often concern multivariate nodal attributes\nsuch as image, textual, and multi-view feature vectors on nodes, rather than\nsimple univariate nodal attributes. The existing graph estimation methods built\non Gaussian graphical models and covariance selection algorithms can not handle\nsuch data, neither can the theories developed around such methods be directly\napplied. In this paper, we propose a new principled framework for estimating\ngraphs from multi-attribute data. Instead of estimating the partial correlation\nas in current literature, our method estimates the partial canonical\ncorrelations that naturally accommodate complex nodal features.\nComputationally, we provide an efficient algorithm which utilizes the\nmulti-attribute structure. Theoretically, we provide sufficient conditions\nwhich guarantee consistent graph recovery. Extensive simulation studies\ndemonstrate performance of our method under various conditions. Furthermore, we\nprovide illustrative applications to uncovering gene regulatory networks from\ngene and protein profiles, and uncovering brain connectivity graph from\nfunctional magnetic resonance imaging data.\n",
        "published": "2012-10-29T13:54:36Z",
        "pdf_link": "http://arxiv.org/pdf/1210.7665v2"
    },
    {
        "id": "http://arxiv.org/abs/1210.8429v1",
        "title": "Anomaly Detection in Time Series of Graphs using Fusion of Graph\n  Invariants",
        "summary": "  Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where the\nfixed vertex set V represents \"actors\" and an edge between vertex u and vertex\nv at time t (uv \\in E(t)) represents the existence of a communications event\nbetween actors u and v during the tth time period, we wish to detect anomalies\nand/or change points. We consider a collection of graph features, or\ninvariants, and demonstrate that adaptive fusion provides superior inferential\nefficacy compared to naive equal weighting for a certain class of anomaly\ndetection problems. Simulation results using a latent process model for time\nseries of graphs, as well as illustrative experimental results for a time\nseries of graphs derived from the Enron email data, show that a fusion\nstatistic can provide superior inference compared to individual invariants\nalone. These results also demonstrate that an adaptive weighting scheme for\nfusion of invariants performs better than naive equal weighting.\n",
        "published": "2012-10-31T18:24:10Z",
        "pdf_link": "http://arxiv.org/pdf/1210.8429v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.0932v1",
        "title": "Kernels and Submodels of Deep Belief Networks",
        "summary": "  We study the mixtures of factorizing probability distributions represented as\nvisible marginal distributions in stochastic layered networks. We take the\nperspective of kernel transitions of distributions, which gives a unified\npicture of distributed representations arising from Deep Belief Networks (DBN)\nand other networks without lateral connections. We describe combinatorial and\ngeometric properties of the set of kernels and products of kernels realizable\nby DBNs as the network parameters vary. We describe explicit classes of\nprobability distributions, including exponential families, that can be learned\nby DBNs. We use these submodels to bound the maximal and the expected\nKullback-Leibler approximation errors of DBNs from above depending on the\nnumber of hidden layers and units that they contain.\n",
        "published": "2012-11-05T17:13:51Z",
        "pdf_link": "http://arxiv.org/pdf/1211.0932v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.1275v3",
        "title": "Kernelized Bayesian Matrix Factorization",
        "summary": "  We extend kernelized matrix factorization with a fully Bayesian treatment and\nwith an ability to work with multiple side information sources expressed as\ndifferent kernels. Kernel functions have been introduced to matrix\nfactorization to integrate side information about the rows and columns (e.g.,\nobjects and users in recommender systems), which is necessary for making\nout-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite\ngraph inference, where the output matrix is binary, but extensions to more\ngeneral matrices are straightforward. We extend the state of the art in two key\naspects: (i) A fully conjugate probabilistic formulation of the kernelized\nmatrix factorization problem enables an efficient variational approximation,\nwhereas fully Bayesian treatments are not computationally feasible in the\nearlier approaches. (ii) Multiple side information sources are included,\ntreated as different kernels in multiple kernel learning that additionally\nreveals which side information sources are informative. Our method outperforms\nalternatives in predicting drug-protein interactions on two data sets. We then\nshow that our framework can also be used for solving multilabel learning\nproblems by considering samples and labels as the two domains where matrix\nfactorization operates on. Our algorithm obtains the lowest Hamming loss values\non 10 out of 14 multilabel classification data sets compared to five\nstate-of-the-art multilabel learning algorithms.\n",
        "published": "2012-11-06T15:54:07Z",
        "pdf_link": "http://arxiv.org/pdf/1211.1275v3"
    },
    {
        "id": "http://arxiv.org/abs/1211.3038v4",
        "title": "Gradient density estimation in arbitrary finite dimensions using the\n  method of stationary phase",
        "summary": "  We prove that the density function of the gradient of a sufficiently smooth\nfunction $S : \\Omega \\subset \\mathbb{R}^d \\rightarrow \\mathbb{R}$, obtained via\na random variable transformation of a uniformly distributed random variable, is\nincreasingly closely approximated by the normalized power spectrum of\n$\\phi=\\exp\\left(\\frac{iS}{\\tau}\\right)$ as the free parameter $\\tau \\rightarrow\n0$. The result is shown using the stationary phase approximation and standard\nintegration techniques and requires proper ordering of limits. We highlight a\nrelationship with the well-known characteristic function approach to density\nestimation, and detail why our result is distinct from this approach.\n",
        "published": "2012-11-13T16:19:31Z",
        "pdf_link": "http://arxiv.org/pdf/1211.3038v4"
    },
    {
        "id": "http://arxiv.org/abs/1211.3589v3",
        "title": "A Truncated EM Approach for Spike-and-Slab Sparse Coding",
        "summary": "  We study inference and learning based on a sparse coding model with\n`spike-and-slab' prior. As in standard sparse coding, the model used assumes\nindependent latent sources that linearly combine to generate data points.\nHowever, instead of using a standard sparse prior such as a Laplace\ndistribution, we study the application of a more flexible `spike-and-slab'\ndistribution which models the absence or presence of a source's contribution\nindependently of its strength if it contributes. We investigate two approaches\nto optimize the parameters of spike-and-slab sparse coding: a novel truncated\nEM approach and, for comparison, an approach based on standard factored\nvariational distributions. The truncated approach can be regarded as a\nvariational approach with truncated posteriors as variational distributions. In\napplications to source separation we find that both approaches improve the\nstate-of-the-art in a number of standard benchmarks, which argues for the use\nof `spike-and-slab' priors for the corresponding data domains. Furthermore, we\nfind that the truncated EM approach improves on the standard factored approach\nin source separation tasks$-$which hints to biases introduced by assuming\nposterior independence in the factored variational approach. Likewise, on a\nstandard benchmark for image denoising, we find that the truncated EM approach\nimproves on the factored variational approach. While the performance of the\nfactored approach saturates with increasing numbers of hidden dimensions, the\nperformance of the truncated approach improves the state-of-the-art for higher\nnoise levels.\n",
        "published": "2012-11-15T12:34:07Z",
        "pdf_link": "http://arxiv.org/pdf/1211.3589v3"
    },
    {
        "id": "http://arxiv.org/abs/1211.3601v4",
        "title": "Statistical inference on errorfully observed graphs",
        "summary": "  Statistical inference on graphs is a burgeoning field in the applied and\ntheoretical statistics communities, as well as throughout the wider world of\nscience, engineering, business, etc. In many applications, we are faced with\nthe reality of errorfully observed graphs. That is, the existence of an edge\nbetween two vertices is based on some imperfect assessment. In this paper, we\nconsider a graph $G = (V,E)$. We wish to perform an inference task -- the\ninference task considered here is \"vertex classification\". However, we do not\nobserve $G$; rather, for each potential edge $uv \\in {{V}\\choose{2}}$ we\nobserve an \"edge-feature\" which we use to classify $uv$ as edge/not-edge. Thus\nwe errorfully observe $G$ when we observe the graph $\\widetilde{G} =\n(V,\\widetilde{E})$ as the edges in $\\widetilde{E}$ arise from the\nclassifications of the \"edge-features\", and are expected to be errorful.\nMoreover, we face a quantity/quality trade-off regarding the edge-features we\nobserve -- more informative edge-features are more expensive, and hence the\nnumber of potential edges that can be assessed decreases with the quality of\nthe edge-features. We studied this problem by formulating a quantity/quality\ntradeoff for a simple class of random graphs model, namely the stochastic\nblockmodel. We then consider a simple but optimal vertex classifier for\nclassifying $v$ and we derive the optimal quantity/quality operating point for\nsubsequent graph inference in the face of this trade-off. The optimal operating\npoints for the quantity/quality trade-off are surprising and illustrate the\nissue that methods for intermediate tasks should be chosen to maximize\nperformance for the ultimate inference task. Finally, we investigate the\nquantity/quality tradeoff for errorful obesrvations of the {\\it C.\\ elegans}\nconnectome graph.\n",
        "published": "2012-11-15T13:22:09Z",
        "pdf_link": "http://arxiv.org/pdf/1211.3601v4"
    },
    {
        "id": "http://arxiv.org/abs/1211.4706v1",
        "title": "Random Input Sampling for Complex Models Using Markov Chain Monte Carlo",
        "summary": "  Many random processes can be simulated as the output of a deterministic model\naccepting random inputs. Such a model usually describes a complex mathematical\nor physical stochastic system and the randomness is introduced in the input\nvariables of the model. When the statistics of the output event are known,\nthese input variables have to be chosen in a specific way for the output to\nhave the prescribed statistics. Because the probability distribution of the\ninput random variables is not directly known but dictated implicitly by the\nstatistics of the output random variables, this problem is usually intractable\nfor classical sampling methods. Based on Markov Chain Monte Carlo we propose a\nnovel method to sample random inputs to such models by introducing a\nmodification to the standard Metropolis-Hastings algorithm. As an example we\nconsider a system described by a stochastic differential equation (sde) and\ndemonstrate how sample paths of a random process satisfying this sde can be\ngenerated with our technique.\n",
        "published": "2012-11-20T10:45:25Z",
        "pdf_link": "http://arxiv.org/pdf/1211.4706v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.7120v1",
        "title": "Exact and Efficient Parallel Inference for Nonparametric Mixture Models",
        "summary": "  Nonparametric mixture models based on the Dirichlet process are an elegant\nalternative to finite models when the number of underlying components is\nunknown, but inference in such models can be slow. Existing attempts to\nparallelize inference in such models have relied on introducing approximations,\nwhich can lead to inaccuracies in the posterior estimate. In this paper, we\ndescribe auxiliary variable representations for the Dirichlet process and the\nhierarchical Dirichlet process that allow us to sample from the true posterior\nin a distributed manner. We show that our approach allows scalable inference\nwithout the deterioration in estimate quality that accompanies existing\nmethods.\n",
        "published": "2012-11-29T23:39:00Z",
        "pdf_link": "http://arxiv.org/pdf/1211.7120v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.1263v1",
        "title": "On the probabilistic continuous complexity conjecture",
        "summary": "  In this paper we prove the probabilistic continuous complexity conjecture. In\ncontinuous complexity theory, this states that the complexity of solving a\ncontinuous problem with probability approaching 1 converges (in this limit) to\nthe complexity of solving the same problem in its worst case. We prove the\nconjecture holds if and only if space of problem elements is uniformly convex.\nThe non-uniformly convex case has a striking counterexample in the problem of\nidentifying a Brownian path in Wiener space, where it is shown that\nprobabilistic complexity converges to only half of the worst case complexity in\nthis limit.\n",
        "published": "2012-12-06T09:17:39Z",
        "pdf_link": "http://arxiv.org/pdf/1212.1263v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.1666v2",
        "title": "Developments in the theory of randomized shortest paths with a\n  comparison of graph node distances",
        "summary": "  There have lately been several suggestions for parametrized distances on a\ngraph that generalize the shortest path distance and the commute time or\nresistance distance. The need for developing such distances has risen from the\nobservation that the above-mentioned common distances in many situations fail\nto take into account the global structure of the graph. In this article, we\ndevelop the theory of one family of graph node distances, known as the\nrandomized shortest path dissimilarity, which has its foundation in statistical\nphysics. We show that the randomized shortest path dissimilarity can be easily\ncomputed in closed form for all pairs of nodes of a graph. Moreover, we come up\nwith a new definition of a distance measure that we call the free energy\ndistance. The free energy distance can be seen as an upgrade of the randomized\nshortest path dissimilarity as it defines a metric, in addition to which it\nsatisfies the graph-geodetic property. The derivation and computation of the\nfree energy distance are also straightforward. We then make a comparison\nbetween a set of generalized distances that interpolate between the shortest\npath distance and the commute time, or resistance distance. This comparison\nfocuses on the applicability of the distances in graph node clustering and\nclassification. The comparison, in general, shows that the parametrized\ndistances perform well in the tasks. In particular, we see that the results\nobtained with the free energy distance are among the best in all the\nexperiments.\n",
        "published": "2012-12-07T17:51:17Z",
        "pdf_link": "http://arxiv.org/pdf/1212.1666v2"
    },
    {
        "id": "http://arxiv.org/abs/1212.1780v1",
        "title": "An Empirical Comparison of V-fold Penalisation and Cross Validation for\n  Model Selection in Distribution-Free Regression",
        "summary": "  Model selection is a crucial issue in machine-learning and a wide variety of\npenalisation methods (with possibly data dependent complexity penalties) have\nrecently been introduced for this purpose. However their empirical performance\nis generally not well documented in the literature. It is the goal of this\npaper to investigate to which extent such recent techniques can be successfully\nused for the tuning of both the regularisation and kernel parameters in support\nvector regression (SVR) and the complexity measure in regression trees (CART).\nThis task is traditionally solved via V-fold cross-validation (VFCV), which\ngives efficient results for a reasonable computational cost. A disadvantage\nhowever of VFCV is that the procedure is known to provide an asymptotically\nsuboptimal risk estimate as the number of examples tends to infinity. Recently,\na penalisation procedure called V-fold penalisation has been proposed to\nimprove on VFCV, supported by theoretical arguments. Here we report on an\nextensive set of experiments comparing V-fold penalisation and VFCV for\nSVR/CART calibration on several benchmark datasets. We highlight cases in which\nVFCV and V-fold penalisation provide poor estimates of the risk respectively\nand introduce a modified penalisation technique to reduce the estimation error.\n",
        "published": "2012-12-08T11:34:10Z",
        "pdf_link": "http://arxiv.org/pdf/1212.1780v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.2126v2",
        "title": "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes",
        "summary": "  The classical mixture of Gaussians model is related to K-means via\nsmall-variance asymptotics: as the covariances of the Gaussians tend to zero,\nthe negative log-likelihood of the mixture of Gaussians model approaches the\nK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis\n& Jordan (2012) used this observation to obtain a novel K-means-like algorithm\nfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We instead\nconsider applying small-variance asymptotics directly to the posterior in\nBayesian nonparametric models. This framework is independent of any specific\nBayesian inference algorithm, and it has the major advantage that it\ngeneralizes immediately to a range of models beyond the DP mixture. To\nillustrate, we apply our framework to the feature learning setting, where the\nbeta process and Indian buffet process provide an appropriate Bayesian\nnonparametric prior. We obtain a novel objective function that goes beyond\nclustering to learn (and penalize new) groupings for which we relax the mutual\nexclusivity and exhaustivity assumptions of clustering. We demonstrate several\nother algorithms, all of which are scalable and simple to implement. Empirical\nresults demonstrate the benefits of the new framework.\n",
        "published": "2012-12-10T16:42:44Z",
        "pdf_link": "http://arxiv.org/pdf/1212.2126v2"
    },
    {
        "id": "http://arxiv.org/abs/1212.4562v1",
        "title": "A complexity analysis of statistical learning algorithms",
        "summary": "  We apply information-based complexity analysis to support vector machine\n(SVM) algorithms, with the goal of a comprehensive continuous algorithmic\nanalysis of such algorithms. This involves complexity measures in which some\nhigher order operations (e.g., certain optimizations) are considered primitive\nfor the purposes of measuring complexity. We consider classes of information\noperators and algorithms made up of scaled families, and investigate the\nutility of scaling the complexities to minimize error. We look at the division\nof statistical learning into information and algorithmic components, at the\ncomplexities of each, and at applications to support vector machine (SVM) and\nmore general machine learning algorithms. We give applications to SVM\nalgorithms graded into linear and higher order components, and give an example\nin biomedical informatics.\n",
        "published": "2012-12-19T03:06:13Z",
        "pdf_link": "http://arxiv.org/pdf/1212.4562v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.4569v2",
        "title": "Feature vector regularization in machine learning",
        "summary": "  Problems in machine learning (ML) can involve noisy input data, and ML\nclassification methods have reached limiting accuracies when based on standard\nML data sets consisting of feature vectors and their classes. Greater accuracy\nwill require incorporation of prior structural information on data into\nlearning. We study methods to regularize feature vectors (unsupervised\nregularization methods), analogous to supervised regularization for estimating\nfunctions in ML. We study regularization (denoising) of ML feature vectors\nusing Tikhonov and other regularization methods for functions on ${\\bf R}^n$. A\nfeature vector ${\\bf x}=(x_1,\\ldots,x_n)=\\{x_q\\}_{q=1}^n$ is viewed as a\nfunction of its index $q$, and smoothed using prior information on its\nstructure. This can involve a penalty functional on feature vectors analogous\nto those in statistical learning, or use of proximity (e.g. graph) structure on\nthe set of indices. Such feature vector regularization inherits a property from\nfunction denoising on ${\\bf R}^n$, in that accuracy is non-monotonic in the\ndenoising (regularization) parameter $\\alpha$. Under some assumptions about the\nnoise level and the data structure, we show that the best reconstruction\naccuracy also occurs at a finite positive $\\alpha$ in index spaces with graph\nstructures. We adapt two standard function denoising methods used on ${\\bf\nR}^n$, local averaging and kernel regression. In general the index space can be\nany discrete set with a notion of proximity, e.g. a metric space, a subset of\n${\\bf R}^n$, or a graph/network, with feature vectors as functions with some\nnotion of continuity. We show this improves feature vector recovery, and thus\nthe subsequent classification or regression done on them. We give an example in\ngene expression analysis for cancer classification with the genome as an index\nspace and network structure based protein-protein interactions.\n",
        "published": "2012-12-19T03:48:24Z",
        "pdf_link": "http://arxiv.org/pdf/1212.4569v2"
    },
    {
        "id": "http://arxiv.org/abs/1212.6936v1",
        "title": "Blind Analysis of EGM Signals: Sparsity-Aware Formulation",
        "summary": "  This technical note considers the problems of blind sparse learning and\ninference of electrogram (EGM) signals under atrial fibrillation (AF)\nconditions. First of all we introduce a mathematical model for the observed\nsignals that takes into account the multiple foci typically appearing inside\nthe heart during AF. Then we propose a reconstruction model based on a fixed\ndictionary and discuss several alternatives for choosing the dictionary. In\norder to obtain a sparse solution that takes into account the biological\nrestrictions of the problem, a first alternative is using LASSO regularization\nfollowed by a post-processing stage that removes low amplitude coefficients\nviolating the refractory period characteristic of cardiac cells. As an\nalternative we propose a novel regularization term, called cross products LASSO\n(CP-LASSO), that is able to incorporate the biological constraints directly\ninto the optimization problem. Unfortunately, the resulting problem is\nnon-convex, but we show how it can be solved efficiently in an approximated way\nmaking use of successive convex approximations (SCA). Finally, spectral\nanalysis is performed on the clean activation sequence obtained from the sparse\nlearning stage in order to estimate the number of latent foci and their\nfrequencies. Simulations on synthetic and real data are provided to validate\nthe proposed approach.\n",
        "published": "2012-12-31T17:56:08Z",
        "pdf_link": "http://arxiv.org/pdf/1212.6936v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.0858v1",
        "title": "A New Geometric Approach to Latent Topic Modeling and Discovery",
        "summary": "  A new geometrically-motivated algorithm for nonnegative matrix factorization\nis developed and applied to the discovery of latent \"topics\" for text and image\n\"document\" corpora. The algorithm is based on robustly finding and clustering\nextreme points of empirical cross-document word-frequencies that correspond to\nnovel \"words\" unique to each topic. In contrast to related approaches that are\nbased on solving non-convex optimization problems using suboptimal\napproximations, locally-optimal methods, or heuristics, the new algorithm is\nconvex, has polynomial complexity, and has competitive qualitative and\nquantitative performance compared to the current state-of-the-art approaches on\nsynthetic and real-world datasets.\n",
        "published": "2013-01-05T02:21:01Z",
        "pdf_link": "http://arxiv.org/pdf/1301.0858v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.1318v4",
        "title": "Efficient Eigen-updating for Spectral Graph Clustering",
        "summary": "  Partitioning a graph into groups of vertices such that those within each\ngroup are more densely connected than vertices assigned to different groups,\nknown as graph clustering, is often used to gain insight into the organisation\nof large scale networks and for visualisation purposes. Whereas a large number\nof dedicated techniques have been recently proposed for static graphs, the\ndesign of on-line graph clustering methods tailored for evolving networks is a\nchallenging problem, and much less documented in the literature. Motivated by\nthe broad variety of applications concerned, ranging from the study of\nbiological networks to the analysis of networks of scientific references\nthrough the exploration of communications networks such as the World Wide Web,\nit is the main purpose of this paper to introduce a novel, computationally\nefficient, approach to graph clustering in the evolutionary context. Namely,\nthe method promoted in this article can be viewed as an incremental eigenvalue\nsolution for the spectral clustering method described by Ng. et al. (2001). The\nincremental eigenvalue solution is a general technique for finding the\napproximate eigenvectors of a symmetric matrix given a change. As well as\noutlining the approach in detail, we present a theoretical bound on the quality\nof the approximate eigenvectors using perturbation theory. We then derive a\nnovel spectral clustering algorithm called Incremental Approximate Spectral\nClustering (IASC). The IASC algorithm is simple to implement and its efficacy\nis demonstrated on both synthetic and real datasets modelling the evolution of\na HIV epidemic, a citation network and the purchase history graph of an\ne-commerce website.\n",
        "published": "2013-01-07T19:52:14Z",
        "pdf_link": "http://arxiv.org/pdf/1301.1318v4"
    },
    {
        "id": "http://arxiv.org/abs/1301.1919v1",
        "title": "Nonparametric Reduced Rank Regression",
        "summary": "  We propose an approach to multivariate nonparametric regression that\ngeneralizes reduced rank regression for linear models. An additive model is\nestimated for each dimension of a $q$-dimensional response, with a shared\n$p$-dimensional predictor variable. To control the complexity of the model, we\nemploy a functional form of the Ky-Fan or nuclear norm, resulting in a set of\nfunction estimates that have low rank. Backfitting algorithms are derived and\njustified using a nonparametric form of the nuclear norm subdifferential.\nOracle inequalities on excess risk are derived that exhibit the scaling\nbehavior of the procedure in the high dimensional setting. The methods are\nillustrated on gene expression data.\n",
        "published": "2013-01-09T16:48:07Z",
        "pdf_link": "http://arxiv.org/pdf/1301.1919v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.1954v5",
        "title": "On the Incommensurability Phenomenon",
        "summary": "  Suppose that two large, multi-dimensional data sets are each noisy\nmeasurements of the same underlying random process, and principle components\nanalysis is performed separately on the data sets to reduce their\ndimensionality. In some circumstances it may happen that the two\nlower-dimensional data sets have an inordinately large Procrustean\nfitting-error between them. The purpose of this manuscript is to quantify this\n\"incommensurability phenomenon.\" In particular, under specified conditions, the\nsquare Procrustean fitting-error of the two normalized lower-dimensional data\nsets is (asymptotically) a convex combination (via a correlation parameter) of\nthe Hausdorff distance between the projection subspaces and the maximum\npossible value of the square Procrustean fitting-error for normalized data. We\nshow how this gives rise to the incommensurability phenomenon, and we employ\nillustrative simulations as well as a real data experiment to explore how the\nincommensurability phenomenon may have an appreciable impact.\n",
        "published": "2013-01-09T19:26:07Z",
        "pdf_link": "http://arxiv.org/pdf/1301.1954v5"
    },
    {
        "id": "http://arxiv.org/abs/1301.2007v1",
        "title": "Spectral Clustering Based on Local PCA",
        "summary": "  We propose a spectral clustering method based on local principal components\nanalysis (PCA). After performing local PCA in selected neighborhoods, the\nalgorithm builds a nearest neighbor graph weighted according to a discrepancy\nbetween the principal subspaces in the neighborhoods, and then applies spectral\nclustering. As opposed to standard spectral methods based solely on pairwise\ndistances between points, our algorithm is able to resolve intersections. We\nestablish theoretical guarantees for simpler variants within a prototypical\nmathematical framework for multi-manifold clustering, and evaluate our\nalgorithm on various simulated data sets.\n",
        "published": "2013-01-09T23:48:15Z",
        "pdf_link": "http://arxiv.org/pdf/1301.2007v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.2724v2",
        "title": "Perturbative Corrections for Approximate Inference in Gaussian Latent\n  Variable Models",
        "summary": "  Expectation Propagation (EP) provides a framework for approximate inference.\nWhen the model under consideration is over a latent Gaussian field, with the\napproximation being Gaussian, we show how these approximations can\nsystematically be corrected. A perturbative expansion is made of the exact but\nintractable correction, and can be applied to the model's partition function\nand other moments of interest. The correction is expressed over the\nhigher-order cumulants which are neglected by EP's local matching of moments.\nThrough the expansion, we see that EP is correct to first order. By considering\nhigher orders, corrections of increasing polynomial complexity can be applied\nto the approximation. The second order provides a correction in quadratic time,\nwhich we apply to an array of Gaussian process and Ising models. The\ncorrections generalize to arbitrarily complex approximating families, which we\nillustrate on tree-structured Ising model approximations. Furthermore, they\nprovide a polynomial-time assessment of the approximation error. We also\nprovide both theoretical and practical insights on the exactness of the EP\nsolution.\n",
        "published": "2013-01-12T22:16:36Z",
        "pdf_link": "http://arxiv.org/pdf/1301.2724v2"
    },
    {
        "id": "http://arxiv.org/abs/1301.3570v1",
        "title": "A Nested HDP for Hierarchical Topic Models",
        "summary": "  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical\ntopic modeling. The nHDP is a generalization of the nested Chinese restaurant\nprocess (nCRP) that allows each word to follow its own path to a topic node\naccording to a document-specific distribution on a shared tree. This alleviates\nthe rigid, single-path formulation of the nCRP, allowing a document to more\neasily express thematic borrowings as a random effect. We demonstrate our\nalgorithm on 1.8 million documents from The New York Times.\n",
        "published": "2013-01-16T03:24:43Z",
        "pdf_link": "http://arxiv.org/pdf/1301.3570v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.3611v4",
        "title": "Jitter-Adaptive Dictionary Learning - Application to Multi-Trial\n  Neuroelectric Signals",
        "summary": "  Dictionary Learning has proven to be a powerful tool for many image\nprocessing tasks, where atoms are typically defined on small image patches. As\na drawback, the dictionary only encodes basic structures. In addition, this\napproach treats patches of different locations in one single set, which means a\nloss of information when features are well-aligned across signals. This is the\ncase, for instance, in multi-trial magneto- or electroencephalography (M/EEG).\nLearning the dictionary on the entire signals could make use of the alignement\nand reveal higher-level features. In this case, however, small missalignements\nor phase variations of features would not be compensated for. In this paper, we\npropose an extension to the common dictionary learning framework to overcome\nthese limitations by allowing atoms to adapt their position across signals. The\nmethod is validated on simulated and real neuroelectric data.\n",
        "published": "2013-01-16T07:41:08Z",
        "pdf_link": "http://arxiv.org/pdf/1301.3611v4"
    },
    {
        "id": "http://arxiv.org/abs/1301.6915v2",
        "title": "An Impossibility Result for High Dimensional Supervised Learning",
        "summary": "  We study high-dimensional asymptotic performance limits of binary supervised\nclassification problems where the class conditional densities are Gaussian with\nunknown means and covariances and the number of signal dimensions scales faster\nthan the number of labeled training samples. We show that the Bayes error,\nnamely the minimum attainable error probability with complete distributional\nknowledge and equally likely classes, can be arbitrarily close to zero and yet\nthe limiting minimax error probability of every supervised learning algorithm\nis no better than a random coin toss. In contrast to related studies where the\nclassification difficulty (Bayes error) is made to vanish, we hold it constant\nwhen taking high-dimensional limits. In contrast to VC-dimension based minimax\nlower bounds that consider the worst case error probability over all\ndistributions that have a fixed Bayes error, our worst case is over the family\nof Gaussian distributions with constant Bayes error. We also show that a\nnontrivial asymptotic minimax error probability can only be attained for\nparametric subsets of zero measure (in a suitable measure space). These results\nexpose the fundamental importance of prior knowledge and suggest that unless we\nimpose strong structural constraints, such as sparsity, on the parametric\nspace, supervised learning may be ineffective in high dimensional small sample\nsettings.\n",
        "published": "2013-01-29T13:01:22Z",
        "pdf_link": "http://arxiv.org/pdf/1301.6915v2"
    },
    {
        "id": "http://arxiv.org/abs/1302.0256v1",
        "title": "Regression shrinkage and grouping of highly correlated predictors with\n  HORSES",
        "summary": "  Identifying homogeneous subgroups of variables can be challenging in high\ndimensional data analysis with highly correlated predictors. We propose a new\nmethod called Hexagonal Operator for Regression with Shrinkage and Equality\nSelection, HORSES for short, that simultaneously selects positively correlated\nvariables and identifies them as predictive clusters. This is achieved via a\nconstrained least-squares problem with regularization that consists of a linear\ncombination of an L_1 penalty for the coefficients and another L_1 penalty for\npairwise differences of the coefficients. This specification of the penalty\nfunction encourages grouping of positively correlated predictors combined with\na sparsity solution. We construct an efficient algorithm to implement the\nHORSES procedure. We show via simulation that the proposed method outperforms\nother variable selection methods in terms of prediction error and parsimony.\nThe technique is demonstrated on two data sets, a small data set from analysis\nof soil in Appalachia, and a high dimensional data set from a near infrared\n(NIR) spectroscopy study, showing the flexibility of the methodology.\n",
        "published": "2013-02-01T19:18:11Z",
        "pdf_link": "http://arxiv.org/pdf/1302.0256v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.2068v1",
        "title": "Efficiency for Regularization Parameter Selection in Penalized\n  Likelihood Estimation of Misspecified Models",
        "summary": "  It has been shown that AIC-type criteria are asymptotically efficient\nselectors of the tuning parameter in non-concave penalized regression methods\nunder the assumption that the population variance is known or that a consistent\nestimator is available. We relax this assumption to prove that AIC itself is\nasymptotically efficient and we study its performance in finite samples. In\nclassical regression, it is known that AIC tends to select overly complex\nmodels when the dimension of the maximum candidate model is large relative to\nthe sample size. Simulation studies suggest that AIC suffers from the same\nshortcomings when used in penalized regression. We therefore propose the use of\nthe classical corrected AIC (AICc) as an alternative and prove that it\nmaintains the desired asymptotic properties. To broaden our results, we further\nprove the efficiency of AIC for penalized likelihood methods in the context of\ngeneralized linear models with no dispersion parameter. Similar results exist\nin the literature but only for a restricted set of candidate models. By\nemploying results from the classical literature on maximum-likelihood\nestimation in misspecified models, we are able to establish this result for a\ngeneral set of candidate models. We use simulations to assess the performance\nof AIC and AICc, as well as that of other selectors, in finite samples for both\nSCAD-penalized and Lasso regressions and a real data example is considered.\n",
        "published": "2013-02-08T16:02:53Z",
        "pdf_link": "http://arxiv.org/pdf/1302.2068v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.2969v1",
        "title": "Towards Identification of Relevant Variables in the observed Aerosol\n  Optical Depth Bias between MODIS and AERONET observations",
        "summary": "  Measurements made by satellite remote sensing, Moderate Resolution Imaging\nSpectroradiometer (MODIS), and globally distributed Aerosol Robotic Network\n(AERONET) are compared. Comparison of the two datasets measurements for aerosol\noptical depth values show that there are biases between the two data products.\nIn this paper, we present a general framework towards identifying relevant set\nof variables responsible for the observed bias. We present a general framework\nto identify the possible factors influencing the bias, which might be\nassociated with the measurement conditions such as the solar and sensor zenith\nangles, the solar and sensor azimuth, scattering angles, and surface\nreflectivity at the various measured wavelengths, etc. Specifically, we\nperformed analysis for remote sensing Aqua-Land data set, and used machine\nlearning technique, neural network in this case, to perform multivariate\nregression between the ground-truth and the training data sets. Finally, we\nused mutual information between the observed and the predicted values as the\nmeasure of similarity to identify the most relevant set of variables. The\nsearch is brute force method as we have to consider all possible combinations.\nThe computations involves a huge number crunching exercise, and we implemented\nit by writing a job-parallel program.\n",
        "published": "2013-02-13T02:11:37Z",
        "pdf_link": "http://arxiv.org/pdf/1302.2969v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.3913v2",
        "title": "Multiclass Data Segmentation using Diffuse Interface Methods on Graphs",
        "summary": "  We present two graph-based algorithms for multiclass segmentation of\nhigh-dimensional data. The algorithms use a diffuse interface model based on\nthe Ginzburg-Landau functional, related to total variation compressed sensing\nand image processing. A multiclass extension is introduced using the Gibbs\nsimplex, with the functional's double-well potential modified to handle the\nmulticlass case. The first algorithm minimizes the functional using a convex\nsplitting numerical scheme. The second algorithm is a uses a graph adaptation\nof the classical numerical Merriman-Bence-Osher (MBO) scheme, which alternates\nbetween diffusion and thresholding. We demonstrate the performance of both\nalgorithms experimentally on synthetic data, grayscale and color images, and\nseveral benchmark data sets such as MNIST, COIL and WebKB. We also make use of\nfast numerical solvers for finding the eigenvectors and eigenvalues of the\ngraph Laplacian, and take advantage of the sparsity of the matrix. Experiments\nindicate that the results are competitive with or better than the current\nstate-of-the-art multiclass segmentation algorithms.\n",
        "published": "2013-02-15T23:49:21Z",
        "pdf_link": "http://arxiv.org/pdf/1302.3913v2"
    },
    {
        "id": "http://arxiv.org/abs/1302.4853v2",
        "title": "Consistency of Online Random Forests",
        "summary": "  As a testament to their success, the theory of random forests has long been\noutpaced by their application in practice. In this paper, we take a step\ntowards narrowing this gap by providing a consistency result for online random\nforests.\n",
        "published": "2013-02-20T09:48:49Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4853v2"
    },
    {
        "id": "http://arxiv.org/abs/1302.5134v1",
        "title": "Spectral Clustering with Unbalanced Data",
        "summary": "  Spectral clustering (SC) and graph-based semi-supervised learning (SSL)\nalgorithms are sensitive to how graphs are constructed from data. In particular\nif the data has proximal and unbalanced clusters these algorithms can lead to\npoor performance on well-known graphs such as $k$-NN, full-RBF,\n$\\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) or\nnormalized cut (NCut) attempt to tradeoff cut values with cluster sizes, which\nare not tailored to unbalanced data. We propose a novel graph partitioning\nframework, which parameterizes a family of graphs by adaptively modulating node\ndegrees in a $k$-NN graph. We then propose a model selection scheme to choose\nsizable clusters which are separated by smallest cut values. Our framework is\nable to adapt to varying levels of unbalancedness of data and can be naturally\nused for small cluster detection. We theoretically justify our ideas through\nlimit cut analysis. Unsupervised and semi-supervised experiments on synthetic\nand real data sets demonstrate the superiority of our method.\n",
        "published": "2013-02-20T21:54:04Z",
        "pdf_link": "http://arxiv.org/pdf/1302.5134v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.6766v3",
        "title": "A bag-of-paths framework for network data analysis",
        "summary": "  This work develops a generic framework, called the bag-of-paths (BoP), for\nlink and network data analysis. The central idea is to assign a probability\ndistribution on the set of all paths in a network. More precisely, a\nGibbs-Boltzmann distribution is defined over a bag of paths in a network, that\nis, on a representation that considers all paths independently. We show that,\nunder this distribution, the probability of drawing a path connecting two nodes\ncan easily be computed in closed form by simple matrix inversion. This\nprobability captures a notion of relatedness between nodes of the graph: two\nnodes are considered as highly related when they are connected by many,\npreferably low-cost, paths. As an application, two families of distances\nbetween nodes are derived from the BoP probabilities. Interestingly, the second\ndistance family interpolates between the shortest path distance and the\nresistance distance. In addition, it extends the Bellman-Ford formula for\ncomputing the shortest path distance in order to integrate sub-optimal paths by\nsimply replacing the minimum operator by the soft minimum operator.\nExperimental results on semi-supervised classification show that both of the\nnew distance families are competitive with other state-of-the-art approaches.\nIn addition to the distance measures studied in this paper, the bag-of-paths\nframework enables straightforward computation of many other relevant network\nmeasures.\n",
        "published": "2013-02-27T13:41:44Z",
        "pdf_link": "http://arxiv.org/pdf/1302.6766v3"
    },
    {
        "id": "http://arxiv.org/abs/1302.7220v2",
        "title": "A New Monte Carlo Based Algorithm for the Gaussian Process\n  Classification Problem",
        "summary": "  Gaussian process is a very promising novel technology that has been applied\nto both the regression problem and the classification problem. While for the\nregression problem it yields simple exact solutions, this is not the case for\nthe classification problem, because we encounter intractable integrals. In this\npaper we develop a new derivation that transforms the problem into that of\nevaluating the ratio of multivariate Gaussian orthant integrals. Moreover, we\ndevelop a new Monte Carlo procedure that evaluates these integrals. It is based\non some aspects of bootstrap sampling and acceptancerejection. The proposed\napproach has beneficial properties compared to the existing Markov Chain Monte\nCarlo approach, such as simplicity, reliability, and speed.\n",
        "published": "2013-02-28T15:02:34Z",
        "pdf_link": "http://arxiv.org/pdf/1302.7220v2"
    },
    {
        "id": "http://arxiv.org/abs/1303.2378v2",
        "title": "Predictive Correlation Screening: Application to Two-stage Predictor\n  Design in High Dimension",
        "summary": "  We introduce a new approach to variable selection, called Predictive\nCorrelation Screening, for predictor design. Predictive Correlation Screening\n(PCS) implements false positive control on the selected variables, is well\nsuited to small sample sizes, and is scalable to high dimensions. We establish\nasymptotic bounds for Familywise Error Rate (FWER), and resultant mean square\nerror of a linear predictor on the selected variables. We apply Predictive\nCorrelation Screening to the following two-stage predictor design problem. An\nexperimenter wants to learn a multivariate predictor of gene expressions based\non successive biological samples assayed on mRNA arrays. She assays the whole\ngenome on a few samples and from these assays she selects a small number of\nvariables using Predictive Correlation Screening. To reduce assay cost, she\nsubsequently assays only the selected variables on the remaining samples, to\nlearn the predictor coefficients. We show superiority of Predictive Correlation\nScreening relative to LASSO and correlation learning (sometimes popularly\nreferred to in the literature as marginal regression or simple thresholding) in\nterms of performance and computational complexity.\n",
        "published": "2013-03-10T21:31:37Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2378v2"
    },
    {
        "id": "http://arxiv.org/abs/1303.2488v1",
        "title": "Visualizing and Interacting with Concept Hierarchies",
        "summary": "  Concept Hierarchies and Formal Concept Analysis are theoretically well\ngrounded and largely experimented methods. They rely on line diagrams called\nGalois lattices for visualizing and analysing object-attribute sets. Galois\nlattices are visually seducing and conceptually rich for experts. However they\npresent important drawbacks due to their concept oriented overall structure:\nanalysing what they show is difficult for non experts, navigation is\ncumbersome, interaction is poor, and scalability is a deep bottleneck for\nvisual interpretation even for experts. In this paper we introduce semantic\nprobes as a means to overcome many of these problems and extend usability and\napplication possibilities of traditional FCA visualization methods. Semantic\nprobes are visual user centred objects which extract and organize reduced\nGalois sub-hierarchies. They are simpler, clearer, and they provide a better\nnavigation support through a rich set of interaction possibilities. Since probe\ndriven sub-hierarchies are limited to users focus, scalability is under control\nand interpretation is facilitated. After some successful experiments, several\napplications are being developed with the remaining problem of finding a\ncompromise between simplicity and conceptual expressivity.\n",
        "published": "2013-03-11T11:20:23Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2488v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.2517v1",
        "title": "Refinement revisited with connections to Bayes error, conditional\n  entropy and calibrated classifiers",
        "summary": "  The concept of refinement from probability elicitation is considered for\nproper scoring rules. Taking directions from the axioms of probability,\nrefinement is further clarified using a Hilbert space interpretation and\nreformulated into the underlying data distribution setting where connections to\nmaximal marginal diversity and conditional entropy are considered and used to\nderive measures that provide arbitrarily tight bounds on the Bayes error.\nRefinement is also reformulated into the classifier output setting and its\nconnections to calibrated classifiers and proper margin losses are established.\n",
        "published": "2013-03-11T13:34:51Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2517v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.2892v1",
        "title": "Toward Optimal Stratification for Stratified Monte-Carlo Integration",
        "summary": "  We consider the problem of adaptive stratified sampling for Monte Carlo\nintegration of a noisy function, given a finite budget n of noisy evaluations\nto the function. We tackle in this paper the problem of adapting to the\nfunction at the same time the number of samples into each stratum and the\npartition itself. More precisely, it is interesting to refine the partition of\nthe domain in area where the noise to the function, or where the variations of\nthe function, are very heterogeneous. On the other hand, having a (too) refined\nstratification is not optimal. Indeed, the more refined the stratification, the\nmore difficult it is to adjust the allocation of the samples to the\nstratification, i.e. sample more points where the noise or variations of the\nfunction are larger. We provide in this paper an algorithm that selects online,\namong a large class of partitions, the partition that provides the optimal\ntrade-off, and allocates the samples almost optimally on this partition.\n",
        "published": "2013-03-12T14:20:27Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2892v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.3265v2",
        "title": "A dependent partition-valued process for multitask clustering and time\n  evolving network modelling",
        "summary": "  The fundamental aim of clustering algorithms is to partition data points. We\nconsider tasks where the discovered partition is allowed to vary with some\ncovariate such as space or time. One approach would be to use\nfragmentation-coagulation processes, but these, being Markov processes, are\nrestricted to linear or tree structured covariate spaces. We define a\npartition-valued process on an arbitrary covariate space using Gaussian\nprocesses. We use the process to construct a multitask clustering model which\npartitions datapoints in a similar way across multiple data sources, and a time\nseries model of network data which allows cluster assignments to vary over\ntime. We describe sampling algorithms for inference and apply our method to\ndefining cancer subtypes based on different types of cellular characteristics,\nfinding regulatory modules from gene expression data from multiple human\npopulations, and discovering time varying community structure in a social\nnetwork.\n",
        "published": "2013-03-13T13:55:20Z",
        "pdf_link": "http://arxiv.org/pdf/1303.3265v2"
    },
    {
        "id": "http://arxiv.org/abs/1303.6938v1",
        "title": "Expectation Propagation for Neural Networks with Sparsity-promoting\n  Priors",
        "summary": "  We propose a novel approach for nonlinear regression using a two-layer neural\nnetwork (NN) model structure with sparsity-favoring hierarchical priors on the\nnetwork weights. We present an expectation propagation (EP) approach for\napproximate integration over the posterior distribution of the weights, the\nhierarchical scale parameters of the priors, and the residual scale. Using a\nfactorized posterior approximation we derive a computationally efficient\nalgorithm, whose complexity scales similarly to an ensemble of independent\nsparse linear models. The approach enables flexible definition of weight priors\nwith different sparseness properties such as independent Laplace priors with a\ncommon scale parameter or Gaussian automatic relevance determination (ARD)\npriors with different relevance parameters for all inputs. The approach can be\nextended beyond standard activation functions and NN model structures to form\nflexible nonlinear predictors from multiple sparse linear models. The effects\nof the hierarchical priors and the predictive performance of the algorithm are\nassessed using both simulated and real-world data. Comparisons are made to two\nalternative models with ARD priors: a Gaussian process with a NN covariance\nfunction and marginal maximum a posteriori estimates of the relevance\nparameters, and a NN with Markov chain Monte Carlo integration over all the\nunknown model parameters.\n",
        "published": "2013-03-27T19:40:26Z",
        "pdf_link": "http://arxiv.org/pdf/1303.6938v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.7410v2",
        "title": "ParceLiNGAM: A causal ordering method robust against latent confounders",
        "summary": "  We consider learning a causal ordering of variables in a linear non-Gaussian\nacyclic model called LiNGAM. Several existing methods have been shown to\nconsistently estimate a causal ordering assuming that all the model assumptions\nare correct. But, the estimation results could be distorted if some assumptions\nactually are violated. In this paper, we propose a new algorithm for learning\ncausal orders that is robust against one typical violation of the model\nassumptions: latent confounders. The key idea is to detect latent confounders\nby testing independence between estimated external influences and find subsets\n(parcels) that include variables that are not affected by latent confounders.\nWe demonstrate the effectiveness of our method using artificial data and\nsimulated brain imaging data.\n",
        "published": "2013-03-29T14:40:24Z",
        "pdf_link": "http://arxiv.org/pdf/1303.7410v2"
    },
    {
        "id": "http://arxiv.org/abs/1304.4549v1",
        "title": "Learning Heteroscedastic Models by Convex Programming under Group\n  Sparsity",
        "summary": "  Popular sparse estimation methods based on $\\ell_1$-relaxation, such as the\nLasso and the Dantzig selector, require the knowledge of the variance of the\nnoise in order to properly tune the regularization parameter. This constitutes\na major obstacle in applying these methods in several frameworks---such as time\nseries, random fields, inverse problems---for which the noise is rarely\nhomoscedastic and its level is hard to know in advance. In this paper, we\npropose a new approach to the joint estimation of the conditional mean and the\nconditional variance in a high-dimensional (auto-) regression setting. An\nattractive feature of the proposed estimator is that it is efficiently\ncomputable even for very large scale problems by solving a second-order cone\nprogram (SOCP). We present theoretical analysis and numerical results assessing\nthe performance of the proposed procedure.\n",
        "published": "2013-04-16T18:54:37Z",
        "pdf_link": "http://arxiv.org/pdf/1304.4549v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.4672v3",
        "title": "Low-Rank Matrix and Tensor Completion via Adaptive Sampling",
        "summary": "  We study low rank matrix and tensor completion and propose novel algorithms\nthat employ adaptive sampling schemes to obtain strong performance guarantees.\nOur algorithms exploit adaptivity to identify entries that are highly\ninformative for learning the column space of the matrix (tensor) and\nconsequently, our results hold even when the row space is highly coherent, in\ncontrast with previous analyses. In the absence of noise, we show that one can\nexactly recover a $n \\times n$ matrix of rank $r$ from merely $\\Omega(n\nr^{3/2}\\log(r))$ matrix entries. We also show that one can recover an order $T$\ntensor using $\\Omega(n r^{T-1/2}T^2 \\log(r))$ entries. For noisy recovery, our\nalgorithm consistently estimates a low rank matrix corrupted with noise using\n$\\Omega(n r^{3/2} \\textrm{polylog}(n))$ entries. We complement our study with\nsimulations that verify our theory and demonstrate the scalability of our\nalgorithms.\n",
        "published": "2013-04-17T03:05:30Z",
        "pdf_link": "http://arxiv.org/pdf/1304.4672v3"
    },
    {
        "id": "http://arxiv.org/abs/1304.5245v2",
        "title": "Feature Elimination in Kernel Machines in moderately high dimensions",
        "summary": "  We develop an approach for feature elimination in statistical learning with\nkernel machines, based on recursive elimination of features.We present\ntheoretical properties of this method and show that it is uniformly consistent\nin finding the correct feature space under certain generalized assumptions.We\npresent four case studies to show that the assumptions are met in most\npractical situations and present simulation results to demonstrate performance\nof the proposed approach.\n",
        "published": "2013-04-18T20:25:15Z",
        "pdf_link": "http://arxiv.org/pdf/1304.5245v2"
    },
    {
        "id": "http://arxiv.org/abs/1304.5417v1",
        "title": "Analytic Expressions for Stochastic Distances Between Relaxed Complex\n  Wishart Distributions",
        "summary": "  The scaled complex Wishart distribution is a widely used model for multilook\nfull polarimetric SAR data whose adequacy has been attested in the literature.\nClassification, segmentation, and image analysis techniques which depend on\nthis model have been devised, and many of them employ some type of\ndissimilarity measure. In this paper we derive analytic expressions for four\nstochastic distances between relaxed scaled complex Wishart distributions in\ntheir most general form and in important particular cases. Using these\ndistances, inequalities are obtained which lead to new ways of deriving the\nBartlett and revised Wishart distances. The expressiveness of the four analytic\ndistances is assessed with respect to the variation of parameters. Such\ndistances are then used for deriving new tests statistics, which are proved to\nhave asymptotic chi-square distribution. Adopting the test size as a comparison\ncriterion, a sensitivity study is performed by means of Monte Carlo experiments\nsuggesting that the Bhattacharyya statistic outperforms all the others. The\npower of the tests is also assessed. Applications to actual data illustrate the\ndiscrimination and homogeneity identification capabilities of these distances.\n",
        "published": "2013-04-19T13:38:59Z",
        "pdf_link": "http://arxiv.org/pdf/1304.5417v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.6803v5",
        "title": "Direct Learning of Sparse Changes in Markov Networks by Density Ratio\n  Estimation",
        "summary": "  We propose a new method for detecting changes in Markov network structure\nbetween two sets of samples. Instead of naively fitting two Markov network\nmodels separately to the two data sets and figuring out their difference, we\n\\emph{directly} learn the network structure change by estimating the ratio of\nMarkov network models. This density-ratio formulation naturally allows us to\nintroduce sparsity in the network structure change, which highly contributes to\nenhancing interpretability. Furthermore, computation of the normalization term,\nwhich is a critical bottleneck of the naive approach, can be remarkably\nmitigated. We also give the dual formulation of the optimization problem, which\nfurther reduces the computation cost for large-scale Markov networks. Through\nexperiments, we demonstrate the usefulness of our method.\n",
        "published": "2013-04-25T05:37:55Z",
        "pdf_link": "http://arxiv.org/pdf/1304.6803v5"
    },
    {
        "id": "http://arxiv.org/abs/1304.7717v2",
        "title": "The Randomized Dependence Coefficient",
        "summary": "  We introduce the Randomized Dependence Coefficient (RDC), a measure of\nnon-linear dependence between random variables of arbitrary dimension based on\nthe Hirschfeld-Gebelein-R\\'enyi Maximum Correlation Coefficient. RDC is defined\nin terms of correlation of random non-linear copula projections; it is\ninvariant with respect to marginal distribution transformations, has low\ncomputational cost and is easy to implement: just five lines of R code,\nincluded at the end of the paper.\n",
        "published": "2013-04-29T17:27:50Z",
        "pdf_link": "http://arxiv.org/pdf/1304.7717v2"
    },
    {
        "id": "http://arxiv.org/abs/1304.7981v5",
        "title": "Generalized Canonical Correlation Analysis for Classification",
        "summary": "  For multiple multivariate data sets, we derive conditions under which\nGeneralized Canonical Correlation Analysis (GCCA) improves classification\nperformance of the projected datasets, compared to standard Canonical\nCorrelation Analysis (CCA) using only two data sets. We illustrate our\ntheoretical results with simulations and a real data experiment.\n",
        "published": "2013-04-30T13:06:35Z",
        "pdf_link": "http://arxiv.org/pdf/1304.7981v5"
    },
    {
        "id": "http://arxiv.org/abs/1305.0047v2",
        "title": "Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation",
        "summary": "  We consider the following signal recovery problem: given a measurement matrix\n$\\Phi\\in \\mathbb{R}^{n\\times p}$ and a noisy observation vector $c\\in\n\\mathbb{R}^{n}$ constructed from $c = \\Phi\\theta^* + \\epsilon$ where\n$\\epsilon\\in \\mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d.\ncentered sub-Gaussian distribution, how to recover the signal $\\theta^*$ if\n$D\\theta^*$ is sparse {\\rca under a linear transformation}\n$D\\in\\mathbb{R}^{m\\times p}$? One natural method using convex optimization is\nto solve the following problem: $$\\min_{\\theta} {1\\over 2}\\|\\Phi\\theta - c\\|^2\n+ \\lambda\\|D\\theta\\|_1.$$ This paper provides an upper bound of the estimate\nerror and shows the consistency property of this method by assuming that the\ndesign matrix $\\Phi$ is a Gaussian random matrix. Specifically, we show 1) in\nthe noiseless case, if the condition number of $D$ is bounded and the\nmeasurement number $n\\geq \\Omega(s\\log(p))$ where $s$ is the sparsity number,\nthen the true solution can be recovered with high probability; and 2) in the\nnoisy case, if the condition number of $D$ is bounded and the measurement\nincreases faster than $s\\log(p)$, that is, $s\\log(p)=o(n)$, the estimate error\nconverges to zero with probability 1 when $p$ and $s$ go to infinity. Our\nresults are consistent with those for the special case $D=\\bold{I}_{p\\times p}$\n(equivalently LASSO) and improve the existing analysis. The condition number of\n$D$ plays a critical role in our analysis. We consider the condition numbers in\ntwo cases including the fused LASSO and the random graph: the condition number\nin the fused LASSO case is bounded by a constant, while the condition number in\nthe random graph case is bounded with high probability if $m\\over p$ (i.e.,\n$#text{edge}\\over #text{vertex}$) is larger than a certain constant. Numerical\nsimulations are consistent with our theoretical results.\n",
        "published": "2013-04-30T22:48:54Z",
        "pdf_link": "http://arxiv.org/pdf/1305.0047v2"
    },
    {
        "id": "http://arxiv.org/abs/1305.0319v6",
        "title": "Learning Mixtures of Bernoulli Templates by Two-Round EM with\n  Performance Guarantee",
        "summary": "  Dasgupta and Shulman showed that a two-round variant of the EM algorithm can\nlearn mixture of Gaussian distributions with near optimal precision with high\nprobability if the Gaussian distributions are well separated and if the\ndimension is sufficiently high. In this paper, we generalize their theory to\nlearning mixture of high-dimensional Bernoulli templates. Each template is a\nbinary vector, and a template generates examples by randomly switching its\nbinary components independently with a certain probability. In computer vision\napplications, a binary vector is a feature map of an image, where each binary\ncomponent indicates whether a local feature or structure is present or absent\nwithin a certain cell of the image domain. A Bernoulli template can be\nconsidered as a statistical model for images of objects (or parts of objects)\nfrom the same category. We show that the two-round EM algorithm can learn\nmixture of Bernoulli templates with near optimal precision with high\nprobability, if the Bernoulli templates are sufficiently different and if the\nnumber of features is sufficiently high. We illustrate the theoretical results\nby synthetic and real examples.\n",
        "published": "2013-05-02T00:31:45Z",
        "pdf_link": "http://arxiv.org/pdf/1305.0319v6"
    },
    {
        "id": "http://arxiv.org/abs/1305.1998v1",
        "title": "Inferring Team Strengths Using a Discrete Markov Random Field",
        "summary": "  We propose an original model for inferring team strengths using a Markov\nRandom Field, which can be used to generate historical estimates of the\noffensive and defensive strengths of a team over time. This model was designed\nto be applied to sports such as soccer or hockey, in which contest outcomes\ntake value in a limited discrete space. We perform inference using a\ncombination of Expectation Maximization and Loopy Belief Propagation. The\nchallenges of working with a non-convex optimization problem and a\nhigh-dimensional parameter space are discussed. The performance of the model is\ndemonstrated on professional soccer data from the English Premier League.\n",
        "published": "2013-05-09T03:15:19Z",
        "pdf_link": "http://arxiv.org/pdf/1305.1998v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.4152v5",
        "title": "Sparse Approximate Inference for Spatio-Temporal Point Process Models",
        "summary": "  Spatio-temporal point process models play a central role in the analysis of\nspatially distributed systems in several disciplines. Yet, scalable inference\nremains computa- tionally challenging both due to the high resolution modelling\ngenerally required and the analytically intractable likelihood function. Here,\nwe exploit the sparsity structure typical of (spatially) discretised\nlog-Gaussian Cox process models by using approximate message-passing\nalgorithms. The proposed algorithms scale well with the state dimension and the\nlength of the temporal horizon with moderate loss in distributional accuracy.\nThey hence provide a flexible and faster alternative to both non-linear\nfiltering-smoothing type algorithms and to approaches that implement the\nLaplace method or expectation propagation on (block) sparse latent Gaussian\nmodels. We infer the parameters of the latent Gaussian model using a structured\nvariational Bayes approach. We demonstrate the proposed framework on simulation\nstudies with both Gaussian and point-process observations and use it to\nreconstruct the conflict intensity and dynamics in Afghanistan from the\nWikiLeaks Afghan War Diary.\n",
        "published": "2013-05-17T18:40:53Z",
        "pdf_link": "http://arxiv.org/pdf/1305.4152v5"
    },
    {
        "id": "http://arxiv.org/abs/1305.4153v1",
        "title": "Factored expectation propagation for input-output FHMM models in systems\n  biology",
        "summary": "  We consider the problem of joint modelling of metabolic signals and gene\nexpression in systems biology applications. We propose an approach based on\ninput-output factorial hidden Markov models and propose a structured\nvariational inference approach to infer the structure and states of the model.\nWe start from the classical free form structured variational mean field\napproach and use a expectation propagation to approximate the expectations\nneeded in the variational loop. We show that this corresponds to a factored\nexpectation constrained approximate inference. We validate our model through\nextensive simulations and demonstrate its applicability on a real world\nbacterial data set.\n",
        "published": "2013-05-17T18:44:50Z",
        "pdf_link": "http://arxiv.org/pdf/1305.4153v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.4893v1",
        "title": "Out-of-sample Extension for Latent Position Graphs",
        "summary": "  We consider the problem of vertex classification for graphs constructed from\nthe latent position model. It was shown previously that the approach of\nembedding the graphs into some Euclidean space followed by classification in\nthat space can yields a universally consistent vertex classifier. However, a\nmajor technical difficulty of the approach arises when classifying unlabeled\nout-of-sample vertices without including them in the embedding stage. In this\npaper, we studied the out-of-sample extension for the graph embedding step and\nits impact on the subsequent inference tasks. We show that, under the latent\nposition graph model and for sufficiently large $n$, the mapping of the\nout-of-sample vertices is close to its true latent position. We then\ndemonstrate that successful inference for the out-of-sample vertices is\npossible.\n",
        "published": "2013-05-21T17:24:57Z",
        "pdf_link": "http://arxiv.org/pdf/1305.4893v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.6526v3",
        "title": "Adaptive estimation of the copula correlation matrix for semiparametric\n  elliptical copulas",
        "summary": "  We study the adaptive estimation of copula correlation matrix $\\Sigma$ for\nthe semi-parametric elliptical copula model. In this context, the correlations\nare connected to Kendall's tau through a sine function transformation. Hence, a\nnatural estimate for $\\Sigma$ is the plug-in estimator $\\hat{\\Sigma}$ with\nKendall's tau statistic. We first obtain a sharp bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$. Then we study a factor model of $\\Sigma$, for which we\npropose a refined estimator $\\widetilde{\\Sigma}$ by fitting a low-rank matrix\nplus a diagonal matrix to $\\hat{\\Sigma}$ using least squares with a nuclear\nnorm penalty on the low-rank matrix. The bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$ serves to scale the penalty term, and we obtain finite\nsample oracle inequalities for $\\widetilde{\\Sigma}$. We also consider an\nelementary factor copula model of $\\Sigma$, for which we propose closed-form\nestimators. All of our estimation procedures are entirely data-driven.\n",
        "published": "2013-05-28T15:14:58Z",
        "pdf_link": "http://arxiv.org/pdf/1305.6526v3"
    },
    {
        "id": "http://arxiv.org/abs/1305.6916v4",
        "title": "Statistical analysis of latent generalized correlation matrix estimation\n  in transelliptical distribution",
        "summary": "  Correlation matrices play a key role in many multivariate methods (e.g.,\ngraphical model estimation and factor analysis). The current state-of-the-art\nin estimating large correlation matrices focuses on the use of Pearson's sample\ncorrelation matrix. Although Pearson's sample correlation matrix enjoys various\ngood properties under Gaussian models, it is not an effective estimator when\nfacing heavy-tailed distributions. As a robust alternative, Han and Liu [J. Am.\nStat. Assoc. 109 (2015) 275-287] advocated the use of a transformed version of\nthe Kendall's tau sample correlation matrix in estimating high dimensional\nlatent generalized correlation matrix under the transelliptical distribution\nfamily (or elliptical copula). The transelliptical family assumes that after\nunspecified marginal monotone transformations, the data follow an elliptical\ndistribution. In this paper, we study the theoretical properties of the\nKendall's tau sample correlation matrix and its transformed version proposed in\nHan and Liu [J. Am. Stat. Assoc. 109 (2015) 275-287] for estimating the\npopulation Kendall's tau correlation matrix and the latent Pearson's\ncorrelation matrix under both spectral and restricted spectral norms. With\nregard to the spectral norm, we highlight the role of \"effective rank\" in\nquantifying the rate of convergence. With regard to the restricted spectral\nnorm, we for the first time present a \"sign sub-Gaussian condition\" which is\nsufficient to guarantee that the rank-based correlation matrix estimator\nattains the fast rate of convergence. In both cases, we do not need any moment\ncondition.\n",
        "published": "2013-05-29T19:39:56Z",
        "pdf_link": "http://arxiv.org/pdf/1305.6916v4"
    },
    {
        "id": "http://arxiv.org/abs/1305.7255v1",
        "title": "Non-linear dimensionality reduction: Riemannian metric estimation and\n  the problem of geometric discovery",
        "summary": "  In recent years, manifold learning has become increasingly popular as a tool\nfor performing non-linear dimensionality reduction. This has led to the\ndevelopment of numerous algorithms of varying degrees of complexity that aim to\nrecover man ifold geometry using either local or global features of the data.\n  Building on the Laplacian Eigenmap and Diffusionmaps framework, we propose a\nnew paradigm that offers a guarantee, under reasonable assumptions, that any\nmanifo ld learning algorithm will preserve the geometry of a data set. Our\napproach is based on augmenting the output of embedding algorithms with\ngeometric informatio n embodied in the Riemannian metric of the manifold. We\nprovide an algorithm for estimating the Riemannian metric from data and\ndemonstrate possible application s of our approach in a variety of examples.\n",
        "published": "2013-05-30T21:16:04Z",
        "pdf_link": "http://arxiv.org/pdf/1305.7255v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.7344v1",
        "title": "Joint Modeling and Registration of Cell Populations in Cohorts of\n  High-Dimensional Flow Cytometric Data",
        "summary": "  In systems biomedicine, an experimenter encounters different potential\nsources of variation in data such as individual samples, multiple experimental\nconditions, and multi-variable network-level responses. In multiparametric\ncytometry, which is often used for analyzing patient samples, such issues are\ncritical. While computational methods can identify cell populations in\nindividual samples, without the ability to automatically match them across\nsamples, it is difficult to compare and characterize the populations in typical\nexperiments, such as those responding to various stimulations or distinctive of\nparticular patients or time-points, especially when there are many samples.\nJoint Clustering and Matching (JCM) is a multi-level framework for simultaneous\nmodeling and registration of populations across a cohort. JCM models every\npopulation with a robust multivariate probability distribution. Simultaneously,\nJCM fits a random-effects model to construct an overall batch template -- used\nfor registering populations across samples, and classifying new samples. By\ntackling systems-level variation, JCM supports practical biomedical\napplications involving large cohorts.\n",
        "published": "2013-05-31T09:53:53Z",
        "pdf_link": "http://arxiv.org/pdf/1305.7344v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.0407v2",
        "title": "Constructive Setting of the Density Ratio Estimation Problem and its\n  Rigorous Solution",
        "summary": "  We introduce a general constructive setting of the density ratio estimation\nproblem as a solution of a (multidimensional) integral equation. In this\nequation, not only its right hand side is known approximately, but also the\nintegral operator is defined approximately. We show that this ill-posed problem\nhas a rigorous solution and obtain the solution in a closed form. The key\nelement of this solution is the novel V-matrix, which captures the geometry of\nthe observed samples. We compare our method with three well-known previously\nproposed ones. Our experimental results demonstrate the good potential of the\nnew approach.\n",
        "published": "2013-06-03T13:54:34Z",
        "pdf_link": "http://arxiv.org/pdf/1306.0407v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.0895v1",
        "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation\n  Distances",
        "summary": "  Optimal transportation distances are a fundamental family of parameterized\ndistances for histograms. Despite their appealing theoretical properties,\nexcellent performance in retrieval tasks and intuitive formulation, their\ncomputation involves the resolution of a linear program whose cost is\nprohibitive whenever the histograms' dimension exceeds a few hundreds. We\npropose in this work a new family of optimal transportation distances that look\nat transportation problems from a maximum-entropy perspective. We smooth the\nclassical optimal transportation problem with an entropic regularization term,\nand show that the resulting optimum is also a distance which can be computed\nthrough Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several\norders of magnitude faster than that of transportation solvers. We also report\nimproved performance over classical optimal transportation distances on the\nMNIST benchmark problem.\n",
        "published": "2013-06-04T14:45:10Z",
        "pdf_link": "http://arxiv.org/pdf/1306.0895v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.1043v2",
        "title": "Structural Intervention Distance (SID) for Evaluating Causal Graphs",
        "summary": "  Causal inference relies on the structure of a graph, often a directed acyclic\ngraph (DAG). Different graphs may result in different causal inference\nstatements and different intervention distributions. To quantify such\ndifferences, we propose a (pre-) distance between DAGs, the structural\nintervention distance (SID). The SID is based on a graphical criterion only and\nquantifies the closeness between two DAGs in terms of their corresponding\ncausal inference statements. It is therefore well-suited for evaluating graphs\nthat are used for computing interventions. Instead of DAGs it is also possible\nto compare CPDAGs, completed partially directed acyclic graphs that represent\nMarkov equivalence classes. Since it differs significantly from the popular\nStructural Hamming Distance (SHD), the SID constitutes a valuable additional\nmeasure. We discuss properties of this distance and provide an efficient\nimplementation with software code available on the first author's homepage (an\nR package is under construction).\n",
        "published": "2013-06-05T10:15:46Z",
        "pdf_link": "http://arxiv.org/pdf/1306.1043v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.3530v2",
        "title": "Generalized Beta Divergence",
        "summary": "  This paper generalizes beta divergence beyond its classical form associated\nwith power variance functions of Tweedie models. Generalized form is\nrepresented by a compact definite integral as a function of variance function\nof the exponential dispersion model. This compact integral form simplifies\nderivations of many properties such as scaling, translation and expectation of\nthe beta divergence. Further, we show that beta divergence and (half of) the\nstatistical deviance are equivalent measures.\n",
        "published": "2013-06-14T23:41:58Z",
        "pdf_link": "http://arxiv.org/pdf/1306.3530v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.3574v1",
        "title": "Early stopping and non-parametric regression: An optimal data-dependent\n  stopping rule",
        "summary": "  The strategy of early stopping is a regularization technique based on\nchoosing a stopping time for an iterative algorithm. Focusing on non-parametric\nregression in a reproducing kernel Hilbert space, we analyze the early stopping\nstrategy for a form of gradient-descent applied to the least-squares loss\nfunction. We propose a data-dependent stopping rule that does not involve\nhold-out or cross-validation data, and we prove upper bounds on the squared\nerror of the resulting function estimate, measured in either the $L^2(P)$ and\n$L^2(P_n)$ norm. These upper bounds lead to minimax-optimal rates for various\nkernel classes, including Sobolev smoothness classes and other forms of\nreproducing kernel Hilbert spaces. We show through simulation that our stopping\nrule compares favorably to two other stopping rules, one based on hold-out data\nand the other based on Stein's unbiased risk estimate. We also establish a\ntight connection between our early stopping strategy and the solution path of a\nkernel ridge regression estimator.\n",
        "published": "2013-06-15T12:52:38Z",
        "pdf_link": "http://arxiv.org/pdf/1306.3574v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.3862v2",
        "title": "Bayesian methods for low-rank matrix estimation: short survey and\n  theoretical study",
        "summary": "  The problem of low-rank matrix estimation recently received a lot of\nattention due to challenging applications. A lot of work has been done on\nrank-penalized methods and convex relaxation, both on the theoretical and\napplied sides. However, only a few papers considered Bayesian estimation. In\nthis paper, we review the different type of priors considered on matrices to\nfavour low-rank. We also prove that the obtained Bayesian estimators, under\nsuitable assumptions, enjoys the same optimality properties as the ones based\non penalization.\n",
        "published": "2013-06-17T13:57:05Z",
        "pdf_link": "http://arxiv.org/pdf/1306.3862v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.4103v1",
        "title": "Group Symmetry and non-Gaussian Covariance Estimation",
        "summary": "  We consider robust covariance estimation with group symmetry constraints.\nNon-Gaussian covariance estimation, e.g., Tyler scatter estimator and\nMultivariate Generalized Gaussian distribution methods, usually involve\nnon-convex minimization problems. Recently, it was shown that the underlying\nprinciple behind their success is an extended form of convexity over the\ngeodesics in the manifold of positive definite matrices. A modern approach to\nimprove estimation accuracy is to exploit prior knowledge via additional\nconstraints, e.g., restricting the attention to specific classes of covariances\nwhich adhere to prior symmetry structures. In this paper, we prove that such\ngroup symmetry constraints are also geodesically convex and can therefore be\nincorporated into various non-Gaussian covariance estimators. Practical\nexamples of such sets include: circulant, persymmetric and complex/quaternion\nproper structures. We provide a simple numerical technique for finding maximum\nlikelihood estimates under such constraints, and demonstrate their performance\nadvantage using synthetic experiments.\n",
        "published": "2013-06-18T08:40:40Z",
        "pdf_link": "http://arxiv.org/pdf/1306.4103v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.4960v5",
        "title": "Optimal computational and statistical rates of convergence for sparse\n  nonconvex learning problems",
        "summary": "  We provide theoretical analysis of the statistical and computational\nproperties of penalized $M$-estimators that can be formulated as the solution\nto a possibly nonconvex optimization problem. Many important estimators fall in\nthis category, including least squares regression with nonconvex\nregularization, generalized linear models with nonconvex regularization and\nsparse elliptical random design regression. For these problems, it is\nintractable to calculate the global solution due to the nonconvex formulation.\nIn this paper, we propose an approximate regularization path-following method\nfor solving a variety of learning problems with nonconvex objective functions.\nUnder a unified analytic framework, we simultaneously provide explicit\nstatistical and computational rates of convergence for any local solution\nattained by the algorithm. Computationally, our algorithm attains a global\ngeometric rate of convergence for calculating the full regularization path,\nwhich is optimal among all first-order algorithms. Unlike most existing methods\nthat only attain geometric rates of convergence for one single regularization\nparameter, our algorithm calculates the full regularization path with the same\niteration complexity. In particular, we provide a refined iteration complexity\nbound to sharply characterize the performance of each stage along the\nregularization path. Statistically, we provide sharp sample complexity analysis\nfor all the approximate local solutions along the regularization path. In\nparticular, our analysis improves upon existing results by providing a more\nrefined sample complexity bound as well as an exact support recovery result for\nthe final estimator. These results show that the final estimator attains an\noracle statistical property due to the usage of nonconvex penalty.\n",
        "published": "2013-06-20T19:04:28Z",
        "pdf_link": "http://arxiv.org/pdf/1306.4960v5"
    },
    {
        "id": "http://arxiv.org/abs/1306.5310v1",
        "title": "Online dictionary learning for kernel LMS. Analysis and forward-backward\n  splitting algorithm",
        "summary": "  Adaptive filtering algorithms operating in reproducing kernel Hilbert spaces\nhave demonstrated superiority over their linear counterpart for nonlinear\nsystem identification. Unfortunately, an undesirable characteristic of these\nmethods is that the order of the filters grows linearly with the number of\ninput data. This dramatically increases the computational burden and memory\nrequirement. A variety of strategies based on dictionary learning have been\nproposed to overcome this severe drawback. Few, if any, of these works analyze\nthe problem of updating the dictionary in a time-varying environment. In this\npaper, we present an analytical study of the convergence behavior of the\nGaussian least-mean-square algorithm in the case where the statistics of the\ndictionary elements only partially match the statistics of the input data. This\nallows us to emphasize the need for updating the dictionary in an online way,\nby discarding the obsolete elements and adding appropriate ones. We introduce a\nkernel least-mean-square algorithm with L1-norm regularization to automatically\nperform this task. The stability in the mean of this method is analyzed, and\nits performance is tested with experiments.\n",
        "published": "2013-06-22T11:11:20Z",
        "pdf_link": "http://arxiv.org/pdf/1306.5310v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.5860v1",
        "title": "Supersparse Linear Integer Models for Predictive Scoring Systems",
        "summary": "  We introduce Supersparse Linear Integer Models (SLIM) as a tool to create\nscoring systems for binary classification. We derive theoretical bounds on the\ntrue risk of SLIM scoring systems, and present experimental results to show\nthat SLIM scoring systems are accurate, sparse, and interpretable\nclassification models.\n",
        "published": "2013-06-25T07:02:20Z",
        "pdf_link": "http://arxiv.org/pdf/1306.5860v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.1196v2",
        "title": "The Group Lasso for Design of Experiments",
        "summary": "  We introduce an application of the group lasso to design of experiments. Note\nthat we are NOT trying to explain experimental design for the group lasso.\nConversely, we explain how we can use the idea of the group lasso in\nexperimental design, showing that the problem of constructing an optimal design\nmatrix can be transformed into a problem of the group lasso. In some numerical\nexamples, we show that we can obtain the orthogonal arrays as the solutions of\nthe group lasso problems.\n",
        "published": "2013-08-06T07:24:36Z",
        "pdf_link": "http://arxiv.org/pdf/1308.1196v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.1479v2",
        "title": "Challenges of Big Data Analysis",
        "summary": "  Big Data bring new opportunities to modern society and challenges to data\nscientists. On one hand, Big Data hold great promises for discovering subtle\npopulation patterns and heterogeneities that are not possible with small-scale\ndata. On the other hand, the massive sample size and high dimensionality of Big\nData introduce unique computational and statistical challenges, including\nscalability and storage bottleneck, noise accumulation, spurious correlation,\nincidental endogeneity, and measurement errors. These challenges are\ndistinguished and require new computational and statistical paradigm. This\narticle give overviews on the salient features of Big Data and how these\nfeatures impact on paradigm change on statistical and computational methods as\nwell as computing architectures. We also provide various new perspectives on\nthe Big Data analysis and computation. In particular, we emphasis on the\nviability of the sparsest solution in high-confidence set and point out that\nexogeneous assumptions in most statistical methods for Big Data can not be\nvalidated due to incidental endogeneity. They can lead to wrong statistical\ninferences and consequently wrong scientific conclusions.\n",
        "published": "2013-08-07T05:09:33Z",
        "pdf_link": "http://arxiv.org/pdf/1308.1479v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.2029v3",
        "title": "Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised\n  Learning",
        "summary": "  Hierarchical probabilistic models, such as Gaussian mixture models, are\nwidely used for unsupervised learning tasks. These models consist of observable\nand latent variables, which represent the observable data and the underlying\ndata-generation process, respectively. Unsupervised learning tasks, such as\ncluster analysis, are regarded as estimations of latent variables based on the\nobservable ones. The estimation of latent variables in semi-supervised\nlearning, where some labels are observed, will be more precise than that in\nunsupervised, and one of the concerns is to clarify the effect of the labeled\ndata. However, there has not been sufficient theoretical analysis of the\naccuracy of the estimation of latent variables. In a previous study, a\ndistribution-based error function was formulated, and its asymptotic form was\ncalculated for unsupervised learning with generative models. It has been shown\nthat, for the estimation of latent variables, the Bayes method is more accurate\nthan the maximum-likelihood method. The present paper reveals the asymptotic\nforms of the error function in Bayesian semi-supervised learning for both\ndiscriminative and generative models. The results show that the generative\nmodel, which uses all of the given data, performs better when the model is well\nspecified.\n",
        "published": "2013-08-09T04:13:09Z",
        "pdf_link": "http://arxiv.org/pdf/1308.2029v3"
    },
    {
        "id": "http://arxiv.org/abs/1308.4211v2",
        "title": "Flexible Low-Rank Statistical Modeling with Side Information",
        "summary": "  We propose a general framework for reduced-rank modeling of matrix-valued\ndata. By applying a generalized nuclear norm penalty we can directly model\nlow-dimensional latent variables associated with rows and columns. Our\nframework flexibly incorporates row and column features, smoothing kernels, and\nother sources of side information by penalizing deviations from the row and\ncolumn models. Moreover, a large class of these models can be estimated\nscalably using convex optimization. The computational bottleneck in each case\nis one singular value decomposition per iteration of a large but easy-to-apply\nmatrix. Our framework generalizes traditional convex matrix completion and\nmulti-task learning methods as well as maximum a posteriori estimation under a\nlarge class of popular hierarchical Bayesian models.\n",
        "published": "2013-08-20T02:33:49Z",
        "pdf_link": "http://arxiv.org/pdf/1308.4211v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.5609v2",
        "title": "Frequency Recognition in SSVEP-based BCI using Multiset Canonical\n  Correlation Analysis",
        "summary": "  Canonical correlation analysis (CCA) has been one of the most popular methods\nfor frequency recognition in steady-state visual evoked potential (SSVEP)-based\nbrain-computer interfaces (BCIs). Despite its efficiency, a potential problem\nis that using pre-constructed sine-cosine waves as the required reference\nsignals in the CCA method often does not result in the optimal recognition\naccuracy due to their lack of features from the real EEG data. To address this\nproblem, this study proposes a novel method based on multiset canonical\ncorrelation analysis (MsetCCA) to optimize the reference signals used in the\nCCA method for SSVEP frequency recognition. The MsetCCA method learns multiple\nlinear transforms that implement joint spatial filtering to maximize the\noverall correlation among canonical variates, and hence extracts SSVEP common\nfeatures from multiple sets of EEG data recorded at the same stimulus\nfrequency. The optimized reference signals are formed by combination of the\ncommon features and completely based on training data. Experimental study with\nEEG data from ten healthy subjects demonstrates that the MsetCCA method\nimproves the recognition accuracy of SSVEP frequency in comparison with the CCA\nmethod and other two competing methods (multiway CCA (MwayCCA) and phase\nconstrained CCA (PCCA)), especially for a small number of channels and a short\ntime window length. The superiority indicates that the proposed MsetCCA method\nis a new promising candidate for frequency recognition in SSVEP-based BCIs.\n",
        "published": "2013-08-26T15:11:12Z",
        "pdf_link": "http://arxiv.org/pdf/1308.5609v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.5712v1",
        "title": "The Generalized Mean Information Coefficient",
        "summary": "  Reshef & Reshef recently published a paper in which they present a method\ncalled the Maximal Information Coefficient (MIC) that can detect all forms of\nstatistical dependence between pairs of variables as sample size goes to\ninfinity. While this method has been praised by some, it has also been\ncriticized for its lack of power in finite samples. We seek to modify MIC so\nthat it has higher power in detecting associations for limited sample sizes.\nHere we present the Generalized Mean Information Coefficient (GMIC), a\ngeneralization of MIC which incorporates a tuning parameter that can be used to\nmodify the complexity of the association favored by the measure. We define GMIC\nand prove it maintains several key asymptotic properties of MIC. Its increased\npower over MIC is demonstrated using a simulation of eight different functional\nrelationships at sixty different noise levels. The results are compared to the\nPearson correlation, distance correlation, and MIC. Simulation results suggest\nthat while generally GMIC has slightly lower power than the distance\ncorrelation measure, it achieves higher power than MIC for many forms of\nunderlying association. For some functional relationships, GMIC surpasses all\nother statistics calculated. Preliminary results suggest choosing a moderate\nvalue of the tuning parameter for GMIC will yield a test that is robust across\nunderlying relationships. GMIC is a promising new method that mitigates the\npower issues suffered by MIC, at the possible expense of equitability.\nNonetheless, distance correlation was in our simulations more powerful for many\nforms of underlying relationships. At a minimum, this work motivates further\nconsideration of maximal information-based nonparametric exploration (MINE)\nmethods as statistical tests of independence.\n",
        "published": "2013-08-26T22:05:46Z",
        "pdf_link": "http://arxiv.org/pdf/1308.5712v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1194v1",
        "title": "Some Options for L1-Subspace Signal Processing",
        "summary": "  We describe ways to define and calculate $L_1$-norm signal subspaces which\nare less sensitive to outlying data than $L_2$-calculated subspaces. We focus\non the computation of the $L_1$ maximum-projection principal component of a\ndata matrix containing N signal samples of dimension D and conclude that the\ngeneral problem is formally NP-hard in asymptotically large N, D. We prove,\nhowever, that the case of engineering interest of fixed dimension D and\nasymptotically large sample support N is not and we present an optimal\nalgorithm of complexity $O(N^D)$. We generalize to multiple\n$L_1$-max-projection components and present an explicit optimal $L_1$ subspace\ncalculation algorithm in the form of matrix nuclear-norm evaluations. We\nconclude with illustrations of $L_1$-subspace signal processing in the fields\nof data dimensionality reduction and direction-of-arrival estimation.\n",
        "published": "2013-09-04T22:00:43Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1194v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1233v2",
        "title": "Noisy Sparse Subspace Clustering",
        "summary": "  This paper considers the problem of subspace clustering under noise.\nSpecifically, we study the behavior of Sparse Subspace Clustering (SSC) when\neither adversarial or random noise is added to the unlabelled input data\npoints, which are assumed to be in a union of low-dimensional subspaces. We\nshow that a modified version of SSC is \\emph{provably effective} in correctly\nidentifying the underlying subspaces, even with noisy data. This extends\ntheoretical guarantee of this algorithm to more practical settings and provides\njustification to the success of SSC in a class of real applications.\n",
        "published": "2013-09-05T04:42:00Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1233v2"
    },
    {
        "id": "http://arxiv.org/abs/1309.2303v1",
        "title": "Spectral Clustering with Imbalanced Data",
        "summary": "  Spectral clustering is sensitive to how graphs are constructed from data\nparticularly when proximal and imbalanced clusters are present. We show that\nRatio-Cut (RCut) or normalized cut (NCut) objectives are not tailored to\nimbalanced data since they tend to emphasize cut sizes over cut values. We\npropose a graph partitioning problem that seeks minimum cut partitions under\nminimum size constraints on partitions to deal with imbalanced data. Our\napproach parameterizes a family of graphs, by adaptively modulating node\ndegrees on a fixed node set, to yield a set of parameter dependent cuts\nreflecting varying levels of imbalance. The solution to our problem is then\nobtained by optimizing over these parameters. We present rigorous limit cut\nanalysis results to justify our approach. We demonstrate the superiority of our\nmethod through unsupervised and semi-supervised experiments on synthetic and\nreal data sets.\n",
        "published": "2013-09-09T20:04:03Z",
        "pdf_link": "http://arxiv.org/pdf/1309.2303v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.2895v5",
        "title": "Sparse and Functional Principal Components Analysis",
        "summary": "  Regularized variants of Principal Components Analysis, especially Sparse PCA\nand Functional PCA, are among the most useful tools for the analysis of complex\nhigh-dimensional data. Many examples of massive data, have both sparse and\nfunctional (smooth) aspects and may benefit from a regularization scheme that\ncan capture both forms of structure. For example, in neuro-imaging data, the\nbrain's response to a stimulus may be restricted to a discrete region of\nactivation (spatial sparsity), while exhibiting a smooth response within that\nregion. We propose a unified approach to regularized PCA which can induce both\nsparsity and smoothness in both the row and column principal components. Our\nframework generalizes much of the previous literature, with sparse, functional,\ntwo-way sparse, and two-way functional PCA all being special cases of our\napproach. Our method permits flexible combinations of sparsity and smoothness\nthat lead to improvements in feature selection and signal recovery, as well as\nmore interpretable PCA factors. We demonstrate the efficacy of our method on\nsimulated data and a neuroimaging example on EEG data.\n",
        "published": "2013-09-11T17:18:30Z",
        "pdf_link": "http://arxiv.org/pdf/1309.2895v5"
    },
    {
        "id": "http://arxiv.org/abs/1309.4859v1",
        "title": "Predictive PAC Learning and Process Decompositions",
        "summary": "  We informally call a stochastic process learnable if it admits a\ngeneralization error approaching zero in probability for any concept class with\nfinite VC-dimension (IID processes are the simplest example). A mixture of\nlearnable processes need not be learnable itself, and certainly its\ngeneralization error need not decay at the same rate. In this paper, we argue\nthat it is natural in predictive PAC to condition not on the past observations\nbut on the mixture component of the sample path. This definition not only\nmatches what a realistic learner might demand, but also allows us to sidestep\nseveral otherwise grave problems in learning from dependent data. In\nparticular, we give a novel PAC generalization bound for mixtures of learnable\nprocesses with a generalization error that is not worse than that of each\nmixture component. We also provide a characterization of mixtures of absolutely\nregular ($\\beta$-mixing) processes, of independent probability-theoretic\ninterest.\n",
        "published": "2013-09-19T04:57:59Z",
        "pdf_link": "http://arxiv.org/pdf/1309.4859v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5701v2",
        "title": "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy\n  Separability",
        "summary": "  We present a numerical algorithm for nonnegative matrix factorization (NMF)\nproblems under noisy separability. An NMF problem under separability can be\nstated as one of finding all vertices of the convex hull of data points. The\nresearch interest of this paper is to find the vectors as close to the vertices\nas possible in a situation in which noise is added to the data points. Our\nalgorithm is designed to capture the shape of the convex hull of data points by\nusing its enclosing ellipsoid. We show that the algorithm has correctness and\nrobustness properties from theoretical and practical perspectives; correctness\nhere means that if the data points do not contain any noise, the algorithm can\nfind the vertices of their convex hull; robustness means that if the data\npoints contain noise, the algorithm can find the near-vertices. Finally, we\napply the algorithm to document clustering, and report the experimental\nresults.\n",
        "published": "2013-09-23T06:19:54Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5701v2"
    },
    {
        "id": "http://arxiv.org/abs/1309.6415v2",
        "title": "Stratified Graphical Models - Context-Specific Independence in Graphical\n  Models",
        "summary": "  Theory of graphical models has matured over more than three decades to\nprovide the backbone for several classes of models that are used in a myriad of\napplications such as genetic mapping of diseases, credit risk evaluation,\nreliability and computer security, etc. Despite of their generic applicability\nand wide adoptance, the constraints imposed by undirected graphical models and\nBayesian networks have also been recognized to be unnecessarily stringent under\ncertain circumstances. This observation has led to the proposal of several\ngeneralizations that aim at more relaxed constraints by which the models can\nimpose local or context-specific dependence structures. Here we consider an\nadditional class of such models, termed as stratified graphical models. We\ndevelop a method for Bayesian learning of these models by deriving an\nanalytical expression for the marginal likelihood of data under a specific\nsubclass of decomposable stratified models. A non-reversible Markov chain Monte\nCarlo approach is further used to identify models that are highly supported by\nthe posterior distribution over the model space. Our method is illustrated and\ncompared with ordinary graphical models through application to several real and\nsynthetic datasets.\n",
        "published": "2013-09-25T07:30:18Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6415v2"
    },
    {
        "id": "http://arxiv.org/abs/1309.6779v4",
        "title": "Causal Discovery with Continuous Additive Noise Models",
        "summary": "  We consider the problem of learning causal directed acyclic graphs from an\nobservational joint distribution. One can use these graphs to predict the\noutcome of interventional experiments, from which data are often not available.\nWe show that if the observational distribution follows a structural equation\nmodel with an additive noise structure, the directed acyclic graph becomes\nidentifiable from the distribution under mild conditions. This constitutes an\ninteresting alternative to traditional methods that assume faithfulness and\nidentify only the Markov equivalence class of the graph, thus leaving some\nedges undirected. We provide practical algorithms for finitely many samples,\nRESIT (Regression with Subsequent Independence Test) and two methods based on\nan independence score. We prove that RESIT is correct in the population setting\nand provide an empirical evaluation.\n",
        "published": "2013-09-26T10:04:45Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6779v4"
    },
    {
        "id": "http://arxiv.org/abs/1309.7821v2",
        "title": "MPBART - Multinomial Probit Bayesian Additive Regression Trees",
        "summary": "  This article proposes Multinomial Probit Bayesian Additive Regression Trees\n(MPBART) as a multinomial probit extension of BART - Bayesian Additive\nRegression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusion\nof predictors that describe the observed units as well as the available choice\nalternatives. Through two simulation studies and four real data examples, we\nshow that MPBART exhibits very good predictive performance in comparison to\nother discrete choice and multiclass classification methods. To implement\nMPBART, we have developed an R package mpbart available freely from CRAN\nrepositories.\n",
        "published": "2013-09-30T12:34:09Z",
        "pdf_link": "http://arxiv.org/pdf/1309.7821v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.0512v2",
        "title": "Jointly Clustering Rows and Columns of Binary Matrices: Algorithms and\n  Trade-offs",
        "summary": "  In standard clustering problems, data points are represented by vectors, and\nby stacking them together, one forms a data matrix with row or column cluster\nstructure. In this paper, we consider a class of binary matrices, arising in\nmany applications, which exhibit both row and column cluster structure, and our\ngoal is to exactly recover the underlying row and column clusters by observing\nonly a small fraction of noisy entries. We first derive a lower bound on the\nminimum number of observations needed for exact cluster recovery. Then, we\npropose three algorithms with different running time and compare the number of\nobservations needed by them for successful cluster recovery. Our analytical\nresults show smooth time-data trade-offs: one can gradually reduce the\ncomputational complexity when increasingly more observations are available.\n",
        "published": "2013-10-01T22:46:06Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0512v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.0532v4",
        "title": "Perfect Clustering for Stochastic Blockmodel Graphs via Adjacency\n  Spectral Embedding",
        "summary": "  Vertex clustering in a stochastic blockmodel graph has wide applicability and\nhas been the subject of extensive research. In thispaper, we provide a short\nproof that the adjacency spectral embedding can be used to obtain perfect\nclustering for the stochastic blockmodel and the degree-corrected stochastic\nblockmodel. We also show an analogous result for the more general random dot\nproduct graph model.\n",
        "published": "2013-10-02T00:33:34Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0532v4"
    },
    {
        "id": "http://arxiv.org/abs/1310.1519v2",
        "title": "Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of\n  Classification Error in the Gaussian Model",
        "summary": "  The most important aspect of any classifier is its error rate, because this\nquantifies its predictive capacity. Thus, the accuracy of error estimation is\ncritical. Error estimation is problematic in small-sample classifier design\nbecause the error must be estimated using the same data from which the\nclassifier has been designed. Use of prior knowledge, in the form of a prior\ndistribution on an uncertainty class of feature-label distributions to which\nthe true, but unknown, feature-distribution belongs, can facilitate accurate\nerror estimation (in the mean-square sense) in circumstances where accurate\ncompletely model-free error estimation is impossible. This paper provides\nanalytic asymptotically exact finite-sample approximations for various\nperformance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE)\nerror estimator in the case of linear discriminant analysis (LDA) in the\nmultivariate Gaussian model. These performance metrics include the first,\nsecond, and cross moments of the Bayesian MMSE error estimator with the true\nerror of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator.\nWe lay down the theoretical groundwork for Kolmogorov double-asymptotics in a\nBayesian setting, which enables us to derive asymptotic expressions of the\ndesired performance metrics. From these we produce analytic finite-sample\napproximations and demonstrate their accuracy via numerical examples. Various\nexamples illustrate the behavior of these approximations and their use in\ndetermining the necessary sample size to achieve a desired RMS. The\nSupplementary Material contains derivations for some equations and added\nfigures.\n",
        "published": "2013-10-05T21:57:02Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1519v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.1562v5",
        "title": "Dependence Measure for non-additive model",
        "summary": "  We proposed a new statistical dependency measure called Copula Dependency\nCoefficient(CDC) for two sets of variables based on copula. It is robust to\noutliers, easy to implement, powerful and appropriate to high-dimensional\nvariables. These properties are important in many applications. Experimental\nresults show that CDC can detect the dependence between variables in both\nadditive and non-additive models.\n",
        "published": "2013-10-06T09:36:55Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1562v5"
    },
    {
        "id": "http://arxiv.org/abs/1310.1867v4",
        "title": "Mean Field Bayes Backpropagation: scalable training of multilayer neural\n  networks with binary weights",
        "summary": "  Significant success has been reported recently using deep neural networks for\nclassification. Such large networks can be computationally intensive, even\nafter training is over. Implementing these trained networks in hardware chips\nwith a limited precision of synaptic weights may improve their speed and energy\nefficiency by several orders of magnitude, thus enabling their integration into\nsmall and low-power electronic devices. With this motivation, we develop a\ncomputationally efficient learning algorithm for multilayer neural networks\nwith binary weights, assuming all the hidden neurons have a fan-out of one.\nThis algorithm, derived within a Bayesian probabilistic online setting, is\nshown to work well for both synthetic and real-world problems, performing\ncomparably to algorithms with real-valued weights, while retaining\ncomputational tractability.\n",
        "published": "2013-10-07T17:32:37Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1867v4"
    },
    {
        "id": "http://arxiv.org/abs/1310.3003v3",
        "title": "Distance-weighted Support Vector Machine",
        "summary": "  A novel linear classification method that possesses the merits of both the\nSupport Vector Machine (SVM) and the Distance-weighted Discrimination (DWD) is\nproposed in this article. The proposed Distance-weighted Support Vector Machine\nmethod can be viewed as a hybrid of SVM and DWD that finds the classification\ndirection by minimizing mainly the DWD loss, and determines the intercept term\nin the SVM manner. We show that our method inheres the merit of DWD, and hence,\novercomes the data-piling and overfitting issue of SVM. On the other hand, the\nnew method is not subject to imbalanced data issue which was a main advantage\nof SVM over DWD. It uses an unusual loss which combines the Hinge loss (of SVM)\nand the DWD loss through a trick of axillary hyperplane. Several theoretical\nproperties, including Fisher consistency and asymptotic normality of the DWSVM\nsolution are developed. We use some simulated examples to show that the new\nmethod can compete DWD and SVM on both classification performance and\ninterpretability. A real data application further establishes the usefulness of\nour approach.\n",
        "published": "2013-10-11T02:21:49Z",
        "pdf_link": "http://arxiv.org/pdf/1310.3003v3"
    },
    {
        "id": "http://arxiv.org/abs/1310.3004v1",
        "title": "Flexible High-dimensional Classification Machines and Their Asymptotic\n  Properties",
        "summary": "  Classification is an important topic in statistics and machine learning with\ngreat potential in many real applications. In this paper, we investigate two\npopular large margin classification methods, Support Vector Machine (SVM) and\nDistance Weighted Discrimination (DWD), under two contexts: the\nhigh-dimensional, low-sample size data and the imbalanced data. A unified\nfamily of classification machines, the FLexible Assortment MachinE (FLAME) is\nproposed, within which DWD and SVM are special cases. The FLAME family helps to\nidentify the similarities and differences between SVM and DWD. It is well known\nthat many classifiers overfit the data in the high-dimensional setting; and\nothers are sensitive to the imbalanced data, that is, the class with a larger\nsample size overly influences the classifier and pushes the decision boundary\ntowards the minority class. SVM is resistant to the imbalanced data issue, but\nit overfits high-dimensional data sets by showing the undesired data-piling\nphenomena. The DWD method was proposed to improve SVM in the high-dimensional\nsetting, but its decision boundary is sensitive to the imbalanced ratio of\nsample sizes. Our FLAME family helps to understand an intrinsic connection\nbetween SVM and DWD, and improves both methods by providing a better trade-off\nbetween sensitivity to the imbalanced data and overfitting the high-dimensional\ndata. Several asymptotic properties of the FLAME classifiers are studied.\nSimulations and real data applications are investigated to illustrate the\nusefulness of the FLAME classifiers.\n",
        "published": "2013-10-11T02:22:20Z",
        "pdf_link": "http://arxiv.org/pdf/1310.3004v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.3561v4",
        "title": "ECA: High Dimensional Elliptical Component Analysis in non-Gaussian\n  Distributions",
        "summary": "  We present a robust alternative to principal component analysis (PCA) ---\ncalled elliptical component analysis (ECA) --- for analyzing high dimensional,\nelliptically distributed data. ECA estimates the eigenspace of the covariance\nmatrix of the elliptical data. To cope with heavy-tailed elliptical\ndistributions, a multivariate rank statistic is exploited. At the model-level,\nwe consider two settings: either that the leading eigenvectors of the\ncovariance matrix are non-sparse or that they are sparse. Methodologically, we\npropose ECA procedures for both non-sparse and sparse settings. Theoretically,\nwe provide both non-asymptotic and asymptotic analyses quantifying the\ntheoretical performances of ECA. In the non-sparse setting, we show that ECA's\nperformance is highly related to the effective rank of the covariance matrix.\nIn the sparse setting, the results are twofold: (i) We show that the sparse ECA\nestimator based on a combinatoric program attains the optimal rate of\nconvergence; (ii) Based on some recent developments in estimating sparse\nleading eigenvectors, we show that a computationally efficient sparse ECA\nestimator attains the optimal rate of convergence under a suboptimal scaling.\n",
        "published": "2013-10-14T04:40:12Z",
        "pdf_link": "http://arxiv.org/pdf/1310.3561v4"
    },
    {
        "id": "http://arxiv.org/abs/1310.3745v2",
        "title": "Alternating Minimization for Mixed Linear Regression",
        "summary": "  Mixed linear regression involves the recovery of two (or more) unknown\nvectors from unlabeled linear measurements; that is, where each sample comes\nfrom exactly one of the vectors, but we do not know which one. It is a classic\nproblem, and the natural and empirically most popular approach to its solution\nhas been the EM algorithm. As in other settings, this is prone to bad local\nminima; however, each iteration is very fast (alternating between guessing\nlabels, and solving with those labels).\n  In this paper we provide a new initialization procedure for EM, based on\nfinding the leading two eigenvectors of an appropriate matrix. We then show\nthat with this, a re-sampled version of the EM algorithm provably converges to\nthe correct vectors, under natural assumptions on the sampling distribution,\nand with nearly optimal (unimprovable) sample complexity. This provides not\nonly the first characterization of EM's performance, but also much lower sample\ncomplexity as compared to both standard (randomly initialized) EM, and other\nmethods for this problem.\n",
        "published": "2013-10-14T16:48:15Z",
        "pdf_link": "http://arxiv.org/pdf/1310.3745v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.4375v3",
        "title": "Fast Computation of Wasserstein Barycenters",
        "summary": "  We present new algorithms to compute the mean of a set of empirical\nprobability measures under the optimal transport metric. This mean, known as\nthe Wasserstein barycenter, is the measure that minimizes the sum of its\nWasserstein distances to each element in that set. We propose two original\nalgorithms to compute Wasserstein barycenters that build upon the subgradient\nmethod. A direct implementation of these algorithms is, however, too costly\nbecause it would require the repeated resolution of large primal and dual\noptimal transport problems to compute subgradients. Extending the work of\nCuturi (2013), we propose to smooth the Wasserstein distance used in the\ndefinition of Wasserstein barycenters with an entropic regularizer and recover\nin doing so a strictly convex objective whose gradients can be computed for a\nconsiderably cheaper computational cost using matrix scaling algorithms. We use\nthese algorithms to visualize a large family of images and to solve a\nconstrained clustering problem.\n",
        "published": "2013-10-16T13:47:14Z",
        "pdf_link": "http://arxiv.org/pdf/1310.4375v3"
    },
    {
        "id": "http://arxiv.org/abs/1310.5415v2",
        "title": "Disease Prediction based on Functional Connectomes using a Scalable and\n  Spatially-Informed Support Vector Machine",
        "summary": "  Substantial evidence indicates that major psychiatric disorders are\nassociated with distributed neural dysconnectivity, leading to strong interest\nin using neuroimaging methods to accurately predict disorder status. In this\nwork, we are specifically interested in a multivariate approach that uses\nfeatures derived from whole-brain resting state functional connectomes.\nHowever, functional connectomes reside in a high dimensional space, which\ncomplicates model interpretation and introduces numerous statistical and\ncomputational challenges. Traditional feature selection techniques are used to\nreduce data dimensionality, but are blind to the spatial structure of the\nconnectomes. We propose a regularization framework where the 6-D structure of\nthe functional connectome is explicitly taken into account via the fused Lasso\nor the GraphNet regularizer. Our method only restricts the loss function to be\nconvex and margin-based, allowing non-differentiable loss functions such as the\nhinge-loss to be used. Using the fused Lasso or GraphNet regularizer with the\nhinge-loss leads to a structured sparse support vector machine (SVM) with\nembedded feature selection. We introduce a novel efficient optimization\nalgorithm based on the augmented Lagrangian and the classical alternating\ndirection method, which can solve both fused Lasso and GraphNet regularized SVM\nwith very little modification. We also demonstrate that the inner subproblems\nof the algorithm can be solved efficiently in analytic form by coupling the\nvariable splitting strategy with a data augmentation scheme. Experiments on\nsimulated data and resting state scans from a large schizophrenia dataset show\nthat our proposed approach can identify predictive regions that are spatially\ncontiguous in the 6-D \"connectome space,\" offering an additional layer of\ninterpretability that could provide new insights about various disease\nprocesses.\n",
        "published": "2013-10-21T04:03:46Z",
        "pdf_link": "http://arxiv.org/pdf/1310.5415v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.5438v4",
        "title": "Variational Bayesian inference for linear and logistic regression",
        "summary": "  The article describe the model, derivation, and implementation of variational\nBayesian inference for linear and logistic regression, both with and without\nautomatic relevance determination. It has the dual function of acting as a\ntutorial for the derivation of variational Bayesian inference for simple\nmodels, as well as documenting, and providing brief examples for the\nMATLAB/Octave functions that implement this inference. These functions are\nfreely available online.\n",
        "published": "2013-10-21T07:10:51Z",
        "pdf_link": "http://arxiv.org/pdf/1310.5438v4"
    },
    {
        "id": "http://arxiv.org/abs/1310.5543v2",
        "title": "Universalities of Reproducing Kernels Revisited",
        "summary": "  Kernel methods have been widely applied to machine learning and other\nquestions of approximating an unknown function from its finite sample data. To\nensure arbitrary accuracy of such approximation, various denseness conditions\nare imposed on the selected kernel. This note contributes to the study of\nuniversal, characteristic, and $C_0$-universal kernels. We first give simple\nand direct description of the difference and relation among these three kinds\nof universalities of kernels. We then focus on translation-invariant and\nweighted polynomial kernels. A simple and shorter proof of the known\ncharacterization of characteristic translation-invariant kernels will be\npresented. The main purpose of the note is to give a delicate discussion on the\nuniversalities of weighted polynomial kernels.\n",
        "published": "2013-10-21T13:51:16Z",
        "pdf_link": "http://arxiv.org/pdf/1310.5543v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.5666v1",
        "title": "Distributed parameter estimation of discrete hierarchical models via\n  marginal likelihoods",
        "summary": "  We consider discrete graphical models Markov with respect to a graph $G$ and\npropose two distributed marginal methods to estimate the maximum likelihood\nestimate of the canonical parameter of the model. Both methods are based on a\nrelaxation of the marginal likelihood obtained by considering the density of\nthe variables represented by a vertex $v$ of $G$ and a neighborhood. The two\nmethods differ by the size of the neighborhood of $v$. We show that the\nestimates are consistent and that those obtained with the larger neighborhood\nhave smaller asymptotic variance than the ones obtained through the smaller\nneighborhood.\n",
        "published": "2013-10-21T18:29:12Z",
        "pdf_link": "http://arxiv.org/pdf/1310.5666v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.6062v1",
        "title": "Combined l_1 and greedy l_0 penalized least squares for linear model\n  selection",
        "summary": "  We introduce a computationally effective algorithm for a linear model\nselection consisting of three steps: screening--ordering--selection (SOS).\nScreening of predictors is based on the thresholded Lasso that is l_1 penalized\nleast squares. The screened predictors are then fitted using least squares (LS)\nand ordered with respect to their t statistics. Finally, a model is selected\nusing greedy generalized information criterion (GIC) that is l_0 penalized LS\nin a nested family induced by the ordering. We give non-asymptotic upper bounds\non error probability of each step of the SOS algorithm in terms of both\npenalties. Then we obtain selection consistency for different (n, p) scenarios\nunder conditions which are needed for screening consistency of the Lasso. For\nthe traditional setting (n >p) we give Sanov-type bounds on the error\nprobabilities of the ordering--selection algorithm. Its surprising consequence\nis that the selection error of greedy GIC is asymptotically not larger than of\nexhaustive GIC. We also obtain new bounds on prediction and estimation errors\nfor the Lasso which are proved in parallel for the algorithm used in practice\nand its formal version.\n",
        "published": "2013-10-22T21:27:18Z",
        "pdf_link": "http://arxiv.org/pdf/1310.6062v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.6067v1",
        "title": "Multiple Kernel Learning for Brain-Computer Interfacing",
        "summary": "  Combining information from different sources is a common way to improve\nclassification accuracy in Brain-Computer Interfacing (BCI). For instance, in\nsmall sample settings it is useful to integrate data from other subjects or\nsessions in order to improve the estimation quality of the spatial filters or\nthe classifier. Since data from different subjects may show large variability,\nit is crucial to weight the contributions according to importance. Many\nmulti-subject learning algorithms determine the optimal weighting in a separate\nstep by using heuristics, however, without ensuring that the selected weights\nare optimal with respect to classification. In this work we apply Multiple\nKernel Learning (MKL) to this problem. MKL has been widely used for feature\nfusion in computer vision and allows to simultaneously learn the classifier and\nthe optimal weighting. We compare the MKL method to two baseline approaches and\ninvestigate the reasons for performance improvement.\n",
        "published": "2013-10-22T21:46:02Z",
        "pdf_link": "http://arxiv.org/pdf/1310.6067v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.6319v2",
        "title": "Efficient State-Space Inference of Periodic Latent Force Models",
        "summary": "  Latent force models (LFM) are principled approaches to incorporating\nsolutions to differential equations within non-parametric inference methods.\nUnfortunately, the development and application of LFMs can be inhibited by\ntheir computational cost, especially when closed-form solutions for the LFM are\nunavailable, as is the case in many real world problems where these latent\nforces exhibit periodic behaviour. Given this, we develop a new sparse\nrepresentation of LFMs which considerably improves their computational\nefficiency, as well as broadening their applicability, in a principled way, to\ndomains with periodic or near periodic latent forces. Our approach uses a\nlinear basis model to approximate one generative model for each periodic force.\nWe assume that the latent forces are generated from Gaussian process priors and\ndevelop a linear basis model which fully expresses these priors. We apply our\napproach to model the thermal dynamics of domestic buildings and show that it\nis effective at predicting day-ahead temperatures within the homes. We also\napply our approach within queueing theory in which quasi-periodic arrival rates\nare modelled as latent forces. In both cases, we demonstrate that our approach\ncan be implemented efficiently using state-space methods which encode the\nlinear dynamic systems via LFMs. Further, we show that state estimates obtained\nusing periodic latent force models can reduce the root mean squared error to\n17% of that from non-periodic models and 27% of the nearest rival approach\nwhich is the resonator model.\n",
        "published": "2013-10-23T18:27:42Z",
        "pdf_link": "http://arxiv.org/pdf/1310.6319v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.6778v2",
        "title": "Bayesian estimation of possible causal direction in the presence of\n  latent confounders using a linear non-Gaussian acyclic structural equation\n  model with individual-specific effects",
        "summary": "  We consider learning the possible causal direction of two observed variables\nin the presence of latent confounding variables. Several existing methods have\nbeen shown to consistently estimate causal direction assuming linear or some\ntype of nonlinear relationship and no latent confounders. However, the\nestimation results could be distorted if either assumption is actually\nviolated. In this paper, we first propose a new linear non-Gaussian acyclic\nstructural equation model with individual-specific effects that allows latent\nconfounders to be considered. We then propose an empirical Bayesian approach\nfor estimating possible causal direction using the new model. We demonstrate\nthe effectiveness of our method using artificial and real-world data.\n",
        "published": "2013-10-24T21:27:57Z",
        "pdf_link": "http://arxiv.org/pdf/1310.6778v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.7855v1",
        "title": "A comparison of bandwidth selectors for mean shift clustering",
        "summary": "  We explore the performance of several automatic bandwidth selectors,\noriginally designed for density gradient estimation, as data-based procedures\nfor nonparametric, modal clustering. The key tool to obtain a clustering from\ndensity gradient estimators is the mean shift algorithm, which allows to obtain\na partition not only of the data sample, but also of the whole space. The\nresults of our simulation study suggest that most of the methods considered\nhere, like cross validation and plug in bandwidth selectors, are useful for\ncluster analysis via the mean shift algorithm.\n",
        "published": "2013-10-29T16:05:44Z",
        "pdf_link": "http://arxiv.org/pdf/1310.7855v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.8612v1",
        "title": "Nonlinear unmixing of hyperspectral images using a semiparametric model\n  and spatial regularization",
        "summary": "  Incorporating spatial information into hyperspectral unmixing procedures has\nbeen shown to have positive effects, due to the inherent spatial-spectral\nduality in hyperspectral scenes. Current research works that consider spatial\ninformation are mainly focused on the linear mixing model. In this paper, we\ninvestigate a variational approach to incorporating spatial correlation into a\nnonlinear unmixing procedure. A nonlinear algorithm operating in reproducing\nkernel Hilbert spaces, associated with an $\\ell_1$ local variation norm as the\nspatial regularizer, is derived. Experimental results, with both synthetic and\nreal data, illustrate the effectiveness of the proposed scheme.\n",
        "published": "2013-10-31T17:40:20Z",
        "pdf_link": "http://arxiv.org/pdf/1310.8612v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.8618v1",
        "title": "Convergence analysis of kernel LMS algorithm with pre-tuned dictionary",
        "summary": "  The kernel least-mean-square (KLMS) algorithm is an appealing tool for online\nidentification of nonlinear systems due to its simplicity and robustness. In\naddition to choosing a reproducing kernel and setting filter parameters,\ndesigning a KLMS adaptive filter requires to select a so-called dictionary in\norder to get a finite-order model. This dictionary has a significant impact on\nperformance, and requires careful consideration. Theoretical analysis of KLMS\nas a function of dictionary setting has rarely, if ever, been addressed in the\nliterature. In an analysis previously published by the authors, the dictionary\nelements were assumed to be governed by the same probability density function\nof the input data. In this paper, we modify this study by considering the\ndictionary as part of the filter parameters to be set. This theoretical\nanalysis paves the way for future investigations on KLMS dictionary design.\n",
        "published": "2013-10-31T17:53:06Z",
        "pdf_link": "http://arxiv.org/pdf/1310.8618v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.0219v2",
        "title": "Joint Estimation of Multiple Graphical Models from High Dimensional Time\n  Series",
        "summary": "  In this manuscript we consider the problem of jointly estimating multiple\ngraphical models in high dimensions. We assume that the data are collected from\nn subjects, each of which consists of T possibly dependent observations. The\ngraphical models of subjects vary, but are assumed to change smoothly\ncorresponding to a measure of closeness between subjects. We propose a kernel\nbased method for jointly estimating all graphical models. Theoretically, under\na double asymptotic framework, where both (T,n) and the dimension d can\nincrease, we provide the explicit rate of convergence in parameter estimation.\nIt characterizes the strength one can borrow across different individuals and\nimpact of data dependence on parameter estimation. Empirically, experiments on\nboth synthetic and real resting state functional magnetic resonance imaging\n(rs-fMRI) data illustrate the effectiveness of the proposed method.\n",
        "published": "2013-11-01T16:32:17Z",
        "pdf_link": "http://arxiv.org/pdf/1311.0219v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.0360v1",
        "title": "Multivariate Generalized Gaussian Process Models",
        "summary": "  We propose a family of multivariate Gaussian process models for correlated\noutputs, based on assuming that the likelihood function takes the generic form\nof the multivariate exponential family distribution (EFD). We denote this model\nas a multivariate generalized Gaussian process model, and derive Taylor and\nLaplace algorithms for approximate inference on the generic model. By\ninstantiating the EFD with specific parameter functions, we obtain two novel GP\nmodels (and corresponding inference algorithms) for correlated outputs: 1) a\nVon-Mises GP for angle regression; and 2) a Dirichlet GP for regressing on the\nmultinomial simplex.\n",
        "published": "2013-11-02T09:09:35Z",
        "pdf_link": "http://arxiv.org/pdf/1311.0360v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.0622v1",
        "title": "Stochastic Dual Coordinate Ascent with Alternating Direction Multiplier\n  Method",
        "summary": "  We propose a new stochastic dual coordinate ascent technique that can be\napplied to a wide range of regularized learning problems. Our method is based\non Alternating Direction Multiplier Method (ADMM) to deal with complex\nregularization functions such as structured regularizations. Although the\noriginal ADMM is a batch method, the proposed method offers a stochastic update\nrule where each iteration requires only one or few sample observations.\nMoreover, our method can naturally afford mini-batch update and it gives speed\nup of convergence. We show that, under mild assumptions, our method converges\nexponentially. The numerical experiments show that our method actually performs\nefficiently.\n",
        "published": "2013-11-04T09:31:51Z",
        "pdf_link": "http://arxiv.org/pdf/1311.0622v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.1033v2",
        "title": "Nonparametric Bayesian models of hierarchical structure in complex\n  networks",
        "summary": "  Analyzing and understanding the structure of complex relational data is\nimportant in many applications including analysis of the connectivity in the\nhuman brain. Such networks can have prominent patterns on different scales,\ncalling for a hierarchically structured model. We propose two non-parametric\nBayesian hierarchical network models based on Gibbs fragmentation tree priors,\nand demonstrate their ability to capture nested patterns in simulated networks.\nOn real networks we demonstrate detection of hierarchical structure and show\npredictive performance on par with the state of the art. We envision that our\nmethods can be employed in exploratory analysis of large scale complex networks\nfor example to model human brain connectivity.\n",
        "published": "2013-11-05T12:39:04Z",
        "pdf_link": "http://arxiv.org/pdf/1311.1033v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.1911v3",
        "title": "Visualizing the Effects of a Changing Distance on Data Using Continuous\n  Embeddings",
        "summary": "  Most Machine Learning (ML) methods, from clustering to classification, rely\non a distance function to describe relationships between datapoints. For\ncomplex datasets it is hard to avoid making some arbitrary choices when\ndefining a distance function. To compare images, one must choose a spatial\nscale, for signals, a temporal scale. The right scale is hard to pin down and\nit is preferable when results do not depend too tightly on the exact value one\npicked. Topological data analysis seeks to address this issue by focusing on\nthe notion of neighbourhood instead of distance. It is shown that in some cases\na simpler solution is available. It can be checked how strongly distance\nrelationships depend on a hyperparameter using dimensionality reduction. A\nvariant of dynamical multi-dimensional scaling (MDS) is formulated, which\nembeds datapoints as curves. The resulting algorithm is based on the\nConcave-Convex Procedure (CCCP) and provides a simple and efficient way of\nvisualizing changes and invariances in distance patterns as a hyperparameter is\nvaried. A variant to analyze the dependence on multiple hyperparameters is also\npresented. A cMDS algorithm that is straightforward to implement, use and\nextend is provided. To illustrate the possibilities of cMDS, cMDS is applied to\nseveral real-world data sets.\n",
        "published": "2013-11-08T09:33:06Z",
        "pdf_link": "http://arxiv.org/pdf/1311.1911v3"
    },
    {
        "id": "http://arxiv.org/abs/1311.2520v3",
        "title": "The Infinite Degree Corrected Stochastic Block Model",
        "summary": "  In Stochastic blockmodels, which are among the most prominent statistical\nmodels for cluster analysis of complex networks, clusters are defined as groups\nof nodes with statistically similar link probabilities within and between\ngroups. A recent extension by Karrer and Newman incorporates a node degree\ncorrection to model degree heterogeneity within each group. Although this\ndemonstrably leads to better performance on several networks it is not obvious\nwhether modelling node degree is always appropriate or necessary. We formulate\nthe degree corrected stochastic blockmodel as a non-parametric Bayesian model,\nincorporating a parameter to control the amount of degree correction which can\nthen be inferred from data. Additionally, our formulation yields principled\nways of inferring the number of groups as well as predicting missing links in\nthe network which can be used to quantify the model's predictive performance.\nOn synthetic data we demonstrate that including the degree correction yields\nbetter performance both on recovering the true group structure and predicting\nmissing links when degree heterogeneity is present, whereas performance is on\npar for data with no degree heterogeneity within clusters. On seven real\nnetworks (with no ground truth group structure available) we show that\npredictive performance is about equal whether or not degree correction is\nincluded; however, for some networks significantly fewer clusters are\ndiscovered when correcting for degree indicating that the data can be more\ncompactly explained by clusters of heterogenous degree nodes.\n",
        "published": "2013-11-11T18:37:35Z",
        "pdf_link": "http://arxiv.org/pdf/1311.2520v3"
    },
    {
        "id": "http://arxiv.org/abs/1311.3257v2",
        "title": "Compressive Nonparametric Graphical Model Selection For Time Series",
        "summary": "  We propose a method for inferring the conditional indepen- dence graph (CIG)\nof a high-dimensional discrete-time Gaus- sian vector random process from\nfinite-length observations. Our approach does not rely on a parametric model\n(such as, e.g., an autoregressive model) for the vector random process; rather,\nit only assumes certain spectral smoothness proper- ties. The proposed\ninference scheme is compressive in that it works for sample sizes that are\n(much) smaller than the number of scalar process components. We provide\nanalytical conditions for our method to correctly identify the CIG with high\nprobability.\n",
        "published": "2013-11-13T19:12:55Z",
        "pdf_link": "http://arxiv.org/pdf/1311.3257v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.4025v3",
        "title": "Signal Recovery from Pooling Representations",
        "summary": "  In this work we compute lower Lipschitz bounds of $\\ell_p$ pooling operators\nfor $p=1, 2, \\infty$ as well as $\\ell_p$ pooling operators preceded by\nhalf-rectification layers. These give sufficient conditions for the design of\ninvertible neural network layers. Numerical experiments on MNIST and image\npatches confirm that pooling layers can be inverted with phase recovery\nalgorithms. Moreover, the regularity of the inverse pooling, controlled by the\nlower Lipschitz constant, is empirically verified with a nearest neighbor\nregression.\n",
        "published": "2013-11-16T06:53:44Z",
        "pdf_link": "http://arxiv.org/pdf/1311.4025v3"
    },
    {
        "id": "http://arxiv.org/abs/1311.4669v1",
        "title": "Nonparametric Bayes dynamic modeling of relational data",
        "summary": "  Symmetric binary matrices representing relations among entities are commonly\ncollected in many areas. Our focus is on dynamically evolving binary relational\nmatrices, with interest being in inference on the relationship structure and\nprediction. We propose a nonparametric Bayesian dynamic model, which reduces\ndimensionality in characterizing the binary matrix through a lower-dimensional\nlatent space representation, with the latent coordinates evolving in continuous\ntime via Gaussian processes. By using a logistic mapping function from the\nprobability matrix space to the latent relational space, we obtain a flexible\nand computational tractable formulation. Employing P\\`olya-Gamma data\naugmentation, an efficient Gibbs sampler is developed for posterior\ncomputation, with the dimension of the latent space automatically inferred. We\nprovide some theoretical results on flexibility of the model, and illustrate\nperformance via simulation experiments. We also consider an application to\nco-movements in world financial markets.\n",
        "published": "2013-11-19T09:39:26Z",
        "pdf_link": "http://arxiv.org/pdf/1311.4669v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.5479v2",
        "title": "Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics",
        "summary": "  We investigate a generic problem of learning pairwise exponential family\ngraphical models with pairwise sufficient statistics defined by a global\nmapping function, e.g., Mercer kernels. This subclass of pairwise graphical\nmodels allow us to flexibly capture complex interactions among variables beyond\npairwise product. We propose two $\\ell_1$-norm penalized maximum likelihood\nestimators to learn the model parameters from i.i.d. samples. The first one is\na joint estimator which estimates all the parameters simultaneously. The second\none is a node-wise conditional estimator which estimates the parameters\nindividually for each node. For both estimators, we show that under proper\nconditions the extra flexibility gained in our model comes at almost no cost of\nstatistical and computational efficiency. We demonstrate the advantages of our\nmodel over state-of-the-art methods on synthetic and real datasets.\n",
        "published": "2013-11-21T16:59:52Z",
        "pdf_link": "http://arxiv.org/pdf/1311.5479v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.5954v2",
        "title": "Robust Vertex Classification",
        "summary": "  For random graphs distributed according to stochastic blockmodels, a special\ncase of latent position graphs, adjacency spectral embedding followed by\nappropriate vertex classification is asymptotically Bayes optimal; but this\napproach requires knowledge of and critically depends on the model dimension.\nIn this paper, we propose a sparse representation vertex classifier which does\nnot require information about the model dimension. This classifier represents a\ntest vertex as a sparse combination of the vertices in the training set and\nuses the recovered coefficients to classify the test vertex. We prove\nconsistency of our proposed classifier for stochastic blockmodels, and\ndemonstrate that the sparse representation classifier can predict vertex labels\nwith higher accuracy than adjacency spectral embedding approaches via both\nsimulation studies and real data experiments. Our results demonstrate the\nrobustness and effectiveness of our proposed vertex classifier when the model\ndimension is unknown.\n",
        "published": "2013-11-23T03:53:31Z",
        "pdf_link": "http://arxiv.org/pdf/1311.5954v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.6182v1",
        "title": "Robust Low-rank Tensor Recovery: Models and Algorithms",
        "summary": "  Robust tensor recovery plays an instrumental role in robustifying tensor\ndecompositions for multilinear data analysis against outliers, gross\ncorruptions and missing values and has a diverse array of applications. In this\npaper, we study the problem of robust low-rank tensor recovery in a convex\noptimization framework, drawing upon recent advances in robust Principal\nComponent Analysis and tensor completion. We propose tailored optimization\nalgorithms with global convergence guarantees for solving both the constrained\nand the Lagrangian formulations of the problem. These algorithms are based on\nthe highly efficient alternating direction augmented Lagrangian and accelerated\nproximal gradient methods. We also propose a nonconvex model that can often\nimprove the recovery results from the convex models. We investigate the\nempirical recoverability properties of the convex and nonconvex formulations\nand compare the computational performance of the algorithms on simulated data.\nWe demonstrate through a number of real applications the practical\neffectiveness of this convex optimization framework for robust low-rank tensor\nrecovery.\n",
        "published": "2013-11-24T22:41:20Z",
        "pdf_link": "http://arxiv.org/pdf/1311.6182v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.6359v3",
        "title": "Score-based Causal Learning in Additive Noise Models",
        "summary": "  Given data sampled from a number of variables, one is often interested in the\nunderlying causal relationships in the form of a directed acyclic graph. In the\ngeneral case, without interventions on some of the variables it is only\npossible to identify the graph up to its Markov equivalence class. However, in\nsome situations one can find the true causal graph just from observational\ndata, for example in structural equation models with additive noise and\nnonlinear edge functions. Most current methods for achieving this rely on\nnonparametric independence tests. One of the problems there is that the null\nhypothesis is independence, which is what one would like to get evidence for.\nWe take a different approach in our work by using a penalized likelihood as a\nscore for model selection. This is practically feasible in many settings and\nhas the advantage of yielding a natural ranking of the candidate models. When\nmaking smoothness assumptions on the probability density space, we prove\nconsistency of the penalized maximum likelihood estimator. We also present\nempirical results for simulated scenarios and real two-dimensional data sets\n(cause-effect pairs) where we obtain similar results as other state-of-the-art\nmethods.\n",
        "published": "2013-11-25T16:42:57Z",
        "pdf_link": "http://arxiv.org/pdf/1311.6359v3"
    },
    {
        "id": "http://arxiv.org/abs/1401.0086v2",
        "title": "Forward-Backward Greedy Algorithms for General Convex Smooth Functions\n  over A Cardinality Constraint",
        "summary": "  We consider forward-backward greedy algorithms for solving sparse feature\nselection problems with general convex smooth functions. A state-of-the-art\ngreedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to\nsolve a large number of optimization problems, thus it is not scalable for\nlarge-size problems. The FoBa-gdt algorithm, which uses the gradient\ninformation for feature selection at each forward iteration, significantly\nimproves the efficiency of FoBa-obj. In this paper, we systematically analyze\nthe theoretical properties of both forward-backward greedy algorithms. Our main\ncontributions are: 1) We derive better theoretical bounds than existing\nanalyses regarding FoBa-obj for general smooth convex functions; 2) We show\nthat FoBa-gdt achieves the same theoretical performance as FoBa-obj under the\nsame condition: restricted strong convexity condition. Our new bounds are\nconsistent with the bounds of a special case (least squares) and fills a\npreviously existing theoretical gap for general convex smooth functions; 3) We\nshow that the restricted strong convexity condition is satisfied if the number\nof independent samples is more than $\\bar{k}\\log d$ where $\\bar{k}$ is the\nsparsity number and $d$ is the dimension of the variable; 4) We apply FoBa-gdt\n(with the conditional random field objective) to the sensor selection problem\nfor human indoor activity recognition and our results show that FoBa-gdt\noutperforms other methods (including the ones based on forward greedy selection\nand L1-regularization).\n",
        "published": "2013-12-31T04:00:31Z",
        "pdf_link": "http://arxiv.org/pdf/1401.0086v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.2451v1",
        "title": "Online Matrix Completion Through Nuclear Norm Regularisation",
        "summary": "  It is the main goal of this paper to propose a novel method to perform matrix\ncompletion on-line. Motivated by a wide variety of applications, ranging from\nthe design of recommender systems to sensor network localization through\nseismic data reconstruction, we consider the matrix completion problem when\nentries of the matrix of interest are observed gradually. Precisely, we place\nourselves in the situation where the predictive rule should be refined\nincrementally, rather than recomputed from scratch each time the sample of\nobserved entries increases. The extension of existing matrix completion methods\nto the sequential prediction context is indeed a major issue in the Big Data\nera, and yet little addressed in the literature. The algorithm promoted in this\narticle builds upon the Soft Impute approach introduced in Mazumder et al.\n(2010). The major novelty essentially arises from the use of a randomised\ntechnique for both computing and updating the Singular Value Decomposition\n(SVD) involved in the algorithm. Though of disarming simplicity, the method\nproposed turns out to be very efficient, while requiring reduced computations.\nSeveral numerical experiments based on real datasets illustrating its\nperformance are displayed, together with preliminary results giving it a\ntheoretical basis.\n",
        "published": "2014-01-10T20:44:50Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2451v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2497v1",
        "title": "Multiscale Shrinkage and Lévy Processes",
        "summary": "  A new shrinkage-based construction is developed for a compressible vector\n$\\boldsymbol{x}\\in\\mathbb{R}^n$, for cases in which the components of $\\xv$ are\nnaturally associated with a tree structure. Important examples are when $\\xv$\ncorresponds to the coefficients of a wavelet or block-DCT representation of\ndata. The method we consider in detail, and for which numerical results are\npresented, is based on increments of a gamma process. However, we demonstrate\nthat the general framework is appropriate for many other types of shrinkage\npriors, all within the L\\'{e}vy process family, with the gamma process a\nspecial case. Bayesian inference is carried out by approximating the posterior\nwith samples from an MCMC algorithm, as well as by constructing a heuristic\nvariational approximation to the posterior. We also consider\nexpectation-maximization (EM) for a MAP (point) solution. State-of-the-art\nresults are manifested for compressive sensing and denoising applications, the\nlatter with spiky (non-Gaussian) noise.\n",
        "published": "2014-01-11T03:13:16Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2497v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2771v1",
        "title": "A variational Bayes framework for sparse adaptive estimation",
        "summary": "  Recently, a number of mostly $\\ell_1$-norm regularized least squares type\ndeterministic algorithms have been proposed to address the problem of\n\\emph{sparse} adaptive signal estimation and system identification. From a\nBayesian perspective, this task is equivalent to maximum a posteriori\nprobability estimation under a sparsity promoting heavy-tailed prior for the\nparameters of interest. Following a different approach, this paper develops a\nunifying framework of sparse \\emph{variational Bayes} algorithms that employ\nheavy-tailed priors in conjugate hierarchical form to facilitate posterior\ninference. The resulting fully automated variational schemes are first\npresented in a batch iterative form. Then it is shown that by properly\nexploiting the structure of the batch estimation task, new sparse adaptive\nvariational Bayes algorithms can be derived, which have the ability to impose\nand track sparsity during real-time processing in a time-varying environment.\nThe most important feature of the proposed algorithms is that they completely\neliminate the need for computationally costly parameter fine-tuning, a\nnecessary ingredient of sparse adaptive deterministic algorithms. Extensive\nsimulation results are provided to demonstrate the effectiveness of the new\nsparse variational Bayes algorithms against state-of-the-art deterministic\ntechniques for adaptive channel estimation. The results show that the proposed\nalgorithms are numerically robust and exhibit in general superior estimation\nperformance compared to their deterministic counterparts.\n",
        "published": "2014-01-13T10:14:08Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2771v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.3291v2",
        "title": "Detection of Anomalous Crowd Behavior Using Spatio-Temporal\n  Multiresolution Model and Kronecker Sum Decompositions",
        "summary": "  In this work we consider the problem of detecting anomalous spatio-temporal\nbehavior in videos. Our approach is to learn the normative multiframe pixel\njoint distribution and detect deviations from it using a likelihood based\napproach. Due to the extreme lack of available training samples relative to the\ndimension of the distribution, we use a mean and covariance approach and\nconsider methods of learning the spatio-temporal covariance in the low-sample\nregime. Our approach is to estimate the covariance using parameter reduction\nand sparse models. The first method considered is the representation of the\ncovariance as a sum of Kronecker products as in (Greenewald et al 2013), which\nis found to be an accurate approximation in this setting. We propose learning\nalgorithms relevant to our problem. We then consider the sparse multiresolution\nmodel of (Choi et al 2010) and apply the Kronecker product methods to it for\nfurther parameter reduction, as well as introducing modifications for enhanced\nefficiency and greater applicability to spatio-temporal covariance matrices. We\napply our methods to the detection of crowd behavior anomalies in the\nUniversity of Minnesota crowd anomaly dataset, and achieve competitive results.\n",
        "published": "2014-01-14T19:15:04Z",
        "pdf_link": "http://arxiv.org/pdf/1401.3291v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.3358v1",
        "title": "Survey On The Estimation Of Mutual Information Methods as a Measure of\n  Dependency Versus Correlation Analysis",
        "summary": "  In this survey, we present and compare different approaches to estimate\nMutual Information (MI) from data to analyse general dependencies between\nvariables of interest in a system. We demonstrate the performance difference of\nMI versus correlation analysis, which is only optimal in case of linear\ndependencies. First, we use a piece-wise constant Bayesian methodology using a\ngeneral Dirichlet prior. In this estimation method, we use a two-stage approach\nwhere we approximate the probability distribution first and then calculate the\nmarginal and joint entropies. Here, we demonstrate the performance of this\nBayesian approach versus the others for computing the dependency between\ndifferent variables. We also compare these with linear correlation analysis.\nFinally, we apply MI and correlation analysis to the identification of the bias\nin the determination of the aerosol optical depth (AOD) by the satellite based\nModerate Resolution Imaging Spectroradiometer (MODIS) and the ground based\nAErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurements\nby these two instruments might be different for the same location. The reason\nof this bias is explored by quantifying the dependencies between the bias and\n15 other variables including cloud cover, surface reflectivity and others.\n",
        "published": "2014-01-14T21:17:21Z",
        "pdf_link": "http://arxiv.org/pdf/1401.3358v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.3940v1",
        "title": "Nonparametric Latent Tree Graphical Models: Inference, Estimation, and\n  Structure Learning",
        "summary": "  Tree structured graphical models are powerful at expressing long range or\nhierarchical dependency among many variables, and have been widely applied in\ndifferent areas of computer science and statistics. However, existing methods\nfor parameter estimation, inference, and structure learning mainly rely on the\nGaussian or discrete assumptions, which are restrictive under many\napplications. In this paper, we propose new nonparametric methods based on\nreproducing kernel Hilbert space embeddings of distributions that can recover\nthe latent tree structures, estimate the parameters, and perform inference for\nhigh dimensional continuous and non-Gaussian variables. The usefulness of the\nproposed methods are illustrated by thorough numerical results.\n",
        "published": "2014-01-16T09:04:20Z",
        "pdf_link": "http://arxiv.org/pdf/1401.3940v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.4408v1",
        "title": "Embedding Graphs under Centrality Constraints for Network Visualization",
        "summary": "  Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates two\ngraph embedding approaches with centrality considerations to comply with node\nhierarchy. The problem is formulated first as one of constrained\nmulti-dimensional scaling (MDS), and it is solved via block coordinate descent\niterations with successive approximations and guaranteed convergence to a KKT\npoint. In addition, a regularization term enforcing graph smoothness is\nincorporated with the goal of reducing edge crossings. A second approach\nleverages the locally-linear embedding (LLE) algorithm which assumes that the\ngraph encodes data sampled from a low-dimensional manifold. Closed-form\nsolutions to the resulting centrality-constrained optimization problems are\ndetermined yielding meaningful embeddings. Experimental results demonstrate the\nefficacy of both approaches, especially for visualizing large networks on the\norder of thousands of nodes.\n",
        "published": "2014-01-17T17:16:25Z",
        "pdf_link": "http://arxiv.org/pdf/1401.4408v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.4988v2",
        "title": "Marginal Pseudo-Likelihood Learning of Markov Network structures",
        "summary": "  Undirected graphical models known as Markov networks are popular for a wide\nvariety of applications ranging from statistical physics to computational\nbiology. Traditionally, learning of the network structure has been done under\nthe assumption of chordality which ensures that efficient scoring methods can\nbe used. In general, non-chordal graphs have intractable normalizing constants\nwhich renders the calculation of Bayesian and other scores difficult beyond\nvery small-scale systems. Recently, there has been a surge of interest towards\nthe use of regularized pseudo-likelihood methods for structural learning of\nlarge-scale Markov network models, as such an approach avoids the assumption of\nchordality. The currently available methods typically necessitate the use of a\ntuning parameter to adapt the level of regularization for a particular dataset,\nwhich can be optimized for example by cross-validation. Here we introduce a\nBayesian version of pseudo-likelihood scoring of Markov networks, which enables\nan automatic regularization through marginalization over the nuisance\nparameters in the model. We prove consistency of the resulting MPL estimator\nfor the network structure via comparison with the pseudo information criterion.\nIdentification of the MPL-optimal network on a prescanned graph space is\nconsidered with both greedy hill climbing and exact pseudo-Boolean optimization\nalgorithms. We find that for reasonable sample sizes the hill climbing approach\nmost often identifies networks that are at a negligible distance from the\nrestricted global optimum. Using synthetic and existing benchmark networks, the\nmarginal pseudo-likelihood method is shown to generally perform favorably\nagainst recent popular inference methods for Markov networks.\n",
        "published": "2014-01-20T17:14:58Z",
        "pdf_link": "http://arxiv.org/pdf/1401.4988v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.5508v3",
        "title": "Hilbert Space Methods for Reduced-Rank Gaussian Process Regression",
        "summary": "  This paper proposes a novel scheme for reduced-rank Gaussian process\nregression. The method is based on an approximate series expansion of the\ncovariance function in terms of an eigenfunction expansion of the Laplace\noperator in a compact subset of $\\mathbb{R}^d$. On this approximate eigenbasis\nthe eigenvalues of the covariance function can be expressed as simple functions\nof the spectral density of the Gaussian process, which allows the GP inference\nto be solved under a computational cost scaling as $\\mathcal{O}(nm^2)$\n(initial) and $\\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis\nfunctions and $n$ data points. Furthermore, the basis functions are independent\nof the parameters of the covariance function, which allows for very fast\nhyperparameter learning. The approach also allows for rigorous error analysis\nwith Hilbert space theory, and we show that the approximation becomes exact\nwhen the size of the compact subset and the number of eigenfunctions go to\ninfinity. We also show that the convergence rate of the truncation error is\nindependent of the input dimensionality provided that the differentiability\norder of the covariance function is increases appropriately, and for the\nsquared exponential covariance function it is always bounded by ${\\sim}1/m$\nregardless of the input dimensionality. The expansion generalizes to Hilbert\nspaces with an inner product which is defined as an integral over a specified\ninput density. The method is compared to previously proposed methods\ntheoretically and through empirical tests with simulated and real data.\n",
        "published": "2014-01-21T22:24:10Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5508v3"
    },
    {
        "id": "http://arxiv.org/abs/1401.5625v1",
        "title": "Identifiability of an Integer Modular Acyclic Additive Noise Model and\n  its Causal Structure Discovery",
        "summary": "  The notion of causality is used in many situations dealing with uncertainty.\nWe consider the problem whether causality can be identified given data set\ngenerated by discrete random variables rather than continuous ones. In\nparticular, for non-binary data, thus far it was only known that causality can\nbe identified except rare cases. In this paper, we present necessary and\nsufficient condition for an integer modular acyclic additive noise (IMAN) of\ntwo variables. In addition, we relate bivariate and multivariate causal\nidentifiability in a more explicit manner, and develop a practical algorithm to\nfind the order of variables and their parent sets. We demonstrate its\nperformance in applications to artificial data and real world body motion data\nwith comparisons to conventional methods.\n",
        "published": "2014-01-22T11:23:50Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5625v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6276v1",
        "title": "The EM algorithm and the Laplace Approximation",
        "summary": "  The Laplace approximation calls for the computation of second derivatives at\nthe likelihood maximum. When the maximum is found by the EM-algorithm, there is\na convenient way to compute these derivatives. The likelihood gradient can be\nobtained from the EM-auxiliary, while the Hessian can be obtained from this\ngradient with the Pearlmutter trick.\n",
        "published": "2014-01-24T07:50:28Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6276v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6623v3",
        "title": "Near-Ideal Behavior of Compressed Sensing Algorithms",
        "summary": "  In a recent paper, it is shown that the LASSO algorithm exhibits \"near-ideal\nbehavior,\" in the following sense: Suppose $y = Az + \\eta$ where $A$ satisfies\nthe restricted isometry property (RIP) with a sufficiently small constant, and\n$\\Vert \\eta \\Vert_2 \\leq \\epsilon$. Then minimizing $\\Vert z \\Vert_1$ subject\nto $\\Vert y - Az \\Vert_2 \\leq \\epsilon$ leads to an estimate $\\hat{x}$ whose\nerror $\\Vert \\hat{x} - x \\Vert_2$ is bounded by a universal constant times the\nerror achieved by an \"oracle\" that knows the location of the nonzero components\nof $x$. In the world of optimization, the LASSO algorithm has been generalized\nin several directions such as the group LASSO, the sparse group LASSO, either\nwithout or with tree-structured overlapping groups, and most recently, the\nsorted LASSO. In this paper, it is shown that {\\it any algorithm\\/} exhibits\nnear-ideal behavior in the above sense, provided only that (i) the norm used to\ndefine the sparsity index is \"decomposable,\" (ii) the penalty norm that is\nminimized in an effort to enforce sparsity is \"$\\gamma$-decomposable,\" and\n(iii) a \"compressibility condition\" in terms of a group restricted isometry\nproperty is satisfied. Specifically, the group LASSO, and the sparse group\nLASSO (with some permissible overlap in the groups), as well as the sorted\n$\\ell_1$-norm minimization all exhibit near-ideal behavior. Explicit bounds on\nthe residual error are derived that contain previously known results as special\ncases.\n",
        "published": "2014-01-26T07:38:23Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6623v3"
    },
    {
        "id": "http://arxiv.org/abs/1401.6740v1",
        "title": "Safe Sample Screening for Support Vector Machines",
        "summary": "  Sparse classifiers such as the support vector machines (SVM) are efficient in\ntest-phases because the classifier is characterized only by a subset of the\nsamples called support vectors (SVs), and the rest of the samples (non SVs)\nhave no influence on the classification result. However, the advantage of the\nsparsity has not been fully exploited in training phases because it is\ngenerally difficult to know which sample turns out to be SV beforehand. In this\npaper, we introduce a new approach called safe sample screening that enables us\nto identify a subset of the non-SVs and screen them out prior to the training\nphase. Our approach is different from existing heuristic approaches in the\nsense that the screened samples are guaranteed to be non-SVs at the optimal\nsolution. We investigate the advantage of the safe sample screening approach\nthrough intensive numerical experiments, and demonstrate that it can\nsubstantially decrease the computational cost of the state-of-the-art SVM\nsolvers such as LIBSVM. In the current big data era, we believe that safe\nsample screening would be of great practical importance since the data size can\nbe reduced without sacrificing the optimality of the final solution.\n",
        "published": "2014-01-27T04:41:37Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6740v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.7145v1",
        "title": "Tempering by Subsampling",
        "summary": "  In this paper we demonstrate that tempering Markov chain Monte Carlo samplers\nfor Bayesian models by recursively subsampling observations without replacement\ncan improve the performance of baseline samplers in terms of effective sample\nsize per computation. We present two tempering by subsampling algorithms,\nsubsampled parallel tempering and subsampled tempered transitions. We provide\nan asymptotic analysis of the computational cost of tempering by subsampling,\nverify that tempering by subsampling costs less than traditional tempering, and\ndemonstrate both algorithms on Bayesian approaches to learning the mean of a\nhigh dimensional multivariate Normal and estimating Gaussian process\nhyperparameters.\n",
        "published": "2014-01-28T11:44:29Z",
        "pdf_link": "http://arxiv.org/pdf/1401.7145v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.8017v1",
        "title": "Sparse Bayesian Unsupervised Learning",
        "summary": "  This paper is about variable selection, clustering and estimation in an\nunsupervised high-dimensional setting. Our approach is based on fitting\nconstrained Gaussian mixture models, where we learn the number of clusters $K$\nand the set of relevant variables $S$ using a generalized Bayesian posterior\nwith a sparsity inducing prior. We prove a sparsity oracle inequality which\nshows that this procedure selects the optimal parameters $K$ and $S$. This\nprocedure is implemented using a Metropolis-Hastings algorithm, based on a\nclustering-oriented greedy proposal, which makes the convergence to the\nposterior very fast.\n",
        "published": "2014-01-30T22:40:35Z",
        "pdf_link": "http://arxiv.org/pdf/1401.8017v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.8066v2",
        "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces\n  for Manifold Regularization and Co-Regularized Multi-view Learning",
        "summary": "  This paper presents a general vector-valued reproducing kernel Hilbert spaces\n(RKHS) framework for the problem of learning an unknown functional dependency\nbetween a structured input space and a structured output space. Our formulation\nencompasses both Vector-valued Manifold Regularization and Co-regularized\nMulti-view Learning, providing in particular a unifying framework linking these\ntwo important learning approaches. In the case of the least square loss\nfunction, we provide a closed form solution, which is obtained by solving a\nsystem of linear equations. In the case of Support Vector Machine (SVM)\nclassification, our formulation generalizes in particular both the binary\nLaplacian SVM to the multi-class, multi-view settings and the multi-class\nSimplex Cone SVM to the semi-supervised, multi-view settings. The solution is\nobtained by solving a single quadratic optimization problem, as in standard\nSVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results\nobtained on the task of object recognition, using several challenging datasets,\ndemonstrate the competitiveness of our algorithms compared with other\nstate-of-the-art methods.\n",
        "published": "2014-01-31T05:29:45Z",
        "pdf_link": "http://arxiv.org/pdf/1401.8066v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.8078v1",
        "title": "Marginal and simultaneous predictive classification using stratified\n  graphical models",
        "summary": "  An inductive probabilistic classification rule must generally obey the\nprinciples of Bayesian predictive inference, such that all observed and\nunobserved stochastic quantities are jointly modeled and the parameter\nuncertainty is fully acknowledged through the posterior predictive\ndistribution. Several such rules have been recently considered and their\nasymptotic behavior has been characterized under the assumption that the\nobserved features or variables used for building a classifier are conditionally\nindependent given a simultaneous labeling of both the training samples and\nthose from an unknown origin. Here we extend the theoretical results to\npredictive classifiers acknowledging feature dependencies either through\ngraphical models or sparser alternatives defined as stratified graphical\nmodels. We also show through experimentation with both synthetic and real data\nthat the predictive classifiers based on stratified graphical models have\nconsistently best accuracy compared with the predictive classifiers based on\neither conditionally independent features or on ordinary graphical models.\n",
        "published": "2014-01-31T07:54:14Z",
        "pdf_link": "http://arxiv.org/pdf/1401.8078v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.1412v2",
        "title": "Variational Inference in Sparse Gaussian Process Regression and Latent\n  Variable Models - a Gentle Tutorial",
        "summary": "  In this tutorial we explain the inference procedures developed for the sparse\nGaussian process (GP) regression and Gaussian process latent variable model\n(GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias &\nLawrence (2010) is brief, hence getting a full picture of it requires\ncollecting results from several different sources and a substantial amount of\nalgebra to fill-in the gaps. Our main goal is thus to collect all the results\nand full derivations into one place to help speed up understanding this work.\nIn doing so we present a re-parametrisation of the inference that allows it to\nbe carried out in parallel. A secondary goal for this document is, therefore,\nto accompany our paper and open-source implementation of the parallel inference\nscheme for the models. We hope that this document will bridge the gap between\nthe equations as implemented in code and those published in the original\npapers, in order to make it easier to extend existing work. We assume prior\nknowledge of Gaussian processes and variational inference, but we also include\nreferences for further reading where appropriate.\n",
        "published": "2014-02-06T17:10:24Z",
        "pdf_link": "http://arxiv.org/pdf/1402.1412v2"
    },
    {
        "id": "http://arxiv.org/abs/1402.2148v1",
        "title": "An Algorithmic Framework for Computing Validation Performance Bounds by\n  Using Suboptimal Models",
        "summary": "  Practical model building processes are often time-consuming because many\ndifferent models must be trained and validated. In this paper, we introduce a\nnovel algorithm that can be used for computing the lower and the upper bounds\nof model validation errors without actually training the model itself. A key\nidea behind our algorithm is using a side information available from a\nsuboptimal model. If a reasonably good suboptimal model is available, our\nalgorithm can compute lower and upper bounds of many useful quantities for\nmaking inferences on the unknown target model. We demonstrate the advantage of\nour algorithm in the context of model selection for regularized learning\nproblems.\n",
        "published": "2014-02-10T13:57:32Z",
        "pdf_link": "http://arxiv.org/pdf/1402.2148v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.2499v1",
        "title": "Justifying Information-Geometric Causal Inference",
        "summary": "  Information Geometric Causal Inference (IGCI) is a new approach to\ndistinguish between cause and effect for two variables. It is based on an\nindependence assumption between input distribution and causal mechanism that\ncan be phrased in terms of orthogonality in information space. We describe two\nintuitive reinterpretations of this approach that makes IGCI more accessible to\na broader audience.\n  Moreover, we show that the described independence is related to the\nhypothesis that unsupervised learning and semi-supervised learning only works\nfor predicting the cause from the effect and not vice versa.\n",
        "published": "2014-02-11T14:24:54Z",
        "pdf_link": "http://arxiv.org/pdf/1402.2499v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.4078v2",
        "title": "Performance Limits of Dictionary Learning for Sparse Coding",
        "summary": "  We consider the problem of dictionary learning under the assumption that the\nobserved signals can be represented as sparse linear combinations of the\ncolumns of a single large dictionary matrix. In particular, we analyze the\nminimax risk of the dictionary learning problem which governs the mean squared\nerror (MSE) performance of any learning scheme, regardless of its computational\ncomplexity. By following an established information-theoretic method based on\nFanos inequality, we derive a lower bound on the minimax risk for a given\ndictionary learning problem. This lower bound yields a characterization of the\nsample-complexity, i.e., a lower bound on the required number of observations\nsuch that consistent dictionary learning schemes exist. Our bounds may be\ncompared with the performance of a given learning scheme, allowing to\ncharacterize how far the method is from optimal performance.\n",
        "published": "2014-02-17T17:39:24Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4078v2"
    },
    {
        "id": "http://arxiv.org/abs/1402.4501v3",
        "title": "A Kernel Independence Test for Random Processes",
        "summary": "  A new non parametric approach to the problem of testing the independence of\ntwo random process is developed. The test statistic is the Hilbert Schmidt\nIndependence Criterion (HSIC), which was used previously in testing\nindependence for i.i.d pairs of variables. The asymptotic behaviour of HSIC is\nestablished when computed from samples drawn from random processes. It is shown\nthat earlier bootstrap procedures which worked in the i.i.d. case will fail for\nrandom processes, and an alternative consistent estimate of the p-values is\nproposed. Tests on artificial data and real-world Forex data indicate that the\nnew test procedure discovers dependence which is missed by linear approaches,\nwhile the earlier bootstrap procedure returns an elevated number of false\npositives. The code is available online:\nhttps://github.com/kacperChwialkowski/HSIC .\n",
        "published": "2014-02-18T21:37:00Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4501v3"
    },
    {
        "id": "http://arxiv.org/abs/1402.4507v1",
        "title": "High Dimensional Semiparametric Scale-Invariant Principal Component\n  Analysis",
        "summary": "  We propose a new high dimensional semiparametric principal component analysis\n(PCA) method, named Copula Component Analysis (COCA). The semiparametric model\nassumes that, after unspecified marginally monotone transformations, the\ndistributions are multivariate Gaussian. COCA improves upon PCA and sparse PCA\nin three aspects: (i) It is robust to modeling assumptions; (ii) It is robust\nto outliers and data contamination; (iii) It is scale-invariant and yields more\ninterpretable results. We prove that the COCA estimators obtain fast estimation\nrates and are feature selection consistent when the dimension is nearly\nexponentially large relative to the sample size. Careful experiments confirm\nthat COCA outperforms sparse PCA on both synthetic and real-world datasets.\n",
        "published": "2014-02-18T21:53:33Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4507v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.4884v2",
        "title": "Le Cam meets LeCun: Deficiency and Generic Feature Learning",
        "summary": "  \"Deep Learning\" methods attempt to learn generic features in an unsupervised\nfashion from a large unlabelled data set. These generic features should perform\nas well as the best hand crafted features for any learning problem that makes\nuse of this data. We provide a definition of generic features, characterize\nwhen it is possible to learn them and provide methods closely related to the\nautoencoder and deep belief network of deep learning. In order to do so we use\nthe notion of deficiency and illustrate its value in studying certain general\nlearning problems.\n",
        "published": "2014-02-20T04:06:28Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4884v2"
    },
    {
        "id": "http://arxiv.org/abs/1402.6262v5",
        "title": "Novel Deviation Bounds for Mixture of Independent Bernoulli Variables\n  with Application to the Missing Mass",
        "summary": "  In this paper, we are concerned with obtaining distribution-free\nconcentration inequalities for mixture of independent Bernoulli variables that\nincorporate a notion of variance. Missing mass is the total probability mass\nassociated to the outcomes that have not been seen in a given sample which is\nan important quantity that connects density estimates obtained from a sample to\nthe population for discrete distributions. Therefore, we are specifically\nmotivated to apply our method to study the concentration of missing mass -\nwhich can be expressed as a mixture of Bernoulli - in a novel way.\n  We not only derive - for the first time - Bernstein-like large deviation\nbounds for the missing mass whose exponents behave almost linearly with respect\nto deviation size, but also sharpen McAllester and Ortiz (2003) and Berend and\nKontorovich (2013) for large sample sizes in the case of small deviations which\nis the most interesting case in learning theory. In the meantime, our approach\nshows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is\nresolvable in the case of missing mass in the sense that one can use standard\ninequalities but it may not lead to strong results. Thus, we postulate that our\nresults are general and can be applied to provide potentially sharp\nBernstein-like bounds under some constraints.\n",
        "published": "2014-02-25T18:05:05Z",
        "pdf_link": "http://arxiv.org/pdf/1402.6262v5"
    },
    {
        "id": "http://arxiv.org/abs/1403.1481v1",
        "title": "New Perspectives on k-Support and Cluster Norms",
        "summary": "  The $k$-support norm is a regularizer which has been successfully applied to\nsparse vector prediction problems. We show that it belongs to a general class\nof norms which can be formulated as a parameterized infimum over quadratics. We\nfurther extend the $k$-support norm to matrices, and we observe that it is a\nspecial case of the matrix cluster norm. Using this formulation we derive an\nefficient algorithm to compute the proximity operator of both norms. This\nimproves upon the standard algorithm for the $k$-support norm and allows us to\napply proximal gradient methods to the cluster norm. We also describe how to\nsolve regularization problems which employ centered versions of these norms.\nFinally, we apply the matrix regularizers to different matrix completion and\nmultitask learning datasets. Our results indicate that the spectral $k$-support\nnorm and the cluster norm give state of the art performance on these problems,\nsignificantly outperforming trace norm and elastic net penalties.\n",
        "published": "2014-03-06T16:25:05Z",
        "pdf_link": "http://arxiv.org/pdf/1403.1481v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.4206v1",
        "title": "A reversible infinite HMM using normalised random measures",
        "summary": "  We present a nonparametric prior over reversible Markov chains. We use\ncompletely random measures, specifically gamma processes, to construct a\ncountably infinite graph with weighted edges. By enforcing symmetry to make the\nedges undirected we define a prior over random walks on graphs that results in\na reversible Markov chain. The resulting prior over infinite transition\nmatrices is closely related to the hierarchical Dirichlet process but enforces\nreversibility. A reinforcement scheme has recently been proposed with similar\nproperties, but the de Finetti measure is not well characterised. We take the\nalternative approach of explicitly constructing the mixing measure, which\nallows more straightforward and efficient inference at the cost of no longer\nhaving a closed form predictive distribution. We use our process to construct a\nreversible infinite HMM which we apply to two real datasets, one from\nepigenomics and one ion channel recording.\n",
        "published": "2014-03-17T18:41:54Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4206v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.4544v3",
        "title": "On the Sensitivity of the Lasso to the Number of Predictor Variables",
        "summary": "  The Lasso is a computationally efficient regression regularization procedure\nthat can produce sparse estimators when the number of predictors (p) is large.\nOracle inequalities provide probability loss bounds for the Lasso estimator at\na deterministic choice of the regularization parameter. These bounds tend to\nzero if p is appropriately controlled, and are thus commonly cited as\ntheoretical justification for the Lasso and its ability to handle\nhigh-dimensional settings. Unfortunately, in practice the regularization\nparameter is not selected to be a deterministic quantity, but is instead chosen\nusing a random, data-dependent procedure. To address this shortcoming of\nprevious theoretical work, we study the loss of the Lasso estimator when tuned\noptimally for prediction. Assuming orthonormal predictors and a sparse true\nmodel, we prove that the probability that the best possible predictive\nperformance of the Lasso deteriorates as p increases is positive and can be\narbitrarily close to one given a sufficiently high signal to noise ratio and\nsufficiently large p. We further demonstrate empirically that the amount of\ndeterioration in performance can be far worse than the oracle inequalities\nsuggest and provide a real data example where deterioration is observed.\n",
        "published": "2014-03-18T17:32:01Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4544v3"
    },
    {
        "id": "http://arxiv.org/abs/1403.5112v1",
        "title": "On The Sample Complexity of Sparse Dictionary Learning",
        "summary": "  In the synthesis model signals are represented as a sparse combinations of\natoms from a dictionary. Dictionary learning describes the acquisition process\nof the underlying dictionary for a given set of training samples. While ideally\nthis would be achieved by optimizing the expectation of the factors over the\nunderlying distribution of the training data, in practice the necessary\ninformation about the distribution is not available. Therefore, in real world\napplications it is achieved by minimizing an empirical average over the\navailable samples. The main goal of this paper is to provide a sample\ncomplexity estimate that controls to what extent the empirical average deviates\nfrom the cost function. This estimate then provides a suitable estimate to the\naccuracy of the representation of the learned dictionary. The presented\napproach exemplifies the general results proposed by the authors in Sample\nComplexity of Dictionary Learning and other Matrix Factorizations, Gribonval et\nal. and gives more concrete bounds of the sample complexity of dictionary\nlearning. We cover a variety of sparsity measures employed in the learning\nprocedure.\n",
        "published": "2014-03-20T12:30:54Z",
        "pdf_link": "http://arxiv.org/pdf/1403.5112v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.5177v1",
        "title": "Sparse Learning over Infinite Subgraph Features",
        "summary": "  We present a supervised-learning algorithm from graph data (a set of graphs)\nfor arbitrary twice-differentiable loss functions and sparse linear models over\nall possible subgraph features. To date, it has been shown that under all\npossible subgraph features, several types of sparse learning, such as Adaboost,\nLPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularly\nemphasis is placed on simultaneous learning of relevant features from an\ninfinite set of candidates. We first generalize techniques used in all these\npreceding studies to derive an unifying bounding technique for arbitrary\nseparable functions. We then carefully use this bounding to make block\ncoordinate gradient descent feasible over infinite subgraph features, resulting\nin a fast converging algorithm that can solve a wider class of sparse learning\nproblems over graph data. We also empirically study the differences from the\nexisting approaches in convergence property, selected subgraph features, and\nsearch-space sizes. We further discuss several unnoticed issues in sparse\nlearning over all possible subgraph features.\n",
        "published": "2014-03-20T15:44:56Z",
        "pdf_link": "http://arxiv.org/pdf/1403.5177v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.5994v1",
        "title": "First Order Methods for Robust Non-negative Matrix Factorization for\n  Large Scale Noisy Data",
        "summary": "  Nonnegative matrix factorization (NMF) has been shown to be identifiable\nunder the separability assumption, under which all the columns(or rows) of the\ninput data matrix belong to the convex cone generated by only a few of these\ncolumns(or rows) [1]. In real applications, however, such separability\nassumption is hard to satisfy. Following [4] and [5], in this paper, we look at\nthe Linear Programming (LP) based reformulation to locate the extreme rays of\nthe convex cone but in a noisy setting. Furthermore, in order to deal with the\nlarge scale data, we employ First-Order Methods (FOM) to mitigate the\ncomputational complexity of LP, which primarily results from a large number of\nconstraints. We show the performance of the algorithm on real and synthetic\ndata sets.\n",
        "published": "2014-03-24T15:22:12Z",
        "pdf_link": "http://arxiv.org/pdf/1403.5994v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.6499v2",
        "title": "Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation",
        "summary": "  In this paper, we consider low rank matrix estimation using either\nmatrix-version Dantzig Selector $\\hat{A}_{\\lambda}^d$ or matrix-version LASSO\nestimator $\\hat{A}_{\\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$,\nthe measurements $X_1,\\ldots,X_n\\in\\mathbb{R}^{m\\times m}$ have $i.i.d.$\nsub-Gaussian entries. Suppose $\\textrm{rank}(A_0)=r$. We proved that, when\n$n\\geq Cm[r^2\\vee r\\log(m)\\log(n)]$ for some $C>0$, both $\\hat{A}_{\\lambda}^d$\nand $\\hat{A}_{\\lambda}^L$ can obtain optimal upper bounds(except some\nlogarithmic terms) for estimation accuracy under spectral norm. By applying\nmetric entropy of Grassmann manifolds, we construct (near) matching minimax\nlower bound for estimation accuracy under spectral norm. We also give upper\nbounds and matching minimax lower bound(except some logarithmic terms) for\nestimation accuracy under Schatten-q norm for every $1\\leq q\\leq\\infty$. As a\ndirect corollary, we show both upper bounds and minimax lower bounds of\nestimation accuracy under Ky-Fan-k norms for every $1\\leq k\\leq m$.\n",
        "published": "2014-03-25T20:33:34Z",
        "pdf_link": "http://arxiv.org/pdf/1403.6499v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.7267v1",
        "title": "Systematic Ensemble Learning for Regression",
        "summary": "  The motivation of this work is to improve the performance of standard\nstacking approaches or ensembles, which are composed of simple, heterogeneous\nbase models, through the integration of the generation and selection stages for\nregression problems. We propose two extensions to the standard stacking\napproach. In the first extension we combine a set of standard stacking\napproaches into an ensemble of ensembles using a two-step ensemble learning in\nthe regression setting. The second extension consists of two parts. In the\ninitial part a diversity mechanism is injected into the original training data\nset, systematically generating different training subsets or partitions, and\ncorresponding ensembles of ensembles. In the final part after measuring the\nquality of the different partitions or ensembles, a max-min rule-based\nselection algorithm is used to select the most appropriate ensemble/partition\non which to make the final prediction. We show, based on experiments over a\nbroad range of data sets, that the second extension performs better than the\nbest of the standard stacking approaches, and is as good as the oracle of\ndatabases, which has the best base model selected by cross-validation for each\ndata set. In addition to that, the second extension performs better than two\nstate-of-the-art ensemble methods for regression, and it is as good as a third\nstate-of-the-art ensemble method.\n",
        "published": "2014-03-28T01:36:27Z",
        "pdf_link": "http://arxiv.org/pdf/1403.7267v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.7304v3",
        "title": "Characteristic Kernels and Infinitely Divisible Distributions",
        "summary": "  We connect shift-invariant characteristic kernels to infinitely divisible\ndistributions on $\\mathbb{R}^{d}$. Characteristic kernels play an important\nrole in machine learning applications with their kernel means to distinguish\nany two probability measures. The contribution of this paper is two-fold.\nFirst, we show, using the L\\'evy-Khintchine formula, that any shift-invariant\nkernel given by a bounded, continuous and symmetric probability density\nfunction (pdf) of an infinitely divisible distribution on $\\mathbb{R}^d$ is\ncharacteristic. We also present some closure property of such characteristic\nkernels under addition, pointwise product, and convolution. Second, in\ndeveloping various kernel mean algorithms, it is fundamental to compute the\nfollowing values: (i) kernel mean values $m_P(x)$, $x \\in \\mathcal{X}$, and\n(ii) kernel mean RKHS inner products ${\\left\\langle m_P, m_Q\n\\right\\rangle_{\\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and\nkernel $k$ are Gaussians, then computation (i) and (ii) results in Gaussian\npdfs that is tractable. We generalize this Gaussian combination to more general\ncases in the class of infinitely divisible distributions. We then introduce a\n{\\it conjugate} kernel and {\\it convolution trick}, so that the above (i) and\n(ii) have the same pdf form, expecting tractable computation at least in some\ncases. As specific instances, we explore $\\alpha$-stable distributions and a\nrich class of generalized hyperbolic distributions, where the Laplace, Cauchy\nand Student-t distributions are included.\n",
        "published": "2014-03-28T08:41:28Z",
        "pdf_link": "http://arxiv.org/pdf/1403.7304v3"
    },
    {
        "id": "http://arxiv.org/abs/1404.0752v1",
        "title": "An Efficient Search Strategy for Aggregation and Discretization of\n  Attributes of Bayesian Networks Using Minimum Description Length",
        "summary": "  Bayesian networks are convenient graphical expressions for high dimensional\nprobability distributions representing complex relationships between a large\nnumber of random variables. They have been employed extensively in areas such\nas bioinformatics, artificial intelligence, diagnosis, and risk management. The\nrecovery of the structure of a network from data is of prime importance for the\npurposes of modeling, analysis, and prediction. Most recovery algorithms in the\nliterature assume either discrete of continuous but Gaussian data. For general\ncontinuous data, discretization is usually employed but often destroys the very\nstructure one is out to recover. Friedman and Goldszmidt suggest an approach\nbased on the minimum description length principle that chooses a discretization\nwhich preserves the information in the original data set, however it is one\nwhich is difficult, if not impossible, to implement for even moderately sized\nnetworks. In this paper we provide an extremely efficient search strategy which\nallows one to use the Friedman and Goldszmidt discretization in practice.\n",
        "published": "2014-04-03T03:15:26Z",
        "pdf_link": "http://arxiv.org/pdf/1404.0752v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.1238v3",
        "title": "Exact Estimation of Multiple Directed Acyclic Graphs",
        "summary": "  This paper considers the problem of estimating the structure of multiple\nrelated directed acyclic graph (DAG) models. Building on recent developments in\nexact estimation of DAGs using integer linear programming (ILP), we present an\nILP approach for joint estimation over multiple DAGs, that does not require\nthat the vertices in each DAG share a common ordering. Furthermore, we allow\nalso for (potentially unknown) dependency structure between the DAGs. Results\nare presented on both simulated data and fMRI data obtained from multiple\nsubjects.\n",
        "published": "2014-04-04T12:50:48Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1238v3"
    },
    {
        "id": "http://arxiv.org/abs/1404.1361v3",
        "title": "Learning the Conditional Independence Structure of Stationary Time\n  Series: A Multitask Learning Approach",
        "summary": "  We propose a method for inferring the conditional independence graph (CIG) of\na high-dimensional Gaussian vector time series (discrete-time process) from a\nfinite-length observation. By contrast to existing approaches, we do not rely\non a parametric process model (such as, e.g., an autoregressive model) for the\nobserved random process. Instead, we only require certain smoothness properties\n(in the Fourier domain) of the process. The proposed inference scheme works\neven for sample sizes much smaller than the number of scalar process components\nif the true underlying CIG is sufficiently sparse. A theoretical performance\nanalysis provides conditions which guarantee that the probability of the\nproposed inference method to deliver a wrong CIG is below a prescribed value.\nThese conditions imply lower bounds on the sample size such that the new method\nis consistent asymptotically. Some numerical experiments validate our\ntheoretical performance analysis and demonstrate superior performance of our\nscheme compared to an existing (parametric) approach in case of model mismatch.\n",
        "published": "2014-04-04T19:41:34Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1361v3"
    },
    {
        "id": "http://arxiv.org/abs/1404.1425v4",
        "title": "Density Estimation via Discrepancy Based Adaptive Sequential Partition",
        "summary": "  Given $iid$ observations from an unknown absolute continuous distribution\ndefined on some domain $\\Omega$, we propose a nonparametric method to learn a\npiecewise constant function to approximate the underlying probability density\nfunction. Our density estimate is a piecewise constant function defined on a\nbinary partition of $\\Omega$. The key ingredient of the algorithm is to use\ndiscrepancy, a concept originates from Quasi Monte Carlo analysis, to control\nthe partition process. The resulting algorithm is simple, efficient, and has a\nprovable convergence rate. We empirically demonstrate its efficiency as a\ndensity estimation method. We present its applications on a wide range of\ntasks, including finding good initializations for k-means.\n",
        "published": "2014-04-05T03:43:28Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1425v4"
    },
    {
        "id": "http://arxiv.org/abs/1404.1935v2",
        "title": "Tyler's Covariance Matrix Estimator in Elliptical Models with Convex\n  Structure",
        "summary": "  We address structured covariance estimation in elliptical distributions by\nassuming that the covariance is a priori known to belong to a given convex set,\ne.g., the set of Toeplitz or banded matrices. We consider the General Method of\nMoments (GMM) optimization applied to robust Tyler's scatter M-estimator\nsubject to these convex constraints. Unfortunately, GMM turns out to be\nnon-convex due to the objective. Instead, we propose a new COCA estimator - a\nconvex relaxation which can be efficiently solved. We prove that the relaxation\nis tight in the unconstrained case for a finite number of samples, and in the\nconstrained case asymptotically. We then illustrate the advantages of COCA in\nsynthetic simulations with structured compound Gaussian distributions. In these\nexamples, COCA outperforms competing methods such as Tyler's estimator and its\nprojection onto the structure set.\n",
        "published": "2014-04-07T20:09:39Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1935v2"
    },
    {
        "id": "http://arxiv.org/abs/1404.2007v1",
        "title": "A Permutation Approach for Selecting the Penalty Parameter in Penalized\n  Model Selection",
        "summary": "  We describe a simple, efficient, permutation based procedure for selecting\nthe penalty parameter in the LASSO. The procedure, which is intended for\napplications where variable selection is the primary focus, can be applied in a\nvariety of structural settings, including generalized linear models. We briefly\ndiscuss connections between permutation selection and existing theory for the\nLASSO. In addition, we present a simulation study and an analysis of three real\ndata sets in which permutation selection is compared with cross-validation\n(CV), the Bayesian information criterion (BIC), and a selection method based on\nrecently developed testing procedures for the LASSO.\n",
        "published": "2014-04-08T04:44:59Z",
        "pdf_link": "http://arxiv.org/pdf/1404.2007v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.2124v1",
        "title": "A Naive Bayes machine learning approach to risk prediction using\n  censored, time-to-event data",
        "summary": "  Predicting an individual's risk of experiencing a future clinical outcome is\na statistical task with important consequences for both practicing clinicians\nand public health experts. Modern observational databases such as electronic\nhealth records (EHRs) provide an alternative to the longitudinal cohort studies\ntraditionally used to construct risk models, bringing with them both\nopportunities and challenges. Large sample sizes and detailed covariate\nhistories enable the use of sophisticated machine learning techniques to\nuncover complex associations and interactions, but observational databases are\noften ``messy,'' with high levels of missing data and incomplete patient\nfollow-up. In this paper, we propose an adaptation of the well-known Naive\nBayes (NB) machine learning approach for classification to time-to-event\noutcomes subject to censoring. We compare the predictive performance of our\nmethod to the Cox proportional hazards model which is commonly used for risk\nprediction in healthcare populations, and illustrate its application to\nprediction of cardiovascular risk using an EHR dataset from a large Midwest\nintegrated healthcare system.\n",
        "published": "2014-04-08T13:34:29Z",
        "pdf_link": "http://arxiv.org/pdf/1404.2124v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5028v1",
        "title": "Clustering via Mode Seeking by Direct Estimation of the Gradient of a\n  Log-Density",
        "summary": "  Mean shift clustering finds the modes of the data probability density by\nidentifying the zero points of the density gradient. Since it does not require\nto fix the number of clusters in advance, the mean shift has been a popular\nclustering algorithm in various application fields. A typical implementation of\nthe mean shift is to first estimate the density by kernel density estimation\nand then compute its gradient. However, since good density estimation does not\nnecessarily imply accurate estimation of the density gradient, such an indirect\ntwo-step approach is not reliable. In this paper, we propose a method to\ndirectly estimate the gradient of the log-density without going through density\nestimation. The proposed method gives the global solution analytically and thus\nis computationally efficient. We then develop a mean-shift-like fixed-point\nalgorithm to find the modes of the density for clustering. As in the mean\nshift, one does not need to set the number of clusters in advance. We\nempirically show that the proposed clustering method works much better than the\nmean shift especially for high-dimensional data. Experimental results further\nindicate that the proposed method outperforms existing clustering methods.\n",
        "published": "2014-04-20T08:43:44Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5028v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5443v1",
        "title": "Approximate Inference for Nonstationary Heteroscedastic Gaussian process\n  Regression",
        "summary": "  This paper presents a novel approach for approximate integration over the\nuncertainty of noise and signal variances in Gaussian process (GP) regression.\nOur efficient and straightforward approach can also be applied to integration\nover input dependent noise variance (heteroscedasticity) and input dependent\nsignal variance (nonstationarity) by setting independent GP priors for the\nnoise and signal variances. We use expectation propagation (EP) for inference\nand compare results to Markov chain Monte Carlo in two simulated data sets and\nthree empirical examples. The results show that EP produces comparable results\nwith less computational burden.\n",
        "published": "2014-04-22T10:04:10Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5443v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5793v2",
        "title": "Bayesian Reconstruction of Missing Observations",
        "summary": "  We focus on an interpolation method referred to Bayesian reconstruction in\nthis paper. Whereas in standard interpolation methods missing data are\ninterpolated deterministically, in Bayesian reconstruction, missing data are\ninterpolated probabilistically using a Bayesian treatment. In this paper, we\naddress the framework of Bayesian reconstruction and its application to the\ntraffic data reconstruction problem in the field of traffic engineering. In the\nlatter part of this paper, we describe the evaluation of the statistical\nperformance of our Bayesian traffic reconstruction model using a statistical\nmechanical approach and clarify its statistical behavior.\n",
        "published": "2014-04-23T12:02:59Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5793v2"
    },
    {
        "id": "http://arxiv.org/abs/1404.7236v1",
        "title": "High Dimensional Semiparametric Latent Graphical Model for Mixed Data",
        "summary": "  Graphical models are commonly used tools for modeling multivariate random\nvariables. While there exist many convenient multivariate distributions such as\nGaussian distribution for continuous data, mixed data with the presence of\ndiscrete variables or a combination of both continuous and discrete variables\nposes new challenges in statistical modeling. In this paper, we propose a\nsemiparametric model named latent Gaussian copula model for binary and mixed\ndata. The observed binary data are assumed to be obtained by dichotomizing a\nlatent variable satisfying the Gaussian copula distribution or the\nnonparanormal distribution. The latent Gaussian model with the assumption that\nthe latent variables are multivariate Gaussian is a special case of the\nproposed model. A novel rank-based approach is proposed for both latent graph\nestimation and latent principal component analysis. Theoretically, the proposed\nmethods achieve the same rates of convergence for both precision matrix\nestimation and eigenvector estimation, as if the latent variables were\nobserved. Under similar conditions, the consistency of graph structure recovery\nand feature selection for leading eigenvectors is established. The performance\nof the proposed methods is numerically assessed through simulation studies, and\nthe usage of our methods is illustrated by a genetic dataset.\n",
        "published": "2014-04-29T05:12:50Z",
        "pdf_link": "http://arxiv.org/pdf/1404.7236v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.0581v10",
        "title": "Support Consistency of Direct Sparse-Change Learning in Markov Networks",
        "summary": "  We study the problem of learning sparse structure changes between two Markov\nnetworks $P$ and $Q$. Rather than fitting two Markov networks separately to two\nsets of data and figuring out their differences, a recent work proposed to\nlearn changes \\emph{directly} via estimating the ratio between two Markov\nnetwork models. In this paper, we give sufficient conditions for\n\\emph{successful change detection} with respect to the sample size $n_p, n_q$,\nthe dimension of data $m$, and the number of changed edges $d$. When using an\nunbounded density ratio model we prove that the true sparse changes can be\nconsistently identified for $n_p = \\Omega(d^2 \\log \\frac{m^2+m}{2})$ and $n_q =\n\\Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error.\nSuch sample complexity can be improved to $\\min(n_p, n_q) = \\Omega(d^2 \\log\n\\frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed.\nOur theoretical guarantee can be applied to a wide range of discrete/continuous\nMarkov networks.\n",
        "published": "2014-07-02T14:27:48Z",
        "pdf_link": "http://arxiv.org/pdf/1407.0581v10"
    },
    {
        "id": "http://arxiv.org/abs/1407.4508v2",
        "title": "Large scale canonical correlation analysis with iterative least squares",
        "summary": "  Canonical Correlation Analysis (CCA) is a widely used statistical tool with\nboth well established theory and favorable performance for a wide range of\nmachine learning problems. However, computing CCA for huge datasets can be very\nslow since it involves implementing QR decomposition or singular value\ndecomposition of huge matrices. In this paper we introduce L-CCA, a iterative\nalgorithm which can compute CCA fast on huge sparse datasets. Theory on both\nthe asymptotic convergence and finite time accuracy of L-CCA are established.\nThe experiments also show that L-CCA outperform other fast CCA approximation\nschemes on two real datasets.\n",
        "published": "2014-07-16T21:51:07Z",
        "pdf_link": "http://arxiv.org/pdf/1407.4508v2"
    },
    {
        "id": "http://arxiv.org/abs/1407.5602v1",
        "title": "Predictive support recovery with TV-Elastic Net penalty and logistic\n  regression: an application to structural MRI",
        "summary": "  The use of machine-learning in neuroimaging offers new perspectives in early\ndiagnosis and prognosis of brain diseases. Although such multivariate methods\ncan capture complex relationships in the data, traditional approaches provide\nirregular (l2 penalty) or scattered (l1 penalty) predictive pattern with a very\nlimited relevance. A penalty like Total Variation (TV) that exploits the\nnatural 3D structure of the images can increase the spatial coherence of the\nweight map. However, TV penalization leads to non-smooth optimization problems\nthat are hard to minimize. We propose an optimization framework that minimizes\nany combination of l1, l2, and TV penalties while preserving the exact l1\npenalty. This algorithm uses Nesterov's smoothing technique to approximate the\nTV penalty with a smooth function such that the loss and the penalties are\nminimized with an exact accelerated proximal gradient algorithm. We propose an\noriginal continuation algorithm that uses successively smaller values of the\nsmoothing parameter to reach a prescribed precision while achieving the best\npossible convergence rate. This algorithm can be used with other losses or\npenalties. The algorithm is applied on a classification problem on the ADNI\ndataset. We observe that the TV penalty does not necessarily improve the\nprediction but provides a major breakthrough in terms of support recovery of\nthe predictive brain regions.\n",
        "published": "2014-07-21T19:12:59Z",
        "pdf_link": "http://arxiv.org/pdf/1407.5602v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.5924v1",
        "title": "Resolution-limit-free and local Non-negative Matrix Factorization\n  quality functions for graph clustering",
        "summary": "  Many graph clustering quality functions suffer from a resolution limit, the\ninability to find small clusters in large graphs. So called\nresolution-limit-free quality functions do not have this limit. This property\nwas previously introduced for hard clustering, that is, graph partitioning.\n  We investigate the resolution-limit-free property in the context of\nNon-negative Matrix Factorization (NMF) for hard and soft graph clustering. To\nuse NMF in the hard clustering setting, a common approach is to assign each\nnode to its highest membership cluster. We show that in this case symmetric NMF\nis not resolution-limit-free, but that it becomes so when hardness constraints\nare used as part of the optimization. The resulting function is strongly linked\nto the Constant Potts Model. In soft clustering, nodes can belong to more than\none cluster, with varying degrees of membership. In this setting\nresolution-limit-free turns out to be too strong a property. Therefore we\nintroduce locality, which roughly states that changing one part of the graph\ndoes not affect the clustering of other parts of the graph. We argue that this\nis a desirable property, provide conditions under which NMF quality functions\nare local, and propose a novel class of local probabilistic NMF quality\nfunctions for soft graph clustering.\n",
        "published": "2014-07-22T16:21:59Z",
        "pdf_link": "http://arxiv.org/pdf/1407.5924v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.6949v1",
        "title": "Efficient Bayesian Nonparametric Modelling of Structured Point Processes",
        "summary": "  This paper presents a Bayesian generative model for dependent Cox point\nprocesses, alongside an efficient inference scheme which scales as if the point\nprocesses were modelled independently. We can handle missing data naturally,\ninfer latent structure, and cope with large numbers of observed processes. A\nfurther novel contribution enables the model to work effectively in higher\ndimensional spaces. Using this method, we achieve vastly improved predictive\nperformance on both 2D and 1D real data, validating our structured approach.\n",
        "published": "2014-07-25T15:57:49Z",
        "pdf_link": "http://arxiv.org/pdf/1407.6949v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.7502v3",
        "title": "Understanding Random Forests: From Theory to Practice",
        "summary": "  Data analysis and machine learning have become an integrative part of the\nmodern scientific methodology, offering automated procedures for the prediction\nof a phenomenon based on past observations, unraveling underlying patterns in\ndata and providing insights about the problem. Yet, caution should avoid using\nmachine learning as a black-box tool, but rather consider it as a methodology,\nwith a rational thought process that is entirely dependent on the problem under\nstudy. In particular, the use of algorithms should ideally require a reasonable\nunderstanding of their mechanisms, properties and limitations, in order to\nbetter apprehend and interpret their results.\n  Accordingly, the goal of this thesis is to provide an in-depth analysis of\nrandom forests, consistently calling into question each and every part of the\nalgorithm, in order to shed new light on its learning capabilities, inner\nworkings and interpretability. The first part of this work studies the\ninduction of decision trees and the construction of ensembles of randomized\ntrees, motivating their design and purpose whenever possible. Our contributions\nfollow with an original complexity analysis of random forests, showing their\ngood computational performance and scalability, along with an in-depth\ndiscussion of their implementation details, as contributed within Scikit-Learn.\n  In the second part of this work, we analyse and discuss the interpretability\nof random forests in the eyes of variable importance measures. The core of our\ncontributions rests in the theoretical characterization of the Mean Decrease of\nImpurity variable importance measure, from which we prove and derive some of\nits properties in the case of multiway totally randomized trees and in\nasymptotic conditions. In consequence of this work, our analysis demonstrates\nthat variable importances [...].\n",
        "published": "2014-07-28T19:16:02Z",
        "pdf_link": "http://arxiv.org/pdf/1407.7502v3"
    },
    {
        "id": "http://arxiv.org/abs/1407.7840v1",
        "title": "Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis",
        "summary": "  Matrix factorization (MF) has become a common approach to collaborative\nfiltering, due to ease of implementation and scalability to large data sets.\nTwo existing drawbacks of the basic model is that it does not incorporate side\ninformation on either users or items, and assumes a common variance for all\nusers. We extend the work of constrained probabilistic matrix factorization by\nderiving the Gibbs updates for the side feature vectors for items\n(Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to the\nconstrained PMF model outperforms simple MAP estimation. We also consider\nextensions to heteroskedastic precision introduced in the literature\n(Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tends\nresult in overfitting for deterministic approximation algorithms (ex:\nVariational inference) when the observed entries in the user / item matrix are\ndistributed in an non-uniform manner. In light of this, we propose a truncated\nprecision model. Our experimental results suggest that this model tends to\ndelay overfitting.\n",
        "published": "2014-07-29T19:38:32Z",
        "pdf_link": "http://arxiv.org/pdf/1407.7840v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.7969v1",
        "title": "Automated Machine Learning on Big Data using Stochastic Algorithm Tuning",
        "summary": "  We introduce a means of automating machine learning (ML) for big data tasks,\nby performing scalable stochastic Bayesian optimisation of ML algorithm\nparameters and hyper-parameters. More often than not, the critical tuning of ML\nalgorithm parameters has relied on domain expertise from experts, along with\nlaborious hand-tuning, brute search or lengthy sampling runs. Against this\nbackground, Bayesian optimisation is finding increasing use in automating\nparameter tuning, making ML algorithms accessible even to non-experts. However,\nthe state of the art in Bayesian optimisation is incapable of scaling to the\nlarge number of evaluations of algorithm performance required to fit realistic\nmodels to complex, big data. We here describe a stochastic, sparse, Bayesian\noptimisation strategy to solve this problem, using many thousands of noisy\nevaluations of algorithm performance on subsets of data in order to effectively\ntrain algorithms for big data. We provide a comprehensive benchmarking of\npossible sparsification strategies for Bayesian optimisation, concluding that a\nNystrom approximation offers the best scaling and performance for real tasks.\nOur proposed algorithm demonstrates substantial improvement over the state of\nthe art in tuning the parameters of a Gaussian Process time series prediction\ntask on real, big data.\n",
        "published": "2014-07-30T08:29:38Z",
        "pdf_link": "http://arxiv.org/pdf/1407.7969v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.0145v2",
        "title": "A convergence and asymptotic analysis of the generalized symmetric\n  FastICA algorithm",
        "summary": "  This contribution deals with the generalized symmetric FastICA algorithm in\nthe domain of Independent Component Analysis (ICA). The generalized symmetric\nversion of FastICA has been shown to have the potential to achieve the\nCram\\'er-Rao Bound (CRB) by allowing the usage of different nonlinearity\nfunctions in its parallel implementations of one-unit FastICA. In spite of this\nappealing property, a rigorous study of the asymptotic error of the generalized\nsymmetric FastICA algorithm is still missing in the community. In fact, all the\nexisting results exhibit certain limitations, such as ignoring the impact of\ndata standardization on the asymptotic statistics or being based on a heuristic\napproach.\n  In this work, we aim at filling this blank.\n  The first result of this contribution is the characterization of the limits\nof the generalized symmetric FastICA. It is shown that the algorithm optimizes\na function that is a sum of the contrast functions used by traditional one-unit\nFastICA with a correction of the sign. Based on this characterization, we\nderive a closed-form analytic expression of the asymptotic covariance matrix of\nthe generalized symmetric FastICA estimator using the method of estimating\nequation and M-estimator.\n",
        "published": "2014-08-01T12:07:53Z",
        "pdf_link": "http://arxiv.org/pdf/1408.0145v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.0337v1",
        "title": "A Bayesian estimation approach to analyze non-Gaussian data-generating\n  processes with latent classes",
        "summary": "  A large amount of observational data has been accumulated in various fields\nin recent times, and there is a growing need to estimate the generating\nprocesses of these data. A linear non-Gaussian acyclic model (LiNGAM) based on\nthe non-Gaussianity of external influences has been proposed to estimate the\ndata-generating processes of variables. However, the results of the estimation\ncan be biased if there are latent classes. In this paper, we first review\nLiNGAM, its extended model, as well as the estimation procedure for LiNGAM in a\nBayesian framework. We then propose a new Bayesian estimation procedure that\nsolves the problem.\n",
        "published": "2014-08-02T04:31:56Z",
        "pdf_link": "http://arxiv.org/pdf/1408.0337v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.0850v5",
        "title": "L0 Sparse Inverse Covariance Estimation",
        "summary": "  Recently, there has been focus on penalized log-likelihood covariance\nestimation for sparse inverse covariance (precision) matrices. The penalty is\nresponsible for inducing sparsity, and a very common choice is the convex $l_1$\nnorm. However, the best estimator performance is not always achieved with this\npenalty. The most natural sparsity promoting \"norm\" is the non-convex $l_0$\npenalty but its lack of convexity has deterred its use in sparse maximum\nlikelihood estimation. In this paper we consider non-convex $l_0$ penalized\nlog-likelihood inverse covariance estimation and present a novel cyclic descent\nalgorithm for its optimization. Convergence to a local minimizer is proved,\nwhich is highly non-trivial, and we demonstrate via simulations the reduced\nbias and superior quality of the $l_0$ penalty as compared to the $l_1$\npenalty.\n",
        "published": "2014-08-05T02:15:46Z",
        "pdf_link": "http://arxiv.org/pdf/1408.0850v5"
    },
    {
        "id": "http://arxiv.org/abs/1408.1336v2",
        "title": "On the Generalization of the C-Bound to Structured Output Ensemble\n  Methods",
        "summary": "  This paper generalizes an important result from the PAC-Bayesian literature\nfor binary classification to the case of ensemble methods for structured\noutputs. We prove a generic version of the \\Cbound, an upper bound over the\nrisk of models expressed as a weighted majority vote that is based on the first\nand second statistical moments of the vote's margin. This bound may\nadvantageously $(i)$ be applied on more complex outputs such as multiclass\nlabels and multilabel, and $(ii)$ allow to consider margin relaxations. These\nresults open the way to develop new ensemble methods for structured output\nprediction with PAC-Bayesian guarantees.\n",
        "published": "2014-08-06T15:50:30Z",
        "pdf_link": "http://arxiv.org/pdf/1408.1336v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.2714v2",
        "title": "Learning From Non-iid Data: Fast Rates for the One-vs-All Multiclass\n  Plug-in Classifiers",
        "summary": "  We prove new fast learning rates for the one-vs-all multiclass plug-in\nclassifiers trained either from exponentially strongly mixing data or from data\ngenerated by a converging drifting distribution. These are two typical\nscenarios where training data are not iid. The learning rates are obtained\nunder a multiclass version of Tsybakov's margin assumption, a type of low-noise\nassumption, and do not depend on the number of classes. Our results are general\nand include a previous result for binary-class plug-in classifiers with iid\ndata as a special case. In contrast to previous works for least squares SVMs\nunder the binary-class setting, our results retain the optimal learning rate in\nthe iid case.\n",
        "published": "2014-08-12T14:04:24Z",
        "pdf_link": "http://arxiv.org/pdf/1408.2714v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.3378v2",
        "title": "Beta diffusion trees and hierarchical feature allocations",
        "summary": "  We define the beta diffusion tree, a random tree structure with a set of\nleaves that defines a collection of overlapping subsets of objects, known as a\nfeature allocation. A generative process for the tree structure is defined in\nterms of particles (representing the objects) diffusing in some continuous\nspace, analogously to the Dirichlet diffusion tree (Neal, 2003), which defines\na tree structure over partitions (i.e., non-overlapping subsets) of the\nobjects. Unlike in the Dirichlet diffusion tree, multiple copies of a particle\nmay exist and diffuse along multiple branches in the beta diffusion tree, and\nan object may therefore belong to multiple subsets of particles. We demonstrate\nhow to build a hierarchically-clustered factor analysis model with the beta\ndiffusion tree and how to perform inference over the random tree structures\nwith a Markov chain Monte Carlo algorithm. We conclude with several numerical\nexperiments on missing data problems with data sets of gene expression\nmicroarrays, international development statistics, and intranational\nsocioeconomic measurements.\n",
        "published": "2014-08-14T18:29:20Z",
        "pdf_link": "http://arxiv.org/pdf/1408.3378v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.5032v1",
        "title": "On the Sample Complexity of Subspace Learning",
        "summary": "  A large number of algorithms in machine learning, from principal component\nanalysis (PCA), and its non-linear (kernel) extensions, to more recent spectral\nembedding and support estimation methods, rely on estimating a linear subspace\nfrom samples. In this paper we introduce a general formulation of this problem\nand derive novel learning error estimates. Our results rely on natural\nassumptions on the spectral properties of the covariance operator associated to\nthe data distribu- tion, and hold for a wide class of metrics between\nsubspaces. As special cases, we discuss sharp error estimates for the\nreconstruction properties of PCA and spectral support estimation. Key to our\nanalysis is an operator theoretic approach that has broad applicability to\nspectral learning methods.\n",
        "published": "2014-08-21T15:12:37Z",
        "pdf_link": "http://arxiv.org/pdf/1408.5032v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.5404v2",
        "title": "A Wild Bootstrap for Degenerate Kernel Tests",
        "summary": "  A wild bootstrap method for nonparametric hypothesis tests based on kernel\ndistribution embeddings is proposed. This bootstrap method is used to construct\nprovably consistent tests that apply to random processes, for which the naive\npermutation-based bootstrap fails. It applies to a large group of kernel tests\nbased on V-statistics, which are degenerate under the null hypothesis, and\nnon-degenerate elsewhere. To illustrate this approach, we construct a\ntwo-sample test, an instantaneous independence test and a multiple lag\nindependence test for time series. In experiments, the wild bootstrap gives\nstrong performance on synthetic examples, on audio data, and in performance\nbenchmarking for the Gibbs sampler.\n",
        "published": "2014-08-23T03:16:41Z",
        "pdf_link": "http://arxiv.org/pdf/1408.5404v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.5810v2",
        "title": "Kernel-based Information Criterion",
        "summary": "  This paper introduces Kernel-based Information Criterion (KIC) for model\nselection in regression analysis. The novel kernel-based complexity measure in\nKIC efficiently computes the interdependency between parameters of the model\nusing a variable-wise variance and yields selection of better, more robust\nregressors. Experimental results show superior performance on both simulated\nand real data sets compared to Leave-One-Out Cross-Validation (LOOCV),\nkernel-based Information Complexity (ICOMP), and maximum log of marginal\nlikelihood in Gaussian Process Regression (GPR).\n",
        "published": "2014-08-25T15:44:05Z",
        "pdf_link": "http://arxiv.org/pdf/1408.5810v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.6693v1",
        "title": "A study of the fixed points and spurious solutions of the FastICA\n  algorithm",
        "summary": "  The FastICA algorithm is one of the most popular iterative algorithms in the\ndomain of linear independent component analysis. Despite its success, it is\nobserved that FastICA occasionally yields outcomes that do not correspond to\nany true solutions (known as demixing vectors) of the ICA problem. These\noutcomes are commonly referred to as spurious solutions. Although FastICA is\namong the most extensively studied ICA algorithms, the occurrence of spurious\nsolutions are not yet completely understood by the community. In this\ncontribution, we aim at addressing this issue. In the first part of this work,\nwe are interested in the relationship between demixing vectors, local\noptimizers of the contrast function and (attractive or unattractive) fixed\npoints of FastICA algorithm. Characterizations of these sets are given, and an\ninclusion relationship is discovered. In the second part, we investigate the\npossible scenarios where spurious solutions occur. We show that when certain\nbimodal Gaussian mixtures distributions are involved, there may exist spurious\nsolutions that are attractive fixed points of FastICA. In this case, popular\nnonlinearities such as \"gauss\" or \"tanh\" tend to yield spurious solutions,\nwhereas only \"kurtosis\" may give reliable results. Some advices are given for\nthe practical choice of nonlinearity function.\n",
        "published": "2014-08-28T12:11:43Z",
        "pdf_link": "http://arxiv.org/pdf/1408.6693v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.0073v1",
        "title": "Learning Mixed Multinomial Logit Model from Ordinal Data",
        "summary": "  Motivated by generating personalized recommendations using ordinal (or\npreference) data, we study the question of learning a mixture of MultiNomial\nLogit (MNL) model, a parameterized class of distributions over permutations,\nfrom partial ordinal or preference data (e.g. pair-wise comparisons). Despite\nits long standing importance across disciplines including social choice,\noperations research and revenue management, little is known about this\nquestion. In case of single MNL models (no mixture), computationally and\nstatistically tractable learning from pair-wise comparisons is feasible.\nHowever, even learning mixture with two MNL components is infeasible in\ngeneral.\n  Given this state of affairs, we seek conditions under which it is feasible to\nlearn the mixture model in both computationally and statistically efficient\nmanner. We present a sufficient condition as well as an efficient algorithm for\nlearning mixed MNL models from partial preferences/comparisons data. In\nparticular, a mixture of $r$ MNL components over $n$ objects can be learnt\nusing samples whose size scales polynomially in $n$ and $r$ (concretely,\n$r^{3.5}n^3(log n)^4$, with $r\\ll n^{2/7}$ when the model parameters are\nsufficiently incoherent). The algorithm has two phases: first, learn the\npair-wise marginals for each component using tensor decomposition; second,\nlearn the model parameters for each component using Rank Centrality introduced\nby Negahban et al. In the process of proving these results, we obtain a\ngeneralization of existing analysis for tensor decomposition to a more\nrealistic regime where only partial information about each sample is available.\n",
        "published": "2014-11-01T05:47:48Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0073v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.0254v3",
        "title": "Variational Inference for Gaussian Process Modulated Poisson Processes",
        "summary": "  We present the first fully variational Bayesian inference scheme for\ncontinuous Gaussian-process-modulated Poisson processes. Such point processes\nare used in a variety of domains, including neuroscience, geo-statistics and\nastronomy, but their use is hindered by the computational cost of existing\ninference schemes. Our scheme: requires no discretisation of the domain; scales\nlinearly in the number of observed events; and is many orders of magnitude\nfaster than previous sampling based approaches. The resulting algorithm is\nshown to outperform standard methods on synthetic examples, coal mining\ndisaster data and in the prediction of Malaria incidences in Kenya.\n",
        "published": "2014-11-02T13:04:30Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0254v3"
    },
    {
        "id": "http://arxiv.org/abs/1411.0439v1",
        "title": "Sampling for Inference in Probabilistic Models with Fast Bayesian\n  Quadrature",
        "summary": "  We propose a novel sampling framework for inference in probabilistic models:\nan active learning approach that converges more quickly (in wall-clock time)\nthan Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in\nprobabilistic inference is numerical integration, to average over ensembles of\nmodels or unknown (hyper-)parameters (for example to compute the marginal\nlikelihood or a partition function). MCMC has provided approaches to numerical\nintegration that deliver state-of-the-art inference, but can suffer from sample\ninefficiency and poor convergence diagnostics. Bayesian quadrature techniques\noffer a model-based solution to such problems, but their uptake has been\nhindered by prohibitive computation costs. We introduce a warped model for\nprobabilistic integrands (likelihoods) that are known to be non-negative,\npermitting a cheap active learning scheme to optimally select sample locations.\nOur algorithm is demonstrated to offer faster convergence (in seconds) relative\nto simple Monte Carlo and annealed importance sampling on both synthetic and\nreal-world examples.\n",
        "published": "2014-11-03T11:48:07Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0439v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.0707v1",
        "title": "A Nonparametric Adaptive Nonlinear Statistical Filter",
        "summary": "  We use statistical learning methods to construct an adaptive state estimator\nfor nonlinear stochastic systems. Optimal state estimation, in the form of a\nKalman filter, requires knowledge of the system's process and measurement\nuncertainty. We propose that these uncertainties can be estimated from\n(conditioned on) past observed data, and without making any assumptions of the\nsystem's prior distribution. The system's prior distribution at each time step\nis constructed from an ensemble of least-squares estimates on sub-sampled sets\nof the data via jackknife sampling. As new data is acquired, the state\nestimates, process uncertainty, and measurement uncertainty are updated\naccordingly, as described in this manuscript.\n",
        "published": "2014-11-03T21:34:24Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0707v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.0939v1",
        "title": "Simple approximate MAP Inference for Dirichlet processes",
        "summary": "  The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesian\nnonparametric statistical model. However, full probabilistic inference in this\nmodel is analytically intractable, so that computationally intensive techniques\nsuch as Gibb's sampling are required. As a result, DPM-based methods, which\nhave considerable potential, are restricted to applications in which\ncomputational resources and time for inference is plentiful. For example, they\nwould not be practical for digital signal processing on embedded hardware,\nwhere computational resources are at a serious premium. Here, we develop\nsimplified yet statistically rigorous approximate maximum a-posteriori (MAP)\ninference algorithms for DPMs. This algorithm is as simple as K-means\nclustering, performs in experiments as well as Gibb's sampling, while requiring\nonly a fraction of the computational effort. Unlike related small variance\nasymptotics, our algorithm is non-degenerate and so inherits the \"rich get\nricher\" property of the Dirichlet process. It also retains a non-degenerate\nclosed-form likelihood which enables standard tools such as cross-validation to\nbe used. This is a well-posed approximation to the MAP solution of the\nprobabilistic DPM model.\n",
        "published": "2014-11-04T15:30:25Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0939v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.1557v1",
        "title": "Proof Supplement - Learning Sparse Causal Models is not NP-hard\n  (UAI2013)",
        "summary": "  This article contains detailed proofs and additional examples related to the\nUAI-2013 submission `Learning Sparse Causal Models is not NP-hard'. It\ndescribes the FCI+ algorithm: a method for sound and complete causal model\ndiscovery in the presence of latent confounders and/or selection bias, that has\nworst case polynomial complexity of order $N^{2(k+1)}$ in the number of\nindependence tests, for sparse graphs over $N$ nodes, bounded by node degree\n$k$. The algorithm is an adaptation of the well-known FCI algorithm by (Spirtes\net al., 2000) that is also sound and complete, but has worst case complexity\nexponential in $N$.\n",
        "published": "2014-11-06T10:41:32Z",
        "pdf_link": "http://arxiv.org/pdf/1411.1557v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.1670v1",
        "title": "Stochastic Variational Inference for Hidden Markov Models",
        "summary": "  Variational inference algorithms have proven successful for Bayesian analysis\nin large data settings, with recent advances using stochastic variational\ninference (SVI). However, such methods have largely been studied in independent\nor exchangeable data settings. We develop an SVI algorithm to learn the\nparameters of hidden Markov models (HMMs) in a time-dependent data setting. The\nchallenge in applying stochastic optimization in this setting arises from\ndependencies in the chain, which must be broken to consider minibatches of\nobservations. We propose an algorithm that harnesses the memory decay of the\nchain to adaptively bound errors arising from edge effects. We demonstrate the\neffectiveness of our algorithm on synthetic experiments and a large genomics\ndataset where a batch algorithm is computationally infeasible.\n",
        "published": "2014-11-06T17:56:21Z",
        "pdf_link": "http://arxiv.org/pdf/1411.1670v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.1690v2",
        "title": "Sublinear-Time Approximate MCMC Transitions for Probabilistic Programs",
        "summary": "  Probabilistic programming languages can simplify the development of machine\nlearning techniques, but only if inference is sufficiently scalable.\nUnfortunately, Bayesian parameter estimation for highly coupled models such as\nregressions and state-space models still scales poorly; each MCMC transition\ntakes linear time in the number of observations. This paper describes a\nsublinear-time algorithm for making Metropolis-Hastings (MH) updates to latent\nvariables in probabilistic programs. The approach generalizes recently\nintroduced approximate MH techniques: instead of subsampling data items assumed\nto be independent, it subsamples edges in a dynamically constructed graphical\nmodel. It thus applies to a broader class of problems and interoperates with\nother general-purpose inference techniques. Empirical results, including\nconfirmation of sublinear per-transition scaling, are presented for Bayesian\nlogistic regression, nonlinear classification via joint Dirichlet process\nmixtures, and parameter estimation for stochastic volatility models (with state\nestimation via particle MCMC). All three applications use the same\nimplementation, and each requires under 20 lines of probabilistic code.\n",
        "published": "2014-11-06T18:32:24Z",
        "pdf_link": "http://arxiv.org/pdf/1411.1690v2"
    },
    {
        "id": "http://arxiv.org/abs/1411.2005v1",
        "title": "Scalable Variational Gaussian Process Classification",
        "summary": "  Gaussian process classification is a popular method with a number of\nappealing properties. We show how to scale the model within a variational\ninducing point framework, outperforming the state of the art on benchmark\ndatasets. Importantly, the variational formulation can be exploited to allow\nclassification in problems with millions of data points, as we demonstrate in\nexperiments.\n",
        "published": "2014-11-07T19:01:19Z",
        "pdf_link": "http://arxiv.org/pdf/1411.2005v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.2540v2",
        "title": "Parameter estimation in spherical symmetry groups",
        "summary": "  This paper considers statistical estimation problems where the probability\ndistribution of the observed random variable is invariant with respect to\nactions of a finite topological group. It is shown that any such distribution\nmust satisfy a restricted finite mixture representation. When specialized to\nthe case of distributions over the sphere that are invariant to the actions of\na finite spherical symmetry group $\\mathcal G$, a group-invariant extension of\nthe Von Mises Fisher (VMF) distribution is obtained. The $\\mathcal G$-invariant\nVMF is parameterized by location and scale parameters that specify the\ndistribution's mean orientation and its concentration about the mean,\nrespectively. Using the restricted finite mixture representation these\nparameters can be estimated using an Expectation Maximization (EM) maximum\nlikelihood (ML) estimation algorithm. This is illustrated for the problem of\nmean crystal orientation estimation under the spherically symmetric group\nassociated with the crystal form, e.g., cubic or octahedral or hexahedral.\nSimulations and experiments establish the advantages of the extended VMF EM-ML\nestimator for data acquired by Electron Backscatter Diffraction (EBSD)\nmicroscopy of a polycrystalline Nickel alloy sample.\n",
        "published": "2014-11-10T19:11:41Z",
        "pdf_link": "http://arxiv.org/pdf/1411.2540v2"
    },
    {
        "id": "http://arxiv.org/abs/1411.3803v1",
        "title": "Stochastic Compositional Gradient Descent: Algorithms for Minimizing\n  Compositions of Expected-Value Functions",
        "summary": "  Classical stochastic gradient methods are well suited for minimizing\nexpected-value objective functions. However, they do not apply to the\nminimization of a nonlinear function involving expected values or a composition\nof two expected-value functions, i.e., problems of the form $\\min_x\n\\mathbf{E}_v [f_v\\big(\\mathbf{E}_w [g_w(x)]\\big)]$. In order to solve this\nstochastic composition problem, we propose a class of stochastic compositional\ngradient descent (SCGD) algorithms that can be viewed as stochastic versions of\nquasi-gradient method. SCGD update the solutions based on noisy sample\ngradients of $f_v,g_{w}$ and use an auxiliary variable to track the unknown\nquantity $\\mathbf{E}_w[g_w(x)]$. We prove that the SCGD converge almost surely\nto an optimal solution for convex optimization problems, as long as such a\nsolution exists. The convergence involves the interplay of two iterations with\ndifferent time scales. For nonsmooth convex problems, the SCGD achieve a\nconvergence rate of $O(k^{-1/4})$ in the general case and $O(k^{-2/3})$ in the\nstrongly convex case, after taking $k$ samples. For smooth convex problems, the\nSCGD can be accelerated to converge at a rate of $O(k^{-2/7})$ in the general\ncase and $O(k^{-4/5})$ in the strongly convex case. For nonconvex problems, we\nprove that any limit point generated by SCGD is a stationary point, for which\nwe also provide the convergence rate analysis. Indeed, the stochastic setting\nwhere one wants to optimize compositions of expected-value functions is very\ncommon in practice. The proposed SCGD methods find wide applications in\nlearning, estimation, dynamic programming, etc.\n",
        "published": "2014-11-14T05:49:01Z",
        "pdf_link": "http://arxiv.org/pdf/1411.3803v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.3972v4",
        "title": "Causal Inference by Identification of Vector Autoregressive Processes\n  with Hidden Components",
        "summary": "  A widely applied approach to causal inference from a non-experimental time\nseries $X$, often referred to as \"(linear) Granger causal analysis\", is to\nregress present on past and interpret the regression matrix $\\hat{B}$ causally.\nHowever, if there is an unmeasured time series $Z$ that influences $X$, then\nthis approach can lead to wrong causal conclusions, i.e., distinct from those\none would draw if one had additional information such as $Z$. In this paper we\ntake a different approach: We assume that $X$ together with some hidden $Z$\nforms a first order vector autoregressive (VAR) process with transition matrix\n$A$, and argue why it is more valid to interpret $A$ causally instead of\n$\\hat{B}$. Then we examine under which conditions the most important parts of\n$A$ are identifiable or almost identifiable from only $X$. Essentially,\nsufficient conditions are (1) non-Gaussian, independent noise or (2) no\ninfluence from $X$ to $Z$. We present two estimation algorithms that are\ntailored towards conditions (1) and (2), respectively, and evaluate them on\nsynthetic and real-world data. We discuss how to check the model using $X$.\n",
        "published": "2014-11-14T16:54:33Z",
        "pdf_link": "http://arxiv.org/pdf/1411.3972v4"
    },
    {
        "id": "http://arxiv.org/abs/1411.4378v1",
        "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert\n  Space",
        "summary": "  While robust parameter estimation has been well studied in parametric density\nestimation, there has been little investigation into robust density estimation\nin the nonparametric setting. We present a robust version of the popular kernel\ndensity estimator (KDE). As with other estimators, a robust version of the KDE\nis useful since sample contamination is a common issue with datasets. What\n\"robustness\" means for a nonparametric density estimate is not straightforward\nand is a topic we explore in this paper. To construct a robust KDE we scale the\ntraditional KDE and project it to its nearest weighted KDE in the $L^2$ norm.\nThis yields a scaled and projected KDE (SPKDE). Because the squared $L^2$ norm\npenalizes point-wise errors superlinearly this causes the weighted KDE to\nallocate more weight to high density regions. We demonstrate the robustness of\nthe SPKDE with numerical experiments and a consistency result which shows that\nasymptotically the SPKDE recovers the uncontaminated density under sufficient\nconditions on the contamination.\n",
        "published": "2014-11-17T06:35:55Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4378v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.4834v1",
        "title": "The NLMS algorithm with time-variant optimum stepsize derived from a\n  Bayesian network perspective",
        "summary": "  In this article, we derive a new stepsize adaptation for the normalized least\nmean square algorithm (NLMS) by describing the task of linear acoustic echo\ncancellation from a Bayesian network perspective. Similar to the well-known\nKalman filter equations, we model the acoustic wave propagation from the\nloudspeaker to the microphone by a latent state vector and define a linear\nobservation equation (to model the relation between the state vector and the\nobservation) as well as a linear process equation (to model the temporal\nprogress of the state vector). Based on additional assumptions on the\nstatistics of the random variables in observation and process equation, we\napply the expectation-maximization (EM) algorithm to derive an NLMS-like filter\nadaptation. By exploiting the conditional independence rules for Bayesian\nnetworks, we reveal that the resulting EM-NLMS algorithm has a stepsize update\nequivalent to the optimal-stepsize calculation proposed by Yamamoto and\nKitayama in 1982, which has been adopted in many textbooks. As main difference,\nthe instantaneous stepsize value is estimated in the M step of the EM algorithm\n(instead of being approximated by artificially extending the acoustic echo\npath). The EM-NLMS algorithm is experimentally verified for synthesized\nscenarios with both, white noise and male speech as input signal.\n",
        "published": "2014-11-18T13:23:11Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4834v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.5271v2",
        "title": "Quantifying error in estimates of human brain fiber directions using\n  Earth Mover's Distance",
        "summary": "  Diffusion-weighted MR imaging (DWI) is the only method we currently have to\nmeasure connections between different parts of the human brain in vivo. To\nelucidate the structure of these connections, algorithms for tracking bundles\nof axonal fibers through the subcortical white matter rely on local estimates\nof the fiber orientation distribution function (fODF) in different parts of the\nbrain. These functions describe the relative abundance of populations of axonal\nfibers crossing each other in each location. Multiple models exist for\nestimating fODFs. The quality of the resulting estimates can be quantified by\nmeans of a suitable measure of distance on the space of fODFs. However, there\nare multiple distance metrics that can be applied for this purpose, including\nsmoothed $L_p$ distances and the Wasserstein metrics. Here, we give four\nreasons for the use of the Earth Mover's Distance (EMD) equipped with the\narc-length, as a distance metric. (continued)\n",
        "published": "2014-11-19T16:02:21Z",
        "pdf_link": "http://arxiv.org/pdf/1411.5271v2"
    },
    {
        "id": "http://arxiv.org/abs/1411.5799v2",
        "title": "Group Factor Analysis",
        "summary": "  Factor analysis provides linear factors that describe relationships between\nindividual variables of a data set. We extend this classical formulation into\nlinear factors that describe relationships between groups of variables, where\neach group represents either a set of related variables or a data set. The\nmodel also naturally extends canonical correlation analysis to more than two\nsets, in a way that is more flexible than previous extensions. Our solution is\nformulated as variational inference of a latent variable model with structural\nsparsity, and it consists of two hierarchical levels: The higher level models\nthe relationships between the groups, whereas the lower models the observed\nvariables given the higher level. We show that the resulting solution solves\nthe group factor analysis problem accurately, outperforming alternative factor\nanalysis based solutions as well as more straightforward implementations of\ngroup factor analysis. The method is demonstrated on two life science data\nsets, one on brain activation and the other on systems biology, illustrating\nits applicability to the analysis of different types of high-dimensional data\nsources.\n",
        "published": "2014-11-21T08:55:35Z",
        "pdf_link": "http://arxiv.org/pdf/1411.5799v2"
    },
    {
        "id": "http://arxiv.org/abs/1411.6203v1",
        "title": "Efficient Minimax Signal Detection on Graphs",
        "summary": "  Several problems such as network intrusion, community detection, and disease\noutbreak can be described by observations attributed to nodes or edges of a\ngraph. In these applications presence of intrusion, community or disease\noutbreak is characterized by novel observations on some unknown connected\nsubgraph. These problems can be formulated in terms of optimization of suitable\nobjectives on connected subgraphs, a problem which is generally computationally\ndifficult. We overcome the combinatorics of connectivity by embedding connected\nsubgraphs into linear matrix inequalities (LMI). Computationally efficient\ntests are then realized by optimizing convex objective functions subject to\nthese LMI constraints. We prove, by means of a novel Euclidean embedding\nargument, that our tests are minimax optimal for exponential family of\ndistributions on 1-D and 2-D lattices. We show that internal conductance of the\nconnected subgraph family plays a fundamental role in characterizing\ndetectability.\n",
        "published": "2014-11-23T07:40:09Z",
        "pdf_link": "http://arxiv.org/pdf/1411.6203v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.6311v1",
        "title": "Optimal variable selection in multi-group sparse discriminant analysis",
        "summary": "  This article considers the problem of multi-group classification in the\nsetting where the number of variables $p$ is larger than the number of\nobservations $n$. Several methods have been proposed in the literature that\naddress this problem, however their variable selection performance is either\nunknown or suboptimal to the results known in the two-group case. In this work\nwe provide sharp conditions for the consistent recovery of relevant variables\nin the multi-group case using the discriminant analysis proposal of Gaynanova\net al., 2014. We achieve the rates of convergence that attain the optimal\nscaling of the sample size $n$, number of variables $p$ and the sparsity level\n$s$. These rates are significantly faster than the best known results in the\nmulti-group case. Moreover, they coincide with the optimal minimax rates for\nthe two-group case. We validate our theoretical results with numerical\nanalysis.\n",
        "published": "2014-11-23T22:42:21Z",
        "pdf_link": "http://arxiv.org/pdf/1411.6311v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.0694v3",
        "title": "Streaming Variational Inference for Bayesian Nonparametric Mixture\n  Models",
        "summary": "  In theory, Bayesian nonparametric (BNP) models are well suited to streaming\ndata scenarios due to their ability to adapt model complexity with the observed\ndata. Unfortunately, such benefits have not been fully realized in practice;\nexisting inference algorithms are either not applicable to streaming\napplications or not extensible to BNP models. For the special case of Dirichlet\nprocesses, streaming inference has been considered. However, there is growing\ninterest in more flexible BNP models building on the class of normalized random\nmeasures (NRMs). We work within this general framework and present a streaming\nvariational inference algorithm for NRM mixture models. Our algorithm is based\non assumed density filtering (ADF), leading straightforwardly to expectation\npropagation (EP) for large-scale batch inference as well. We demonstrate the\nefficacy of the algorithm on clustering documents in large, streaming text\ncorpora.\n",
        "published": "2014-12-01T21:26:33Z",
        "pdf_link": "http://arxiv.org/pdf/1412.0694v3"
    },
    {
        "id": "http://arxiv.org/abs/1412.1370v1",
        "title": "Nested Variational Compression in Deep Gaussian Processes",
        "summary": "  Deep Gaussian processes provide a flexible approach to probabilistic\nmodelling of data using either supervised or unsupervised learning. For\ntractable inference approximations to the marginal likelihood of the model must\nbe made. The original approach to approximate inference in these models used\nvariational compression to allow for approximate variational marginalization of\nthe hidden variables leading to a lower bound on the marginal likelihood of the\nmodel [Damianou and Lawrence, 2013]. In this paper we extend this idea with a\nnested variational compression. The resulting lower bound on the likelihood can\nbe easily parallelized or adapted for stochastic variational inference.\n",
        "published": "2014-12-03T15:39:55Z",
        "pdf_link": "http://arxiv.org/pdf/1412.1370v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.2295v2",
        "title": "A Likelihood Ratio Framework for High Dimensional Semiparametric\n  Regression",
        "summary": "  We propose a likelihood ratio based inferential framework for high\ndimensional semiparametric generalized linear models. This framework addresses\na variety of challenging problems in high dimensional data analysis, including\nincomplete data, selection bias, and heterogeneous multitask learning. Our work\nhas three main contributions. (i) We develop a regularized statistical\nchromatography approach to infer the parameter of interest under the proposed\nsemiparametric generalized linear model without the need of estimating the\nunknown base measure function. (ii) We propose a new framework to construct\npost-regularization confidence regions and tests for the low dimensional\ncomponents of high dimensional parameters. Unlike existing post-regularization\ninferential methods, our approach is based on a novel directional likelihood.\nIn particular, the framework naturally handles generic regularized estimators\nwith nonconvex penalty functions and it can be used to infer least false\nparameters under misspecified models. (iii) We develop new concentration\ninequalities and normal approximation results for U-statistics with unbounded\nkernels, which are of independent interest. We demonstrate the consequences of\nthe general theory by using an example of missing data problem. Extensive\nsimulation studies and real data analysis are provided to illustrate our\nproposed approach.\n",
        "published": "2014-12-06T22:52:52Z",
        "pdf_link": "http://arxiv.org/pdf/1412.2295v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.3432v4",
        "title": "Detecting Overlapping Communities in Networks Using Spectral Methods",
        "summary": "  Community detection is a fundamental problem in network analysis which is\nmade more challenging by overlaps between communities which often occur in\npractice. Here we propose a general, flexible, and interpretable generative\nmodel for overlapping communities, which can be thought of as a generalization\nof the degree-corrected stochastic block model. We develop an efficient\nspectral algorithm for estimating the community memberships, which deals with\nthe overlaps by employing the K-medians algorithm rather than the usual K-means\nfor clustering in the spectral domain. We show that the algorithm is\nasymptotically consistent when networks are not too sparse and the overlaps\nbetween communities not too large. Numerical experiments on both simulated\nnetworks and many real social networks demonstrate that our method performs\nvery well compared to a number of benchmark methods for overlapping community\ndetection.\n",
        "published": "2014-12-10T20:06:24Z",
        "pdf_link": "http://arxiv.org/pdf/1412.3432v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.4098v4",
        "title": "Manifold Matching using Shortest-Path Distance and Joint Neighborhood\n  Selection",
        "summary": "  Matching datasets of multiple modalities has become an important task in data\nanalysis. Existing methods often rely on the embedding and transformation of\neach single modality without utilizing any correspondence information, which\noften results in sub-optimal matching performance. In this paper, we propose a\nnonlinear manifold matching algorithm using shortest-path distance and joint\nneighborhood selection. Specifically, a joint nearest-neighbor graph is built\nfor all modalities. Then the shortest-path distance within each modality is\ncalculated from the joint neighborhood graph, followed by embedding into and\nmatching in a common low-dimensional Euclidean space. Compared to existing\nalgorithms, our approach exhibits superior performance for matching disparate\ndatasets of multiple modalities.\n",
        "published": "2014-12-12T19:51:22Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4098v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.4679v5",
        "title": "Bayesian multi-tensor factorization",
        "summary": "  We introduce Bayesian multi-tensor factorization, a model that is the first\nBayesian formulation for joint factorization of multiple matrices and tensors.\nThe research problem generalizes the joint matrix-tensor factorization problem\nto arbitrary sets of tensors of any depth, including matrices, can be\ninterpreted as unsupervised multi-view learning from multiple data tensors, and\ncan be generalized to relax the usual trilinear tensor factorization\nassumptions. The result is a factorization of the set of tensors into factors\nshared by any subsets of the tensors, and factors private to individual\ntensors. We demonstrate the performance against existing baselines in multiple\ntensor factorization tasks in structural toxicogenomics and functional\nneuroimaging.\n",
        "published": "2014-12-15T17:10:55Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4679v5"
    },
    {
        "id": "http://arxiv.org/abs/1412.6515v4",
        "title": "On distinguishability criteria for estimating generative models",
        "summary": "  Two recently introduced criteria for estimation of generative models are both\nbased on a reduction to binary classification. Noise-contrastive estimation\n(NCE) is an estimation procedure in which a generative model is trained to be\nable to distinguish data samples from noise samples. Generative adversarial\nnetworks (GANs) are pairs of generator and discriminator networks, with the\ngenerator network learning to generate samples by attempting to fool the\ndiscriminator network into believing its samples are real data. Both estimation\nprocedures use the same function to drive learning, which naturally raises\nquestions about how they are related to each other, as well as whether this\nfunction is related to maximum likelihood estimation (MLE). NCE corresponds to\ntraining an internal data model belonging to the {\\em discriminator} network\nbut using a fixed generator network. We show that a variant of NCE, with a\ndynamic generator network, is equivalent to maximum likelihood estimation.\nSince pairing a learned discriminator with an appropriate dynamically selected\ngenerator recovers MLE, one might expect the reverse to hold for pairing a\nlearned generator with a certain discriminator. However, we show that\nrecovering MLE for a learned generator requires departing from the\ndistinguishability game. Specifically:\n  (i) The expected gradient of the NCE discriminator can be made to match the\nexpected gradient of\n  MLE, if one is allowed to use a non-stationary noise distribution for NCE,\n  (ii) No choice of discriminator network can make the expected gradient for\nthe GAN generator match that of MLE, and\n  (iii) The existing theory does not guarantee that GANs will converge in the\nnon-convex case.\n  This suggests that the key next step in GAN research is to determine whether\nGANs converge, and if not, to modify their training algorithm to force\nconvergence.\n",
        "published": "2014-12-19T20:28:41Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6515v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.7260v2",
        "title": "Approximate Subspace-Sparse Recovery with Corrupted Data via Constrained\n  $\\ell_1$-Minimization",
        "summary": "  High-dimensional data often lie in low-dimensional subspaces corresponding to\ndifferent classes they belong to. Finding sparse representations of data points\nin a dictionary built using the collection of data helps to uncover\nlow-dimensional subspaces and address problems such as clustering,\nclassification, subset selection and more. In this paper, we address the\nproblem of recovering sparse representations for noisy data points in a\ndictionary whose columns correspond to corrupted data lying close to a union of\nsubspaces. We consider a constrained $\\ell_1$-minimization and study conditions\nunder which the solution of the proposed optimization satisfies the approximate\nsubspace-sparse recovery condition. More specifically, we show that each noisy\ndata point, perturbed from a subspace by a noise of the magnitude of\n$\\varepsilon$, will be reconstructed using data points from the same subspace\nwith a small error of the order of $O(\\varepsilon)$ and that the coefficients\ncorresponding to data points in other subspaces will be sufficiently small,\n\\ie, of the order of $O(\\varepsilon)$. We do not impose any randomness\nassumption on the arrangement of subspaces or distribution of data points in\neach subspace. Our framework is based on a novel generalization of the\nnull-space property to the setting where data lie in multiple subspaces, the\nnumber of data points in each subspace exceeds the dimension of the subspace,\nand all data points are corrupted by noise. Moreover, assuming a random\ndistribution for data points, we further show that coefficients from the\ndesired support not only reconstruct a given point with high accuracy, but also\nhave sufficiently large values, \\ie, of the order of $O(1)$.\n",
        "published": "2014-12-23T06:10:34Z",
        "pdf_link": "http://arxiv.org/pdf/1412.7260v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.7638v1",
        "title": "Inference for Sparse Conditional Precision Matrices",
        "summary": "  Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is a\nhigh-dimensional vector and $Z$ is a low-dimensional index variable, we study\nthe problem of estimating the conditional inverse covariance matrix $\\Omega(z)\n= (E[(X-E[X \\mid Z])(X-E[X \\mid Z])^T \\mid Z=z])^{-1}$ under the assumption\nthat the set of non-zero elements is small and does not depend on the index\nvariable. We develop a novel procedure that combines the ideas of the local\nconstant smoothing and the group Lasso for estimating the conditional inverse\ncovariance matrix. A proximal iterative smoothing algorithm is used to solve\nthe corresponding convex optimization problems. We prove that our procedure\nrecovers the conditional independence assumptions of the distribution $X \\mid\nZ$ with high probability. This result is established by developing a uniform\ndeviation bound for the high-dimensional conditional covariance matrix from its\npopulation counterpart, which may be of independent interest. Furthermore, we\ndevelop point-wise confidence intervals for individual elements of the\nconditional inverse covariance matrix. We perform extensive simulation studies,\nin which we demonstrate that our proposed procedure outperforms sensible\ncompetitors. We illustrate our proposal on a S&P 500 stock price data set.\n",
        "published": "2014-12-24T10:32:02Z",
        "pdf_link": "http://arxiv.org/pdf/1412.7638v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.7983v2",
        "title": "Exploring Sparsity in Multi-class Linear Discriminant Analysis",
        "summary": "  Recent studies in the literature have paid much attention to the sparsity in\nlinear classification tasks. One motivation of imposing sparsity assumption on\nthe linear discriminant direction is to rule out the noninformative features,\nmaking hardly contribution to the classification problem. Most of those work\nwere focused on the scenarios of binary classification. In the presence of\nmulti-class data, preceding researches recommended individually pairwise sparse\nlinear discriminant analysis(LDA). However, further sparsity should be\nexplored. In this paper, an estimator of grouped LASSO type is proposed to take\nadvantage of sparsity for multi-class data. It enjoys appealing non-asymptotic\nproperties which allows insignificant correlations among features. This\nestimator exhibits superior capability on both simulated and real data.\n",
        "published": "2014-12-26T20:01:21Z",
        "pdf_link": "http://arxiv.org/pdf/1412.7983v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.8697v2",
        "title": "On Semiparametric Exponential Family Graphical Models",
        "summary": "  We propose a new class of semiparametric exponential family graphical models\nfor the analysis of high dimensional mixed data. Different from the existing\nmixed graphical models, we allow the nodewise conditional distributions to be\nsemiparametric generalized linear models with unspecified base measure\nfunctions. Thus, one advantage of our method is that it is unnecessary to\nspecify the type of each node and the method is more convenient to apply in\npractice. Under the proposed model, we consider both problems of parameter\nestimation and hypothesis testing in high dimensions. In particular, we propose\na symmetric pairwise score test for the presence of a single edge in the graph.\nCompared to the existing methods for hypothesis tests, our approach takes into\naccount of the symmetry of the parameters, such that the inferential results\nare invariant with respect to the different parametrizations of the same edge.\nThorough numerical simulations and a real data example are provided to back up\nour results.\n",
        "published": "2014-12-30T17:39:48Z",
        "pdf_link": "http://arxiv.org/pdf/1412.8697v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.8724v2",
        "title": "A General Framework for Robust Testing and Confidence Regions in\n  High-Dimensional Quantile Regression",
        "summary": "  We propose a robust inferential procedure for assessing uncertainties of\nparameter estimation in high-dimensional linear models, where the dimension $p$\ncan grow exponentially fast with the sample size $n$. Our method combines the\nde-biasing technique with the composite quantile function to construct an\nestimator that is asymptotically normal. Hence it can be used to construct\nvalid confidence intervals and conduct hypothesis tests. Our estimator is\nrobust and does not require the existence of first or second moment of the\nnoise distribution. It also preserves efficiency in the sense that the worst\ncase efficiency loss is less than 30\\% compared to the square-loss-based\nde-biased Lasso estimator. In many cases our estimator is close to or better\nthan the latter, especially when the noise is heavy-tailed. Our de-biasing\nprocedure does not require solving the $L_1$-penalized composite quantile\nregression. Instead, it allows for any first-stage estimator with desired\nconvergence rate and empirical sparsity. The paper also provides new proof\ntechniques for developing theoretical guarantees of inferential procedures with\nnon-smooth loss functions. To establish the main results, we exploit the local\ncurvature of the conditional expectation of composite quantile loss and apply\nempirical process theories to control the difference between empirical\nquantities and their conditional expectations. Our results are established\nunder weaker assumptions compared to existing work on inference for\nhigh-dimensional quantile regression. Furthermore, we consider a\nhigh-dimensional simultaneous test for the regression parameters by applying\nthe Gaussian approximation and multiplier bootstrap theories. We also study\ndistributed learning and exploit the divide-and-conquer estimator to reduce\ncomputation complexity when the sample size is massive. Finally, we provide\nempirical results to verify the theory.\n",
        "published": "2014-12-30T18:44:04Z",
        "pdf_link": "http://arxiv.org/pdf/1412.8724v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.8729v2",
        "title": "High Dimensional Expectation-Maximization Algorithm: Statistical\n  Optimization and Asymptotic Normality",
        "summary": "  We provide a general theory of the expectation-maximization (EM) algorithm\nfor inferring high dimensional latent variable models. In particular, we make\ntwo contributions: (i) For parameter estimation, we propose a novel high\ndimensional EM algorithm which naturally incorporates sparsity structure into\nparameter estimation. With an appropriate initialization, this algorithm\nconverges at a geometric rate and attains an estimator with the (near-)optimal\nstatistical rate of convergence. (ii) Based on the obtained estimator, we\npropose new inferential procedures for testing hypotheses and constructing\nconfidence intervals for low dimensional components of high dimensional\nparameters. For a broad family of statistical models, our framework establishes\nthe first computationally feasible approach for optimal estimation and\nasymptotic inference in high dimensions. Our theory is supported by thorough\nnumerical results.\n",
        "published": "2014-12-30T18:53:22Z",
        "pdf_link": "http://arxiv.org/pdf/1412.8729v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.8765v2",
        "title": "A General Theory of Hypothesis Tests and Confidence Regions for Sparse\n  High Dimensional Models",
        "summary": "  We consider the problem of uncertainty assessment for low dimensional\ncomponents in high dimensional models. Specifically, we propose a decorrelated\nscore function to handle the impact of high dimensional nuisance parameters. We\nconsider both hypothesis tests and confidence regions for generic penalized\nM-estimators. Unlike most existing inferential methods which are tailored for\nindividual models, our approach provides a general framework for high\ndimensional inference and is applicable to a wide range of applications. From\nthe testing perspective, we develop general theorems to characterize the\nlimiting distributions of the decorrelated score test statistic under both null\nhypothesis and local alternatives. These results provide asymptotic guarantees\non the type I errors and local powers of the proposed test. Furthermore, we\nshow that the decorrelated score function can be used to construct point and\nconfidence region estimators that are semiparametrically efficient. We also\ngeneralize this framework to broaden its applications. First, we extend it to\nhandle high dimensional null hypothesis, where the number of parameters of\ninterest can increase exponentially fast with the sample size. Second, we\nestablish the theory for model misspecification. Third, we go beyond the\nlikelihood framework, by introducing the generalized score test based on\ngeneral loss functions. Thorough numerical studies are conducted to back up the\ndeveloped theoretical results.\n",
        "published": "2014-12-30T20:54:27Z",
        "pdf_link": "http://arxiv.org/pdf/1412.8765v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.00727v7",
        "title": "Laplacian Mixture Modeling for Network Analysis and Unsupervised\n  Learning on Graphs",
        "summary": "  Laplacian mixture models identify overlapping regions of influence in\nunlabeled graph and network data in a scalable and computationally efficient\nway, yielding useful low-dimensional representations. By combining Laplacian\neigenspace and finite mixture modeling methods, they provide probabilistic or\nfuzzy dimensionality reductions or domain decompositions for a variety of input\ndata types, including mixture distributions, feature vectors, and graphs or\nnetworks. Provable optimal recovery using the algorithm is analytically shown\nfor a nontrivial class of cluster graphs. Heuristic approximations for scalable\nhigh-performance implementations are described and empirically tested.\nConnections to PageRank and community detection in network analysis demonstrate\nthe wide applicability of this approach. The origins of fuzzy spectral methods,\nbeginning with generalized heat or diffusion equations in physics, are reviewed\nand summarized. Comparisons to other dimensionality reduction and clustering\nmethods for challenging unsupervised machine learning problems are also\ndiscussed.\n",
        "published": "2015-02-03T03:50:16Z",
        "pdf_link": "http://arxiv.org/pdf/1502.00727v7"
    },
    {
        "id": "http://arxiv.org/abs/1502.00916v1",
        "title": "Learning Planar Ising Models",
        "summary": "  Inference and learning of graphical models are both well-studied problems in\nstatistics and machine learning that have found many applications in science\nand engineering. However, exact inference is intractable in general graphical\nmodels, which suggests the problem of seeking the best approximation to a\ncollection of random variables within some tractable family of graphical\nmodels. In this paper, we focus on the class of planar Ising models, for which\nexact inference is tractable using techniques of statistical physics. Based on\nthese techniques and recent methods for planarity testing and planar embedding,\nwe propose a simple greedy algorithm for learning the best planar Ising model\nto approximate an arbitrary collection of binary random variables (possibly\nfrom sample data). Given the set of all pairwise correlations among variables,\nwe select a planar graph and optimal planar Ising model defined on this graph\nto best approximate that set of correlations. We demonstrate our method in\nsimulations and for the application of modeling senate voting records.\n",
        "published": "2015-02-03T16:35:43Z",
        "pdf_link": "http://arxiv.org/pdf/1502.00916v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.01368v4",
        "title": "Sparse Representation Classification Beyond L1 Minimization and the\n  Subspace Assumption",
        "summary": "  The sparse representation classifier (SRC) has been utilized in various\nclassification problems, which makes use of L1 minimization and works well for\nimage recognition satisfying a subspace assumption. In this paper we propose a\nnew implementation of SRC via screening, establish its equivalence to the\noriginal SRC under regularity conditions, and prove its classification\nconsistency under a latent subspace model and contamination. The results are\ndemonstrated via simulations and real data experiments, where the new algorithm\nachieves comparable numerical performance and significantly faster.\n",
        "published": "2015-02-04T21:50:55Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01368v4"
    },
    {
        "id": "http://arxiv.org/abs/1502.01425v3",
        "title": "Provable Sparse Tensor Decomposition",
        "summary": "  We propose a novel sparse tensor decomposition method, namely Tensor\nTruncated Power (TTP) method, that incorporates variable selection into the\nestimation of decomposition components. The sparsity is achieved via an\nefficient truncation step embedded in the tensor power iteration. Our method\napplies to a broad family of high dimensional latent variable models, including\nhigh dimensional Gaussian mixture and mixtures of sparse regressions. A\nthorough theoretical investigation is further conducted. In particular, we show\nthat the final decomposition estimator is guaranteed to achieve a local\nstatistical rate, and further strengthen it to the global statistical rate by\nintroducing a proper initialization procedure. In high dimensional regimes, the\nobtained statistical rate significantly improves those shown in the existing\nnon-sparse decomposition methods. The empirical advantages of TTP are confirmed\nin extensive simulated results and two real applications of click-through rate\nprediction and high-dimensional gene clustering.\n",
        "published": "2015-02-05T03:37:40Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01425v3"
    },
    {
        "id": "http://arxiv.org/abs/1502.01943v1",
        "title": "Active Function Cross-Entropy Clustering",
        "summary": "  Gaussian Mixture Models (GMM) have found many applications in density\nestimation and data clustering. However, the model does not adapt well to\ncurved and strongly nonlinear data. Recently there appeared an improvement\ncalled AcaGMM (Active curve axis Gaussian Mixture Model), which fits Gaussians\nalong curves using an EM-like (Expectation Maximization) approach.\n  Using the ideas standing behind AcaGMM, we build an alternative active\nfunction model of clustering, which has some advantages over AcaGMM. In\nparticular it is naturally defined in arbitrary dimensions and enables an easy\nadaptation to clustering of complicated datasets along the predefined family of\nfunctions. Moreover, it does not need external methods to determine the number\nof clusters as it automatically reduces the number of groups on-line.\n",
        "published": "2015-02-06T16:40:06Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01943v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.02089v1",
        "title": "Discriminative training for Convolved Multiple-Output Gaussian processes",
        "summary": "  Multi-output Gaussian processes (MOGP) are probability distributions over\nvector-valued functions, and have been previously used for multi-output\nregression and for multi-class classification. A less explored facet of the\nmulti-output Gaussian process is that it can be used as a generative model for\nvector-valued random fields in the context of pattern recognition. As a\ngenerative model, the multi-output GP is able to handle vector-valued functions\nwith continuous inputs, as opposed, for example, to hidden Markov models. It\nalso offers the ability to model multivariate random functions with high\ndimensional inputs. In this report, we use a discriminative training criteria\nknown as Minimum Classification Error to fit the parameters of a multi-output\nGaussian process. We compare the performance of generative training and\ndiscriminative training of MOGP in emotion recognition, activity recognition,\nand face recognition. We also compare the proposed methodology against hidden\nMarkov models trained in a generative and in a discriminative way.\n",
        "published": "2015-02-07T01:45:55Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02089v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.02309v1",
        "title": "Measuring the functional connectome \"on-the-fly\": towards a new control\n  signal for fMRI-based brain-computer interfaces",
        "summary": "  There has been an explosion of interest in functional Magnetic Resonance\nImaging (MRI) during the past two decades. Naturally, this has been accompanied\nby many major advances in the understanding of the human connectome. These\nadvances have served to pose novel challenges as well as open new avenues for\nresearch. One of the most promising and exciting of such avenues is the study\nof functional MRI in real-time. Such studies have recently gained momentum and\nhave been applied in a wide variety of settings; ranging from training of\nhealthy subjects to self-regulate neuronal activity to being suggested as\npotential treatments for clinical populations. To date, the vast majority of\nthese studies have focused on a single region at a time. This is due in part to\nthe many challenges faced when estimating dynamic functional connectivity\nnetworks in real-time. In this work we propose a novel methodology with which\nto accurately track changes in functional connectivity networks in real-time.\nWe adapt the recently proposed SINGLE algorithm for estimating sparse and\ntemporally homo- geneous dynamic networks to be applicable in real-time. The\nproposed method is applied to motor task data from the Human Connectome Project\nas well as to real-time data ob- tained while exploring a virtual environment.\nWe show that the algorithm is able to estimate significant task-related changes\nin network structure quickly enough to be useful in future brain-computer\ninterface applications.\n",
        "published": "2015-02-08T22:09:17Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02309v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.02344v2",
        "title": "Regularization Path of Cross-Validation Error Lower Bounds",
        "summary": "  Careful tuning of a regularization parameter is indispensable in many machine\nlearning tasks because it has a significant impact on generalization\nperformances. Nevertheless, current practice of regularization parameter tuning\nis more of an art than a science, e.g., it is hard to tell how many grid-points\nwould be needed in cross-validation (CV) for obtaining a solution with\nsufficiently small CV error. In this paper we propose a novel framework for\ncomputing a lower bound of the CV errors as a function of the regularization\nparameter, which we call regularization path of CV error lower bounds. The\nproposed framework can be used for providing a theoretical approximation\nguarantee on a set of solutions in the sense that how far the CV error of the\ncurrent best solution could be away from best possible CV error in the entire\nrange of the regularization parameters. We demonstrate through numerical\nexperiments that a theoretically guaranteed a choice of regularization\nparameter in the above sense is possible with reasonable computational costs.\n",
        "published": "2015-02-09T03:35:38Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02344v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.02347v2",
        "title": "Local and Global Inference for High Dimensional Nonparanormal Graphical\n  Models",
        "summary": "  This paper proposes a unified framework to quantify local and global\ninferential uncertainty for high dimensional nonparanormal graphical models. In\nparticular, we consider the problems of testing the presence of a single edge\nand constructing a uniform confidence subgraph. Due to the presence of unknown\nmarginal transformations, we propose a pseudo likelihood based inferential\napproach. In sharp contrast to the existing high dimensional score test method,\nour method is free of tuning parameters given an initial estimator, and extends\nthe scope of the existing likelihood based inferential framework. Furthermore,\nwe propose a U-statistic multiplier bootstrap method to construct the\nconfidence subgraph. We show that the constructed subgraph is contained in the\ntrue graph with probability greater than a given nominal level. Compared with\nexisting methods for constructing confidence subgraphs, our method does not\nrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the\nproposed inferential methods are verified by thorough numerical experiments and\nreal data analysis.\n",
        "published": "2015-02-09T04:06:43Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02347v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.02843v3",
        "title": "Distributed Gaussian Processes",
        "summary": "  To scale Gaussian processes (GPs) to large data sets we introduce the robust\nBayesian Committee Machine (rBCM), a practical and scalable product-of-experts\nmodel for large-scale distributed GP regression. Unlike state-of-the-art sparse\nGP approximations, the rBCM is conceptually simple and does not rely on\ninducing or variational parameters. The key idea is to recursively distribute\ncomputations to independent computational units and, subsequently, recombine\nthem to form an overall result. Efficient closed-form inference allows for\nstraightforward parallelisation and distributed computations with a small\nmemory footprint. The rBCM is independent of the computational graph and can be\nused on heterogeneous computing infrastructures, ranging from laptops to\nclusters. With sufficient computing resources our distributed GP model can\nhandle arbitrarily large data sets.\n",
        "published": "2015-02-10T10:31:41Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02843v3"
    },
    {
        "id": "http://arxiv.org/abs/1502.03365v1",
        "title": "Reconstruction in the Labeled Stochastic Block Model",
        "summary": "  The labeled stochastic block model is a random graph model representing\nnetworks with community structure and interactions of multiple types. In its\nsimplest form, it consists of two communities of approximately equal size, and\nthe edges are drawn and labeled at random with probability depending on whether\ntheir two endpoints belong to the same community or not.\n  It has been conjectured in \\cite{Heimlicher12} that correlated reconstruction\n(i.e.\\ identification of a partition correlated with the true partition into\nthe underlying communities) would be feasible if and only if a model parameter\nexceeds a threshold. We prove one half of this conjecture, i.e., reconstruction\nis impossible when below the threshold. In the positive direction, we introduce\na weighted graph to exploit the label information. With a suitable choice of\nweight function, we show that when above the threshold by a specific constant,\nreconstruction is achieved by (1) minimum bisection, (2) a semidefinite\nrelaxation of minimum bisection, and (3) a spectral method combined with\nremoval of edges incident to vertices of high degree. Furthermore, we show that\nhypothesis testing between the labeled stochastic block model and the labeled\nErd\\H{o}s-R\\'enyi random graph model exhibits a phase transition at the\nconjectured reconstruction threshold.\n",
        "published": "2015-02-11T16:33:19Z",
        "pdf_link": "http://arxiv.org/pdf/1502.03365v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.03466v1",
        "title": "Dependent Matérn Processes for Multivariate Time Series",
        "summary": "  For the challenging task of modeling multivariate time series, we propose a\nnew class of models that use dependent Mat\\'ern processes to capture the\nunderlying structure of data, explain their interdependencies, and predict\ntheir unknown values. Although similar models have been proposed in the\neconometric, statistics, and machine learning literature, our approach has\nseveral advantages that distinguish it from existing methods: 1) it is flexible\nto provide high prediction accuracy, yet its complexity is controlled to avoid\noverfitting; 2) its interpretability separates it from black-box methods; 3)\nfinally, its computational efficiency makes it scalable for high-dimensional\ntime series. In this paper, we use several simulated and real data sets to\nillustrate these advantages. We will also briefly discuss some extensions of\nour model.\n",
        "published": "2015-02-11T21:56:13Z",
        "pdf_link": "http://arxiv.org/pdf/1502.03466v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.03696v7",
        "title": "Monte Carlo Planning method estimates planning horizons during\n  interactive social exchange",
        "summary": "  Reciprocating interactions represent a central feature of all human\nexchanges. They have been the target of various recent experiments, with\nhealthy participants and psychiatric populations engaging as dyads in\nmulti-round exchanges such as a repeated trust task. Behaviour in such\nexchanges involves complexities related to each agent's preference for equity\nwith their partner, beliefs about the partner's appetite for equity, beliefs\nabout the partner's model of their partner, and so on. Agents may also plan\ndifferent numbers of steps into the future. Providing a computationally precise\naccount of the behaviour is an essential step towards understanding what\nunderlies choices. A natural framework for this is that of an interactive\npartially observable Markov decision process (IPOMDP). However, the various\ncomplexities make IPOMDPs inordinately computationally challenging. Here, we\nshow how to approximate the solution for the multi-round trust task using a\nvariant of the Monte-Carlo tree search algorithm. We demonstrate that the\nalgorithm is efficient and effective, and therefore can be used to invert\nobservations of behavioural choices. We use generated behaviour to elucidate\nthe richness and sophistication of interactive inference.\n",
        "published": "2015-02-12T15:18:43Z",
        "pdf_link": "http://arxiv.org/pdf/1502.03696v7"
    },
    {
        "id": "http://arxiv.org/abs/1502.04315v1",
        "title": "Fast and Memory-Efficient Significant Pattern Mining via Permutation\n  Testing",
        "summary": "  We present a novel algorithm, Westfall-Young light, for detecting patterns,\nsuch as itemsets and subgraphs, which are statistically significantly enriched\nin one of two classes. Our method corrects rigorously for multiple hypothesis\ntesting and correlations between patterns through the Westfall-Young\npermutation procedure, which empirically estimates the null distribution of\npattern frequencies in each class via permutations. In our experiments,\nWestfall-Young light dramatically outperforms the current state-of-the-art\napproach in terms of both runtime and memory efficiency on popular real-world\nbenchmark datasets for pattern mining. The key to this efficiency is that\nunlike all existing methods, our algorithm neither needs to solve the\nunderlying frequent itemset mining problem anew for each permutation nor needs\nto store the occurrence list of all frequent patterns. Westfall-Young light\nopens the door to significant pattern mining on large datasets that previously\nled to prohibitive runtime or memory costs.\n",
        "published": "2015-02-15T14:46:13Z",
        "pdf_link": "http://arxiv.org/pdf/1502.04315v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.04416v5",
        "title": "Random Subspace Learning Approach to High-Dimensional Outliers Detection",
        "summary": "  We introduce and develop a novel approach to outlier detection based on\nadaptation of random subspace learning. Our proposed method handles both\nhigh-dimension low-sample size and traditional low-dimensional high-sample size\ndatasets. Essentially, we avoid the computational bottleneck of techniques like\nminimum covariance determinant (MCD) by computing the needed determinants and\nassociated measures in much lower dimensional subspaces. Both theoretical and\ncomputational development of our approach reveal that it is computationally\nmore efficient than the regularized methods in high-dimensional low-sample\nsize, and often competes favorably with existing methods as far as the\npercentage of correct outlier detection is concerned.\n",
        "published": "2015-02-16T03:31:02Z",
        "pdf_link": "http://arxiv.org/pdf/1502.04416v5"
    },
    {
        "id": "http://arxiv.org/abs/1502.04631v2",
        "title": "Clustering and Inference From Pairwise Comparisons",
        "summary": "  Given a set of pairwise comparisons, the classical ranking problem computes a\nsingle ranking that best represents the preferences of all users. In this\npaper, we study the problem of inferring individual preferences, arising in the\ncontext of making personalized recommendations. In particular, we assume that\nthere are $n$ users of $r$ types; users of the same type provide similar\npairwise comparisons for $m$ items according to the Bradley-Terry model. We\npropose an efficient algorithm that accurately estimates the individual\npreferences for almost all users, if there are $r \\max \\{m, n\\}\\log m \\log^2 n$\npairwise comparisons per type, which is near optimal in sample complexity when\n$r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps:\nfirst, for each user, compute the \\emph{net-win} vector which is a projection\nof its $\\binom{m}{2}$-dimensional vector of pairwise comparisons onto an\n$m$-dimensional linear subspace; second, cluster the users based on the net-win\nvectors; third, estimate a single preference for each cluster separately. The\nnet-win vectors are much less noisy than the high dimensional vectors of\npairwise comparisons and clustering is more accurate after the projection as\nconfirmed by numerical experiments. Moreover, we show that, when a cluster is\nonly approximately correct, the maximum likelihood estimation for the\nBradley-Terry model is still close to the true preference.\n",
        "published": "2015-02-16T17:08:48Z",
        "pdf_link": "http://arxiv.org/pdf/1502.04631v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.05312v2",
        "title": "Predictive Entropy Search for Bayesian Optimization with Unknown\n  Constraints",
        "summary": "  Unknown constraints arise in many types of expensive black-box optimization\nproblems. Several methods have been proposed recently for performing Bayesian\noptimization with constraints, based on the expected improvement (EI)\nheuristic. However, EI can lead to pathologies when used with constraints. For\nexample, in the case of decoupled constraints---i.e., when one can\nindependently evaluate the objective or the constraints---EI can encounter a\npathology that prevents exploration. Additionally, computing EI requires a\ncurrent best solution, which may not exist if none of the data collected so far\nsatisfy the constraints. By contrast, information-based approaches do not\nsuffer from these failure modes. In this paper, we present a new\ninformation-based method called Predictive Entropy Search with Constraints\n(PESC). We analyze the performance of PESC and show that it compares favorably\nto EI-based approaches on synthetic and benchmark problems, as well as several\nreal-world examples. We demonstrate that PESC is an effective algorithm that\nprovides a promising direction towards a unified solution for constrained\nBayesian optimization.\n",
        "published": "2015-02-18T17:39:30Z",
        "pdf_link": "http://arxiv.org/pdf/1502.05312v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.05313v2",
        "title": "Variational Optimization of Annealing Schedules",
        "summary": "  Annealed importance sampling (AIS) is a common algorithm to estimate\npartition functions of useful stochastic models. One important problem for\nobtaining accurate AIS estimates is the selection of an annealing schedule.\nConventionally, an annealing schedule is often determined heuristically or is\nsimply set as a linearly increasing sequence. In this paper, we propose an\nalgorithm for the optimal schedule by deriving a functional that dominates the\nAIS estimation error and by numerically minimizing this functional. We\nexperimentally demonstrate that the proposed algorithm mostly outperforms\nconventional scheduling schemes with large quantization numbers.\n",
        "published": "2015-02-18T17:45:44Z",
        "pdf_link": "http://arxiv.org/pdf/1502.05313v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.05336v2",
        "title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural\n  Networks",
        "summary": "  Large multilayer neural networks trained with backpropagation have recently\nachieved state-of-the-art results in a wide range of problems. However, using\nbackprop for neural net learning still has some disadvantages, e.g., having to\ntune a large number of hyperparameters to the data, lack of calibrated\nprobabilistic predictions, and a tendency to overfit the training data. In\nprinciple, the Bayesian approach to learning neural networks does not have\nthese problems. However, existing Bayesian techniques lack scalability to large\ndataset and network sizes. In this work we present a novel scalable method for\nlearning Bayesian neural networks, called probabilistic backpropagation (PBP).\nSimilar to classical backpropagation, PBP works by computing a forward\npropagation of probabilities through the network and then doing a backward\ncomputation of gradients. A series of experiments on ten real-world datasets\nshow that PBP is significantly faster than other techniques, while offering\ncompetitive predictive abilities. Our experiments also show that PBP provides\naccurate estimates of the posterior variance on the network weights.\n",
        "published": "2015-02-18T18:45:17Z",
        "pdf_link": "http://arxiv.org/pdf/1502.05336v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.05700v2",
        "title": "Scalable Bayesian Optimization Using Deep Neural Networks",
        "summary": "  Bayesian optimization is an effective methodology for the global optimization\nof functions with expensive evaluations. It relies on querying a distribution\nover functions defined by a relatively cheap surrogate model. An accurate model\nfor this distribution over functions is critical to the effectiveness of the\napproach, and is typically fit using Gaussian processes (GPs). However, since\nGPs scale cubically with the number of observations, it has been challenging to\nhandle objectives whose optimization requires many evaluations, and as such,\nmassively parallelizing the optimization.\n  In this work, we explore the use of neural networks as an alternative to GPs\nto model distributions over functions. We show that performing adaptive basis\nfunction regression with a neural network as the parametric form performs\ncompetitively with state-of-the-art GP-based approaches, but scales linearly\nwith the number of data rather than cubically. This allows us to achieve a\npreviously intractable degree of parallelism, which we apply to large scale\nhyperparameter optimization, rapidly finding competitive models on benchmark\nobject recognition tasks using convolutional networks, and image caption\ngeneration using neural language models.\n",
        "published": "2015-02-19T20:51:27Z",
        "pdf_link": "http://arxiv.org/pdf/1502.05700v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.06689v1",
        "title": "1-Bit Matrix Completion under Exact Low-Rank Constraint",
        "summary": "  We consider the problem of noisy 1-bit matrix completion under an exact rank\nconstraint on the true underlying matrix $M^*$. Instead of observing a subset\nof the noisy continuous-valued entries of a matrix $M^*$, we observe a subset\nof noisy 1-bit (or binary) measurements generated according to a probabilistic\nmodel. We consider constrained maximum likelihood estimation of $M^*$, under a\nconstraint on the entry-wise infinity-norm of $M^*$ and an exact rank\nconstraint. This is in contrast to previous work which has used convex\nrelaxations for the rank. We provide an upper bound on the matrix estimation\nerror under this model. Compared to the existing results, our bound has faster\nconvergence rate with matrix dimensions when the fraction of revealed 1-bit\nobservations is fixed, independent of the matrix dimensions. We also propose an\niterative algorithm for solving our nonconvex optimization with a certificate\nof global optimality of the limiting point. This algorithm is based on low rank\nfactorization of $M^*$. We validate the method on synthetic and real data with\nimproved performance over existing methods.\n",
        "published": "2015-02-24T05:38:31Z",
        "pdf_link": "http://arxiv.org/pdf/1502.06689v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.07017v1",
        "title": "On Convolutional Approximations to Linear Dimensionality Reduction\n  Operators for Large Scale Data Processing",
        "summary": "  In this paper, we examine the problem of approximating a general linear\ndimensionality reduction (LDR) operator, represented as a matrix $A \\in\n\\mathbb{R}^{m \\times n}$ with $m < n$, by a partial circulant matrix with rows\nrelated by circular shifts. Partial circulant matrices admit fast\nimplementations via Fourier transform methods and subsampling operations; our\ninvestigation here is motivated by a desire to leverage these potential\ncomputational improvements in large-scale data processing tasks. We establish a\nfundamental result, that most large LDR matrices (whose row spaces are\nuniformly distributed) in fact cannot be well approximated by partial circulant\nmatrices. Then, we propose a natural generalization of the partial circulant\napproximation framework that entails approximating the range space of a given\nLDR operator $A$ over a restricted domain of inputs, using a matrix formed as a\nproduct of a partial circulant matrix having $m '> m$ rows and a $m \\times k$\n'post processing' matrix. We introduce a novel algorithmic technique, based on\nsparse matrix factorization, for identifying the factors comprising such\napproximations, and provide preliminary evidence to demonstrate the potential\nof this approach.\n",
        "published": "2015-02-25T00:45:41Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07017v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.07104v1",
        "title": "A Note on the Kullback-Leibler Divergence for the von Mises-Fisher\n  distribution",
        "summary": "  We present a derivation of the Kullback Leibler (KL)-Divergence (also known\nas Relative Entropy) for the von Mises Fisher (VMF) Distribution in\n$d$-dimensions.\n",
        "published": "2015-02-25T10:08:34Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07104v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.07334v5",
        "title": "Sparse Multivariate Factor Regression",
        "summary": "  We consider the problem of multivariate regression in a setting where the\nrelevant predictors could be shared among different responses. We propose an\nalgorithm which decomposes the coefficient matrix into the product of a long\nmatrix and a wide matrix, with an elastic net penalty on the former and an\n$\\ell_1$ penalty on the latter. The first matrix linearly transforms the\npredictors to a set of latent factors, and the second one regresses the\nresponses on these factors. Our algorithm simultaneously performs dimension\nreduction and coefficient estimation and automatically estimates the number of\nlatent factors from the data. Our formulation results in a non-convex\noptimization problem, which despite its flexibility to impose effective\nlow-dimensional structure, is difficult, or even impossible, to solve exactly\nin a reasonable time. We specify an optimization algorithm based on alternating\nminimization with three different sets of updates to solve this non-convex\nproblem and provide theoretical results on its convergence and optimality.\nFinally, we demonstrate the effectiveness of our algorithm via experiments on\nsimulated and real data.\n",
        "published": "2015-02-25T20:40:30Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07334v5"
    },
    {
        "id": "http://arxiv.org/abs/1503.00214v3",
        "title": "Matrix Completion with Noisy Entries and Outliers",
        "summary": "  This paper considers the problem of matrix completion when the observed\nentries are noisy and contain outliers. It begins with introducing a new\noptimization criterion for which the recovered matrix is defined as its\nsolution. This criterion uses the celebrated Huber function from the robust\nstatistics literature to downweigh the effects of outliers. A practical\nalgorithm is developed to solve the optimization involved. This algorithm is\nfast, straightforward to implement, and monotonic convergent. Furthermore, the\nproposed methodology is theoretically shown to be stable in a well defined\nsense. Its promising empirical performance is demonstrated via a sequence of\nsimulation experiments, including image inpainting.\n",
        "published": "2015-03-01T04:24:42Z",
        "pdf_link": "http://arxiv.org/pdf/1503.00214v3"
    },
    {
        "id": "http://arxiv.org/abs/1503.01442v2",
        "title": "Statistical Limits of Convex Relaxations",
        "summary": "  Many high dimensional sparse learning problems are formulated as nonconvex\noptimization. A popular approach to solve these nonconvex optimization problems\nis through convex relaxations such as linear and semidefinite programming. In\nthis paper, we study the statistical limits of convex relaxations.\nParticularly, we consider two problems: Mean estimation for sparse principal\nsubmatrix and edge probability estimation for stochastic block model. We\nexploit the sum-of-squares relaxation hierarchy to sharply characterize the\nlimits of a broad class of convex relaxations. Our result shows statistical\noptimality needs to be compromised for achieving computational tractability\nusing convex relaxations. Compared with existing results on computational lower\nbounds for statistical problems, which consider general polynomial-time\nalgorithms and rely on computational hardness hypotheses on problems like\nplanted clique detection, our theory focuses on a broad class of convex\nrelaxations and does not rely on unproven hypotheses.\n",
        "published": "2015-03-04T20:12:11Z",
        "pdf_link": "http://arxiv.org/pdf/1503.01442v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.01494v1",
        "title": "Local Expectation Gradients for Doubly Stochastic Variational Inference",
        "summary": "  We introduce local expectation gradients which is a general purpose\nstochastic variational inference algorithm for constructing stochastic\ngradients through sampling from the variational distribution. This algorithm\ndivides the problem of estimating the stochastic gradients over multiple\nvariational parameters into smaller sub-tasks so that each sub-task exploits\nintelligently the information coming from the most relevant part of the\nvariational distribution. This is achieved by performing an exact expectation\nover the single random variable that mostly correlates with the variational\nparameter of interest resulting in a Rao-Blackwellized estimate that has low\nvariance and can work efficiently for both continuous and discrete random\nvariables. Furthermore, the proposed algorithm has interesting similarities\nwith Gibbs sampling but at the same time, unlike Gibbs sampling, it can be\ntrivially parallelized.\n",
        "published": "2015-03-04T22:53:41Z",
        "pdf_link": "http://arxiv.org/pdf/1503.01494v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.02182v1",
        "title": "Latent Gaussian Processes for Distribution Estimation of Multivariate\n  Categorical Data",
        "summary": "  Multivariate categorical data occur in many applications of machine learning.\nOne of the main difficulties with these vectors of categorical variables is\nsparsity. The number of possible observations grows exponentially with vector\nlength, but dataset diversity might be poor in comparison. Recent models have\ngained significant improvement in supervised tasks with this data. These models\nembed observations in a continuous space to capture similarities between them.\nBuilding on these ideas we propose a Bayesian model for the unsupervised task\nof distribution estimation of multivariate categorical data. We model vectors\nof categorical variables as generated from a non-linear transformation of a\ncontinuous latent space. Non-linearity captures multi-modality in the\ndistribution. The continuous representation addresses sparsity. Our model ties\ntogether many existing models, linking the linear categorical latent Gaussian\nmodel, the Gaussian process latent variable model, and Gaussian process\nclassification. We derive inference for our model based on recent developments\nin sampling based variational inference. We show empirically that the model\noutperforms its linear and discrete counterparts in imputation tasks of sparse\ndata.\n",
        "published": "2015-03-07T14:53:39Z",
        "pdf_link": "http://arxiv.org/pdf/1503.02182v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.02424v2",
        "title": "Improving the Gaussian Process Sparse Spectrum Approximation by\n  Representing Uncertainty in Frequency Inputs",
        "summary": "  Standard sparse pseudo-input approximations to the Gaussian process (GP)\ncannot handle complex functions well. Sparse spectrum alternatives attempt to\nanswer this but are known to over-fit. We suggest the use of variational\ninference for the sparse spectrum approximation to avoid both issues. We model\nthe covariance function with a finite Fourier series approximation and treat it\nas a random variable. The random covariance function has a posterior, on which\na variational distribution is placed. The variational distribution transforms\nthe random covariance function to fit the data. We study the properties of our\napproximate inference, compare it to alternative ones, and extend it to the\ndistributed and stochastic domains. Our approximation captures complex\nfunctions better than standard approaches and avoids over-fitting.\n",
        "published": "2015-03-09T11:04:58Z",
        "pdf_link": "http://arxiv.org/pdf/1503.02424v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.02698v2",
        "title": "Graphical Exponential Screening",
        "summary": "  In high dimensions we propose and analyze an aggregation estimator of the\nprecision matrix for Gaussian graphical models. This estimator, called\ngraphical Exponential Screening (gES), linearly combines a suitable set of\nindividual estimators with different underlying graphs, and balances the\nestimation error and sparsity. We study the risk of this aggregation estimator\nand show that it is comparable to that of the best estimator based on a single\ngraph, chosen by an oracle. Numerical performance of our method is investigated\nusing both simulated and real datasets, in comparison with some state-of-art\nestimation procedures.\n",
        "published": "2015-03-09T21:10:48Z",
        "pdf_link": "http://arxiv.org/pdf/1503.02698v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.02768v2",
        "title": "Novel Bernstein-like Concentration Inequalities for the Missing Mass",
        "summary": "  We are concerned with obtaining novel concentration inequalities for the\nmissing mass, i.e. the total probability mass of the outcomes not observed in\nthe sample. We not only derive - for the first time - distribution-free\nBernstein-like deviation bounds with sublinear exponents in deviation size for\nmissing mass, but also improve the results of McAllester and Ortiz (2003)\nandBerend and Kontorovich (2013, 2012) for small deviations which is the most\ninteresting case in learning theory. It is known that the majority of standard\ninequalities cannot be directly used to analyze heterogeneous sums i.e. sums\nwhose terms have large difference in magnitude. Our generic and intuitive\napproach shows that the heterogeneity issue introduced in McAllester and Ortiz\n(2003) is resolvable at least in the case of missing mass via regulating the\nterms using our novel thresholding technique.\n",
        "published": "2015-03-10T04:38:46Z",
        "pdf_link": "http://arxiv.org/pdf/1503.02768v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.03082v2",
        "title": "Learning the Structure for Structured Sparsity",
        "summary": "  Structured sparsity has recently emerged in statistics, machine learning and\nsignal processing as a promising paradigm for learning in high-dimensional\nsettings. All existing methods for learning under the assumption of structured\nsparsity rely on prior knowledge on how to weight (or how to penalize)\nindividual subsets of variables during the subset selection process, which is\nnot available in general. Inferring group weights from data is a key open\nresearch problem in structured sparsity.In this paper, we propose a Bayesian\napproach to the problem of group weight learning. We model the group weights as\nhyperparameters of heavy-tailed priors on groups of variables and derive an\napproximate inference scheme to infer these hyperparameters. We empirically\nshow that we are able to recover the model hyperparameters when the data are\ngenerated from the model, and we demonstrate the utility of learning weights in\nsynthetic and real denoising problems.\n",
        "published": "2015-03-10T20:09:13Z",
        "pdf_link": "http://arxiv.org/pdf/1503.03082v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.05684v1",
        "title": "Non-parametric Bayesian Models of Response Function in Dynamic Image\n  Sequences",
        "summary": "  Estimation of response functions is an important task in dynamic medical\nimaging. This task arises for example in dynamic renal scintigraphy, where\nimpulse response or retention functions are estimated, or in functional\nmagnetic resonance imaging where hemodynamic response functions are required.\nThese functions can not be observed directly and their estimation is\ncomplicated because the recorded images are subject to superposition of\nunderlying signals. Therefore, the response functions are estimated via blind\nsource separation and deconvolution. Performance of this algorithm heavily\ndepends on the used models of the response functions. Response functions in\nreal image sequences are rather complicated and finding a suitable parametric\nform is problematic. In this paper, we study estimation of the response\nfunctions using non-parametric Bayesian priors. These priors were designed to\nfavor desirable properties of the functions, such as sparsity or smoothness.\nThese assumptions are used within hierarchical priors of the blind source\nseparation and deconvolution algorithm. Comparison of the resulting algorithms\nwith these priors is performed on synthetic dataset as well as on real datasets\nfrom dynamic renal scintigraphy. It is shown that flexible non-parametric\npriors improve estimation of response functions in both cases. MATLAB\nimplementation of the resulting algorithms is freely available for download.\n",
        "published": "2015-03-19T09:53:34Z",
        "pdf_link": "http://arxiv.org/pdf/1503.05684v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.06134v2",
        "title": "A Bennett Inequality for the Missing Mass",
        "summary": "  Novel concentration inequalities are obtained for the missing mass, i.e. the\ntotal probability mass of the outcomes not observed in the sample. We derive\ndistribution-free deviation bounds with sublinear exponents in deviation size\nfor missing mass and improve the results of Berend and Kontorovich (2013) and\nYari Saeed Khanloo and Haffari (2015) for small deviations which is the most\nimportant case in learning theory.\n",
        "published": "2015-03-20T16:08:56Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06134v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.06432v1",
        "title": "Indian Buffet process for model selection in convolved multiple-output\n  Gaussian processes",
        "summary": "  Multi-output Gaussian processes have received increasing attention during the\nlast few years as a natural mechanism to extend the powerful flexibility of\nGaussian processes to the setup of multiple output variables. The key point\nhere is the ability to design kernel functions that allow exploiting the\ncorrelations between the outputs while fulfilling the positive definiteness\nrequisite for the covariance function. Alternatives to construct these\ncovariance functions are the linear model of coregionalization and process\nconvolutions. Each of these methods demand the specification of the number of\nlatent Gaussian process used to build the covariance function for the outputs.\nWe propose in this paper, the use of an Indian Buffet process as a way to\nperform model selection over the number of latent Gaussian processes. This type\nof model is particularly important in the context of latent force models, where\nthe latent forces are associated to physical quantities like protein profiles\nor latent forces in mechanical systems. We use variational inference to\nestimate posterior distributions over the variables involved, and show examples\nof the model performance over artificial data, a motion capture dataset, and a\ngene expression dataset.\n",
        "published": "2015-03-22T14:15:04Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06432v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.08356v4",
        "title": "Efficient Online Minimization for Low-Rank Subspace Clustering",
        "summary": "  Low-rank representation~(LRR) has been a significant method for segmenting\ndata that are generated from a union of subspaces. It is, however, known that\nsolving the LRR program is challenging in terms of time complexity and memory\nfootprint, in that the size of the nuclear norm regularized matrix is\n$n$-by-$n$ (where $n$ is the number of samples). In this paper, we thereby\ndevelop a fast online implementation of LRR that reduces the memory cost from\n$O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and $d$ being some\nestimated rank~($d < p \\ll n$). The crux for this end is a non-convex\nreformulation of the LRR program, which pursues the basis dictionary that\ngenerates the (uncorrupted) observations. We build the theoretical guarantee\nthat the sequence of the solutions produced by our algorithm converges to a\nstationary point of the empirical and the expected loss function\nasymptotically. Extensive experiments on synthetic and realistic datasets\nfurther substantiate that our algorithm is fast, robust and memory efficient.\n",
        "published": "2015-03-28T21:56:35Z",
        "pdf_link": "http://arxiv.org/pdf/1503.08356v4"
    },
    {
        "id": "http://arxiv.org/abs/1503.08727v1",
        "title": "A Parzen-based distance between probability measures as an alternative\n  of summary statistics in Approximate Bayesian Computation",
        "summary": "  Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo\nmethods. ABC methods use a comparison between simulated data, using different\nparameters drew from a prior distribution, and observed data. This comparison\nprocess is based on computing a distance between the summary statistics from\nthe simulated data and the observed data. For complex models, it is usually\ndifficult to define a methodology for choosing or constructing the summary\nstatistics. Recently, a nonparametric ABC has been proposed, that uses a\ndissimilarity measure between discrete distributions based on empirical kernel\nembeddings as an alternative for summary statistics. The nonparametric ABC\noutperforms other methods including ABC, kernel ABC or synthetic likelihood\nABC. However, it assumes that the probability distributions are discrete, and\nit is not robust when dealing with few observations. In this paper, we propose\nto apply kernel embeddings using an smoother density estimator or Parzen\nestimator for comparing the empirical data distributions, and computing the ABC\nposterior. Synthetic data and real data were used to test the Bayesian\ninference of our method. We compare our method with respect to state-of-the-art\nmethods, and demonstrate that our method is a robust estimator of the posterior\ndistribution in terms of the number of observations.\n",
        "published": "2015-03-30T16:10:23Z",
        "pdf_link": "http://arxiv.org/pdf/1503.08727v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.00052v2",
        "title": "Gaussian Process for Noisy Inputs with Ordering Constraints",
        "summary": "  We study the Gaussian Process regression model in the context of training\ndata with noise in both input and output. The presence of two sources of noise\nmakes the task of learning accurate predictive models extremely challenging.\nHowever, in some instances additional constraints may be available that can\nreduce the uncertainty in the resulting predictive models. In particular, we\nconsider the case of monotonically ordered latent input, which occurs in many\napplication domains that deal with temporal data. We present a novel inference\nand learning approach based on non-parametric Gaussian variational\napproximation to learn the GP model while taking into account the new\nconstraints. The resulting strategy allows one to gain access to posterior\nestimates of both the input and the output and results in improved predictive\nperformance. We compare our proposed models to state-of-the-art Noisy Input\nGaussian Process (NIGP) and other competing approaches on synthetic and real\nsea-level rise data. Experimental results suggest that the proposed approach\nconsistently outperforms selected methods while, at the same time, reducing the\ncomputational costs of learning and inference.\n",
        "published": "2015-06-30T22:28:06Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00052v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.00507v1",
        "title": "Identification of stable models via nonparametric prediction error\n  methods",
        "summary": "  A new Bayesian approach to linear system identification has been proposed in\na series of recent papers. The main idea is to frame linear system\nidentification as predictor estimation in an infinite dimensional space, with\nthe aid of regularization/Bayesian techniques. This approach guarantees the\nidentification of stable predictors based on the prediction error minimization.\nUnluckily, the stability of the predictors does not guarantee the stability of\nthe impulse response of the system. In this paper we propose and compare\nvarious techniques to address this issue. Simulations results comparing these\ntechniques will be provided.\n",
        "published": "2015-07-02T10:20:52Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00507v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.00543v1",
        "title": "Classical vs. Bayesian methods for linear system identification: point\n  estimators and confidence sets",
        "summary": "  This paper compares classical parametric methods with recently developed\nBayesian methods for system identification. A Full Bayes solution is considered\ntogether with one of the standard approximations based on the Empirical Bayes\nparadigm. Results regarding point estimators for the impulse response as well\nas for confidence regions are reported.\n",
        "published": "2015-07-02T12:28:41Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00543v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.00566v1",
        "title": "Anomaly Detection and Removal Using Non-Stationary Gaussian Processes",
        "summary": "  This paper proposes a novel Gaussian process approach to fault removal in\ntime-series data. Fault removal does not delete the faulty signal data but,\ninstead, massages the fault from the data. We assume that only one fault occurs\nat any one time and model the signal by two separate non-parametric Gaussian\nprocess models for both the physical phenomenon and the fault. In order to\nfacilitate fault removal we introduce the Markov Region Link kernel for\nhandling non-stationary Gaussian processes. This kernel is piece-wise\nstationary but guarantees that functions generated by it and their derivatives\n(when required) are everywhere continuous. We apply this kernel to the removal\nof drift and bias errors in faulty sensor data and also to the recovery of EOG\nartifact corrupted EEG signals.\n",
        "published": "2015-07-02T13:11:04Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00566v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.01059v2",
        "title": "Remarks on kernel Bayes' rule",
        "summary": "  Kernel Bayes' rule has been proposed as a nonparametric kernel-based method\nto realize Bayesian inference in reproducing kernel Hilbert spaces. However, we\ndemonstrate both theoretically and experimentally that the prediction result by\nkernel Bayes' rule is in some cases unnatural. We consider that this phenomenon\nis in part due to the fact that the assumptions in kernel Bayes' rule do not\nhold in general.\n",
        "published": "2015-07-04T02:03:02Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01059v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.01661v1",
        "title": "Semiblind Hyperspectral Unmixing in the Presence of Spectral Library\n  Mismatches",
        "summary": "  The dictionary-aided sparse regression (SR) approach has recently emerged as\na promising alternative to hyperspectral unmixing (HU) in remote sensing. By\nusing an available spectral library as a dictionary, the SR approach identifies\nthe underlying materials in a given hyperspectral image by selecting a small\nsubset of spectral samples in the dictionary to represent the whole image. A\ndrawback with the current SR developments is that an actual spectral signature\nin the scene is often assumed to have zero mismatch with its corresponding\ndictionary sample, and such an assumption is considered too ideal in practice.\nIn this paper, we tackle the spectral signature mismatch problem by proposing a\ndictionary-adjusted nonconvex sparsity-encouraging regression (DANSER)\nframework. The main idea is to incorporate dictionary correcting variables in\nan SR formulation. A simple and low per-iteration complexity algorithm is\ntailor-designed for practical realization of DANSER. Using the same dictionary\ncorrecting idea, we also propose a robust subspace solution for dictionary\npruning. Extensive simulations and real-data experiments show that the proposed\nmethod is effective in mitigating the undesirable spectral signature mismatch\neffects.\n",
        "published": "2015-07-07T03:00:17Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01661v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.02925v3",
        "title": "Completely random measures for modelling block-structured networks",
        "summary": "  Many statistical methods for network data parameterize the edge-probability\nby attributing latent traits to the vertices such as block structure and assume\nexchangeability in the sense of the Aldous-Hoover representation theorem.\nEmpirical studies of networks indicate that many real-world networks have a\npower-law distribution of the vertices which in turn implies the number of\nedges scale slower than quadratically in the number of vertices. These\nassumptions are fundamentally irreconcilable as the Aldous-Hoover theorem\nimplies quadratic scaling of the number of edges. Recently Caron and Fox (2014)\nproposed the use of a different notion of exchangeability due to Kallenberg\n(2009) and obtained a network model which admits power-law behaviour while\nretaining desirable statistical properties, however this model does not capture\nlatent vertex traits such as block-structure. In this work we re-introduce the\nuse of block-structure for network models obeying Kallenberg's notion of\nexchangeability and thereby obtain a model which admits the inference of\nblock-structure and edge inhomogeneity. We derive a simple expression for the\nlikelihood and an efficient sampling method. The obtained model is not\nsignificantly more difficult to implement than existing approaches to\nblock-modelling and performs well on real network datasets.\n",
        "published": "2015-07-10T14:49:52Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02925v3"
    },
    {
        "id": "http://arxiv.org/abs/1507.03092v2",
        "title": "On the use of Harrell's C for clinical risk prediction via random\n  survival forests",
        "summary": "  Random survival forests (RSF) are a powerful method for risk prediction of\nright-censored outcomes in biomedical research. RSF use the log-rank split\ncriterion to form an ensemble of survival trees. The most common approach to\nevaluate the prediction accuracy of a RSF model is Harrell's concordance index\nfor survival data ('C index'). Conceptually, this strategy implies that the\nsplit criterion in RSF is different from the evaluation criterion of interest.\nThis discrepancy can be overcome by using Harrell's C for both node splitting\nand evaluation. We compare the difference between the two split criteria\nanalytically and in simulation studies with respect to the preference of more\nunbalanced splits, termed end-cut preference (ECP). Specifically, we show that\nthe log-rank statistic has a stronger ECP compared to the C index. In\nsimulation studies and with the help of two medical data sets we demonstrate\nthat the accuracy of RSF predictions, as measured by Harrell's C, can be\nimproved if the log-rank statistic is replaced by the C index for node\nsplitting. This is especially true in situations where the censoring rate or\nthe fraction of informative continuous predictor variables is high. Conversely,\nlog-rank splitting is preferable in noisy scenarios. Both C-based and log-rank\nsplitting are implemented in the R~package ranger. We recommend Harrell's C as\nsplit criterion for use in smaller scale clinical studies and the log-rank\nsplit criterion for use in large-scale 'omics' studies.\n",
        "published": "2015-07-11T10:31:50Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03092v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.03176v1",
        "title": "Dependent Indian Buffet Process-based Sparse Nonparametric Nonnegative\n  Matrix Factorization",
        "summary": "  Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two\noptimized nonnegative matrices appropriate for the intended applications. The\nmethod has been widely used for unsupervised learning tasks, including\nrecommender systems (rating matrix of users by items) and document clustering\n(weighting matrix of papers by keywords). However, traditional NMF methods\ntypically assume the number of latent factors (i.e., dimensionality of the\nloading matrices) to be fixed. This assumption makes them inflexible for many\napplications. In this paper, we propose a nonparametric NMF framework to\nmitigate this issue by using dependent Indian Buffet Processes (dIBP). In a\nnutshell, we apply a correlation function for the generation of two stick\nweights associated with each pair of columns of loading matrices, while still\nmaintaining their respective marginal distribution specified by IBP. As a\nconsequence, the generation of two loading matrices will be column-wise\n(indirectly) correlated. Under this same framework, two classes of correlation\nfunction are proposed (1) using Bivariate beta distribution and (2) using\nCopula function. Both methods allow us to adopt our work for various\napplications by flexibly choosing an appropriate parameter settings. Compared\nwith the other state-of-the art approaches in this area, such as using Gaussian\nProcess (GP)-based dIBP, our work is seen to be much more flexible in terms of\nallowing the two corresponding binary matrix columns to have greater variations\nin their non-zero entries. Our experiments on the real-world and synthetic\ndatasets show that three proposed models perform well on the document\nclustering task comparing standard NMF without predefining the dimension for\nthe factor matrices, and the Bivariate beta distribution-based and Copula-based\nmodels have better flexibility than the GP-based model.\n",
        "published": "2015-07-12T01:41:12Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03176v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03228v1",
        "title": "Scalable Bayesian Inference for Excitatory Point Process Networks",
        "summary": "  Networks capture our intuition about relationships in the world. They\ndescribe the friendships between Facebook users, interactions in financial\nmarkets, and synapses connecting neurons in the brain. These networks are\nrichly structured with cliques of friends, sectors of stocks, and a smorgasbord\nof cell types that govern how neurons connect. Some networks, like social\nnetwork friendships, can be directly observed, but in many cases we only have\nan indirect view of the network through the actions of its constituents and an\nunderstanding of how the network mediates that activity. In this work, we focus\non the problem of latent network discovery in the case where the observable\nactivity takes the form of a mutually-excitatory point process known as a\nHawkes process. We build on previous work that has taken a Bayesian approach to\nthis problem, specifying prior distributions over the latent network structure\nand a likelihood of observed activity given this network. We extend this work\nby proposing a discrete-time formulation and developing a computationally\nefficient stochastic variational inference (SVI) algorithm that allows us to\nscale the approach to long sequences of observations. We demonstrate our\nalgorithm on the calcium imaging data used in the Chalearn neural connectomics\nchallenge.\n",
        "published": "2015-07-12T12:59:28Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03228v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03285v1",
        "title": "Scatter Matrix Concordance: A Diagnostic for Regressions on Subsets of\n  Data",
        "summary": "  Linear regression models depend directly on the design matrix and its\nproperties. Techniques that efficiently estimate model coefficients by\npartitioning rows of the design matrix are increasingly popular for large-scale\nproblems because they fit well with modern parallel computing architectures. We\npropose a simple measure of {\\em concordance} between a design matrix and a\nsubset of its rows that estimates how well a subset captures the\nvariance-covariance structure of a larger data set. We illustrate the use of\nthis measure in a heuristic method for selecting row partition sizes that\nbalance statistical and computational efficiency goals in real-world problems.\n",
        "published": "2015-07-12T22:51:07Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03285v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.04436v1",
        "title": "Joint Tensor Factorization and Outlying Slab Suppression with\n  Applications",
        "summary": "  We consider factoring low-rank tensors in the presence of outlying slabs.\nThis problem is important in practice, because data collected in many\nreal-world applications, such as speech, fluorescence, and some social network\ndata, fit this paradigm. Prior work tackles this problem by iteratively\nselecting a fixed number of slabs and fitting, a procedure which may not\nconverge. We formulate this problem from a group-sparsity promoting point of\nview, and propose an alternating optimization framework to handle the\ncorresponding $\\ell_p$ ($0<p\\leq 1$) minimization-based low-rank tensor\nfactorization problem. The proposed algorithm features a similar per-iteration\ncomplexity as the plain trilinear alternating least squares (TALS) algorithm.\nConvergence of the proposed algorithm is also easy to analyze under the\nframework of alternating optimization and its variants. In addition,\nregularization and constraints can be easily incorporated to make use of\n\\emph{a priori} information on the latent loading factors. Simulations and real\ndata experiments on blind speech separation, fluorescence data analysis, and\nsocial network mining are used to showcase the effectiveness of the proposed\nalgorithm.\n",
        "published": "2015-07-16T02:47:58Z",
        "pdf_link": "http://arxiv.org/pdf/1507.04436v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.04505v1",
        "title": "On the Convergence of Stochastic Variational Inference in Bayesian\n  Networks",
        "summary": "  We highlight a pitfall when applying stochastic variational inference to\ngeneral Bayesian networks. For global random variables approximated by an\nexponential family distribution, natural gradient steps, commonly starting from\na unit length step size, are averaged to convergence. This useful insight into\nthe scaling of initial step sizes is lost when the approximation factorizes\nacross a general Bayesian network, and care must be taken to ensure practical\nconvergence. We experimentally investigate how much of the baby (well-scaled\nsteps) is thrown out with the bath water (exact gradients).\n",
        "published": "2015-07-16T09:37:32Z",
        "pdf_link": "http://arxiv.org/pdf/1507.04505v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.04513v1",
        "title": "Scalable Gaussian Process Classification via Expectation Propagation",
        "summary": "  Variational methods have been recently considered for scaling the training\nprocess of Gaussian process classifiers to large datasets. As an alternative,\nwe describe here how to train these classifiers efficiently using expectation\npropagation. The proposed method allows for handling datasets with millions of\ndata instances. More precisely, it can be used for (i) training in a\ndistributed fashion where the data instances are sent to different nodes in\nwhich the required computations are carried out, and for (ii) maximizing an\nestimate of the marginal likelihood using a stochastic approximation of the\ngradient. Several experiments indicate that the method described is competitive\nwith the variational approach.\n",
        "published": "2015-07-16T10:11:44Z",
        "pdf_link": "http://arxiv.org/pdf/1507.04513v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.05016v2",
        "title": "Incremental Variational Inference for Latent Dirichlet Allocation",
        "summary": "  We introduce incremental variational inference and apply it to latent\nDirichlet allocation (LDA). Incremental variational inference is inspired by\nincremental EM and provides an alternative to stochastic variational inference.\nIncremental LDA can process massive document collections, does not require to\nset a learning rate, converges faster to a local optimum of the variational\nbound and enjoys the attractive property of monotonically increasing it. We\nstudy the performance of incremental LDA on large benchmark data sets. We\nfurther introduce a stochastic approximation of incremental variational\ninference which extends to the asynchronous distributed setting. The resulting\ndistributed algorithm achieves comparable performance as single host\nincremental variational inference, but with a significant speed-up.\n",
        "published": "2015-07-17T16:14:54Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05016v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.05117v1",
        "title": "Fast Approximate Bayesian Computation for Estimating Parameters in\n  Differential Equations",
        "summary": "  Approximate Bayesian computation (ABC) using a sequential Monte Carlo method\nprovides a comprehensive platform for parameter estimation, model selection and\nsensitivity analysis in differential equations. However, this method, like\nother Monte Carlo methods, incurs a significant computational cost as it\nrequires explicit numerical integration of differential equations to carry out\ninference. In this paper we propose a novel method for circumventing the\nrequirement of explicit integration by using derivatives of Gaussian processes\nto smooth the observations from which parameters are estimated. We evaluate our\nmethods using synthetic data generated from model biological systems described\nby ordinary and delay differential equations. Upon comparing the performance of\nour method to existing ABC techniques, we demonstrate that it produces\ncomparably reliable parameter estimates at a significantly reduced execution\ntime.\n",
        "published": "2015-07-17T21:03:53Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05117v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.05131v4",
        "title": "Optimal Estimation of Low Rank Density Matrices",
        "summary": "  The density matrices are positively semi-definite Hermitian matrices of unit\ntrace that describe the state of a quantum system. The goal of the paper is to\ndevelop minimax lower bounds on error rates of estimation of low rank density\nmatrices in trace regression models used in quantum state tomography (in\nparticular, in the case of Pauli measurements) with explicit dependence of the\nbounds on the rank and other complexity parameters. Such bounds are established\nfor several statistically relevant distances, including quantum versions of\nKullback-Leibler divergence (relative entropy distance) and of Hellinger\ndistance (so called Bures distance), and Schatten $p$-norm distances. Sharp\nupper bounds and oracle inequalities for least squares estimator with von\nNeumann entropy penalization are obtained showing that minimax lower bounds are\nattained (up to logarithmic factors) for these distances.\n",
        "published": "2015-07-17T23:05:40Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05131v4"
    },
    {
        "id": "http://arxiv.org/abs/1507.05253v2",
        "title": "The Population Posterior and Bayesian Inference on Streams",
        "summary": "  Many modern data analysis problems involve inferences from streaming data.\nHowever, streaming data is not easily amenable to the standard probabilistic\nmodeling approaches, which assume that we condition on finite data. We develop\npopulation variational Bayes, a new approach for using Bayesian modeling to\nanalyze streams of data. It approximates a new type of distribution, the\npopulation posterior, which combines the notion of a population distribution of\nthe data with Bayesian inference in a probabilistic model. We study our method\nwith latent Dirichlet allocation and Dirichlet process mixtures on several\nlarge-scale data sets.\n",
        "published": "2015-07-19T07:19:22Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05253v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.05333v4",
        "title": "Invariant Models for Causal Transfer Learning",
        "summary": "  Methods of transfer learning try to combine knowledge from several related\ntasks (or domains) to improve performance on a test task. Inspired by causal\nmethodology, we relax the usual covariate shift assumption and assume that it\nholds true for a subset of predictor variables: the conditional distribution of\nthe target variable given this subset of predictors is invariant over all\ntasks. We show how this assumption can be motivated from ideas in the field of\ncausality. We focus on the problem of Domain Generalization, in which no\nexamples from the test task are observed. We prove that in an adversarial\nsetting using this subset for prediction is optimal in Domain Generalization;\nwe further provide examples, in which the tasks are sufficiently diverse and\nthe estimator therefore outperforms pooling the data, even on average. If\nexamples from the test task are available, we also provide a method to transfer\nknowledge from the training tasks and exploit all available features for\nprediction. However, we provide no guarantees for this method. We introduce a\npractical method which allows for automatic inference of the above subset and\nprovide corresponding code. We present results on synthetic data sets and a\ngene deletion data set.\n",
        "published": "2015-07-19T20:36:10Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05333v4"
    },
    {
        "id": "http://arxiv.org/abs/1507.05781v1",
        "title": "Gradient Importance Sampling",
        "summary": "  Adaptive Monte Carlo schemes developed over the last years usually seek to\nensure ergodicity of the sampling process in line with MCMC tradition. This\nposes constraints on what is possible in terms of adaptation. In the general\ncase ergodicity can only be guaranteed if adaptation is diminished at a certain\nrate. Importance Sampling approaches offer a way to circumvent this limitation\nand design sampling algorithms that keep adapting. Here I present a gradient\ninformed variant of SMC (and its special case Population Monte Carlo) for\nstatic problems.\n",
        "published": "2015-07-21T10:51:49Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05781v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.05870v2",
        "title": "A statistical perspective of sampling scores for linear regression",
        "summary": "  In this paper, we consider a statistical problem of learning a linear model\nfrom noisy samples. Existing work has focused on approximating the least\nsquares solution by using leverage-based scores as an importance sampling\ndistribution. However, no finite sample statistical guarantees and no\ncomputationally efficient optimal sampling strategies have been proposed. To\nevaluate the statistical properties of different sampling strategies, we\npropose a simple yet effective estimator, which is easy for theoretical\nanalysis and is useful in multitask linear regression. We derive the exact mean\nsquare error of the proposed estimator for any given sampling scores. Based on\nminimizing the mean square error, we propose the optimal sampling scores for\nboth estimator and predictor, and show that they are influenced by the\nnoise-to-signal ratio. Numerical simulations match the theoretical analysis\nwell.\n",
        "published": "2015-07-21T15:25:49Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05870v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.05899v2",
        "title": "Sparsity in Multivariate Extremes with Applications to Anomaly Detection",
        "summary": "  Capturing the dependence structure of multivariate extreme events is a major\nconcern in many fields involving the management of risks stemming from multiple\nsources, e.g. portfolio monitoring, insurance, environmental risk management\nand anomaly detection. One convenient (non-parametric) characterization of\nextremal dependence in the framework of multivariate Extreme Value Theory (EVT)\nis the angular measure, which provides direct information about the probable\n'directions' of extremes, that is, the relative contribution of each\nfeature/coordinate of the 'largest' observations. Modeling the angular measure\nin high dimensional problems is a major challenge for the multivariate analysis\nof rare events. The present paper proposes a novel methodology aiming at\nexhibiting a sparsity pattern within the dependence structure of extremes. This\nis done by estimating the amount of mass spread by the angular measure on\nrepresentative sets of directions, corresponding to specific sub-cones of\n$R^d\\_+$. This dimension reduction technique paves the way towards scaling up\nexisting multivariate EVT methods. Beyond a non-asymptotic study providing a\ntheoretical validity framework for our method, we propose as a direct\napplication a --first-- anomaly detection algorithm based on multivariate EVT.\nThis algorithm builds a sparse 'normal profile' of extreme behaviours, to be\nconfronted with new (possibly abnormal) extreme observations. Illustrative\nexperimental results provide strong empirical evidence of the relevance of our\napproach.\n",
        "published": "2015-07-21T16:27:08Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05899v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.06350v7",
        "title": "Admissibility of a posterior predictive decision rule",
        "summary": "  Recent decades have seen an interest in prediction problems for which\nBayesian methodology has been used ubiquitously. Sampling from or approximating\nthe posterior predictive distribution in a Bayesian model allows one to make\ninferential statements about potentially observable random quantities given\nobserved data. The purpose of this note is to use statistical decision theory\nas a basis to justify the use of a posterior predictive distribution for making\na point prediction.\n",
        "published": "2015-07-22T22:29:30Z",
        "pdf_link": "http://arxiv.org/pdf/1507.06350v7"
    },
    {
        "id": "http://arxiv.org/abs/1507.06615v1",
        "title": "Optimal Learning Rates for Localized SVMs",
        "summary": "  One of the limiting factors of using support vector machines (SVMs) in large\nscale applications are their super-linear computational requirements in terms\nof the number of training samples. To address this issue, several approaches\nthat train SVMs on many small chunks of large data sets separately have been\nproposed in the literature. So far, however, almost all these approaches have\nonly been empirically investigated. In addition, their motivation was always\nbased on computational requirements. In this work, we consider a localized SVM\napproach based upon a partition of the input space. For this local SVM, we\nderive a general oracle inequality. Then we apply this oracle inequality to\nleast squares regression using Gaussian kernels and deduce local learning rates\nthat are essentially minimax optimal under some standard smoothness assumptions\non the regression function. This gives the first motivation for using local\nSVMs that is not based on computational requirements but on theoretical\npredictions on the generalization performance. We further introduce a\ndata-dependent parameter selection method for our local SVM approach and show\nthat this method achieves the same learning rates as before. Finally, we\npresent some larger scale experiments for our localized SVM showing that it\nachieves essentially the same test performance as a global SVM for a fraction\nof the computational requirements. In addition, it turns out that the\ncomputational requirements for the local SVMs are similar to those of a vanilla\nrandom chunk approach, while the achieved test errors are significantly better.\n",
        "published": "2015-07-23T19:03:48Z",
        "pdf_link": "http://arxiv.org/pdf/1507.06615v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.06683v1",
        "title": "Clustering of Modal Valued Symbolic Data",
        "summary": "  Symbolic Data Analysis is based on special descriptions of data - symbolic\nobjects (SO). Such descriptions preserve more detailed information about units\nand their clusters than the usual representations with mean values. A special\nkind of symbolic object is a representation with frequency or probability\ndistributions (modal values). This representation enables us to consider in the\nclustering process the variables of all measurement types at the same time. In\nthe paper a clustering criterion function for SOs is proposed such that the\nrepresentative of each cluster is again composed of distributions of variables'\nvalues over the cluster. The corresponding leaders clustering method is based\non this result. It is also shown that for the corresponding agglomerative\nhierarchical method a generalized Ward's formula holds. Both methods are\ncompatible - they are solving the same clustering optimization problem. The\nleaders method efficiently solves clustering problems with large number of\nunits; while the agglomerative method can be applied alone on the smaller data\nset, or it could be applied on leaders, obtained with compatible\nnonhierarchical clustering method. Such a combination of two compatible methods\nenables us to decide upon the right number of clusters on the basis of the\ncorresponding dendrogram. The proposed methods were applied on different data\nsets. In the paper, some results of clustering of ESS data are presented.\n",
        "published": "2015-07-23T21:07:11Z",
        "pdf_link": "http://arxiv.org/pdf/1507.06683v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.06977v4",
        "title": "String and Membrane Gaussian Processes",
        "summary": "  In this paper we introduce a novel framework for making exact nonparametric\nBayesian inference on latent functions, that is particularly suitable for Big\nData tasks. Firstly, we introduce a class of stochastic processes we refer to\nas string Gaussian processes (string GPs), which are not to be mistaken for\nGaussian processes operating on text. We construct string GPs so that their\nfinite-dimensional marginals exhibit suitable local conditional independence\nstructures, which allow for scalable, distributed, and flexible nonparametric\nBayesian inference, without resorting to approximations, and while ensuring\nsome mild global regularity constraints. Furthermore, string GP priors\nnaturally cope with heterogeneous input data, and the gradient of the learned\nlatent function is readily available for explanatory analysis. Secondly, we\nprovide some theoretical results relating our approach to the standard GP\nparadigm. In particular, we prove that some string GPs are Gaussian processes,\nwhich provides a complementary global perspective on our framework. Finally, we\nderive a scalable and distributed MCMC scheme for supervised learning tasks\nunder string GP priors. The proposed MCMC scheme has computational time\ncomplexity $\\mathcal{O}(N)$ and memory requirement $\\mathcal{O}(dN)$, where $N$\nis the data size and $d$ the dimension of the input space. We illustrate the\nefficacy of the proposed approach on several synthetic and real-world datasets,\nincluding a dataset with $6$ millions input points and $8$ attributes.\n",
        "published": "2015-07-24T19:53:47Z",
        "pdf_link": "http://arxiv.org/pdf/1507.06977v4"
    },
    {
        "id": "http://arxiv.org/abs/1507.08272v2",
        "title": "Context-aware learning for generative models",
        "summary": "  This work studies the class of algorithms for learning with side-information\nthat emerge by extending generative models with embedded context-related\nvariables. Using finite mixture models (FMM) as the prototypical Bayesian\nnetwork, we show that maximum-likelihood estimation (MLE) of parameters through\nexpectation-maximization (EM) improves over the regular unsupervised case and\ncan approach the performances of supervised learning, despite the absence of\nany explicit ground truth data labeling. By direct application of the missing\ninformation principle (MIP), the algorithms' performances are proven to range\nbetween the conventional supervised and unsupervised MLE extremities\nproportionally to the information content of the contextual assistance\nprovided. The acquired benefits regard higher estimation precision, smaller\nstandard errors, faster convergence rates and improved classification accuracy\nor regression fitness shown in various scenarios, while also highlighting\nimportant properties and differences among the outlined situations.\nApplicability is showcased with three real-world unsupervised classification\nscenarios employing Gaussian Mixture Models. Importantly, we exemplify the\nnatural extension of this methodology to any type of generative model by\nderiving an equivalent context-aware algorithm for variational autoencoders\n(VAs), thus broadening the spectrum of applicability to unsupervised deep\nlearning with artificial neural networks. The latter is contrasted with a\nneural-symbolic algorithm exploiting side-information.\n",
        "published": "2015-07-29T19:54:54Z",
        "pdf_link": "http://arxiv.org/pdf/1507.08272v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.00085v1",
        "title": "Regularized Multi-Task Learning for Multi-Dimensional Log-Density\n  Gradient Estimation",
        "summary": "  Log-density gradient estimation is a fundamental statistical problem and\npossesses various practical applications such as clustering and measuring\nnon-Gaussianity. A naive two-step approach of first estimating the density and\nthen taking its log-gradient is unreliable because an accurate density estimate\ndoes not necessarily lead to an accurate log-density gradient estimate. To cope\nwith this problem, a method to directly estimate the log-density gradient\nwithout density estimation has been explored, and demonstrated to work much\nbetter than the two-step method. The objective of this paper is to further\nimprove the performance of this direct method in multi-dimensional cases. Our\nidea is to regard the problem of log-density gradient estimation in each\ndimension as a task, and apply regularized multi-task learning to the direct\nlog-density gradient estimator. We experimentally demonstrate the usefulness of\nthe proposed multi-task method in log-density gradient estimation and\nmode-seeking clustering.\n",
        "published": "2015-08-01T06:43:02Z",
        "pdf_link": "http://arxiv.org/pdf/1508.00085v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01019v1",
        "title": "Direct Estimation of the Derivative of Quadratic Mutual Information with\n  Application in Supervised Dimension Reduction",
        "summary": "  A typical goal of supervised dimension reduction is to find a low-dimensional\nsubspace of the input space such that the projected input variables preserve\nmaximal information about the output variables. The dependence maximization\napproach solves the supervised dimension reduction problem through maximizing a\nstatistical dependence between projected input variables and output variables.\nA well-known statistical dependence measure is mutual information (MI) which is\nbased on the Kullback-Leibler (KL) divergence. However, it is known that the KL\ndivergence is sensitive to outliers. On the other hand, quadratic MI (QMI) is a\nvariant of MI based on the $L_2$ distance which is more robust against outliers\nthan the KL divergence, and a computationally efficient method to estimate QMI\nfrom data, called least-squares QMI (LSQMI), has been proposed recently. For\nthese reasons, developing a supervised dimension reduction method based on\nLSQMI seems promising. However, not QMI itself, but the derivative of QMI is\nneeded for subspace search in supervised dimension reduction, and the\nderivative of an accurate QMI estimator is not necessarily a good estimator of\nthe derivative of QMI. In this paper, we propose to directly estimate the\nderivative of QMI without estimating QMI itself. We show that the direct\nestimation of the derivative of QMI is more accurate than the derivative of the\nestimated QMI. Finally, we develop a supervised dimension reduction algorithm\nwhich efficiently uses the proposed derivative estimator, and demonstrate\nthrough experiments that the proposed method is more robust against outliers\nthan existing methods.\n",
        "published": "2015-08-05T09:44:32Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01019v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01240v4",
        "title": "A Bayesian framework for functional calibration of expensive\n  computational models through non-isometric matching",
        "summary": "  We study statistical calibration, i.e., adjusting features of a computational\nmodel that are not observable or controllable in its associated physical\nsystem. We focus on functional calibration, which arises in many manufacturing\nprocesses where the unobservable features, called calibration variables, are a\nfunction of the input variables. A major challenge in many applications is that\ncomputational models are expensive and can only be evaluated a limited number\nof times. Furthermore, without making strong assumptions, the calibration\nvariables are not identifiable. We propose Bayesian non-isometric matching\ncalibration (BNMC) that allows calibration of expensive computational models\nwith only a limited number of samples taken from a computational model and its\nassociated physical system. BNMC replaces the computational model with a\ndynamic Gaussian process (GP) whose parameters are trained in the calibration\nprocedure. To resolve the identifiability issue, we present the calibration\nproblem from a geometric perspective of non-isometric curve to surface\nmatching, which enables us to take advantage of combinatorial optimization\ntechniques to extract necessary information for constructing prior\ndistributions. Our numerical experiments demonstrate that in terms of\nprediction accuracy BNMC outperforms, or is comparable to, other existing\ncalibration frameworks.\n",
        "published": "2015-08-05T22:17:56Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01240v4"
    },
    {
        "id": "http://arxiv.org/abs/1508.01248v4",
        "title": "Sparse Pseudo-input Local Kriging for Large Spatial Datasets with\n  Exogenous Variables",
        "summary": "  We study large-scale spatial systems that contain exogenous variables, e.g.\nenvironmental factors that are significant predictors in spatial processes.\nBuilding predictive models for such processes is challenging because the large\nnumbers of observations present makes it inefficient to apply full Kriging. In\norder to reduce computational complexity, this paper proposes Sparse\nPseudo-input Local Kriging (SPLK), which utilizes hyperplanes to partition a\ndomain into smaller subdomains and then applies a sparse approximation of the\nfull Kriging to each subdomain. We also develop an optimization procedure to\nfind the desired hyperplanes. To alleviate the problem of discontinuity in the\nglobal predictor, we impose continuity constraints on the boundaries of the\nneighboring subdomains. Furthermore, partitioning the domain into smaller\nsubdomains makes it possible to use different parameter values for the\ncovariance function in each region and, therefore, the heterogeneity in the\ndata structure can be effectively captured. Numerical experiments demonstrate\nthat SPLK outperforms, or is comparable to, the algorithms commonly applied to\nspatial datasets.\n",
        "published": "2015-08-05T23:44:56Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01248v4"
    },
    {
        "id": "http://arxiv.org/abs/1508.01717v4",
        "title": "Distributional Equivalence and Structure Learning for Bow-free Acyclic\n  Path Diagrams",
        "summary": "  We consider the problem of structure learning for bow-free acyclic path\ndiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG\nmodels that allow for certain hidden variables. We present a first method for\nthis problem using a greedy score-based search algorithm. We also prove some\nnecessary and some sufficient conditions for distributional equivalence of BAPs\nwhich are used in an algorithmic ap- proach to compute (nearly) equivalent\nmodel structures. This allows us to infer lower bounds of causal effects. We\nalso present applications to real and simulated datasets using our publicly\navailable R-package.\n",
        "published": "2015-08-07T15:06:04Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01717v4"
    },
    {
        "id": "http://arxiv.org/abs/1508.02810v2",
        "title": "Convergence rates of sub-sampled Newton methods",
        "summary": "  We consider the problem of minimizing a sum of $n$ functions over a convex\nparameter set $\\mathcal{C} \\subset \\mathbb{R}^p$ where $n\\gg p\\gg 1$. In this\nregime, algorithms which utilize sub-sampling techniques are known to be\neffective. In this paper, we use sub-sampling techniques together with low-rank\napproximation to design a new randomized batch algorithm which possesses\ncomparable convergence rate to Newton's method, yet has much smaller\nper-iteration cost. The proposed algorithm is robust in terms of starting point\nand step size, and enjoys a composite convergence rate, namely, quadratic\nconvergence at start and linear convergence when the iterate is close to the\nminimizer. We develop its theoretical analysis which also allows us to select\nnear-optimal algorithm parameters. Our theoretical results can be used to\nobtain convergence rates of previously proposed sub-sampling based algorithms\nas well. We demonstrate how our results apply to well-known machine learning\nproblems. Lastly, we evaluate the performance of our algorithm on several\ndatasets under various scenarios.\n",
        "published": "2015-08-12T04:52:58Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02810v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.02905v2",
        "title": "Bayesian Dropout",
        "summary": "  Dropout has recently emerged as a powerful and simple method for training\nneural networks preventing co-adaptation by stochastically omitting neurons.\nDropout is currently not grounded in explicit modelling assumptions which so\nfar has precluded its adoption in Bayesian modelling. Using Bayesian entropic\nreasoning we show that dropout can be interpreted as optimal inference under\nconstraints. We demonstrate this on an analytically tractable regression model\nproviding a Bayesian interpretation of its mechanism for regularizing and\npreventing co-adaptation as well as its connection to other Bayesian\ntechniques. We also discuss two general approximate techniques for applying\nBayesian dropout for general models, one based on an analytical approximation\nand the other on stochastic variational techniques. These techniques are then\napplied to a Baysian logistic regression problem and are shown to improve\nperformance as the model become more misspecified. Our framework roots dropout\nas a theoretically justified and practical tool for statistical modelling\nallowing Bayesians to tap into the benefits of dropout training.\n",
        "published": "2015-08-12T13:09:19Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02905v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.03106v2",
        "title": "Neyman-Pearson Classification under High-Dimensional Settings",
        "summary": "  Most existing binary classification methods target on the optimization of the\noverall classification risk and may fail to serve some real-world applications\nsuch as cancer diagnosis, where users are more concerned with the risk of\nmisclassifying one specific class than the other. Neyman-Pearson (NP) paradigm\nwas introduced in this context as a novel statistical framework for handling\nasymmetric type I/II error priorities. It seeks classifiers with a minimal type\nII error and a constrained type I error under a user specified level. This\narticle is the first attempt to construct classifiers with guaranteed\ntheoretical performance under the NP paradigm in high-dimensional settings.\nBased on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to\nconstruct NP-type classifiers for Naive Bayes models. The proposed classifiers\nsatisfy the NP oracle inequalities, which are natural NP paradigm counterparts\nof the oracle inequalities in classical binary classification. Besides their\ndesirable theoretical properties, we also demonstrated their numerical\nadvantages in prioritized error control via both simulation and real data\nstudies.\n",
        "published": "2015-08-13T02:47:53Z",
        "pdf_link": "http://arxiv.org/pdf/1508.03106v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.03666v1",
        "title": "Unbounded Bayesian Optimization via Regularization",
        "summary": "  Bayesian optimization has recently emerged as a popular and efficient tool\nfor global optimization and hyperparameter tuning. Currently, the established\nBayesian optimization practice requires a user-defined bounding box which is\nassumed to contain the optimizer. However, when little is known about the\nprobed objective function, it can be difficult to prescribe such bounds. In\nthis work we modify the standard Bayesian optimization framework in a\nprincipled way to allow automatic resizing of the search space. We introduce\ntwo alternative methods and compare them on two common synthetic benchmarking\ntest functions as well as the tasks of tuning the stochastic gradient descent\noptimizer of a multi-layered perceptron and a convolutional neural network on\nMNIST.\n",
        "published": "2015-08-14T21:10:46Z",
        "pdf_link": "http://arxiv.org/pdf/1508.03666v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.04319v1",
        "title": "Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo",
        "summary": "  We present a novel approach for fully non-stationary Gaussian process\nregression (GPR), where all three key parameters -- noise variance, signal\nvariance and lengthscale -- can be simultaneously input-dependent. We develop\ngradient-based inference methods to learn the unknown function and the\nnon-stationary model parameters, without requiring any model approximations. We\npropose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),\nwhich conveniently extends the analytical gradient-based GPR learning by\nguiding the sampling with model gradients. We also learn the MAP solution from\nthe posterior by gradient ascent. In experiments on several synthetic datasets\nand in modelling of temporal gene expression, the nonstationary GPR is shown to\nbe necessary for modeling realistic input-dependent dynamics, while it performs\ncomparably to conventional stationary or previous non-stationary GPR models\notherwise.\n",
        "published": "2015-08-18T13:48:02Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04319v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.04556v1",
        "title": "Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector\n  Problems",
        "summary": "  We are interested in solving the multiple measurement vector (MMV) problem\nfor instances, where the underlying sparsity pattern exhibit spatio-temporal\nstructure motivated by the electroencephalogram (EEG) source localization\nproblem. We propose a probabilistic model that takes this structure into\naccount by generalizing the structured spike and slab prior and the associated\nExpectation Propagation inference scheme. Based on numerical experiments, we\ndemonstrate the viability of the model and the approximate inference scheme.\n",
        "published": "2015-08-19T08:09:06Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04556v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.05913v1",
        "title": "Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in\n  RKHS",
        "summary": "  Distance weighted discrimination (DWD) is a margin-based classifier with an\ninteresting geometric motivation. DWD was originally proposed as a superior\nalternative to the support vector machine (SVM), however DWD is yet to be\npopular compared with the SVM. The main reasons are twofold. First, the\nstate-of-the-art algorithm for solving DWD is based on the second-order-cone\nprogramming (SOCP), while the SVM is a quadratic programming problem which is\nmuch more efficient to solve. Second, the current statistical theory of DWD\nmainly focuses on the linear DWD for the high-dimension-low-sample-size setting\nand data-piling, while the learning theory for the SVM mainly focuses on the\nBayes risk consistency of the kernel SVM. In fact, the Bayes risk consistency\nof DWD is presented as an open problem in the original DWD paper. In this work,\nwe advance the current understanding of DWD from both computational and\ntheoretical perspectives. We propose a novel efficient algorithm for solving\nDWD, and our algorithm can be several hundred times faster than the existing\nstate-of-the-art algorithm based on the SOCP. In addition, our algorithm can\nhandle the generalized DWD, while the SOCP algorithm only works well for a\nspecial DWD but not the generalized DWD. Furthermore, we consider a natural\nkernel DWD in a reproducing kernel Hilbert space and then establish the Bayes\nrisk consistency of the kernel DWD. We compare DWD and the SVM on several\nbenchmark data sets and show that the two have comparable classification\naccuracy, but DWD equipped with our new algorithm can be much faster to compute\nthan the SVM.\n",
        "published": "2015-08-24T18:59:03Z",
        "pdf_link": "http://arxiv.org/pdf/1508.05913v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.07535v1",
        "title": "Calibration of One-Class SVM for MV set estimation",
        "summary": "  A general approach for anomaly detection or novelty detection consists in\nestimating high density regions or Minimum Volume (MV) sets. The One-Class\nSupport Vector Machine (OCSVM) is a state-of-the-art algorithm for estimating\nsuch regions from high dimensional data. Yet it suffers from practical\nlimitations. When applied to a limited number of samples it can lead to poor\nperformance even when picking the best hyperparameters. Moreover the solution\nof OCSVM is very sensitive to the selection of hyperparameters which makes it\nhard to optimize in an unsupervised setting. We present a new approach to\nestimate MV sets using the OCSVM with a different choice of the parameter\ncontrolling the proportion of outliers. The solution function of the OCSVM is\nlearnt on a training set and the desired probability mass is obtained by\nadjusting the offset on a test set to prevent overfitting. Models learnt on\ndifferent train/test splits are then aggregated to reduce the variance induced\nby such random splits. Our approach makes it possible to tune the\nhyperparameters automatically and obtain nested set estimates. Experimental\nresults show that our approach outperforms the standard OCSVM formulation while\nsuffering less from the curse of dimensionality than kernel density estimates.\nResults on actual data sets are also presented.\n",
        "published": "2015-08-30T06:51:31Z",
        "pdf_link": "http://arxiv.org/pdf/1508.07535v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.01631v1",
        "title": "Stochastic gradient variational Bayes for gamma approximating\n  distributions",
        "summary": "  While stochastic variational inference is relatively well known for scaling\ninference in Bayesian probabilistic models, related methods also offer ways to\ncircumnavigate the approximation of analytically intractable expectations. The\nkey challenge in either setting is controlling the variance of gradient\nestimates: recent work has shown that for continuous latent variables,\nparticularly multivariate Gaussians, this can be achieved by using the gradient\nof the log posterior. In this paper we apply the same idea to gamma distributed\nlatent variables given gamma variational distributions, enabling\nstraightforward \"black box\" variational inference in models where sparsity and\nnon-negativity are appropriate. We demonstrate the method on a recently\nproposed gamma process model for network data, as well as a novel sparse factor\nanalysis. We outperform generic sampling algorithms and the approach of using\nGaussian variational distributions on transformed variables.\n",
        "published": "2015-09-04T23:13:27Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01631v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02088v1",
        "title": "Matrix Factorisation with Linear Filters",
        "summary": "  This text investigates relations between two well-known family of algorithms,\nmatrix factorisations and recursive linear filters, by describing a\nprobabilistic model in which approximate inference corresponds to a matrix\nfactorisation algorithm. Using the probabilistic model, we derive a matrix\nfactorisation algorithm as a recursive linear filter. More precisely, we derive\na matrix-variate recursive linear filter in order to perform efficient\ninference in high dimensions. We also show that it is possible to interpret our\nalgorithm as a nontrivial stochastic gradient algorithm. Demonstrations and\ncomparisons on an image restoration task are given.\n",
        "published": "2015-09-07T15:47:39Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02088v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02116v3",
        "title": "Poisson Subsampling Algorithms for Large Sample Linear Regression in\n  Massive Data",
        "summary": "  Large sample size brings the computation bottleneck for modern data analysis.\nSubsampling is one of efficient strategies to handle this problem. In previous\nstudies, researchers make more fo- cus on subsampling with replacement (SSR)\nthan on subsampling without replacement (SSWR). In this paper we investigate a\nkind of SSWR, poisson subsampling (PSS), for fast algorithm in ordinary\nleast-square problem. We establish non-asymptotic property, i.e, the error\nbound of the correspond- ing subsample estimator, which provide a tradeoff\nbetween computation cost and approximation efficiency. Besides the\nnon-asymptotic result, we provide asymptotic consistency and normality of the\nsubsample estimator. Methodologically, we propose a two-step subsampling\nalgorithm, which is efficient with respect to a statistical objective and\nindependent on the linear model assumption.. Synthetic and real data are used\nto empirically study our proposed subsampling strategies. We argue by these\nempirical studies that, (1) our proposed two-step algorithm has obvious\nadvantage when the assumed linear model does not accurate, and (2) the PSS\nstrategy performs obviously better than SSR when the subsampling ratio\nincreases.\n",
        "published": "2015-09-07T16:46:56Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02116v3"
    },
    {
        "id": "http://arxiv.org/abs/1509.02347v1",
        "title": "Modelling time evolving interactions in networks through a non\n  stationary extension of stochastic block models",
        "summary": "  In this paper, we focus on the stochastic block model (SBM),a probabilistic\ntool describing interactions between nodes of a network using latent clusters.\nThe SBM assumes that the networkhas a stationary structure, in which\nconnections of time varying intensity are not taken into account. In other\nwords, interactions between two groups are forced to have the same features\nduring the whole observation time. To overcome this limitation,we propose a\npartition of the whole time horizon, in which interactions are observed, and\ndevelop a non stationary extension of the SBM,allowing to simultaneously\ncluster the nodes in a network along with fixed time intervals in which the\ninteractions take place. The number of clusters (K for nodes, D for time\nintervals) as well as the class memberships are finallyobtained through\nmaximizing the complete-data integrated likelihood by means of a greedy search\napproach. After showing that the model works properly with simulated data, we\nfocus on a real data set. We thus consider the three days ACM Hypertext\nconference held in Turin,June 29th - July 1st 2009. Proximity interactions\nbetween attendees during the first day are modelled and an\ninterestingclustering of the daily hours is finally obtained, with times of\nsocial gathering (e.g. coffee breaks) recovered by the approach. Applications\nto large networks are limited due to the computational complexity of the greedy\nsearch which is dominated bythe number $K\\_{max}$ and $D\\_{max}$ of clusters\nused in the initialization. Therefore,advanced clustering tools are considered\nto reduce the number of clusters expected in the data, making the greedy search\napplicable to large networks.\n",
        "published": "2015-09-08T12:59:19Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02347v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02357v1",
        "title": "Empirical risk minimization is consistent with the mean absolute\n  percentage error",
        "summary": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We show that\nfinding the best model under the MAPE is equivalent to doing weighted Mean\nAbsolute Error (MAE) regression. We also show that, under some asumptions,\nuniversal consistency of Empirical Risk Minimization remains possible using the\nMAPE.\n",
        "published": "2015-09-08T13:17:28Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02357v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02438v1",
        "title": "A Variational Bayesian State-Space Approach to Online Passive-Aggressive\n  Regression",
        "summary": "  Online Passive-Aggressive (PA) learning is a class of online margin-based\nalgorithms suitable for a wide range of real-time prediction tasks, including\nclassification and regression. PA algorithms are formulated in terms of\ndeterministic point-estimation problems governed by a set of user-defined\nhyperparameters: the approach fails to capture model/prediction uncertainty and\nmakes their performance highly sensitive to hyperparameter configurations. In\nthis paper, we introduce a novel PA learning framework for regression that\novercomes the above limitations. We contribute a Bayesian state-space\ninterpretation of PA regression, along with a novel online variational\ninference scheme, that not only produces probabilistic predictions, but also\noffers the benefit of automatic hyperparameter tuning. Experiments with various\nreal-world data sets show that our approach performs significantly better than\na more standard, linear Gaussian state-space model.\n",
        "published": "2015-09-08T16:42:39Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02438v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02866v2",
        "title": "Fast Second-Order Stochastic Backpropagation for Variational Inference",
        "summary": "  We propose a second-order (Hessian or Hessian-free) based optimization method\nfor variational inference inspired by Gaussian backpropagation, and argue that\nquasi-Newton optimization can be developed as well. This is accomplished by\ngeneralizing the gradient computation in stochastic backpropagation via a\nreparametrization trick with lower complexity. As an illustrative example, we\napply this approach to the problems of Bayesian logistic regression and\nvariational auto-encoder (VAE). Additionally, we compute bounds on the\nestimator variance of intractable expectations for the family of Lipschitz\ncontinuous function. Our method is practical, scalable and model free. We\ndemonstrate our method on several real-world datasets and provide comparisons\nwith other stochastic gradient methods to show substantial enhancement in\nconvergence rates.\n",
        "published": "2015-09-09T17:44:37Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02866v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.02873v1",
        "title": "Sélection de variables par le GLM-Lasso pour la prédiction du risque\n  palustre",
        "summary": "  In this study, we propose an automatic learning method for variables\nselection based on Lasso in epidemiology context. One of the aim of this\napproach is to overcome the pretreatment of experts in medicine and\nepidemiology on collected data. These pretreatment consist in recoding some\nvariables and to choose some interactions based on expertise. The approach\nproposed uses all available explanatory variables without treatment and\ngenerate automatically all interactions between them. This lead to high\ndimension. We use Lasso, one of the robust methods of variable selection in\nhigh dimension. To avoid over fitting a two levels cross-validation is used.\nBecause the target variable is account variable and the lasso estimators are\nbiased, variables selected by lasso are debiased by a GLM and used to predict\nthe distribution of the main vector of malaria which is Anopheles. Results show\nthat only few climatic and environmental variables are the mains factors\nassociated to the malaria risk exposure.\n",
        "published": "2015-09-09T17:59:23Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02873v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.03381v1",
        "title": "Learning the Number of Autoregressive Mixtures in Time Series Using the\n  Gap Statistics",
        "summary": "  Using a proper model to characterize a time series is crucial in making\naccurate predictions. In this work we use time-varying autoregressive process\n(TVAR) to describe non-stationary time series and model it as a mixture of\nmultiple stable autoregressive (AR) processes. We introduce a new model\nselection technique based on Gap statistics to learn the appropriate number of\nAR filters needed to model a time series. We define a new distance measure\nbetween stable AR filters and draw a reference curve that is used to measure\nhow much adding a new AR filter improves the performance of the model, and then\nchoose the number of AR filters that has the maximum gap with the reference\ncurve. To that end, we propose a new method in order to generate uniform random\nstable AR filters in root domain. Numerical results are provided demonstrating\nthe performance of the proposed approach.\n",
        "published": "2015-09-11T03:16:52Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03381v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.04541v1",
        "title": "When are Kalman-filter restless bandits indexable?",
        "summary": "  We study the restless bandit associated with an extremely simple scalar\nKalman filter model in discrete time. Under certain assumptions, we prove that\nthe problem is indexable in the sense that the Whittle index is a\nnon-decreasing function of the relevant belief state. In spite of the long\nhistory of this problem, this appears to be the first such proof. We use\nresults about Schur-convexity and mechanical words, which are particular binary\nstrings intimately related to palindromes.\n",
        "published": "2015-09-15T13:33:52Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04541v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.04610v2",
        "title": "Macau: Scalable Bayesian Multi-relational Factorization with Side\n  Information using MCMC",
        "summary": "  We propose Macau, a powerful and flexible Bayesian factorization method for\nheterogeneous data. Our model can factorize any set of entities and relations\nthat can be represented by a relational model, including tensors and also\nmultiple relations for each entity. Macau can also incorporate side\ninformation, specifically entity and relation features, which are crucial for\npredicting sparsely observed relations. Macau scales to millions of entity\ninstances, hundred millions of observations, and sparse entity features with\nmillions of dimensions. To achieve the scale up, we specially designed sampling\nprocedure for entity and relation features that relies primarily on noise\ninjection in linear regressions. We show performance and advanced features of\nMacau in a set of experiments, including challenging drug-protein activity\nprediction task.\n",
        "published": "2015-09-15T15:52:22Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04610v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.04681v2",
        "title": "Large-Scale Optimization Algorithms for Sparse Conditional Gaussian\n  Graphical Models",
        "summary": "  This paper addresses the problem of scalable optimization for L1-regularized\nconditional Gaussian graphical models. Conditional Gaussian graphical models\ngeneralize the well-known Gaussian graphical models to conditional\ndistributions to model the output network influenced by conditioning input\nvariables. While highly scalable optimization methods exist for sparse Gaussian\ngraphical model estimation, state-of-the-art methods for conditional Gaussian\ngraphical models are not efficient enough and more importantly, fail due to\nmemory constraints for very large problems. In this paper, we propose a new\noptimization procedure based on a Newton method that efficiently iterates over\ntwo sub-problems, leading to drastic improvement in computation time compared\nto the previous methods. We then extend our method to scale to large problems\nunder memory constraints, using block coordinate descent to limit memory usage\nwhile achieving fast convergence. Using synthetic and genomic data, we show\nthat our methods can solve one million dimensional problems to high accuracy in\na little over a day on a single machine.\n",
        "published": "2015-09-15T19:03:48Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04681v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.04781v1",
        "title": "Dirichlet Fragmentation Processes",
        "summary": "  Tree structures are ubiquitous in data across many domains, and many datasets\nare naturally modelled by unobserved tree structures. In this paper, first we\nreview the theory of random fragmentation processes [Bertoin, 2006], and a\nnumber of existing methods for modelling trees, including the popular nested\nChinese restaurant process (nCRP). Then we define a general class of\nprobability distributions over trees: the Dirichlet fragmentation process (DFP)\nthrough a novel combination of the theory of Dirichlet processes and random\nfragmentation processes. This DFP presents a stick-breaking construction, and\nrelates to the nCRP in the same way the Dirichlet process relates to the\nChinese restaurant process. Furthermore, we develop a novel hierarchical\nmixture model with the DFP, and empirically compare the new model to similar\nmodels in machine learning. Experiments show the DFP mixture model to be\nconvincingly better than existing state-of-the-art approaches for hierarchical\nclustering and density modelling.\n",
        "published": "2015-09-16T01:07:24Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04781v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.06061v1",
        "title": "A Statistical Theory of Deep Learning via Proximal Splitting",
        "summary": "  In this paper we develop a statistical theory and an implementation of deep\nlearning models. We show that an elegant variable splitting scheme for the\nalternating direction method of multipliers optimises a deep learning\nobjective. We allow for non-smooth non-convex regularisation penalties to\ninduce sparsity in parameter weights. We provide a link between traditional\nshallow layer statistical models such as principal component and sliced inverse\nregression and deep layer models. We also define the degrees of freedom of a\ndeep learning predictor and a predictive MSE criteria to perform model\nselection for comparing architecture designs. We focus on deep multiclass\nlogistic learning although our methods apply more generally. Our results\nsuggest an interesting and previously under-exploited relationship between deep\nlearning and proximal splitting techniques. To illustrate our methodology, we\nprovide a multi-class logit classification analysis of Fisher's Iris data where\nwe illustrate the convergence of our algorithm. Finally, we conclude with\ndirections for future research.\n",
        "published": "2015-09-20T21:39:47Z",
        "pdf_link": "http://arxiv.org/pdf/1509.06061v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.06831v1",
        "title": "Density Estimation via Discrepancy",
        "summary": "  Given i.i.d samples from some unknown continuous density on hyper-rectangle\n$[0, 1]^d$, we attempt to learn a piecewise constant function that approximates\nthis underlying density non-parametrically. Our density estimate is defined on\na binary split of $[0, 1]^d$ and built up sequentially according to discrepancy\ncriteria; the key ingredient is to control the discrepancy adaptively in each\nsub-rectangle to achieve overall bound. We prove that the estimate, even though\nsimple as it appears, preserves most of the estimation power. By exploiting its\nstructure, it can be directly applied to some important pattern recognition\ntasks such as mode seeking and density landscape exploration. We demonstrate\nits applicability through simulations and examples.\n",
        "published": "2015-09-23T03:20:28Z",
        "pdf_link": "http://arxiv.org/pdf/1509.06831v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.07497v2",
        "title": "High Dimensional Data Modeling Techniques for Detection of Chemical\n  Plumes and Anomalies in Hyperspectral Images and Movies",
        "summary": "  We briefly review recent progress in techniques for modeling and analyzing\nhyperspectral images and movies, in particular for detecting plumes of both\nknown and unknown chemicals. For detecting chemicals of known spectrum, we\nextend the technique of using a single subspace for modeling the background to\na \"mixture of subspaces\" model to tackle more complicated background.\nFurthermore, we use partial least squares regression on a resampled training\nset to boost performance. For the detection of unknown chemicals we view the\nproblem as an anomaly detection problem, and use novel estimators with\nlow-sampled complexity for intrinsically low-dimensional data in\nhigh-dimensions that enable us to model the \"normal\" spectra and detect\nanomalies. We apply these algorithms to benchmark data sets made available by\nthe Automated Target Detection program co-funded by NSF, DTRA and NGA, and\ncompare, when applicable, to current state-of-the-art algorithms, with\nfavorable results.\n",
        "published": "2015-09-24T19:59:46Z",
        "pdf_link": "http://arxiv.org/pdf/1509.07497v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.08327v2",
        "title": "Unbiased Bayesian Inference for Population Markov Jump Processes via\n  Random Truncations",
        "summary": "  We consider continuous time Markovian processes where populations of\nindividual agents interact stochastically according to kinetic rules. Despite\nthe increasing prominence of such models in fields ranging from biology to\nsmart cities, Bayesian inference for such systems remains challenging, as these\nare continuous time, discrete state systems with potentially infinite\nstate-space. Here we propose a novel efficient algorithm for joint state /\nparameter posterior sampling in population Markov Jump processes. We introduce\na class of pseudo-marginal sampling algorithms based on a random truncation\nmethod which enables a principled treatment of infinite state spaces. Extensive\nevaluation on a number of benchmark models shows that this approach achieves\nconsiderable savings compared to state of the art methods, retaining accuracy\nand fast convergence. We also present results on a synthetic biology data set\nshowing the potential for practical usefulness of our work.\n",
        "published": "2015-09-28T14:14:07Z",
        "pdf_link": "http://arxiv.org/pdf/1509.08327v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.08582v1",
        "title": "Tractable Fully Bayesian Inference via Convex Optimization and Optimal\n  Transport Theory",
        "summary": "  We consider the problem of transforming samples from one continuous source\ndistribution into samples from another target distribution. We demonstrate with\noptimal transport theory that when the source distribution can be easily\nsampled from and the target distribution is log-concave, this can be tractably\nsolved with convex optimization. We show that a special case of this, when the\nsource is the prior and the target is the posterior, is Bayesian inference.\nHere, we can tractably calculate the normalization constant and draw posterior\ni.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log\nconcavity of the prior and likelihood: the same criterion for tractable\ncalculation of the maximum a posteriori point estimate. With simulated data, we\ndemonstrate how we can attain the Bayes risk in simulations. With physiologic\ndata, we demonstrate improvements over point estimation in intensive care unit\noutcome prediction and electroencephalography-based sleep staging.\n",
        "published": "2015-09-29T03:44:36Z",
        "pdf_link": "http://arxiv.org/pdf/1509.08582v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.08588v3",
        "title": "Estimating network edge probabilities by neighborhood smoothing",
        "summary": "  The estimation of probabilities of network edges from the observed adjacency\nmatrix has important applications to predicting missing links and network\ndenoising. It has usually been addressed by estimating the graphon, a function\nthat determines the matrix of edge probabilities, but this is ill-defined\nwithout strong assumptions on the network structure. Here we propose a novel\ncomputationally efficient method, based on neighborhood smoothing to estimate\nthe expectation of the adjacency matrix directly, without making the structural\nassumptions that graphon estimation requires. The neighborhood smoothing method\nrequires little tuning, has a competitive mean-squared error rate, and\noutperforms many benchmark methods on link prediction in simulated and real\nnetworks.\n",
        "published": "2015-09-29T04:51:24Z",
        "pdf_link": "http://arxiv.org/pdf/1509.08588v3"
    },
    {
        "id": "http://arxiv.org/abs/1511.00831v1",
        "title": "PCA-Based Out-of-Sample Extension for Dimensionality Reduction",
        "summary": "  Dimensionality reduction methods are very common in the field of high\ndimensional data analysis. Typically, algorithms for dimensionality reduction\nare computationally expensive. Therefore, their applications for the analysis\nof massive amounts of data are impractical. For example, repeated computations\ndue to accumulated data are computationally prohibitive. In this paper, an\nout-of-sample extension scheme, which is used as a complementary method for\ndimensionality reduction, is presented. We describe an algorithm which performs\nan out-of-sample extension to newly-arrived data points. Unlike other extension\nalgorithms such as Nystr\\\"om algorithm, the proposed algorithm uses the\nintrinsic geometry of the data and properties for dimensionality reduction map.\nWe prove that the error of the proposed algorithm is bounded. Additionally to\nthe out-of-sample extension, the algorithm provides a degree of the abnormality\nof any newly-arrived data point.\n",
        "published": "2015-11-03T09:30:44Z",
        "pdf_link": "http://arxiv.org/pdf/1511.00831v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.01284v1",
        "title": "Lasso based feature selection for malaria risk exposure prediction",
        "summary": "  In life sciences, the experts generally use empirical knowledge to recode\nvariables, choose interactions and perform selection by classical approach. The\naim of this work is to perform automatic learning algorithm for variables\nselection which can lead to know if experts can be help in they decision or\nsimply replaced by the machine and improve they knowledge and results. The\nLasso method can detect the optimal subset of variables for estimation and\nprediction under some conditions. In this paper, we propose a novel approach\nwhich uses automatically all variables available and all interactions. By a\ndouble cross-validation combine with Lasso, we select a best subset of\nvariables and with GLM through a simple cross-validation perform predictions.\nThe algorithm assures the stability and the the consistency of estimators.\n",
        "published": "2015-11-04T10:53:41Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01284v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.01987v1",
        "title": "Neutralized Empirical Risk Minimization with Generalization Neutrality\n  Bound",
        "summary": "  Currently, machine learning plays an important role in the lives and\nindividual activities of numerous people. Accordingly, it has become necessary\nto design machine learning algorithms to ensure that discrimination, biased\nviews, or unfair treatment do not result from decision making or predictions\nmade via machine learning. In this work, we introduce a novel empirical risk\nminimization (ERM) framework for supervised learning, neutralized ERM (NERM)\nthat ensures that any classifiers obtained can be guaranteed to be neutral with\nrespect to a viewpoint hypothesis. More specifically, given a viewpoint\nhypothesis, NERM works to find a target hypothesis that minimizes the empirical\nrisk while simultaneously identifying a target hypothesis that is neutral to\nthe viewpoint hypothesis. Within the NERM framework, we derive a theoretical\nbound on empirical and generalization neutrality risks. Furthermore, as a\nrealization of NERM with linear classification, we derive a max-margin\nalgorithm, neutral support vector machine (SVM). Experimental results show that\nour neutral SVM shows improved classification performance in real datasets\nwithout sacrificing the neutrality guarantee.\n",
        "published": "2015-11-06T05:11:21Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01987v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.02187v3",
        "title": "Streaming regularization parameter selection via stochastic gradient\n  descent",
        "summary": "  We propose a framework to perform streaming covariance selection. Our\napproach employs regularization constraints where a time-varying sparsity\nparameter is iteratively estimated via stochastic gradient descent. This allows\nfor the regularization parameter to be efficiently learnt in an online manner.\nThe proposed framework is developed for linear regression models and extended\nto graphical models via neighbourhood selection. Under mild assumptions, we are\nable to obtain convergence results in a non-stochastic setting. The\ncapabilities of such an approach are demonstrated using both synthetic data as\nwell as neuroimaging data.\n",
        "published": "2015-11-06T18:38:17Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02187v3"
    },
    {
        "id": "http://arxiv.org/abs/1511.02722v1",
        "title": "Learning Instrumental Variables with Non-Gaussianity Assumptions:\n  Theoretical Limitations and Practical Algorithms",
        "summary": "  Learning a causal effect from observational data is not straightforward, as\nthis is not possible without further assumptions. If hidden common causes\nbetween treatment $X$ and outcome $Y$ cannot be blocked by other measurements,\none possibility is to use an instrumental variable. In principle, it is\npossible under some assumptions to discover whether a variable is structurally\ninstrumental to a target causal effect $X \\rightarrow Y$, but current\nframeworks are somewhat lacking on how general these assumptions can be. A\ninstrumental variable discovery problem is challenging, as no variable can be\ntested as an instrument in isolation but only in groups, but different\nvariables might require different conditions to be considered an instrument.\nMoreover, identification constraints might be hard to detect statistically. In\nthis paper, we give a theoretical characterization of instrumental variable\ndiscovery, highlighting identifiability problems and solutions, the need for\nnon-Gaussianity assumptions, and how they fit within existing methods.\n",
        "published": "2015-11-09T15:40:50Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02722v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.03243v3",
        "title": "Black-box $α$-divergence Minimization",
        "summary": "  Black-box alpha (BB-$\\alpha$) is a new approximate inference method based on\nthe minimization of $\\alpha$-divergences. BB-$\\alpha$ scales to large datasets\nbecause it can be implemented using stochastic gradient descent. BB-$\\alpha$\ncan be applied to complex probabilistic models with little effort since it only\nrequires as input the likelihood function and its gradients. These gradients\ncan be easily obtained using automatic differentiation. By changing the\ndivergence parameter $\\alpha$, the method is able to interpolate between\nvariational Bayes (VB) ($\\alpha \\rightarrow 0$) and an algorithm similar to\nexpectation propagation (EP) ($\\alpha = 1$). Experiments on probit regression\nand neural network regression and classification problems show that BB-$\\alpha$\nwith non-standard settings of $\\alpha$, such as $\\alpha = 0.5$, usually\nproduces better predictions than with $\\alpha \\rightarrow 0$ (VB) or $\\alpha =\n1$ (EP).\n",
        "published": "2015-11-10T20:02:48Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03243v3"
    },
    {
        "id": "http://arxiv.org/abs/1511.03249v3",
        "title": "Stochastic Expectation Propagation for Large Scale Gaussian Process\n  Classification",
        "summary": "  A method for large scale Gaussian process classification has been recently\nproposed based on expectation propagation (EP). Such a method allows Gaussian\nprocess classifiers to be trained on very large datasets that were out of the\nreach of previous deployments of EP and has been shown to be competitive with\nrelated techniques based on stochastic variational inference. Nevertheless, the\nmemory resources required scale linearly with the dataset size, unlike in\nvariational methods. This is a severe limitation when the number of instances\nis very large. Here we show that this problem is avoided when stochastic EP is\nused to train the model.\n",
        "published": "2015-11-10T20:11:10Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03249v3"
    },
    {
        "id": "http://arxiv.org/abs/1511.03405v1",
        "title": "Training Deep Gaussian Processes using Stochastic Expectation\n  Propagation and Probabilistic Backpropagation",
        "summary": "  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\nmultiple, infinitely wide hidden layers. DGPs are probabilistic and\nnon-parametric and as such are arguably more flexible, have a greater capacity\nto generalise, and provide better calibrated uncertainty estimates than\nalternative deep models. The focus of this paper is scalable approximate\nBayesian learning of these networks. The paper develops a novel and efficient\nextension of probabilistic backpropagation, a state-of-the-art method for\ntraining Bayesian neural networks, that can be used to train DGPs. The new\nmethod leverages a recently proposed method for scaling Expectation\nPropagation, called stochastic Expectation Propagation. The method is able to\nautomatically discover useful input warping, expansion or compression, and it\nis therefore is a flexible form of Bayesian kernel design. We demonstrate the\nsuccess of the new method for supervised learning on several real-world\ndatasets, showing that it typically outperforms GP regression and is never much\nworse.\n",
        "published": "2015-11-11T07:40:48Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03405v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.03990v1",
        "title": "Automatic Inference of the Quantile Parameter",
        "summary": "  Supervised learning is an active research area, with numerous applications in\ndiverse fields such as data analytics, computer vision, speech and audio\nprocessing, and image understanding. In most cases, the loss functions used in\nmachine learning assume symmetric noise models, and seek to estimate the\nunknown function parameters. However, loss functions such as quantile and\nquantile Huber generalize the symmetric $\\ell_1$ and Huber losses to the\nasymmetric setting, for a fixed quantile parameter. In this paper, we propose\nto jointly infer the quantile parameter and the unknown function parameters,\nfor the asymmetric quantile Huber and quantile losses. We explore various\nproperties of the quantile Huber loss and implement a convexity certificate\nthat can be used to check convexity in the quantile parameter. When the loss if\nconvex with respect to the parameter of the function, we prove that it is\nbiconvex in both the function and the quantile parameters, and propose an\nalgorithm to jointly estimate these. Results with synthetic and real data\ndemonstrate that the proposed approach can automatically recover the quantile\nparameter corresponding to the noise and also provide an improved recovery of\nfunction parameters. To illustrate the potential of the framework, we extend\nthe gradient boosting machines with quantile losses to automatically estimate\nthe quantile parameter at each iteration.\n",
        "published": "2015-11-12T17:54:46Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03990v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.04157v1",
        "title": "$k$-means: Fighting against Degeneracy in Sequential Monte Carlo with an\n  Application to Tracking",
        "summary": "  For regular particle filter algorithm or Sequential Monte Carlo (SMC)\nmethods, the initial weights are traditionally dependent on the proposed\ndistribution, the posterior distribution at the current timestamp in the\nsampled sequence, and the target is the posterior distribution of the previous\ntimestamp. This is technically correct, but leads to algorithms which usually\nhave practical issues with degeneracy, where all particles eventually collapse\nonto a single particle. In this paper, we propose and evaluate using $k$ means\nclustering to attack and even take advantage of this degeneracy. Specifically,\nwe propose a Stochastic SMC algorithm which initializes the set of $k$ means,\nproviding the initial centers chosen from the collapsed particles. To fight\nagainst degeneracy, we adjust the regular SMC weights, mediated by cluster\nproportions, and then correct them to retain the same expectation as before. We\nexperimentally demonstrate that our approach has better performance than\nvanilla algorithms.\n",
        "published": "2015-11-13T04:47:58Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04157v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.04402v2",
        "title": "Lass-0: sparse non-convex regression by local search",
        "summary": "  We compute approximate solutions to L0 regularized linear regression using L1\nregularization, also known as the Lasso, as an initialization step. Our\nalgorithm, the Lass-0 (\"Lass-zero\"), uses a computationally efficient stepwise\nsearch to determine a locally optimal L0 solution given any L1 regularization\nsolution. We present theoretical results of consistency under orthogonality and\nappropriate handling of redundant features. Empirically, we use synthetic data\nto demonstrate that Lass-0 solutions are closer to the true sparse support than\nL1 regularization models. Additionally, in real-world data Lass-0 finds more\nparsimonious solutions than L1 regularization while maintaining similar\npredictive accuracy.\n",
        "published": "2015-11-13T19:07:50Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04402v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.04408v1",
        "title": "Scalable Gaussian Processes for Characterizing Multidimensional Change\n  Surfaces",
        "summary": "  We present a scalable Gaussian process model for identifying and\ncharacterizing smooth multidimensional changepoints, and automatically learning\nchanges in expressive covariance structure. We use Random Kitchen Sink features\nto flexibly define a change surface in combination with expressive spectral\nmixture kernels to capture the complex statistical structure. Finally, through\nthe use of novel methods for additive non-separable kernels, we can scale the\nmodel to large datasets. We demonstrate the model on numerical and real world\ndata, including a large spatio-temporal disease dataset where we identify\npreviously unknown heterogeneous changes in space and time.\n",
        "published": "2015-11-13T19:38:17Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04408v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.04817v1",
        "title": "Probabilistic Segmentation via Total Variation Regularization",
        "summary": "  We present a convex approach to probabilistic segmentation and modeling of\ntime series data. Our approach builds upon recent advances in multivariate\ntotal variation regularization, and seeks to learn a separate set of parameters\nfor the distribution over the observations at each time point, but with an\nadditional penalty that encourages the parameters to remain constant over time.\nWe propose efficient optimization methods for solving the resulting (large)\noptimization problems, and a two-stage procedure for estimating recurring\nclusters under such models, based upon kernel density estimation. Finally, we\nshow on a number of real-world segmentation tasks, the resulting methods often\nperform as well or better than existing latent variable models, while being\nsubstantially easier to train.\n",
        "published": "2015-11-16T04:11:00Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04817v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.05467v3",
        "title": "Predictive Entropy Search for Multi-objective Bayesian Optimization",
        "summary": "  We present PESMO, a Bayesian method for identifying the Pareto set of\nmulti-objective optimization problems, when the functions are expensive to\nevaluate. The central idea of PESMO is to choose evaluation points so as to\nmaximally reduce the entropy of the posterior distribution over the Pareto set.\nCritically, the PESMO multi-objective acquisition function can be decomposed as\na sum of objective-specific acquisition functions, which enables the algorithm\nto be used in \\emph{decoupled} scenarios in which the objectives can be\nevaluated separately and perhaps with different costs. This decoupling\ncapability also makes it possible to identify difficult objectives that require\nmore evaluations. PESMO also offers gains in efficiency, as its cost scales\nlinearly with the number of objectives, in comparison to the exponential cost\nof other methods. We compare PESMO with other related methods for\nmulti-objective Bayesian optimization on synthetic and real-world problems. The\nresults show that PESMO produces better recommendations with a smaller number\nof evaluations of the objectives, and that a decoupled evaluation can lead to\nimprovements in performance, particularly when the number of objectives is\nlarge.\n",
        "published": "2015-11-17T16:59:33Z",
        "pdf_link": "http://arxiv.org/pdf/1511.05467v3"
    },
    {
        "id": "http://arxiv.org/abs/1511.06120v1",
        "title": "The Kernel Two-Sample Test for Brain Networks",
        "summary": "  In clinical and neuroscientific studies, systematic differences between two\npopulations of brain networks are investigated in order to characterize mental\ndiseases or processes. Those networks are usually represented as graphs built\nfrom neuroimaging data and studied by means of graph analysis methods. The\ntypical machine learning approach to study these brain graphs creates a\nclassifier and tests its ability to discriminate the two populations. In\ncontrast to this approach, in this work we propose to directly test whether two\npopulations of graphs are different or not, by using the kernel two-sample test\n(KTST), without creating the intermediate classifier. We claim that, in\ngeneral, the two approaches provides similar results and that the KTST requires\nmuch less computation. Additionally, in the regime of low sample size, we claim\nthat the KTST has lower frequency of Type II error than the classification\napproach. Besides providing algorithmic considerations to support these claims,\nwe show strong evidence through experiments and one simulation.\n",
        "published": "2015-11-19T11:01:54Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06120v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.06772v1",
        "title": "PLDA with Two Sources of Inter-session Variability",
        "summary": "  In some speaker recognition scenarios we find conversations recorded\nsimultaneously over multiple channels. That is the case of the interviews in\nthe NIST SRE dataset. To take advantage of that, we propose a modification of\nthe PLDA model that considers two different inter-session variability terms.\nThe first term is tied between all the recordings belonging to the same\nconversation whereas the second is not. Thus, the former mainly intends to\ncapture the variability due to the phonetic content of the conversation while\nthe latter tries to capture the channel variability. In this document, we\nderive the equations for this model. This model was applied in the paper\n\"Handling Recordings Acquired Simultaneously over Multiple Channels with PLDA\"\npublished at Interspeech 2013.\n",
        "published": "2015-11-20T21:08:04Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06772v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07294v1",
        "title": "Stochastic Parallel Block Coordinate Descent for Large-scale Saddle\n  Point Problems",
        "summary": "  We consider convex-concave saddle point problems with a separable structure\nand non-strongly convex functions. We propose an efficient stochastic block\ncoordinate descent method using adaptive primal-dual updates, which enables\nflexible parallel optimization for large-scale problems. Our method shares the\nefficiency and flexibility of block coordinate descent methods with the\nsimplicity of primal-dual methods and utilizing the structure of the separable\nconvex-concave saddle point problem. It is capable of solving a wide range of\nmachine learning applications, including robust principal component analysis,\nLasso, and feature selection by group Lasso, etc. Theoretically and\nempirically, we demonstrate significantly better performance than\nstate-of-the-art methods in all these applications.\n",
        "published": "2015-11-23T16:12:11Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07294v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07318v1",
        "title": "Bayesian SPLDA",
        "summary": "  In this document we are going to derive the equations needed to implement a\nVariational Bayes estimation of the parameters of the simplified probabilistic\nlinear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA\nfrom one database to another with few development data or to implement the\nfully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.\n",
        "published": "2015-11-20T20:43:43Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07318v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07367v1",
        "title": "Black box variational inference for state space models",
        "summary": "  Latent variable time-series models are among the most heavily used tools from\nmachine learning and applied statistics. These models have the advantage of\nlearning latent structure both from noisy observations and from the temporal\nordering in the data, where it is assumed that meaningful correlation structure\nexists across time. A few highly-structured models, such as the linear\ndynamical system with linear-Gaussian observations, have closed-form inference\nprocedures (e.g. the Kalman Filter), but this case is an exception to the\ngeneral rule that exact posterior inference in more complex generative models\nis intractable. Consequently, much work in time-series modeling focuses on\napproximate inference procedures for one particular class of models. Here, we\nextend recent developments in stochastic variational inference to develop a\n`black-box' approximate inference technique for latent variable models with\nlatent dynamical structure. We propose a structured Gaussian variational\napproximate posterior that carries the same intuition as the standard Kalman\nfilter-smoother but, importantly, permits us to use the same inference approach\nto approximate the posterior of much more general, nonlinear latent variable\ngenerative models. We show that our approach recovers accurate estimates in the\ncase of basic models with closed-form posteriors, and more interestingly\nperforms well in comparison to variational approaches that were designed in a\nbespoke fashion for specific non-conjugate models.\n",
        "published": "2015-11-23T19:08:08Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07367v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07421v1",
        "title": "Unsupervised Adaptation of SPLDA",
        "summary": "  State-of-the-art speaker recognition relays on models that need a large\namount of training data. This models are successful in tasks like NIST SRE\nbecause there is sufficient data available. However, in real applications, we\nusually do not have so much data and, in many cases, the speaker labels are\nunknown. We present a method to adapt a PLDA model from a domain with a large\namount of labeled data to another with unlabeled data. We describe a generative\nmodel that produces both sets of data where the unknown labels are modeled like\nlatent variables. We used variational Bayes to estimate the hidden variables.\nHere, we derive the equations for this model. This model has been used in the\npapers: \"UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS\"\npublised at ICASSP 2014, \"Unsupervised Training of PLDA with Variational Bayes\"\npublished at Iberspeech 2014, and \"VARIATIONAL BAYESIAN PLDA FOR SPEAKER\nDIARIZATION IN THE MGB CHALLENGE\" published at ASRU 2015.\n",
        "published": "2015-11-20T21:25:59Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07421v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07422v1",
        "title": "Variational Bayes Factor Analysis for i-Vector Extraction",
        "summary": "  In this document we are going to derive the equations needed to implement a\nVariational Bayes i-vector extractor. This can be used to extract longer\ni-vectors reducing the risk of overfittig or to adapt an i-vector extractor\nfrom a database to another with scarce development data. This work is based on\nPatrick Kenny's joint factor analysis and Christopher Bishop's variational\nprincipal components.\n",
        "published": "2015-11-20T21:38:25Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07422v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.07715v2",
        "title": "Statistical Properties of the Single Linkage Hierarchical Clustering\n  Estimator",
        "summary": "  Distance-based hierarchical clustering (HC) methods are widely used in\nunsupervised data analysis but few authors take account of uncertainty in the\ndistance data. We incorporate a statistical model of the uncertainty through\ncorruption or noise in the pairwise distances and investigate the problem of\nestimating the HC as unknown parameters from measurements. Specifically, we\nfocus on single linkage hierarchical clustering (SLHC) and study its geometry.\nWe prove that under fairly reasonable conditions on the probability\ndistribution governing measurements, SLHC is equivalent to maximum partial\nprofile likelihood estimation (MPPLE) with some of the information contained in\nthe data ignored. At the same time, we show that direct evaluation of SLHC on\nmaximum likelihood estimation (MLE) of pairwise distances yields a consistent\nestimator. Consequently, a full MLE is expected to perform better than SLHC in\ngetting the correct HC results for the ground truth metric.\n",
        "published": "2015-11-24T14:15:11Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07715v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.07944v1",
        "title": "Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering",
        "summary": "  We derive a statistical model for estimation of a dendrogram from single\nlinkage hierarchical clustering (SLHC) that takes account of uncertainty\nthrough noise or corruption in the measurements of separation of data. Our\nfocus is on just the estimation of the hierarchy of partitions afforded by the\ndendrogram, rather than the heights in the latter. The concept of estimating\nthis \"dendrogram structure'' is introduced, and an approximate maximum\nlikelihood estimator (MLE) for the dendrogram structure is described. These\nideas are illustrated by a simple Monte Carlo simulation that, at least for\nsmall data sets, suggests the method outperforms SLHC in the presence of noise.\n",
        "published": "2015-11-25T03:35:46Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07944v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.08768v2",
        "title": "Gradient Estimation with Simultaneous Perturbation and Compressive\n  Sensing",
        "summary": "  This paper aims at achieving a \"good\" estimator for the gradient of a\nfunction on a high-dimensional space. Often such functions are not sensitive in\nall coordinates and the gradient of the function is almost sparse. We propose a\nmethod for gradient estimation that combines ideas from Spall's Simultaneous\nPerturbation Stochastic Approximation with compressive sensing. The aim is to\nobtain \"good\" estimator without too many function evaluations. Application to\nestimating gradient outer product matrix as well as standard optimization\nproblems are illustrated via simulations.\n",
        "published": "2015-11-27T18:51:29Z",
        "pdf_link": "http://arxiv.org/pdf/1511.08768v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.09422v2",
        "title": "A General Framework for Constrained Bayesian Optimization using\n  Information-based Search",
        "summary": "  We present an information-theoretic framework for solving global black-box\noptimization problems that also have black-box constraints. Of particular\ninterest to us is to efficiently solve problems with decoupled constraints, in\nwhich subsets of the objective and constraint functions may be evaluated\nindependently. For example, when the objective is evaluated on a CPU and the\nconstraints are evaluated independently on a GPU. These problems require an\nacquisition function that can be separated into the contributions of the\nindividual function evaluations. We develop one such acquisition function and\ncall it Predictive Entropy Search with Constraints (PESC). PESC is an\napproximation to the expected information gain criterion and it compares\nfavorably to alternative approaches based on improvement in several synthetic\nand real-world problems. In addition to this, we consider problems with a mix\nof functions that are fast and slow to evaluate. These problems require\nbalancing the amount of time spent in the meta-computation of PESC and in the\nactual evaluation of the target objective. We take a bounded rationality\napproach and develop partial update for PESC which trades off accuracy against\nspeed. We then propose a method for adaptively switching between the partial\nand full updates for PESC. This allows us to interpolate between versions of\nPESC that are efficient in terms of function evaluations and those that are\nefficient in terms of wall-clock time. Overall, we demonstrate that PESC is an\neffective algorithm that provides a promising direction towards a unified\nsolution for constrained Bayesian optimization.\n",
        "published": "2015-11-30T18:36:45Z",
        "pdf_link": "http://arxiv.org/pdf/1511.09422v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.00315v1",
        "title": "Highly Scalable Tensor Factorization for Prediction of Drug-Protein\n  Interaction Type",
        "summary": "  The understanding of the type of inhibitory interaction plays an important\nrole in drug design. Therefore, researchers are interested to know whether a\ndrug has competitive or non-competitive interaction to particular protein\ntargets.\n  Method: to analyze the interaction types we propose factorization method\nMacau which allows us to combine different measurement types into a single\ntensor together with proteins and compounds. The compounds are characterized by\nhigh dimensional 2D ECFP fingerprints. The novelty of the proposed method is\nthat using a specially designed noise injection MCMC sampler it can incorporate\nhigh dimensional side information, i.e., millions of unique 2D ECFP compound\nfeatures, even for large scale datasets of millions of compounds. Without the\nside information, in this case, the tensor factorization would be practically\nfutile.\n  Results: using public IC50 and Ki data from ChEMBL we trained a model from\nwhere we can identify the latent subspace separating the two measurement types\n(IC50 and Ki). The results suggest the proposed method can detect the\ncompetitive inhibitory activity between compounds and proteins.\n",
        "published": "2015-12-01T16:03:14Z",
        "pdf_link": "http://arxiv.org/pdf/1512.00315v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01286v1",
        "title": "Adjusting for Chance Clustering Comparison Measures",
        "summary": "  Adjusted for chance measures are widely used to compare\npartitions/clusterings of the same data set. In particular, the Adjusted Rand\nIndex (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI)\nbased on Shannon information theory are very popular in the clustering\ncommunity. Nonetheless it is an open problem as to what are the best\napplication scenarios for each measure and guidelines in the literature for\ntheir usage are sparse, with the result that users often resort to using both.\nGeneralized Information Theoretic (IT) measures based on the Tsallis entropy\nhave been shown to link pair-counting and Shannon IT measures. In this paper,\nwe aim to bridge the gap between adjustment of measures based on pair-counting\nand measures based on information theory. We solve the key technical challenge\nof analytically computing the expected value and variance of generalized IT\nmeasures. This allows us to propose adjustments of generalized IT measures,\nwhich reduce to well known adjusted clustering comparison measures as special\ncases. Using the theory of generalized IT measures, we are able to propose the\nfollowing guidelines for using ARI and AMI as external validation indices: ARI\nshould be used when the reference clustering has large equal sized clusters;\nAMI should be used when the reference clustering is unbalanced and there exist\nsmall clusters.\n",
        "published": "2015-12-03T23:56:55Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01286v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01665v1",
        "title": "Stochastic Collapsed Variational Inference for Hidden Markov Models",
        "summary": "  Stochastic variational inference for collapsed models has recently been\nsuccessfully applied to large scale topic modelling. In this paper, we propose\na stochastic collapsed variational inference algorithm for hidden Markov\nmodels, in a sequential data setting. Given a collapsed hidden Markov Model, we\nbreak its long Markov chain into a set of short subchains. We propose a novel\nsum-product algorithm to update the posteriors of the subchains, taking into\naccount their boundary transitions due to the sequential dependencies. Our\nexperiments on two discrete datasets show that our collapsed algorithm is\nscalable to very large datasets, memory efficient and significantly more\naccurate than the existing uncollapsed algorithm.\n",
        "published": "2015-12-05T13:39:18Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01665v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01666v1",
        "title": "Stochastic Collapsed Variational Inference for Sequential Data",
        "summary": "  Stochastic variational inference for collapsed models has recently been\nsuccessfully applied to large scale topic modelling. In this paper, we propose\na stochastic collapsed variational inference algorithm in the sequential data\nsetting. Our algorithm is applicable to both finite hidden Markov models and\nhierarchical Dirichlet process hidden Markov models, and to any datasets\ngenerated by emission distributions in the exponential family. Our experiment\nresults on two discrete datasets show that our inference is both more efficient\nand more accurate than its uncollapsed version, stochastic variational\ninference.\n",
        "published": "2015-12-05T13:45:47Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01666v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01947v1",
        "title": "Learning population and subject-specific brain connectivity networks via\n  Mixed Neighborhood Selection",
        "summary": "  In neuroimaging data analysis, Gaussian graphical models are often used to\nmodel statistical dependencies across spatially remote brain regions known as\nfunctional connectivity. Typically, data is collected across a cohort of\nsubjects and the scientific objectives consist of estimating population and\nsubject-specific graphical models. A third objective that is often overlooked\ninvolves quantifying inter-subject variability and thus identifying regions or\nsub-networks that demonstrate heterogeneity across subjects. Such information\nis fundamental in order to thoroughly understand the human connectome. We\npropose Mixed Neighborhood Selection in order to simultaneously address the\nthree aforementioned objectives. By recasting covariance selection as a\nneighborhood selection problem we are able to efficiently learn the topology of\neach node. We introduce an additional mixed effect component to neighborhood\nselection in order to simultaneously estimate a graphical model for the\npopulation of subjects as well as for each individual subject. The proposed\nmethod is validated empirically through a series of simulations and applied to\nresting state data for healthy subjects taken from the ABIDE consortium.\n",
        "published": "2015-12-07T09:07:35Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01947v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.02543v2",
        "title": "Gibbs-type Indian buffet processes",
        "summary": "  We investigate a class of feature allocation models that generalize the\nIndian buffet process and are parameterized by Gibbs-type random measures. Two\nexisting classes are contained as special cases: the original two-parameter\nIndian buffet process, corresponding to the Dirichlet process, and the stable\n(or three-parameter) Indian buffet process, corresponding to the Pitman--Yor\nprocess. Asymptotic behavior of the Gibbs-type partitions, such as power laws\nholding for the number of latent clusters, translates into analogous\ncharacteristics for this class of Gibbs-type feature allocation models. Despite\ncontaining several different distinct subclasses, the properties of Gibbs-type\npartitions allow us to develop a black-box procedure for posterior inference\nwithin any subclass of models. Through numerical experiments, we compare and\ncontrast a few of these subclasses and highlight the utility of varying\npower-law behaviors in the latent features.\n",
        "published": "2015-12-08T17:01:05Z",
        "pdf_link": "http://arxiv.org/pdf/1512.02543v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.03300v1",
        "title": "Inference in topic models: sparsity and trade-off",
        "summary": "  Topic models are popular for modeling discrete data (e.g., texts, images,\nvideos, links), and provide an efficient way to discover hidden\nstructures/semantics in massive data. One of the core problems in this field is\nthe posterior inference for individual data instances. This problem is\nparticularly important in streaming environments, but is often intractable. In\nthis paper, we investigate the use of the Frank-Wolfe algorithm (FW) for\nrecovering sparse solutions to posterior inference. From detailed elucidation\nof both theoretical and practical aspects, FW exhibits many interesting\nproperties which are beneficial to topic modeling. We then employ FW to design\nfast methods, including ML-FW, for learning latent Dirichlet allocation (LDA)\nat large scales. Extensive experiments show that to reach the same\npredictiveness level, ML-FW can perform tens to thousand times faster than\nexisting state-of-the-art methods for learning LDA from massive/streaming data.\n",
        "published": "2015-12-10T16:12:10Z",
        "pdf_link": "http://arxiv.org/pdf/1512.03300v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.03308v2",
        "title": "Guaranteed inference in topic models",
        "summary": "  One of the core problems in statistical models is the estimation of a\nposterior distribution. For topic models, the problem of posterior inference\nfor individual texts is particularly important, especially when dealing with\ndata streams, but is often intractable in the worst case. As a consequence,\nexisting methods for posterior inference are approximate and do not have any\nguarantee on neither quality nor convergence rate. In this paper, we introduce\na provably fast algorithm, namely Online Maximum a Posteriori Estimation (OPE),\nfor posterior inference in topic models. OPE has more attractive properties\nthan existing inference approaches, including theoretical guarantees on quality\nand fast rate of convergence to a local maximal/stationary point of the\ninference problem. The discussions about OPE are very general and hence can be\neasily employed in a wide range of contexts. Finally, we employ OPE to design\nthree methods for learning Latent Dirichlet Allocation from text streams or\nlarge corpora. Extensive experiments demonstrate some superior behaviors of OPE\nand of our new learning methods.\n",
        "published": "2015-12-10T16:24:44Z",
        "pdf_link": "http://arxiv.org/pdf/1512.03308v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.03444v1",
        "title": "Cross-Validated Variable Selection in Tree-Based Methods Improves\n  Predictive Performance",
        "summary": "  Recursive partitioning approaches producing tree-like models are a long\nstanding staple of predictive modeling, in the last decade mostly as\n``sub-learners'' within state of the art ensemble methods like Boosting and\nRandom Forest. However, a fundamental flaw in the partitioning (or splitting)\nrule of commonly used tree building methods precludes them from treating\ndifferent types of variables equally. This most clearly manifests in these\nmethods' inability to properly utilize categorical variables with a large\nnumber of categories, which are ubiquitous in the new age of big data. Such\nvariables can often be very informative, but current tree methods essentially\nleave us a choice of either not using them, or exposing our models to severe\noverfitting. We propose a conceptual framework to splitting using leave-one-out\n(LOO) cross validation for selecting the splitting variable, then performing a\nregular split (in our case, following CART's approach) for the selected\nvariable. The most important consequence of our approach is that categorical\nvariables with many categories can be safely used in tree building and are only\nchosen if they contribute to predictive power. We demonstrate in extensive\nsimulation and real data analysis that our novel splitting approach\nsignificantly improves the performance of both single tree models and ensemble\nmethods that utilize trees. Importantly, we design an algorithm for LOO\nsplitting variable selection which under reasonable assumptions does not\nincrease the overall computational complexity compared to CART for two-class\nclassification. For regression tasks, our approach carries an increased\ncomputational burden, replacing a O(log(n)) factor in CART splitting rule\nsearch with an O(n) term.\n",
        "published": "2015-12-10T21:20:14Z",
        "pdf_link": "http://arxiv.org/pdf/1512.03444v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.04937v1",
        "title": "Relative Density and Exact Recovery in Heterogeneous Stochastic Block\n  Models",
        "summary": "  The Stochastic Block Model (SBM) is a widely used random graph model for\nnetworks with communities. Despite the recent burst of interest in recovering\ncommunities in the SBM from statistical and computational points of view, there\nare still gaps in understanding the fundamental information theoretic and\ncomputational limits of recovery. In this paper, we consider the SBM in its\nfull generality, where there is no restriction on the number and sizes of\ncommunities or how they grow with the number of nodes, as well as on the\nconnection probabilities inside or across communities. This generality allows\nus to move past the artifacts of homogenous SBM, and understand the right\nparameters (such as the relative densities of communities) that define the\nvarious recovery thresholds. We outline the implications of our generalizations\nvia a set of illustrative examples. For instance, $\\log n$ is considered to be\nthe standard lower bound on the cluster size for exact recovery via convex\nmethods, for homogenous SBM. We show that it is possible, in the right\ncircumstances (when sizes are spread and the smaller the cluster, the denser),\nto recover very small clusters (up to $\\sqrt{\\log n}$ size), if there are just\na few of them (at most polylogarithmic in $n$).\n",
        "published": "2015-12-15T20:57:28Z",
        "pdf_link": "http://arxiv.org/pdf/1512.04937v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.05219v1",
        "title": "Learning a Hybrid Architecture for Sequence Regression and Annotation",
        "summary": "  When learning a hidden Markov model (HMM), sequen- tial observations can\noften be complemented by real-valued summary response variables generated from\nthe path of hid- den states. Such settings arise in numerous domains, includ-\ning many applications in biology, like motif discovery and genome annotation.\nIn this paper, we present a flexible frame- work for jointly modeling both\nlatent sequence features and the functional mapping that relates the summary\nresponse variables to the hidden state sequence. The algorithm is com- patible\nwith a rich set of mapping functions. Results show that the availability of\nadditional continuous response vari- ables can simultaneously improve the\nannotation of the se- quential observations and yield good prediction\nperformance in both synthetic data and real-world datasets.\n",
        "published": "2015-12-16T15:48:40Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05219v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.05287v5",
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural\n  Networks",
        "summary": "  Recurrent neural networks (RNNs) stand at the forefront of many recent\ndevelopments in deep learning. Yet a major difficulty with these models is\ntheir tendency to overfit, with dropout shown to fail when applied to recurrent\nlayers. Recent results at the intersection of Bayesian modelling and deep\nlearning offer a Bayesian interpretation of common deep learning techniques\nsuch as dropout. This grounding of dropout in approximate Bayesian inference\nsuggests an extension of the theoretical results, offering insights into the\nuse of dropout with RNN models. We apply this new variational inference based\ndropout technique in LSTM and GRU models, assessing it on language modelling\nand sentiment analysis tasks. The new approach outperforms existing techniques,\nand to the best of our knowledge improves on the single model state-of-the-art\nin language modelling with the Penn Treebank (73.4 test perplexity). This\nextends our arsenal of variational tools in deep learning.\n",
        "published": "2015-12-16T19:18:43Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05287v5"
    },
    {
        "id": "http://arxiv.org/abs/1512.05469v2",
        "title": "Private Causal Inference",
        "summary": "  Causal inference deals with identifying which random variables \"cause\" or\ncontrol other random variables. Recent advances on the topic of causal\ninference based on tools from statistical estimation and machine learning have\nresulted in practical algorithms for causal inference. Causal inference has the\npotential to have significant impact on medical research, prevention and\ncontrol of diseases, and identifying factors that impact economic changes to\nname just a few. However, these promising applications for causal inference are\noften ones that involve sensitive or personal data of users that need to be\nkept private (e.g., medical records, personal finances, etc). Therefore, there\nis a need for the development of causal inference methods that preserve data\nprivacy. We study the problem of inferring causality using the current, popular\ncausal inference framework, the additive noise model (ANM) while simultaneously\nensuring privacy of the users. Our framework provides differential privacy\nguarantees for a variety of ANM variants. We run extensive experiments, and\ndemonstrate that our techniques are practical and easy to implement.\n",
        "published": "2015-12-17T05:46:56Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05469v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.05610v2",
        "title": "Classification of weak multi-view signals by sharing factors in a\n  mixture of Bayesian group factor analyzers",
        "summary": "  We propose a novel classification model for weak signal data, building upon a\nrecent model for Bayesian multi-view learning, Group Factor Analysis (GFA).\nInstead of assuming all data to come from a single GFA model, we allow latent\nclusters, each having a different GFA model and producing a different class\ndistribution. We show that sharing information across the clusters, by sharing\nfactors, increases the classification accuracy considerably; the shared factors\nessentially form a flexible noise model that explains away the part of data not\nrelated to classification. Motivation for the setting comes from single-trial\nfunctional brain imaging data, having a very low signal-to-noise ratio and a\nnatural multi-view setting, with the different sensors, measurement modalities\n(EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrate\nour model on a MEG dataset.\n",
        "published": "2015-12-17T14:46:20Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05610v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.05698v1",
        "title": "Oracle inequalities for ranking and U-processes with Lasso penalty",
        "summary": "  We investigate properties of estimators obtained by minimization of\nU-processes with the Lasso penalty in high-dimensional settings. Our attention\nis focused on the ranking problem that is popular in machine learning. It is\nrelated to guessing the ordering between objects on the basis of their observed\npredictors. We prove the oracle inequality for the excess risk of the\nconsidered estimator as well as the bound for the l1 distance between the\nestimator and the oracle.\n",
        "published": "2015-12-17T17:57:21Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05698v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.06098v2",
        "title": "Expectation propagation for continuous time stochastic processes",
        "summary": "  We consider the inverse problem of reconstructing the posterior measure over\nthe trajec- tories of a diffusion process from discrete time observations and\ncontinuous time constraints. We cast the problem in a Bayesian framework and\nderive approximations to the posterior distributions of single time marginals\nusing variational approximate inference. We then show how the approximation can\nbe extended to a wide class of discrete-state Markov jump pro- cesses by making\nuse of the chemical Langevin equation. Our empirical results show that the\nproposed method is computationally efficient and provides good approximations\nfor these classes of inverse problems.\n",
        "published": "2015-12-18T20:26:00Z",
        "pdf_link": "http://arxiv.org/pdf/1512.06098v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.07548v1",
        "title": "k-Means Clustering Is Matrix Factorization",
        "summary": "  We show that the objective function of conventional k-means clustering can be\nexpressed as the Frobenius norm of the difference of a data matrix and a low\nrank approximation of that data matrix. In short, we show that k-means\nclustering is a matrix factorization problem. These notes are meant as a\nreference and intended to provide a guided tour towards a result that is often\nmentioned but seldom made explicit in the literature.\n",
        "published": "2015-12-23T17:12:06Z",
        "pdf_link": "http://arxiv.org/pdf/1512.07548v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.07662v1",
        "title": "High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep\n  Models",
        "summary": "  Learning in deep models using Bayesian methods has generated significant\nattention recently. This is largely because of the feasibility of modern\nBayesian methods to yield scalable learning and inference, while maintaining a\nmeasure of uncertainty in the model parameters. Stochastic gradient MCMC\nalgorithms (SG-MCMC) are a family of diffusion-based sampling methods for\nlarge-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient\nthermostats (mSGNHT) augment each parameter of interest, with a momentum and a\nthermostat variable to maintain stationary distributions as target posterior\ndistributions. As the number of variables in a continuous-time diffusion\nincreases, its numerical approximation error becomes a practical bottleneck, so\nbetter use of a numerical integrator is desirable. To this end, we propose use\nof an efficient symmetric splitting integrator in mSGNHT, instead of the\ntraditional Euler integrator. We demonstrate that the proposed scheme is more\naccurate, robust, and converges faster. These properties are demonstrated to be\ndesirable in Bayesian deep learning. Extensive experiments on two canonical\nmodels and their deep extensions demonstrate that the proposed scheme improves\ngeneral Bayesian posterior sampling, particularly for deep models.\n",
        "published": "2015-12-23T23:21:40Z",
        "pdf_link": "http://arxiv.org/pdf/1512.07662v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.07666v1",
        "title": "Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural\n  Networks",
        "summary": "  Effective training of deep neural networks suffers from two main issues. The\nfirst is that the parameter spaces of these models exhibit pathological\ncurvature. Recent methods address this problem by using adaptive\npreconditioning for Stochastic Gradient Descent (SGD). These methods improve\nconvergence by adapting to the local geometry of parameter space. A second\nissue is overfitting, which is typically addressed by early stopping. However,\nrecent work has demonstrated that Bayesian model averaging mitigates this\nproblem. The posterior can be sampled by using Stochastic Gradient Langevin\nDynamics (SGLD). However, the rapidly changing curvature renders default SGLD\nmethods inefficient. Here, we propose combining adaptive preconditioners with\nSGLD. In support of this idea, we give theoretical properties on asymptotic\nconvergence and predictive risk. We also provide empirical results for Logistic\nRegression, Feedforward Neural Nets, and Convolutional Neural Nets,\ndemonstrating that our preconditioned SGLD method gives state-of-the-art\nperformance on these models.\n",
        "published": "2015-12-23T23:45:03Z",
        "pdf_link": "http://arxiv.org/pdf/1512.07666v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.07960v1",
        "title": "Histogram Meets Topic Model: Density Estimation by Mixture of Histograms",
        "summary": "  The histogram method is a powerful non-parametric approach for estimating the\nprobability density function of a continuous variable. But the construction of\na histogram, compared to the parametric approaches, demands a large number of\nobservations to capture the underlying density function. Thus it is not\nsuitable for analyzing a sparse data set, a collection of units with a small\nsize of data. In this paper, by employing the probabilistic topic model, we\ndevelop a novel Bayesian approach to alleviating the sparsity problem in the\nconventional histogram estimation. Our method estimates a unit's density\nfunction as a mixture of basis histograms, in which the number of bins for each\nbasis, as well as their heights, is determined automatically. The estimation\nprocedure is performed by using the fast and easy-to-implement collapsed Gibbs\nsampling. We apply the proposed method to synthetic data, showing that it\nperforms well.\n",
        "published": "2015-12-25T05:30:20Z",
        "pdf_link": "http://arxiv.org/pdf/1512.07960v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.08298v3",
        "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical\n  Models",
        "summary": "  We propose a novel class of time-varying nonparanormal graphical models,\nwhich allows us to model high dimensional heavy-tailed systems and the\nevolution of their latent network structures. Under this model, we develop\nstatistical tests for presence of edges both locally at a fixed index value and\nglobally over a range of values. The tests are developed for a high-dimensional\nregime, are robust to model selection mistakes and do not require commonly\nassumed minimum signal strength. The testing procedures are based on a high\ndimensional, debiasing-free moment estimator, which uses a novel kernel\nsmoothed Kendall's tau correlation matrix as an input statistic. The estimator\nconsistently estimates the latent inverse Pearson correlation matrix uniformly\nin both the index variable and kernel bandwidth. Its rate of convergence is\nshown to be minimax optimal. Our method is supported by thorough numerical\nsimulations and an application to a neural imaging data set.\n",
        "published": "2015-12-28T01:27:07Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08298v3"
    },
    {
        "id": "http://arxiv.org/abs/1512.08643v2",
        "title": "Testing for Differences in Gaussian Graphical Models: Applications to\n  Brain Connectivity",
        "summary": "  Functional brain networks are well described and estimated from data with\nGaussian Graphical Models (GGMs), e.g. using sparse inverse covariance\nestimators. Comparing functional connectivity of subjects in two populations\ncalls for comparing these estimated GGMs. Our goal is to identify differences\nin GGMs known to have similar structure. We characterize the uncertainty of\ndifferences with confidence intervals obtained using a parametric distribution\non parameters of a sparse estimator. Sparse penalties enable statistical\nguarantees and interpretable models even in high-dimensional and low-sample\nsettings. Characterizing the distributions of sparse models is inherently\nchallenging as the penalties produce a biased estimator. Recent work invokes\nthe sparsity assumptions to effectively remove the bias from a sparse estimator\nsuch as the lasso. These distributions can be used to give confidence intervals\non edges in GGMs, and by extension their differences. However, in the case of\ncomparing GGMs, these estimators do not make use of any assumed joint structure\namong the GGMs. Inspired by priors from brain functional connectivity we derive\nthe distribution of parameter differences under a joint penalty when parameters\nare known to be sparse in the difference. This leads us to introduce the\ndebiased multi-task fused lasso, whose distribution can be characterized in an\nefficient manner. We then show how the debiased lasso and multi-task fused\nlasso can be used to obtain confidence intervals on edge differences in GGMs.\nWe validate the techniques proposed on a set of synthetic examples as well as\nneuro-imaging dataset created for the study of autism.\n",
        "published": "2015-12-29T10:07:20Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08643v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.08673v1",
        "title": "Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A\n  Unified Approach",
        "summary": "  In compressed sensing, in order to recover a sparse or nearly sparse vector\nfrom possibly noisy measurements, the most popular approach is $\\ell_1$-norm\nminimization. Upper bounds for the $\\ell_2$- norm of the error between the true\nand estimated vectors are given in [1] and reviewed in [2], while bounds for\nthe $\\ell_1$-norm are given in [3]. When the unknown vector is not\nconventionally sparse but is \"group sparse\" instead, a variety of alternatives\nto the $\\ell_1$-norm have been proposed in the literature, including the group\nLASSO, sparse group LASSO, and group LASSO with tree structured overlapping\ngroups. However, no error bounds are available for any of these modified\nobjective functions. In the present paper, a unified approach is presented for\nderiving upper bounds on the error between the true vector and its\napproximation, based on the notion of decomposable and $\\gamma$-decomposable\nnorms. The bounds presented cover all of the norms mentioned above, and also\nprovide a guideline for choosing norms in future to accommodate alternate forms\nof sparsity.\n",
        "published": "2015-12-29T13:10:25Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08673v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.08861v1",
        "title": "Sharp Computational-Statistical Phase Transitions via Oracle\n  Computational Model",
        "summary": "  We study the fundamental tradeoffs between computational tractability and\nstatistical accuracy for a general family of hypothesis testing problems with\ncombinatorial structures. Based upon an oracle model of computation, which\ncaptures the interactions between algorithms and data, we establish a general\nlower bound that explicitly connects the minimum testing risk under\ncomputational budget constraints with the intrinsic probabilistic and\ncombinatorial structures of statistical problems. This lower bound mirrors the\nclassical statistical lower bound by Le Cam (1986) and allows us to quantify\nthe optimal statistical performance achievable given limited computational\nbudgets in a systematic fashion. Under this unified framework, we sharply\ncharacterize the statistical-computational phase transition for two testing\nproblems, namely, normal mean detection and sparse principal component\ndetection. For normal mean detection, we consider two combinatorial structures,\nnamely, sparse set and perfect matching. For these problems we identify\nsignificant gaps between the optimal statistical accuracy that is achievable\nunder computational tractability constraints and the classical statistical\nlower bounds. Compared with existing works on computational lower bounds for\nstatistical problems, which consider general polynomial-time algorithms on\nTuring machines, and rely on computational hardness hypotheses on problems like\nplanted clique detection, we focus on the oracle computational model, which\ncovers a broad range of popular algorithms, and do not rely on unproven\nhypotheses. Moreover, our result provides an intuitive and concrete\ninterpretation for the intrinsic computational intractability of\nhigh-dimensional statistical problems. One byproduct of our result is a lower\nbound for a strict generalization of the matrix permanent problem, which is of\nindependent interest.\n",
        "published": "2015-12-30T06:16:46Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08861v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.00142v1",
        "title": "Joint Estimation of Precision Matrices in Heterogeneous Populations",
        "summary": "  We introduce a general framework for estimation of inverse covariance, or\nprecision, matrices from heterogeneous populations. The proposed framework uses\na Laplacian shrinkage penalty to encourage similarity among estimates from\ndisparate, but related, subpopulations, while allowing for differences among\nmatrices. We propose an efficient alternating direction method of multipliers\n(ADMM) algorithm for parameter estimation, as well as its extension for faster\ncomputation in high dimensions by thresholding the empirical covariance matrix\nto identify the joint block diagonal structure in the estimated precision\nmatrices. We establish both variable selection and norm consistency of the\nproposed estimator for distributions with exponential or polynomial tails.\nFurther, to extend the applicability of the method to the settings with unknown\npopulations structure, we propose a Laplacian penalty based on hierarchical\nclustering, and discuss conditions under which this data-driven choice results\nin consistent estimation of precision matrices in heterogenous populations.\nExtensive numerical studies and applications to gene expression data from\nsubtypes of cancer with distinct clinical outcomes indicate the potential\nadvantages of the proposed method over existing approaches.\n",
        "published": "2016-01-02T06:11:06Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00142v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.00504v1",
        "title": "Learning relationships between data obtained independently",
        "summary": "  The aim of this paper is to provide a new method for learning the\nrelationships between data that have been obtained independently. Unlike\nexisting methods like matching, the proposed technique does not require any\ncontextual information, provided that the dependency between the variables of\ninterest is monotone. It can therefore be easily combined with matching in\norder to exploit the advantages of both methods. This technique can be\ndescribed as a mix between quantile matching, and deconvolution. We provide for\nit a theoretical and an empirical validation.\n",
        "published": "2016-01-04T13:52:49Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00504v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.01190v3",
        "title": "On Bayesian index policies for sequential resource allocation",
        "summary": "  This paper is about index policies for minimizing (frequentist) regret in a\nstochastic multi-armed bandit model, inspired by a Bayesian view on the\nproblem. Our main contribution is to prove that the Bayes-UCB algorithm, which\nrelies on quantiles of posterior distributions, is asymptotically optimal when\nthe reward distributions belong to a one-dimensional exponential family, for a\nlarge class of prior distributions. We also show that the Bayesian literature\ngives new insight on what kind of exploration rates could be used in\nfrequentist, UCB-type algorithms. Indeed, approximations of the Bayesian\noptimal solution or the Finite Horizon Gittins indices provide a justification\nfor the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also\nestablished.\n",
        "published": "2016-01-06T14:24:59Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01190v3"
    },
    {
        "id": "http://arxiv.org/abs/1601.01966v1",
        "title": "Numerical Coding of Nominal Data",
        "summary": "  In this paper, a novel approach for coding nominal data is proposed. For the\ngiven nominal data, a rank in a form of complex number is assigned. The\nproposed method does not lose any information about the attribute and brings\nother properties previously unknown. The approach based on these knew\nproperties can been used for classification. The analyzed example shows that\nclassification with the use of coded nominal data or both numerical as well as\ncoded nominal data is more effective than the classification, which uses only\nnumerical data.\n",
        "published": "2016-01-08T18:24:52Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01966v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.04674v2",
        "title": "A Framework for Individualizing Predictions of Disease Trajectories by\n  Exploiting Multi-Resolution Structure",
        "summary": "  For many complex diseases, there is a wide variety of ways in which an\nindividual can manifest the disease. The challenge of personalized medicine is\nto develop tools that can accurately predict the trajectory of an individual's\ndisease, which can in turn enable clinicians to optimize treatments. We\nrepresent an individual's disease trajectory as a continuous-valued\ncontinuous-time function describing the severity of the disease over time. We\npropose a hierarchical latent variable model that individualizes predictions of\ndisease trajectories. This model shares statistical strength across\nobservations at different resolutions--the population, subpopulation and the\nindividual level. We describe an algorithm for learning population and\nsubpopulation parameters offline, and an online procedure for dynamically\nlearning individual-specific parameters. Finally, we validate our model on the\ntask of predicting the course of interstitial lung disease, a leading cause of\ndeath among patients with the autoimmune disease scleroderma. We compare our\napproach against state-of-the-art and demonstrate significant improvements in\npredictive accuracy.\n",
        "published": "2016-01-18T20:01:16Z",
        "pdf_link": "http://arxiv.org/pdf/1601.04674v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.05285v4",
        "title": "Nonlinear variable selection with continuous outcome: a nonparametric\n  incremental forward stagewise approach",
        "summary": "  We present a method of variable selection for the sparse generalized additive\nmodel. The method doesn't assume any specific functional form, and can select\nfrom a large number of candidates. It takes the form of incremental forward\nstagewise regression. Given no functional form is assumed, we devised an\napproach termed roughening to adjust the residuals in the iterations. In\nsimulations, we show the new method is competitive against popular machine\nlearning approaches. We also demonstrate its performance using some real\ndatasets. The method is available as a part of the nlnet package on CRAN\nhttps://cran.r-project.org/package=nlnet.\n",
        "published": "2016-01-20T14:44:54Z",
        "pdf_link": "http://arxiv.org/pdf/1601.05285v4"
    },
    {
        "id": "http://arxiv.org/abs/1601.07665v1",
        "title": "Non-Gaussian Component Analysis with Log-Density Gradient Estimation",
        "summary": "  Non-Gaussian component analysis (NGCA) is aimed at identifying a linear\nsubspace such that the projected data follows a non-Gaussian distribution. In\nthis paper, we propose a novel NGCA algorithm based on log-density gradient\nestimation. Unlike existing methods, the proposed NGCA algorithm identifies the\nlinear subspace by using the eigenvalue decomposition without any iterative\nprocedures, and thus is computationally reasonable. Furthermore, through\ntheoretical analysis, we prove that the identified subspace converges to the\ntrue subspace at the optimal parametric rate. Finally, the practical\nperformance of the proposed algorithm is demonstrated on both artificial and\nbenchmark datasets.\n",
        "published": "2016-01-28T06:49:34Z",
        "pdf_link": "http://arxiv.org/pdf/1601.07665v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.00221v1",
        "title": "Principal Polynomial Analysis",
        "summary": "  This paper presents a new framework for manifold learning based on a sequence\nof principal polynomials that capture the possibly nonlinear nature of the\ndata. The proposed Principal Polynomial Analysis (PPA) generalizes PCA by\nmodeling the directions of maximal variance by means of curves, instead of\nstraight lines. Contrarily to previous approaches, PPA reduces to performing\nsimple univariate regressions, which makes it computationally feasible and\nrobust. Moreover, PPA shows a number of interesting analytical properties.\nFirst, PPA is a volume-preserving map, which in turn guarantees the existence\nof the inverse. Second, such an inverse can be obtained in closed form.\nInvertibility is an important advantage over other learning methods, because it\npermits to understand the identified features in the input domain where the\ndata has physical meaning. Moreover, it allows to evaluate the performance of\ndimensionality reduction in sensible (input-domain) units. Volume preservation\nalso allows an easy computation of information theoretic quantities, such as\nthe reduction in multi-information after the transform. Third, the analytical\nnature of PPA leads to a clear geometrical interpretation of the manifold: it\nallows the computation of Frenet-Serret frames (local features) and of\ngeneralized curvatures at any point of the space. And fourth, the analytical\nJacobian allows the computation of the metric induced by the data, thus\ngeneralizing the Mahalanobis distance. These properties are demonstrated\ntheoretically and illustrated experimentally. The performance of PPA is\nevaluated in dimensionality and redundancy reduction, in both synthetic and\nreal datasets from the UCI repository.\n",
        "published": "2016-01-31T10:46:44Z",
        "pdf_link": "http://arxiv.org/pdf/1602.00221v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.00229v1",
        "title": "Iterative Gaussianization: from ICA to Random Rotations",
        "summary": "  Most signal processing problems involve the challenging task of\nmultidimensional probability density function (PDF) estimation. In this work,\nwe propose a solution to this problem by using a family of Rotation-based\nIterative Gaussianization (RBIG) transforms. The general framework consists of\nthe sequential application of a univariate marginal Gaussianization transform\nfollowed by an orthonormal transform. The proposed procedure looks for\ndifferentiable transforms to a known PDF so that the unknown PDF can be\nestimated at any point of the original domain. In particular, we aim at a zero\nmean unit covariance Gaussian for convenience. RBIG is formally similar to\nclassical iterative Projection Pursuit (PP) algorithms. However, we show that,\nunlike in PP methods, the particular class of rotations used has no special\nqualitative relevance in this context, since looking for interestingness is not\na critical issue for PDF estimation. The key difference is that our approach\nfocuses on the univariate part (marginal Gaussianization) of the problem rather\nthan on the multivariate part (rotation). This difference implies that one may\nselect the most convenient rotation suited to each practical application. The\ndifferentiability, invertibility and convergence of RBIG are theoretically and\nexperimentally analyzed. Relation to other methods, such as Radial\nGaussianization (RG), one-class support vector domain description (SVDD), and\ndeep neural networks (DNN) is also pointed out. The practical performance of\nRBIG is successfully illustrated in a number of multidimensional problems such\nas image synthesis, classification, denoising, and multi-information\nestimation.\n",
        "published": "2016-01-31T11:30:50Z",
        "pdf_link": "http://arxiv.org/pdf/1602.00229v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.00260v2",
        "title": "DOLDA - a regularized supervised topic model for high-dimensional\n  multi-class regression",
        "summary": "  Generating user interpretable multi-class predictions in data rich\nenvironments with many classes and explanatory covariates is a daunting task.\nWe introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised\ntopic model for multi-class classification that can handle both many classes as\nwell as many covariates. To handle many classes we use the recently proposed\nDiagonal Orthant (DO) probit model (Johndrow et al., 2013) together with an\nefficient Horseshoe prior for variable selection/shrinkage (Carvalho et al.,\n2010). We propose a computationally efficient parallel Gibbs sampler for the\nnew model. An important advantage of DOLDA is that learned topics are directly\nconnected to individual classes without the need for a reference class. We\nevaluate the model's predictive accuracy on two datasets and demonstrate\nDOLDA's advantage in interpreting the generated predictions.\n",
        "published": "2016-01-31T15:33:10Z",
        "pdf_link": "http://arxiv.org/pdf/1602.00260v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.00360v1",
        "title": "Semi-supervised K-means++",
        "summary": "  Traditionally, practitioners initialize the {\\tt k-means} algorithm with\ncenters chosen uniformly at random. Randomized initialization with uneven\nweights ({\\tt k-means++}) has recently been used to improve the performance\nover this strategy in cost and run-time. We consider the k-means problem with\nsemi-supervised information, where some of the data are pre-labeled, and we\nseek to label the rest according to the minimum cost solution. By extending the\n{\\tt k-means++} algorithm and analysis to account for the labels, we derive an\nimproved theoretical bound on expected cost and observe improved performance in\nsimulated and real data examples. This analysis provides theoretical\njustification for a roughly linear semi-supervised clustering algorithm.\n",
        "published": "2016-02-01T01:49:23Z",
        "pdf_link": "http://arxiv.org/pdf/1602.00360v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.01182v2",
        "title": "High-Dimensional Regularized Discriminant Analysis",
        "summary": "  Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a\nwidely popular classifier that lacks interpretability and is impractical for\nhigh-dimensional data sets. Here, we present an interpretable and\ncomputationally efficient classifier called high-dimensional RDA (HDRDA),\ndesigned for the small-sample, high-dimensional setting. For HDRDA, we show\nthat each training observation, regardless of class, contributes to the class\ncovariance matrix, resulting in an interpretable estimator that borrows from\nthe pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent\nto a classifier in a reduced-feature space with dimension approximately equal\nto the training sample size. As a result, the matrix operations employed by\nHDRDA are computationally linear in the number of features, making the\nclassifier well-suited for high-dimensional classification in practice. We\ndemonstrate that HDRDA is often superior to several sparse and regularized\nclassifiers in terms of classification accuracy with three artificial and six\nreal high-dimensional data sets. Also, timing comparisons between our HDRDA\nimplementation in the sparsediscrim R package and the standard RDA formulation\nin the klaR R package demonstrate that as the number of features increases, the\ncomputational runtime of HDRDA is drastically smaller than that of RDA.\n",
        "published": "2016-02-03T03:56:08Z",
        "pdf_link": "http://arxiv.org/pdf/1602.01182v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.01345v2",
        "title": "A Probabilistic Modeling Approach to Hearing Loss Compensation",
        "summary": "  Hearing Aid (HA) algorithms need to be tuned (\"fitted\") to match the\nimpairment of each specific patient. The lack of a fundamental HA fitting\ntheory is a strong contributing factor to an unsatisfying sound experience for\nabout 20% of hearing aid patients. This paper proposes a probabilistic modeling\napproach to the design of HA algorithms. The proposed method relies on a\ngenerative probabilistic model for the hearing loss problem and provides for\nautomated inference of the corresponding (1) signal processing algorithm, (2)\nthe fitting solution as well as a principled (3) performance evaluation metric.\nAll three tasks are realized as message passing algorithms in a factor graph\nrepresentation of the generative model, which in principle allows for fast\nimplementation on hearing aid or mobile device hardware. The methods are\ntheoretically worked out and simulated with a custom-built factor graph toolbox\nfor a specific hearing loss model.\n",
        "published": "2016-02-03T15:45:47Z",
        "pdf_link": "http://arxiv.org/pdf/1602.01345v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.02485v1",
        "title": "Simultaneous Safe Screening of Features and Samples in Doubly Sparse\n  Modeling",
        "summary": "  The problem of learning a sparse model is conceptually interpreted as the\nprocess of identifying active features/samples and then optimizing the model\nover them. Recently introduced safe screening allows us to identify a part of\nnon-active features/samples. So far, safe screening has been individually\nstudied either for feature screening or for sample screening. In this paper, we\nintroduce a new approach for safely screening features and samples\nsimultaneously by alternatively iterating feature and sample screening steps. A\nsignificant advantage of considering them simultaneously rather than\nindividually is that they have a synergy effect in the sense that the results\nof the previous safe feature screening can be exploited for improving the next\nsafe sample screening performances, and vice-versa. We first theoretically\ninvestigate the synergy effect, and then illustrate the practical advantage\nthrough intensive numerical experiments for problems with large numbers of\nfeatures and samples.\n",
        "published": "2016-02-08T07:48:38Z",
        "pdf_link": "http://arxiv.org/pdf/1602.02485v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.02964v4",
        "title": "A Kernel Test of Goodness of Fit",
        "summary": "  We propose a nonparametric statistical test for goodness-of-fit: given a set\nof samples, the test determines how likely it is that these were generated from\na target density function. The measure of goodness-of-fit is a divergence\nconstructed via Stein's method using functions from a Reproducing Kernel\nHilbert Space. Our test statistic is based on an empirical estimate of this\ndivergence, taking the form of a V-statistic in terms of the log gradients of\nthe target density and the kernel. We derive a statistical test, both for\ni.i.d. and non-i.i.d. samples, where we estimate the null distribution\nquantiles using a wild bootstrap procedure. We apply our test to quantifying\nconvergence of approximate Markov Chain Monte Carlo methods, statistical model\ncriticism, and evaluating quality of fit vs model complexity in nonparametric\ndensity estimation.\n",
        "published": "2016-02-09T12:54:16Z",
        "pdf_link": "http://arxiv.org/pdf/1602.02964v4"
    },
    {
        "id": "http://arxiv.org/abs/1602.03048v1",
        "title": "Bayesian nonparametric image segmentation using a generalized\n  Swendsen-Wang algorithm",
        "summary": "  Unsupervised image segmentation aims at clustering the set of pixels of an\nimage into spatially homogeneous regions. We introduce here a class of Bayesian\nnonparametric models to address this problem. These models are based on a\ncombination of a Potts-like spatial smoothness component and a prior on\npartitions which is used to control both the number and size of clusters. This\nclass of models is flexible enough to include the standard Potts model and the\nmore recent Potts-Dirichlet Process model \\cite{Orbanz2008}. More importantly,\nany prior on partitions can be introduced to control the global clustering\nstructure so that it is possible to penalize small or large clusters if\nnecessary. Bayesian computation is carried out using an original generalized\nSwendsen-Wang algorithm. Experiments demonstrate that our method is competitive\nin terms of RAND\\ index compared to popular image segmentation methods, such as\nmean-shift, and recent alternative Bayesian nonparametric models.\n",
        "published": "2016-02-09T16:02:00Z",
        "pdf_link": "http://arxiv.org/pdf/1602.03048v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.03253v2",
        "title": "A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model\n  Evaluation",
        "summary": "  We derive a new discrepancy statistic for measuring differences between two\nprobability distributions based on combining Stein's identity with the\nreproducing kernel Hilbert space theory. We apply our result to test how well a\nprobabilistic model fits a set of observations, and derive a new class of\npowerful goodness-of-fit tests that are widely applicable for complex and high\ndimensional distributions, even for those with computationally intractable\nnormalization constants. Both theoretical and empirical properties of our\nmethods are studied thoroughly.\n",
        "published": "2016-02-10T03:38:52Z",
        "pdf_link": "http://arxiv.org/pdf/1602.03253v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.03442v2",
        "title": "Stochastic Quasi-Newton Langevin Monte Carlo",
        "summary": "  Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have\nbeen proposed for scaling up Monte Carlo computations to large data problems.\nWhilst these approaches have proven useful in many applications, vanilla\nSG-MCMC might suffer from poor mixing rates when random variables exhibit\nstrong couplings under the target densities or big scale differences. In this\nstudy, we propose a novel SG-MCMC method that takes the local geometry into\naccount by using ideas from Quasi-Newton optimization methods. These second\norder methods directly approximate the inverse Hessian by using a limited\nhistory of samples and their gradients. Our method uses dense approximations of\nthe inverse Hessian while keeping the time and memory complexities linear with\nthe dimension of the problem. We provide a formal theoretical analysis where we\nshow that the proposed method is asymptotically unbiased and consistent with\nthe posterior expectations. We illustrate the effectiveness of the approach on\nboth synthetic and real datasets. Our experiments on two challenging\napplications show that our method achieves fast convergence rates similar to\nRiemannian approaches while at the same time having low computational\nrequirements similar to diagonal preconditioning approaches.\n",
        "published": "2016-02-10T16:53:36Z",
        "pdf_link": "http://arxiv.org/pdf/1602.03442v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.03670v2",
        "title": "Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View",
        "summary": "  Extracting the underlying low-dimensional space where high-dimensional\nsignals often reside has long been at the center of numerous algorithms in the\nsignal processing and machine learning literature during the past few decades.\nAt the same time, working with incomplete (partly observed) large scale\ndatasets has recently been commonplace for diverse reasons. This so called {\\it\nbig data era} we are currently living calls for devising online subspace\nlearning algorithms that can suitably handle incomplete data. Their envisaged\nobjective is to {\\it recursively} estimate the unknown subspace by processing\nstreaming data sequentially, thus reducing computational complexity, while\nobviating the need for storing the whole dataset in memory. In this paper, an\nonline variational Bayes subspace learning algorithm from partial observations\nis presented. To account for the unawareness of the true rank of the subspace,\ncommonly met in practice, low-rankness is explicitly imposed on the sought\nsubspace data matrix by exploiting sparse Bayesian learning principles.\nMoreover, sparsity, {\\it simultaneously} to low-rankness, is favored on the\nsubspace matrix by the sophisticated hierarchical Bayesian scheme that is\nadopted. In doing so, the proposed algorithm becomes adept in dealing with\napplications whereby the underlying subspace may be also sparse, as, e.g., in\nsparse dictionary learning problems. As shown, the new subspace tracking scheme\noutperforms its state-of-the-art counterparts in terms of estimation accuracy,\nin a variety of experiments conducted on simulated and real data.\n",
        "published": "2016-02-11T10:47:31Z",
        "pdf_link": "http://arxiv.org/pdf/1602.03670v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.03683v1",
        "title": "A Universal Approximation Theorem for Mixture of Experts Models",
        "summary": "  The mixture of experts (MoE) model is a popular neural network architecture\nfor nonlinear regression and classification. The class of MoE mean functions is\nknown to be uniformly convergent to any unknown target function, assuming that\nthe target function is from Sobolev space that is sufficiently differentiable\nand that the domain of estimation is a compact unit hypercube. We provide an\nalternative result, which shows that the class of MoE mean functions is dense\nin the class of all continuous functions over arbitrary compact domains of\nestimation. Our result can be viewed as a universal approximation theorem for\nMoE models.\n",
        "published": "2016-02-11T11:42:15Z",
        "pdf_link": "http://arxiv.org/pdf/1602.03683v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.04548v1",
        "title": "Safe Pattern Pruning: An Efficient Approach for Predictive Pattern\n  Mining",
        "summary": "  In this paper we study predictive pattern mining problems where the goal is\nto construct a predictive model based on a subset of predictive patterns in the\ndatabase. Our main contribution is to introduce a novel method called safe\npattern pruning (SPP) for a class of predictive pattern mining problems. The\nSPP method allows us to efficiently find a superset of all the predictive\npatterns in the database that are needed for the optimal predictive model. The\nadvantage of the SPP method over existing boosting-type method is that the\nformer can find the superset by a single search over the database, while the\nlatter requires multiple searches. The SPP method is inspired by recent\ndevelopment of safe feature screening. In order to extend the idea of safe\nfeature screening into predictive pattern mining, we derive a novel pruning\nrule called safe pattern pruning (SPP) rule that can be used for searching over\nthe tree defined among patterns in the database. The SPP rule has a property\nthat, if a node corresponding to a pattern in the database is pruned out by the\nSPP rule, then it is guaranteed that all the patterns corresponding to its\ndescendant nodes are never needed for the optimal predictive model. We apply\nthe SPP method to graph mining and item-set mining problems, and demonstrate\nits computational advantage.\n",
        "published": "2016-02-15T02:16:16Z",
        "pdf_link": "http://arxiv.org/pdf/1602.04548v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.04601v2",
        "title": "Selective Inference Approach for Statistically Sound Predictive Pattern\n  Mining",
        "summary": "  Discovering statistically significant patterns from databases is an important\nchallenging problem. The main obstacle of this problem is in the difficulty of\ntaking into account the selection bias, i.e., the bias arising from the fact\nthat patterns are selected from extremely large number of candidates in\ndatabases. In this paper, we introduce a new approach for predictive pattern\nmining problems that can address the selection bias issue. Our approach is\nbuilt on a recently popularized statistical inference framework called\nselective inference. In selective inference, statistical inferences (such as\nstatistical hypothesis testing) are conducted based on sampling distributions\nconditional on a selection event. If the selection event is characterized in a\ntractable way, statistical inferences can be made without minding selection\nbias issue. However, in pattern mining problems, it is difficult to\ncharacterize the entire selection process of mining algorithms. Our main\ncontribution in this paper is to solve this challenging problem for a class of\npredictive pattern mining problems by introducing a novel algorithmic\nframework. We demonstrate that our approach is useful for finding statistically\nsignificant patterns from databases.\n",
        "published": "2016-02-15T09:52:00Z",
        "pdf_link": "http://arxiv.org/pdf/1602.04601v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.05003v6",
        "title": "The Multivariate Generalised von Mises distribution: Inference and\n  applications",
        "summary": "  Circular variables arise in a multitude of data-modelling contexts ranging\nfrom robotics to the social sciences, but they have been largely overlooked by\nthe machine learning community. This paper partially redresses this imbalance\nby extending some standard probabilistic modelling tools to the circular\ndomain. First we introduce a new multivariate distribution over circular\nvariables, called the multivariate Generalised von Mises (mGvM) distribution.\nThis distribution can be constructed by restricting and renormalising a general\nmultivariate Gaussian distribution to the unit hyper-torus. Previously proposed\nmultivariate circular distributions are shown to be special cases of this\nconstruction. Second, we introduce a new probabilistic model for circular\nregression, that is inspired by Gaussian Processes, and a method for\nprobabilistic principal component analysis with circular hidden variables.\nThese models can leverage standard modelling tools (e.g. covariance functions\nand methods for automatic relevance determination). Third, we show that the\nposterior distribution in these models is a mGvM distribution which enables\ndevelopment of an efficient variational free-energy scheme for performing\napproximate inference and approximate maximum-likelihood learning.\n",
        "published": "2016-02-16T12:50:16Z",
        "pdf_link": "http://arxiv.org/pdf/1602.05003v6"
    },
    {
        "id": "http://arxiv.org/abs/1602.05221v2",
        "title": "Patterns of Scalable Bayesian Inference",
        "summary": "  Datasets are growing not just in size but in complexity, creating a demand\nfor rich models and quantification of uncertainty. Bayesian methods are an\nexcellent fit for this demand, but scaling Bayesian inference is a challenge.\nIn response to this challenge, there has been considerable recent work based on\nvarying assumptions about model structure, underlying computational resources,\nand the importance of asymptotic correctness. As a result, there is a zoo of\nideas with few clear overarching principles.\n  In this paper, we seek to identify unifying principles, patterns, and\nintuitions for scaling Bayesian inference. We review existing work on utilizing\nmodern computing resources with both MCMC and variational approximation\ntechniques. From this taxonomy of ideas, we characterize the general principles\nthat have proven successful for designing scalable inference procedures and\ncomment on the path forward.\n",
        "published": "2016-02-16T22:13:00Z",
        "pdf_link": "http://arxiv.org/pdf/1602.05221v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.05563v1",
        "title": "Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel\n  Hilbert Space toward Kernel Methods",
        "summary": "  To the best of our knowledge, there are no general well-founded robust\nmethods for statistical unsupervised learning. Most of the unsupervised methods\nexplicitly or implicitly depend on the kernel covariance operator (kernel CO)\nor kernel cross-covariance operator (kernel CCO). They are sensitive to\ncontaminated data, even when using bounded positive definite kernels. First, we\npropose robust kernel covariance operator (robust kernel CO) and robust kernel\ncrosscovariance operator (robust kernel CCO) based on a generalized loss\nfunction instead of the quadratic loss function. Second, we propose influence\nfunction of classical kernel canonical correlation analysis (classical kernel\nCCA). Third, using this influence function, we propose a visualization method\nto detect influential observations from two sets of data. Finally, we propose a\nmethod based on robust kernel CO and robust kernel CCO, called robust kernel\nCCA, which is designed for contaminated data and less sensitive to noise than\nclassical kernel CCA. The principles we describe also apply to many kernel\nmethods which must deal with the issue of kernel CO or kernel CCO. Experiments\non synthesized and imaging genetics analysis demonstrate that the proposed\nvisualization and robust kernel CCA can be applied effectively to both ideal\ndata and contaminated data. The robust methods show the superior performance\nover the state-of-the-art methods.\n",
        "published": "2016-02-17T20:37:40Z",
        "pdf_link": "http://arxiv.org/pdf/1602.05563v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.05822v1",
        "title": "What is the distribution of the number of unique original items in a\n  bootstrap sample?",
        "summary": "  Sampling with replacement occurs in many settings in machine learning,\nnotably in the bagging ensemble technique and the .632+ validation scheme. The\nnumber of unique original items in a bootstrap sample can have an important\nrole in the behaviour of prediction models learned on it. Indeed, there are\nuncontrived examples where duplicate items have no effect. The purpose of this\nreport is to present the distribution of the number of unique original items in\na bootstrap sample clearly and concisely, with a view to enabling other machine\nlearning researchers to understand and control this quantity in existing and\nfuture resampling techniques. We describe the key characteristics of this\ndistribution along with the generalisation for the case where items come from\ndistinct categories, as in classification. In both cases we discuss the normal\nlimit, and conduct an empirical investigation to derive a heuristic for when a\nnormal approximation is permissible.\n",
        "published": "2016-02-18T14:56:47Z",
        "pdf_link": "http://arxiv.org/pdf/1602.05822v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.06049v1",
        "title": "Scaling up Dynamic Topic Models",
        "summary": "  Dynamic topic models (DTMs) are very effective in discovering topics and\ncapturing their evolution trends in time series data. To do posterior inference\nof DTMs, existing methods are all batch algorithms that scan the full dataset\nbefore each update of the model and make inexact variational approximations\nwith mean-field assumptions. Due to a lack of a more scalable inference\nalgorithm, despite the usefulness, DTMs have not captured large topic dynamics.\n  This paper fills this research void, and presents a fast and parallelizable\ninference algorithm using Gibbs Sampling with Stochastic Gradient Langevin\nDynamics that does not make any unwarranted assumptions. We also present a\nMetropolis-Hastings based $O(1)$ sampler for topic assignments for each word\ntoken. In a distributed environment, our algorithm requires very little\ncommunication between workers during sampling (almost embarrassingly parallel)\nand scales up to large-scale applications. We are able to learn the largest\nDynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics\nfrom 2.6 million documents in less than half an hour, and our empirical results\nshow that our algorithm is not only orders of magnitude faster than the\nbaselines but also achieves lower perplexity.\n",
        "published": "2016-02-19T05:55:08Z",
        "pdf_link": "http://arxiv.org/pdf/1602.06049v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.06235v1",
        "title": "A Mutual Contamination Analysis of Mixed Membership and Partial Label\n  Models",
        "summary": "  Many machine learning problems can be characterized by mutual contamination\nmodels. In these problems, one observes several random samples from different\nconvex combinations of a set of unknown base distributions. It is of interest\nto decontaminate mutual contamination models, i.e., to recover the base\ndistributions either exactly or up to a permutation. This paper considers the\ngeneral setting where the base distributions are defined on arbitrary\nprobability spaces. We examine the decontamination problem in two mutual\ncontamination models that describe popular machine learning tasks: recovering\nthe base distributions up to a permutation in a mixed membership model, and\nrecovering the base distributions exactly in a partial label model for\nclassification. We give necessary and sufficient conditions for identifiability\nof both mutual contamination models, algorithms for both problems in the\ninfinite and finite sample cases, and introduce novel proof techniques based on\naffine geometry.\n",
        "published": "2016-02-19T17:40:58Z",
        "pdf_link": "http://arxiv.org/pdf/1602.06235v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.06349v1",
        "title": "The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM",
        "summary": "  We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov\nmodel (iHMM) that supports a simple, efficient inference scheme. The siHMM is\nwell suited to segmentation problems, where the goal is to identify points at\nwhich a time series transitions from one relatively stable regime to a new\nregime. Conventional iHMMs often struggle with such problems, since they have\nno mechanism for distinguishing between high- and low-level dynamics.\nHierarchical HMMs (HHMMs) can do better, but they require much more complex and\nexpensive inference algorithms. The siHMM retains the simplicity and efficiency\nof the iHMM, but outperforms it on a variety of segmentation problems,\nachieving performance that matches or exceeds that of a more complicated HHMM.\n",
        "published": "2016-02-20T00:30:03Z",
        "pdf_link": "http://arxiv.org/pdf/1602.06349v1"
    },
    {
        "id": "http://arxiv.org/abs/1602.06701v2",
        "title": "Inference Networks for Sequential Monte Carlo in Graphical Models",
        "summary": "  We introduce a new approach for amortizing inference in directed graphical\nmodels by learning heuristic approximations to stochastic inverses, designed\nspecifically for use as proposal distributions in sequential Monte Carlo\nmethods. We describe a procedure for constructing and learning a structured\nneural network which represents an inverse factorization of the graphical\nmodel, resulting in a conditional density estimator that takes as input\nparticular values of the observed random variables, and returns an\napproximation to the distribution of the latent variables. This recognition\nmodel can be learned offline, independent from any particular dataset, prior to\nperforming inference. The output of these networks can be used as\nautomatically-learned high-quality proposal distributions to accelerate\nsequential Monte Carlo across a diverse range of problem settings.\n",
        "published": "2016-02-22T09:39:09Z",
        "pdf_link": "http://arxiv.org/pdf/1602.06701v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.07277v2",
        "title": "A Simple Approach to Sparse Clustering",
        "summary": "  Consider the problem of sparse clustering, where it is assumed that only a\nsubset of the features are useful for clustering purposes. In the framework of\nthe COSA method of Friedman and Meulman, subsequently improved in the form of\nthe Sparse K-means method of Witten and Tibshirani, a natural and simpler\nhill-climbing approach is introduced. The new method is shown to be competitive\nwith these two methods and others.\n",
        "published": "2016-02-23T19:49:16Z",
        "pdf_link": "http://arxiv.org/pdf/1602.07277v2"
    },
    {
        "id": "http://arxiv.org/abs/1602.08418v1",
        "title": "Multivariate Hawkes Processes for Large-scale Inference",
        "summary": "  In this paper, we present a framework for fitting multivariate Hawkes\nprocesses for large-scale problems both in the number of events in the observed\nhistory $n$ and the number of event types $d$ (i.e. dimensions). The proposed\nLow-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation of\nthe kernel matrix that allows to perform the nonparametric learning of the\n$d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is the\nrank of the approximation ($r \\ll d,n$). This comes as a major improvement to\nthe existing state-of-the-art inference algorithms that are in $O(nd^2)$.\nFurthermore, the low-rank approximation allows LRHP to learn representative\npatterns of interaction between event types, which may be valuable for the\nanalysis of such complex processes in real world datasets. The efficiency and\nscalability of our approach is illustrated with numerical experiments on\nsimulated as well as real datasets.\n",
        "published": "2016-02-26T17:56:13Z",
        "pdf_link": "http://arxiv.org/pdf/1602.08418v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.00316v1",
        "title": "Directional Statistics in Machine Learning: a Brief Review",
        "summary": "  The modern data analyst must cope with data encoded in various forms,\nvectors, matrices, strings, graphs, or more. Consequently, statistical and\nmachine learning models tailored to different data encodings are important. We\nfocus on data encoded as normalized vectors, so that their \"direction\" is more\nimportant than their magnitude. Specifically, we consider high-dimensional\nvectors that lie either on the surface of the unit hypersphere or on the real\nprojective plane. For such data, we briefly review common mathematical models\nprevalent in machine learning, while also outlining some technical aspects,\nsoftware, applications, and open mathematical challenges.\n",
        "published": "2016-05-01T22:37:24Z",
        "pdf_link": "http://arxiv.org/pdf/1605.00316v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.00355v1",
        "title": "Contrastive Structured Anomaly Detection for Gaussian Graphical Models",
        "summary": "  Gaussian graphical models (GGMs) are probabilistic tools of choice for\nanalyzing conditional dependencies between variables in complex systems.\nFinding changepoints in the structural evolution of a GGM is therefore\nessential to detecting anomalies in the underlying system modeled by the GGM.\nIn order to detect structural anomalies in a GGM, we consider the problem of\nestimating changes in the precision matrix of the corresponding Gaussian\ndistribution. We take a two-step approach to solving this problem:- (i)\nestimating a background precision matrix using system observations from the\npast without any anomalies, and (ii) estimating a foreground precision matrix\nusing a sliding temporal window during anomaly monitoring. Our primary\ncontribution is in estimating the foreground precision using a novel\ncontrastive inverse covariance estimation procedure. In order to accurately\nlearn only the structural changes to the GGM, we maximize a penalized\nlog-likelihood where the penalty is the $l_1$ norm of difference between the\nforeground precision being estimated and the already learned background\nprecision. We modify the alternating direction method of multipliers (ADMM)\nalgorithm for sparse inverse covariance estimation to perform contrastive\nestimation of the foreground precision matrix. Our results on simulated GGM\ndata show significant improvement in precision and recall for detecting\nstructural changes to the GGM, compared to a non-contrastive sliding window\nbaseline.\n",
        "published": "2016-05-02T05:42:10Z",
        "pdf_link": "http://arxiv.org/pdf/1605.00355v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.00388v2",
        "title": "Highly Accurate Prediction of Jobs Runtime Classes",
        "summary": "  Separating the short jobs from the long is a known technique to improve\nscheduling performance. In this paper we describe a method we developed for\naccurately predicting the runtimes classes of the jobs to enable this\nseparation. Our method uses the fact that the runtimes can be represented as a\nmixture of overlapping Gaussian distributions, in order to train a CART\nclassifier to provide the prediction. The threshold that separates the short\njobs from the long jobs is determined during the evaluation of the classifier\nto maximize prediction accuracy. Our results indicate overall accuracy of 90%\nfor the data set used in our study, with sensitivity and specificity both above\n90%.\n",
        "published": "2016-05-02T08:31:48Z",
        "pdf_link": "http://arxiv.org/pdf/1605.00388v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.00513v1",
        "title": "Fuzzy clustering of distribution-valued data using adaptive L2\n  Wasserstein distances",
        "summary": "  Distributional (or distribution-valued) data are a new type of data arising\nfrom several sources and are considered as realizations of distributional\nvariables. A new set of fuzzy c-means algorithms for data described by\ndistributional variables is proposed.\n  The algorithms use the $L2$ Wasserstein distance between distributions as\ndissimilarity measures. Beside the extension of the fuzzy c-means algorithm for\ndistributional data, and considering a decomposition of the squared $L2$\nWasserstein distance, we propose a set of algorithms using different automatic\nway to compute the weights associated with the variables as well as with their\ncomponents, globally or cluster-wise. The relevance weights are computed in the\nclustering process introducing product-to-one constraints.\n  The relevance weights induce adaptive distances expressing the importance of\neach variable or of each component in the clustering process, acting also as a\nvariable selection method in clustering. We have tested the proposed algorithms\non artificial and real-world data. Results confirm that the proposed methods\nare able to better take into account the cluster structure of the data with\nrespect to the standard fuzzy c-means, with non-adaptive distances.\n",
        "published": "2016-05-02T14:56:18Z",
        "pdf_link": "http://arxiv.org/pdf/1605.00513v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.01573v1",
        "title": "Observational-Interventional Priors for Dose-Response Learning",
        "summary": "  Controlled interventions provide the most direct source of information for\nlearning causal effects. In particular, a dose-response curve can be learned by\nvarying the treatment level and observing the corresponding outcomes. However,\ninterventions can be expensive and time-consuming. Observational data, where\nthe treatment is not controlled by a known mechanism, is sometimes available.\nUnder some strong assumptions, observational data allows for the estimation of\ndose-response curves. Estimating such curves nonparametrically is hard: sample\nsizes for controlled interventions may be small, while in the observational\ncase a large number of measured confounders may need to be marginalized. In\nthis paper, we introduce a hierarchical Gaussian process prior that constructs\na distribution over the dose-response curve by learning from observational\ndata, and reshapes the distribution with a nonparametric affine transform\nlearned from controlled interventions. This function composition from different\nsources is shown to speed-up learning, which we demonstrate with a thorough\nsensitivity analysis and an application to modeling the effect of therapy on\ncognitive skills of premature infants.\n",
        "published": "2016-05-05T12:50:44Z",
        "pdf_link": "http://arxiv.org/pdf/1605.01573v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.01779v1",
        "title": "Clustering on the Edge: Learning Structure in Graphs",
        "summary": "  With the recent popularity of graphical clustering methods, there has been an\nincreased focus on the information between samples. We show how learning\ncluster structure using edge features naturally and simultaneously determines\nthe most likely number of clusters and addresses data scale issues. These\nresults are particularly useful in instances where (a) there are a large number\nof clusters and (b) we have some labeled edges. Applications in this domain\ninclude image segmentation, community discovery and entity resolution. Our\nmodel is an extension of the planted partition model and our solution uses\nresults of correlation clustering, which achieves a partition O(log(n))-close\nto the log-likelihood of the true clustering.\n",
        "published": "2016-05-05T22:23:21Z",
        "pdf_link": "http://arxiv.org/pdf/1605.01779v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.02190v1",
        "title": "Matching models across abstraction levels with Gaussian Processes",
        "summary": "  Biological systems are often modelled at different levels of abstraction\ndepending on the particular aims/resources of a study. Such different models\noften provide qualitatively concordant predictions over specific\nparametrisations, but it is generally unclear whether model predictions are\nquantitatively in agreement, and whether such agreement holds for different\nparametrisations. Here we present a generally applicable statistical machine\nlearning methodology to automatically reconcile the predictions of different\nmodels across abstraction levels. Our approach is based on defining a\ncorrection map, a random function which modifies the output of a model in order\nto match the statistics of the output of a different model of the same system.\nWe use two biological examples to give a proof-of-principle demonstration of\nthe methodology, and discuss its advantages and potential further applications.\n",
        "published": "2016-05-07T13:00:05Z",
        "pdf_link": "http://arxiv.org/pdf/1605.02190v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.02541v2",
        "title": "Mean Absolute Percentage Error for regression models",
        "summary": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We prove the\nexistence of an optimal MAPE model and we show the universal consistency of\nEmpirical Risk Minimization based on the MAPE. We also show that finding the\nbest model under the MAPE is equivalent to doing weighted Mean Absolute Error\n(MAE) regression, and we apply this weighting strategy to kernel regression.\nThe behavior of the MAPE kernel regression is illustrated on simulated data.\n",
        "published": "2016-05-09T11:46:26Z",
        "pdf_link": "http://arxiv.org/pdf/1605.02541v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.02674v2",
        "title": "Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate\n  Analysis",
        "summary": "  Multivariate Analysis (MVA) comprises a family of well-known methods for\nfeature extraction that exploit correlations among input variables of the data\nrepresentation. One important property that is enjoyed by most such methods is\nuncorrelation among the extracted features. Recently, regularized versions of\nMVA methods have appeared in the literature, mainly with the goal to gain\ninterpretability of the solution. In these cases, the solutions can no longer\nbe obtained in a closed manner, and it is frequent to recur to the iteration of\ntwo steps, one of them being an orthogonal Procrustes problem. This letter\nshows that the Procrustes solution is not optimal from the perspective of the\noverall MVA method, and proposes an alternative approach based on the solution\nof an eigenvalue problem. Our method ensures the preservation of several\nproperties of the original methods, most notably the uncorrelation of the\nextracted features, as demonstrated theoretically and through a collection of\nselected experiments.\n",
        "published": "2016-05-09T18:07:06Z",
        "pdf_link": "http://arxiv.org/pdf/1605.02674v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.03027v1",
        "title": "Destination Prediction by Trajectory Distribution Based Model",
        "summary": "  In this paper we propose a new method to predict the final destination of\nvehicle trips based on their initial partial trajectories. We first review how\nwe obtained clustering of trajectories that describes user behaviour. Then, we\nexplain how we model main traffic flow patterns by a mixture of 2d Gaussian\ndistributions. This yielded a density based clustering of locations, which\nproduces a data driven grid of similar points within each pattern. We present\nhow this model can be used to predict the final destination of a new trajectory\nbased on their first locations using a two step procedure: We first assign the\nnew trajectory to the clusters it mot likely belongs. Secondly, we use\ncharacteristics from trajectories inside these clusters to predict the final\ndestination. Finally, we present experimental results of our methods for\nclassification of trajectories and final destination prediction on datasets of\ntimestamped GPS-Location of taxi trips. We test our methods on two different\ndatasets, to assess the capacity of our method to adapt automatically to\ndifferent subsets.\n",
        "published": "2016-05-10T14:22:45Z",
        "pdf_link": "http://arxiv.org/pdf/1605.03027v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.03040v1",
        "title": "A note on the statistical view of matrix completion",
        "summary": "  A very simple interpretation of matrix completion problem is introduced based\non statistical models. Combined with the well-known results from missing data\nanalysis, such interpretation indicates that matrix completion is still a valid\nand principled estimation procedure even without the missing completely at\nrandom (MCAR) assumption, which almost all of the current theoretical studies\nof matrix completion assume.\n",
        "published": "2016-05-10T14:55:46Z",
        "pdf_link": "http://arxiv.org/pdf/1605.03040v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.03122v1",
        "title": "Kernel-Based Structural Equation Models for Topology Identification of\n  Directed Networks",
        "summary": "  Structural equation models (SEMs) have been widely adopted for inference of\ncausal interactions in complex networks. Recent examples include unveiling\ntopologies of hidden causal networks over which processes such as spreading\ndiseases, or rumors propagate. The appeal of SEMs in these settings stems from\ntheir simplicity and tractability, since they typically assume linear\ndependencies among observable variables. Acknowledging the limitations inherent\nto adopting linear models, the present paper advocates nonlinear SEMs, which\naccount for (possible) nonlinear dependencies among network nodes. The\nadvocated approach leverages kernels as a powerful encompassing framework for\nnonlinear modeling, and an efficient estimator with affordable tradeoffs is put\nforth. Interestingly, pursuit of the novel kernel-based approach yields a\nconvex regularized estimator that promotes edge sparsity, and is amenable to\nproximal-splitting optimization methods. To this end, solvers with\ncomplementary merits are developed by leveraging the alternating direction\nmethod of multipliers, and proximal gradient iterations. Experiments conducted\non simulated data demonstrate that the novel approach outperforms linear SEMs\nwith respect to edge detection errors. Furthermore, tests on a real gene\nexpression dataset unveil interesting new edges that were not revealed by\nlinear SEMs, which could shed more light on regulatory behavior of human genes.\n",
        "published": "2016-05-10T17:40:20Z",
        "pdf_link": "http://arxiv.org/pdf/1605.03122v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.03267v2",
        "title": "Generalized Sparse Precision Matrix Selection for Fitting Multivariate\n  Gaussian Random Fields to Large Data Sets",
        "summary": "  We present a new method for estimating multivariate, second-order stationary\nGaussian Random Field (GRF) models based on the Sparse Precision matrix\nSelection (SPS) algorithm, proposed by Davanloo et al. (2015) for estimating\nscalar GRF models. Theoretical convergence rates for the estimated\nbetween-response covariance matrix and for the estimated parameters of the\nunderlying spatial correlation function are established. Numerical tests using\nsimulated and real datasets validate our theoretical findings. Data\nsegmentation is used to handle large data sets.\n",
        "published": "2016-05-11T03:10:20Z",
        "pdf_link": "http://arxiv.org/pdf/1605.03267v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.04262v1",
        "title": "ABtree: An Algorithm for Subgroup-Based Treatment Assignment",
        "summary": "  Given two possible treatments, there may exist subgroups who benefit greater\nfrom one treatment than the other. This problem is relevant to the field of\nmarketing, where treatments may correspond to different ways of selling a\nproduct. It is similarly relevant to the field of public policy, where\ntreatments may correspond to specific government programs. And finally,\npersonalized medicine is a field wholly devoted to understanding which\nsubgroups of individuals will benefit from particular medical treatments. We\npresent a computationally fast tree-based method, ABtree, for treatment effect\ndifferentiation. Unlike other methods, ABtree specifically produces decision\nrules for optimal treatment assignment on a per-individual basis. The treatment\nchoices are selected for maximizing the overall occurrence of a desired binary\noutcome, conditional on a set of covariates. In this poster, we present the\nmethodology on tree growth and pruning, and show performance results when\napplied to simulated data as well as real data.\n",
        "published": "2016-05-13T17:27:55Z",
        "pdf_link": "http://arxiv.org/pdf/1605.04262v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.04435v1",
        "title": "Proceedings of the 5th Workshop on Machine Learning and Interpretation\n  in Neuroimaging (MLINI) at NIPS 2015",
        "summary": "  This volume is a collection of contributions from the 5th Workshop on Machine\nLearning and Interpretation in Neuroimaging (MLINI) at the Neural Information\nProcessing Systems (NIPS 2015) conference. Modern multivariate statistical\nmethods developed in the rapidly growing field of machine learning are being\nincreasingly applied to various problems in neuroimaging, from cognitive state\ndetection to clinical diagnosis and prognosis. Multivariate pattern analysis\nmethods are designed to examine complex relationships between high-dimensional\nsignals, such as brain images, and outcomes of interest, such as the category\nof a stimulus, a type of a mental state of a subject, or a specific mental\ndisorder. Such techniques are in contrast with the traditional mass-univariate\napproaches that dominated neuroimaging in the past and treated each individual\nimaging measurement in isolation.\n  We believe that machine learning has a prominent role in shaping how\nquestions in neuroscience are framed, and that the machine-learning mind set is\nnow entering modern psychology and behavioral studies. It is also equally\nimportant that practical applications in these fields motivate a rapidly\nevolving line or research in the machine learning community. In parallel, there\nis an intense interest in learning more about brain function in the context of\nrich naturalistic environments and scenes. Efforts to go beyond highly specific\nparadigms that pinpoint a single function, towards schemes for measuring the\ninteraction with natural and more varied scene are made. The goal of the\nworkshop is to pinpoint the most pressing issues and common challenges across\nthe neuroscience, neuroimaging, psychology and machine learning fields, and to\nsketch future directions and open questions in the light of novel methodology.\n",
        "published": "2016-05-14T16:37:54Z",
        "pdf_link": "http://arxiv.org/pdf/1605.04435v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.04955v2",
        "title": "Probing the Geometry of Data with Diffusion Fréchet Functions",
        "summary": "  Many complex ecosystems, such as those formed by multiple microbial taxa,\ninvolve intricate interactions amongst various sub-communities. The most basic\nrelationships are frequently modeled as co-occurrence networks in which the\nnodes represent the various players in the community and the weighted edges\nencode levels of interaction. In this setting, the composition of a community\nmay be viewed as a probability distribution on the nodes of the network. This\npaper develops methods for modeling the organization of such data, as well as\ntheir Euclidean counterparts, across spatial scales. Using the notion of\ndiffusion distance, we introduce diffusion Frechet functions and diffusion\nFrechet vectors associated with probability distributions on Euclidean space\nand the vertex set of a weighted network, respectively. We prove that these\nfunctional statistics are stable with respect to the Wasserstein distance\nbetween probability measures, thus yielding robust descriptors of their shapes.\nWe apply the methodology to investigate bacterial communities in the human gut,\nseeking to characterize divergence from intestinal homeostasis in patients with\nClostridium difficile infection (CDI) and the effects of fecal microbiota\ntransplantation, a treatment used in CDI patients that has proven to be\nsignificantly more effective than traditional treatment with antibiotics. The\nproposed method proves useful in deriving a biomarker that might help elucidate\nthe mechanisms that drive these processes.\n",
        "published": "2016-05-16T21:11:08Z",
        "pdf_link": "http://arxiv.org/pdf/1605.04955v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.05349v1",
        "title": "Orthogonal symmetric non-negative matrix factorization under the\n  stochastic block model",
        "summary": "  We present a method based on the orthogonal symmetric non-negative matrix\ntri-factorization of the normalized Laplacian matrix for community detection in\ncomplex networks. While the exact factorization of a given order may not exist\nand is NP hard to compute, we obtain an approximate factorization by solving an\noptimization problem. We establish the connection of the factors obtained\nthrough the factorization to a non-negative basis of an invariant subspace of\nthe estimated matrix, drawing parallel with the spectral clustering. Using such\nfactorization for clustering in networks is motivated by analyzing a\nblock-diagonal Laplacian matrix with the blocks representing the connected\ncomponents of a graph. The method is shown to be consistent for community\ndetection in graphs generated from the stochastic block model and the degree\ncorrected stochastic block model. Simulation results and real data analysis\nshow the effectiveness of these methods under a wide variety of situations,\nincluding sparse and highly heterogeneous graphs where the usual spectral\nclustering is known to fail. Our method also performs better than the state of\nthe art in popular benchmark network datasets, e.g., the political web blogs\nand the karate club data.\n",
        "published": "2016-05-17T20:22:12Z",
        "pdf_link": "http://arxiv.org/pdf/1605.05349v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.05697v1",
        "title": "Online Algorithms For Parameter Mean And Variance Estimation In Dynamic\n  Regression Models",
        "summary": "  We study the problem of estimating the parameters of a regression model from\na set of observations, each consisting of a response and a predictor. The\nresponse is assumed to be related to the predictor via a regression model of\nunknown parameters. Often, in such models the parameters to be estimated are\nassumed to be constant. Here we consider the more general scenario where the\nparameters are allowed to evolve over time, a more natural assumption for many\napplications. We model these dynamics via a linear update equation with\nadditive noise that is often used in a wide range of engineering applications,\nparticularly in the well-known and widely used Kalman filter (where the system\nstate it seeks to estimate maps to the parameter values here). We derive an\napproximate algorithm to estimate both the mean and the variance of the\nparameter estimates in an online fashion for a generic regression model. This\nalgorithm turns out to be equivalent to the extended Kalman filter. We\nspecialize our algorithm to the multivariate exponential family distribution to\nobtain a generalization of the generalized linear model (GLM). Because the\ncommon regression models encountered in practice such as logistic, exponential\nand multinomial all have observations modeled through an exponential family\ndistribution, our results are used to easily obtain algorithms for online mean\nand variance parameter estimation for all these regression models in the\ncontext of time-dependent parameters. Lastly, we propose to use these\nalgorithms in the contextual multi-armed bandit scenario, where so far model\nparameters are assumed static and observations univariate and Gaussian or\nBernoulli. Both of these restrictions can be relaxed using the algorithms\ndescribed here, which we combine with Thompson sampling to show the resulting\nperformance on a simulation.\n",
        "published": "2016-05-18T19:00:39Z",
        "pdf_link": "http://arxiv.org/pdf/1605.05697v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.05860v3",
        "title": "False Discovery Rate Control and Statistical Quality Assessment of\n  Annotators in Crowdsourced Ranking",
        "summary": "  With the rapid growth of crowdsourcing platforms it has become easy and\nrelatively inexpensive to collect a dataset labeled by multiple annotators in a\nshort time. However due to the lack of control over the quality of the\nannotators, some abnormal annotators may be affected by position bias which can\npotentially degrade the quality of the final consensus labels. In this paper we\nintroduce a statistical framework to model and detect annotator's position bias\nin order to control the false discovery rate (FDR) without a prior knowledge on\nthe amount of biased annotators - the expected fraction of false discoveries\namong all discoveries being not too high, in order to assure that most of the\ndiscoveries are indeed true and replicable. The key technical development\nrelies on some new knockoff filters adapted to our problem and new algorithms\nbased on the Inverse Scale Space dynamics whose discretization is potentially\nsuitable for large scale crowdsourcing data analysis. Our studies are supported\nby experiments with both simulated examples and real-world data. The proposed\nframework provides us a useful tool for quantitatively studying annotator's\nabnormal behavior in crowdsourcing data arising from machine learning,\nsociology, computer vision, multimedia, etc.\n",
        "published": "2016-05-19T09:14:39Z",
        "pdf_link": "http://arxiv.org/pdf/1605.05860v3"
    },
    {
        "id": "http://arxiv.org/abs/1605.05918v2",
        "title": "Bayesian Variable Selection for Globally Sparse Probabilistic PCA",
        "summary": "  Sparse versions of principal component analysis (PCA) have imposed themselves\nas simple, yet powerful ways of selecting relevant features of high-dimensional\ndata in an unsupervised manner. However, when several sparse principal\ncomponents are computed, the interpretation of the selected variables is\ndifficult since each axis has its own sparsity pattern and has to be\ninterpreted separately. To overcome this drawback, we propose a Bayesian\nprocedure called globally sparse probabilistic PCA (GSPPCA) that allows to\nobtain several sparse components with the same sparsity pattern. This allows\nthe practitioner to identify the original variables which are relevant to\ndescribe the data. To this end, using Roweis' probabilistic interpretation of\nPCA and a Gaussian prior on the loading matrix, we provide the first exact\ncomputation of the marginal likelihood of a Bayesian PCA model. To avoid the\ndrawbacks of discrete model selection, a simple relaxation of this framework is\npresented. It allows to find a path of models using a variational\nexpectation-maximization algorithm. The exact marginal likelihood is then\nmaximized over this path. This approach is illustrated on real and synthetic\ndata sets. In particular, using unlabeled microarray data, GSPPCA infers much\nmore relevant gene subsets than traditional sparse PCA algorithms.\n",
        "published": "2016-05-19T12:34:34Z",
        "pdf_link": "http://arxiv.org/pdf/1605.05918v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.06197v3",
        "title": "Stick-Breaking Variational Autoencoders",
        "summary": "  We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE's.\n",
        "published": "2016-05-20T02:20:13Z",
        "pdf_link": "http://arxiv.org/pdf/1605.06197v3"
    },
    {
        "id": "http://arxiv.org/abs/1605.06359v3",
        "title": "Learning to Discover Sparse Graphical Models",
        "summary": "  We consider structure discovery of undirected graphical models from\nobservational data. Inferring likely structures from few examples is a complex\ntask often requiring the formulation of priors and sophisticated inference\nprocedures. Popular methods rely on estimating a penalized maximum likelihood\nof the precision matrix. However, in these approaches structure recovery is an\nindirect consequence of the data-fit term, the penalty can be difficult to\nadapt for domain-specific knowledge, and the inference is computationally\ndemanding. By contrast, it may be easier to generate training samples of data\nthat arise from graphs with the desired structure properties. We propose here\nto leverage this latter source of information as training data to learn a\nfunction, parametrized by a neural network that maps empirical covariance\nmatrices to estimated graph structures. Learning this function brings two\nbenefits: it implicitly models the desired structure or sparsity properties to\nform suitable priors, and it can be tailored to the specific problem of edge\nstructure discovery, rather than maximizing data likelihood. Applying this\nframework, we find our learnable graph-discovery method trained on synthetic\ndata generalizes well: identifying relevant edges in both synthetic and real\ndata, completely unknown at training time. We find that on genetics, brain\nimaging, and simulation data we obtain performance generally superior to\nanalytical methods.\n",
        "published": "2016-05-20T13:58:21Z",
        "pdf_link": "http://arxiv.org/pdf/1605.06359v3"
    },
    {
        "id": "http://arxiv.org/abs/1605.07254v2",
        "title": "Convergence guarantees for kernel-based quadrature rules in misspecified\n  settings",
        "summary": "  Kernel-based quadrature rules are becoming important in machine learning and\nstatistics, as they achieve super-$\\sqrt{n}$ convergence rates in numerical\nintegration, and thus provide alternatives to Monte Carlo integration in\nchallenging settings where integrands are expensive to evaluate or where\nintegrands are high dimensional. These rules are based on the assumption that\nthe integrand has a certain degree of smoothness, which is expressed as that\nthe integrand belongs to a certain reproducing kernel Hilbert space (RKHS).\nHowever, this assumption can be violated in practice (e.g., when the integrand\nis a black box function), and no general theory has been established for the\nconvergence of kernel quadratures in such misspecified settings. Our\ncontribution is in proving that kernel quadratures can be consistent even when\nthe integrand does not belong to the assumed RKHS, i.e., when the integrand is\nless smooth than assumed. Specifically, we derive convergence rates that depend\non the (unknown) lesser smoothness of the integrand, where the degree of\nsmoothness is expressed via powers of RKHSs or via Sobolev spaces.\n",
        "published": "2016-05-24T01:41:25Z",
        "pdf_link": "http://arxiv.org/pdf/1605.07254v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.07332v2",
        "title": "Relevant sparse codes with variational information bottleneck",
        "summary": "  In many applications, it is desirable to extract only the relevant aspects of\ndata. A principled way to do this is the information bottleneck (IB) method,\nwhere one seeks a code that maximizes information about a 'relevance' variable,\nY, while constraining the information encoded about the original data, X.\nUnfortunately however, the IB method is computationally demanding when data are\nhigh-dimensional and/or non-gaussian. Here we propose an approximate\nvariational scheme for maximizing a lower bound on the IB objective, analogous\nto variational EM. Using this method, we derive an IB algorithm to recover\nfeatures that are both relevant and sparse. Finally, we demonstrate how\nkernelized versions of the algorithm can be used to address a broad range of\nproblems with non-linear relation between X and Y.\n",
        "published": "2016-05-24T08:16:54Z",
        "pdf_link": "http://arxiv.org/pdf/1605.07332v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.07596v3",
        "title": "Local Minimax Complexity of Stochastic Convex Optimization",
        "summary": "  We extend the traditional worst-case, minimax analysis of stochastic convex\noptimization by introducing a localized form of minimax complexity for\nindividual functions. Our main result gives function-specific lower and upper\nbounds on the number of stochastic subgradient evaluations needed to optimize\neither the function or its \"hardest local alternative\" to a given numerical\nprecision. The bounds are expressed in terms of a localized and computational\nanalogue of the modulus of continuity that is central to statistical minimax\nanalysis. We show how the computational modulus of continuity can be explicitly\ncalculated in concrete cases, and relates to the curvature of the function at\nthe optimum. We also prove a superefficiency result that demonstrates it is a\nmeaningful benchmark, acting as a computational analogue of the Fisher\ninformation in statistical estimation. The nature and practical implications of\nthe results are demonstrated in simulations.\n",
        "published": "2016-05-24T19:37:41Z",
        "pdf_link": "http://arxiv.org/pdf/1605.07596v3"
    },
    {
        "id": "http://arxiv.org/abs/1605.07870v1",
        "title": "Simultaneous Sparse Dictionary Learning and Pruning",
        "summary": "  Dictionary learning is a cutting-edge area in imaging processing, that has\nrecently led to state-of-the-art results in many signal processing tasks. The\nidea is to conduct a linear decomposition of a signal using a few atoms of a\nlearned and usually over-completed dictionary instead of a pre-defined basis.\nDetermining a proper size of the to-be-learned dictionary is crucial for both\nprecision and efficiency of the process, while most of the existing dictionary\nlearning algorithms choose the size quite arbitrarily. In this paper, a novel\nregularization method called the Grouped Smoothly Clipped Absolute Deviation\n(GSCAD) is employed for learning the dictionary. The proposed method can\nsimultaneously learn a sparse dictionary and select the appropriate dictionary\nsize. Efficient algorithm is designed based on the alternative direction method\nof multipliers (ADMM) which decomposes the joint non-convex problem with the\nnon-convex penalty into two convex optimization problems. Several examples are\npresented for image denoising and the experimental results are compared with\nother state-of-the-art approaches.\n",
        "published": "2016-05-25T13:24:39Z",
        "pdf_link": "http://arxiv.org/pdf/1605.07870v1"
    },
    {
        "id": "http://arxiv.org/abs/1605.07906v2",
        "title": "How priors of initial hyperparameters affect Gaussian process regression\n  models",
        "summary": "  The hyperparameters in Gaussian process regression (GPR) model with a\nspecified kernel are often estimated from the data via the maximum marginal\nlikelihood. Due to the non-convexity of marginal likelihood with respect to the\nhyperparameters, the optimization may not converge to the global maxima. A\ncommon approach to tackle this issue is to use multiple starting points\nrandomly selected from a specific prior distribution. As a result the choice of\nprior distribution may play a vital role in the predictability of this\napproach. However, there exists little research in the literature to study the\nimpact of the prior distributions on the hyperparameter estimation and the\nperformance of GPR. In this paper, we provide the first empirical study on this\nproblem using simulated and real data experiments. We consider different types\nof priors for the initial values of hyperparameters for some commonly used\nkernels and investigate the influence of the priors on the predictability of\nGPR models. The results reveal that, once a kernel is chosen, different priors\nfor the initial hyperparameters have no significant impact on the performance\nof GPR prediction, despite that the estimates of the hyperparameters are very\ndifferent to the true values in some cases.\n",
        "published": "2016-05-25T14:45:26Z",
        "pdf_link": "http://arxiv.org/pdf/1605.07906v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.08299v2",
        "title": "A General Family of Trimmed Estimators for Robust High-dimensional Data\n  Analysis",
        "summary": "  We consider the problem of robustifying high-dimensional structured\nestimation. Robust techniques are key in real-world applications which often\ninvolve outliers and data corruption. We focus on trimmed versions of\nstructurally regularized M-estimators in the high-dimensional setting,\nincluding the popular Least Trimmed Squares estimator, as well as analogous\nestimators for generalized linear models and graphical models, using possibly\nnon-convex loss functions. We present a general analysis of their statistical\nconvergence rates and consistency, and then take a closer look at the trimmed\nversions of the Lasso and Graphical Lasso estimators as special cases. On the\noptimization side, we show how to extend algorithms for M-estimators to fit\ntrimmed variants and provide guarantees on their numerical convergence. The\ngenerality and competitive performance of high-dimensional trimmed estimators\nare illustrated numerically on both simulated and real-world genomics data.\n",
        "published": "2016-05-26T14:26:20Z",
        "pdf_link": "http://arxiv.org/pdf/1605.08299v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.08301v2",
        "title": "Predictive Coarse-Graining",
        "summary": "  We propose a data-driven, coarse-graining formulation in the context of\nequilibrium statistical mechanics. In contrast to existing techniques which are\nbased on a fine-to-coarse map, we adopt the opposite strategy by prescribing a\nprobabilistic coarse-to-fine map. This corresponds to a directed probabilistic\nmodel where the coarse variables play the role of latent generators of the fine\nscale (all-atom) data. From an information-theoretic perspective, the framework\nproposed provides an improvement upon the relative entropy method and is\ncapable of quantifying the uncertainty due to the information loss that\nunavoidably takes place during the CG process. Furthermore, it can be readily\nextended to a fully Bayesian model where various sources of uncertainties are\nreflected in the posterior of the model parameters. The latter can be used to\nproduce not only point estimates of fine-scale reconstructions or macroscopic\nobservables, but more importantly, predictive posterior distributions on these\nquantities. Predictive posterior distributions reflect the confidence of the\nmodel as a function of the amount of data and the level of coarse-graining. The\nissues of model complexity and model selection are seamlessly addressed by\nemploying a hierarchical prior that favors the discovery of sparse solutions,\nrevealing the most prominent features in the coarse-grained model. A flexible\nand parallelizable Monte Carlo - Expectation-Maximization (MC-EM) scheme is\nproposed for carrying out inference and learning tasks. A comparative\nassessment of the proposed methodology is presented for a lattice spin system\nand the SPC/E water model.\n",
        "published": "2016-05-26T14:37:12Z",
        "pdf_link": "http://arxiv.org/pdf/1605.08301v2"
    },
    {
        "id": "http://arxiv.org/abs/1605.09459v4",
        "title": "Scalable and Flexible Multiview MAX-VAR Canonical Correlation Analysis",
        "summary": "  Generalized canonical correlation analysis (GCCA) aims at finding latent\nlow-dimensional common structure from multiple views (feature vectors in\ndifferent domains) of the same entities. Unlike principal component analysis\n(PCA) that handles a single view, (G)CCA is able to integrate information from\ndifferent feature spaces. Here we focus on MAX-VAR GCCA, a popular formulation\nwhich has recently gained renewed interest in multilingual processing and\nspeech modeling. The classic MAX-VAR GCCA problem can be solved optimally via\neigen-decomposition of a matrix that compounds the (whitened) correlation\nmatrices of the views; but this solution has serious scalability issues, and is\nnot directly amenable to incorporating pertinent structural constraints such as\nnon-negativity and sparsity on the canonical components. We posit regularized\nMAX-VAR GCCA as a non-convex optimization problem and propose an alternating\noptimization (AO)-based algorithm to handle it. Our algorithm alternates\nbetween {\\em inexact} solutions of a regularized least squares subproblem and a\nmanifold-constrained non-convex subproblem, thereby achieving substantial\nmemory and computational savings. An important benefit of our design is that it\ncan easily handle structure-promoting regularization. We show that the\nalgorithm globally converges to a critical point at a sublinear rate, and\napproaches a global optimal solution at a linear rate when no regularization is\nconsidered. Judiciously designed simulations and large-scale word embedding\ntasks are employed to showcase the effectiveness of the proposed algorithm.\n",
        "published": "2016-05-31T01:01:52Z",
        "pdf_link": "http://arxiv.org/pdf/1605.09459v4"
    },
    {
        "id": "http://arxiv.org/abs/1605.09499v9",
        "title": "Extreme Stochastic Variational Inference: Distributed and Asynchronous",
        "summary": "  Stochastic variational inference (SVI), the state-of-the-art algorithm for\nscaling variational inference to large-datasets, is inherently serial.\nMoreover, it requires the parameters to fit in the memory of a single\nprocessor; this is problematic when the number of parameters is in billions. In\nthis paper, we propose extreme stochastic variational inference (ESVI), an\nasynchronous and lock-free algorithm to perform variational inference for\nmixture models on massive real world datasets. ESVI overcomes the limitations\nof SVI by requiring that each processor only access a subset of the data and a\nsubset of the parameters, thus providing data and model parallelism\nsimultaneously. We demonstrate the effectiveness of ESVI by running Latent\nDirichlet Allocation (LDA) on UMBC-3B, a dataset that has a vocabulary of 3\nmillion and a token size of 3 billion. In our experiments, we found that ESVI\nnot only outperforms VI and SVI in wallclock-time, but also achieves a better\nquality solution. In addition, we propose a strategy to speed up computation\nand save memory when fitting large number of topics.\n",
        "published": "2016-05-31T05:10:51Z",
        "pdf_link": "http://arxiv.org/pdf/1605.09499v9"
    },
    {
        "id": "http://arxiv.org/abs/1605.09658v6",
        "title": "Continuation of Nesterov's Smoothing for Regression with Structured\n  Sparsity in High-Dimensional Neuroimaging",
        "summary": "  Predictive models can be used on high-dimensional brain images for diagnosis\nof a clinical condition. Spatial regularization through structured sparsity\noffers new perspectives in this context and reduces the risk of overfitting the\nmodel while providing interpretable neuroimaging signatures by forcing the\nsolution to adhere to domain-specific constraints. Total Variation (TV)\nenforces spatial smoothness of the solution while segmenting predictive regions\nfrom the background. We consider the problem of minimizing the sum of a smooth\nconvex loss, a non-smooth convex penalty (whose proximal operator is known) and\na wide range of possible complex, non-smooth convex structured penalties such\nas TV or overlapping group Lasso. Existing solvers are either limited in the\nfunctions they can minimize or in their practical capacity to scale to\nhigh-dimensional imaging data. Nesterov's smoothing technique can be used to\nminimize a large number of non-smooth convex structured penalties but\nreasonable precision requires a small smoothing parameter, which slows down the\nconvergence speed. To benefit from the versatility of Nesterov's smoothing\ntechnique, we propose a first order continuation algorithm, CONESTA, which\nautomatically generates a sequence of decreasing smoothing parameters. The\ngenerated sequence maintains the optimal convergence speed towards any globally\ndesired precision. Our main contributions are: To propose an expression of the\nduality gap to probe the current distance to the global optimum in order to\nadapt the smoothing parameter and the convergence speed. We provide a\nconvergence rate, which is an improvement over classical proximal gradient\nsmoothing methods. We demonstrate on both simulated and high-dimensional\nstructural neuroimaging data that CONESTA significantly outperforms many\nstate-of-the-art solvers in regard to convergence speed and precision.\n",
        "published": "2016-05-31T15:09:13Z",
        "pdf_link": "http://arxiv.org/pdf/1605.09658v6"
    },
    {
        "id": "http://arxiv.org/abs/1606.00113v1",
        "title": "Identifying Outliers using Influence Function of Multiple Kernel\n  Canonical Correlation Analysis",
        "summary": "  Imaging genetic research has essentially focused on discovering unique and\nco-association effects, but typically ignoring to identify outliers or atypical\nobjects in genetic as well as non-genetics variables. Identifying significant\noutliers is an essential and challenging issue for imaging genetics and\nmultiple sources data analysis. Therefore, we need to examine for transcription\nerrors of identified outliers. First, we address the influence function (IF) of\nkernel mean element, kernel covariance operator, kernel cross-covariance\noperator, kernel canonical correlation analysis (kernel CCA) and multiple\nkernel CCA. Second, we propose an IF of multiple kernel CCA, which can be\napplied for more than two datasets. Third, we propose a visualization method to\ndetect influential observations of multiple sources of data based on the IF of\nkernel CCA and multiple kernel CCA. Finally, the proposed methods are capable\nof analyzing outliers of subjects usually found in biomedical applications, in\nwhich the number of dimension is large. To examine the outliers, we use the\nstem-and-leaf display. Experiments on both synthesized and imaging genetics\ndata (e.g., SNP, fMRI, and DNA methylation) demonstrate that the proposed\nvisualization can be applied effectively.\n",
        "published": "2016-06-01T04:45:21Z",
        "pdf_link": "http://arxiv.org/pdf/1606.00113v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.00118v1",
        "title": "Gene-Gene association for Imaging Genetics Data using Robust Kernel\n  Canonical Correlation Analysis",
        "summary": "  In genome-wide interaction studies, to detect gene-gene interactions, most\nmethods are divided into two folds: single nucleotide polymorphisms (SNP) based\nand gene-based methods. Basically, the methods based on the gene are more\neffective than the methods based on a single SNP. Recent years, while the\nkernel canonical correlation analysis (Classical kernel CCA) based U statistic\n(KCCU) has proposed to detect the nonlinear relationship between genes. To\nestimate the variance in KCCU, they have used resampling based methods which\nare highly computationally intensive. In addition, classical kernel CCA is not\nrobust to contaminated data. We, therefore, first discuss robust kernel mean\nelement, the robust kernel covariance, and cross-covariance operators. Second,\nwe propose a method based on influence function to estimate the variance of the\nKCCU. Third, we propose a nonparametric robust KCCU method based on robust\nkernel CCA, which is designed for contaminated data and less sensitive to noise\nthan classical kernel CCA. Finally, we investigate the proposed methods to\nsynthesized data and imaging genetic data set. Based on gene ontology and\npathway analysis, the synthesized and genetics analysis demonstrate that the\nproposed robust method shows the superior performance of the state-of-the-art\nmethods.\n",
        "published": "2016-06-01T05:14:03Z",
        "pdf_link": "http://arxiv.org/pdf/1606.00118v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.00813v1",
        "title": "Generalized Root Models: Beyond Pairwise Graphical Models for Univariate\n  Exponential Families",
        "summary": "  We present a novel k-way high-dimensional graphical model called the\nGeneralized Root Model (GRM) that explicitly models dependencies between\nvariable sets of size k > 2---where k = 2 is the standard pairwise graphical\nmodel. This model is based on taking the k-th root of the original sufficient\nstatistics of any univariate exponential family with positive sufficient\nstatistics, including the Poisson and exponential distributions. As in the\nrecent work with square root graphical (SQR) models [Inouye et al.\n2016]---which was restricted to pairwise dependencies---we give the conditions\nof the parameters that are needed for normalization using the radial\nconditionals similar to the pairwise case [Inouye et al. 2016]. In particular,\nwe show that the Poisson GRM has no restrictions on the parameters and the\nexponential GRM only has a restriction akin to negative definiteness. We\ndevelop a simple but general learning algorithm based on L1-regularized\nnode-wise regressions. We also present a general way of numerically\napproximating the log partition function and associated derivatives of the GRM\nunivariate node conditionals---in contrast to [Inouye et al. 2016], which only\nprovided algorithm for estimating the exponential SQR. To illustrate GRM, we\nmodel word counts with a Poisson GRM and show the associated k-sized variable\nsets. We finish by discussing methods for reducing the parameter space in\nvarious situations.\n",
        "published": "2016-06-02T19:13:23Z",
        "pdf_link": "http://arxiv.org/pdf/1606.00813v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.00832v1",
        "title": "High Dimensional Multivariate Regression and Precision Matrix Estimation\n  via Nonconvex Optimization",
        "summary": "  We propose a nonconvex estimator for joint multivariate regression and\nprecision matrix estimation in the high dimensional regime, under sparsity\nconstraints. A gradient descent algorithm with hard thresholding is developed\nto solve the nonconvex estimator, and it attains a linear rate of convergence\nto the true regression coefficients and precision matrix simultaneously, up to\nthe statistical error. Compared with existing methods along this line of\nresearch, which have little theoretical guarantee, the proposed algorithm not\nonly is computationally much more efficient with provable convergence\nguarantee, but also attains the optimal finite sample statistical rate up to a\nlogarithmic factor. Thorough experiments on both synthetic and real datasets\nback up our theory.\n",
        "published": "2016-06-02T19:59:44Z",
        "pdf_link": "http://arxiv.org/pdf/1606.00832v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.00906v2",
        "title": "Nonlinear Statistical Learning with Truncated Gaussian Graphical Models",
        "summary": "  We introduce the truncated Gaussian graphical model (TGGM) as a novel\nframework for designing statistical models for nonlinear learning. A TGGM is a\nGaussian graphical model (GGM) with a subset of variables truncated to be\nnonnegative. The truncated variables are assumed latent and integrated out to\ninduce a marginal model. We show that the variables in the marginal model are\nnon-Gaussian distributed and their expected relations are nonlinear. We use\nexpectation-maximization to break the inference of the nonlinear model into a\nsequence of TGGM inference problems, each of which is efficiently solved by\nusing the properties and numerical methods of multivariate Gaussian\ndistributions. We use the TGGM to design models for nonlinear regression and\nclassification, with the performances of these models demonstrated on extensive\nbenchmark datasets and compared to state-of-the-art competing results.\n",
        "published": "2016-06-02T21:39:40Z",
        "pdf_link": "http://arxiv.org/pdf/1606.00906v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.01822v3",
        "title": "ROCS-Derived Features for Virtual Screening",
        "summary": "  Rapid overlay of chemical structures (ROCS) is a standard tool for the\ncalculation of 3D shape and chemical (\"color\") similarity. ROCS uses unweighted\nsums to combine many aspects of similarity, yielding parameter-free models for\nvirtual screening. In this report, we decompose the ROCS color force field into\n\"color components\" and \"color atom overlaps\", novel color similarity features\nthat can be weighted in a system-specific manner by machine learning\nalgorithms. In cross-validation experiments, these additional features\nsignificantly improve virtual screening performance (ROC AUC scores) relative\nto standard ROCS.\n",
        "published": "2016-06-06T16:48:28Z",
        "pdf_link": "http://arxiv.org/pdf/1606.01822v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.01869v3",
        "title": "On Robustness of Kernel Clustering",
        "summary": "  Clustering is one of the most important unsupervised problems in machine\nlearning and statistics. Among many existing algorithms, kernel k-means has\ndrawn much research attention due to its ability to find non-linear cluster\nboundaries and its inherent simplicity. There are two main approaches for\nkernel k-means: SVD of the kernel matrix and convex relaxations. Despite the\nattention kernel clustering has received both from theoretical and applied\nquarters, not much is known about robustness of the methods. In this paper we\nfirst introduce a semidefinite programming relaxation for the kernel clustering\nproblem, then prove that under a suitable model specification, both the K-SVD\nand SDP approaches are consistent in the limit, albeit SDP is strongly\nconsistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent,\ni.e. the fraction of misclassified nodes vanish.\n",
        "published": "2016-06-06T19:26:23Z",
        "pdf_link": "http://arxiv.org/pdf/1606.01869v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.02261v1",
        "title": "Reducing the error of Monte Carlo Algorithms by Learning Control\n  Variates",
        "summary": "  Monte Carlo (MC) sampling algorithms are an extremely widely-used technique\nto estimate expectations of functions f(x), especially in high dimensions.\nControl variates are a very powerful technique to reduce the error of such\nestimates, but in their conventional form rely on having an accurate\napproximation of f, a priori. Stacked Monte Carlo (StackMC) is a recently\nintroduced technique designed to overcome this limitation by fitting a control\nvariate to the data samples themselves. Done naively, forming a control variate\nto the data would result in overfitting, typically worsening the MC algorithm's\nperformance. StackMC uses in-sample / out-sample techniques to remove this\noverfitting. Crucially, it is a post-processing technique, requiring no\nadditional samples, and can be applied to data generated by any MC estimator.\nOur preliminary experiments demonstrated that StackMC improved the estimates of\nexpectations when it was used to post-process samples produces by a \"simple\nsampling\" MC estimator. Here we substantially extend this earlier work. We\nprovide an in-depth analysis of the StackMC algorithm, which we use to\nconstruct an improved version of the original algorithm, with lower estimation\nerror. We then perform experiments of StackMC on several additional kinds of MC\nestimators, demonstrating improved performance when the samples are generated\nvia importance sampling, Latin-hypercube sampling and quasi-Monte Carlo\nsampling. We also show how to extend StackMC to combine multiple fitting\nfunctions, and how to apply it to discrete input spaces x.\n",
        "published": "2016-06-07T18:52:38Z",
        "pdf_link": "http://arxiv.org/pdf/1606.02261v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.02321v1",
        "title": "Better Conditional Density Estimation for Neural Networks",
        "summary": "  The vast majority of the neural network literature focuses on predicting\npoint values for a given set of response variables, conditioned on a feature\nvector. In many cases we need to model the full joint conditional distribution\nover the response variables rather than simply making point predictions. In\nthis paper, we present two novel approaches to such conditional density\nestimation (CDE): Multiscale Nets (MSNs) and CDE Trend Filtering. Multiscale\nnets transform the CDE regression task into a hierarchical classification task\nby decomposing the density into a series of half-spaces and learning boolean\nprobabilities of each split. CDE Trend Filtering applies a k-th order graph\ntrend filtering penalty to the unnormalized logits of a multinomial classifier\nnetwork, with each edge in the graph corresponding to a neighboring point on a\ndiscretized version of the density. We compare both methods against plain\nmultinomial classifier networks and mixture density networks (MDNs) on a\nsimulated dataset and three real-world datasets. The results suggest the two\nmethods are complementary: MSNs work well in a high-data-per-feature regime and\nCDE-TF is well suited for few-samples-per-feature scenarios where overfitting\nis a primary concern.\n",
        "published": "2016-06-07T20:19:59Z",
        "pdf_link": "http://arxiv.org/pdf/1606.02321v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.02518v3",
        "title": "A Locally Adaptive Normal Distribution",
        "summary": "  The multivariate normal density is a monotonic function of the distance to\nthe mean, and its ellipsoidal shape is due to the underlying Euclidean metric.\nWe suggest to replace this metric with a locally adaptive, smoothly changing\n(Riemannian) metric that favors regions of high local density. The resulting\nlocally adaptive normal distribution (LAND) is a generalization of the normal\ndistribution to the \"manifold\" setting, where data is assumed to lie near a\npotentially low-dimensional manifold embedded in $\\mathbb{R}^D$. The LAND is\nparametric, depending only on a mean and a covariance, and is the maximum\nentropy distribution under the given metric. The underlying metric is, however,\nnon-parametric. We develop a maximum likelihood algorithm to infer the\ndistribution parameters that relies on a combination of gradient descent and\nMonte Carlo integration. We further extend the LAND to mixture models, and\nprovide the corresponding EM algorithm. We demonstrate the efficiency of the\nLAND to fit non-trivial probability distributions over both synthetic data, and\nEEG measurements of human sleep.\n",
        "published": "2016-06-08T11:49:08Z",
        "pdf_link": "http://arxiv.org/pdf/1606.02518v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.03865v3",
        "title": "Prediction performance after learning in Gaussian process regression",
        "summary": "  This paper considers the quantification of the prediction performance in\nGaussian process regression. The standard approach is to base the prediction\nerror bars on the theoretical predictive variance, which is a lower bound on\nthe mean square-error (MSE). This approach, however, does not take into account\nthat the statistical model is learned from the data. We show that this omission\nleads to a systematic underestimation of the prediction errors. Starting from a\ngeneralization of the Cram\\'er-Rao bound, we derive a more accurate MSE bound\nwhich provides a measure of uncertainty for prediction of Gaussian processes.\nThe improved bound is easily computed and we illustrate it using synthetic and\nreal data examples. of uncertainty for prediction of Gaussian processes and\nillustrate it using synthetic and real data examples.\n",
        "published": "2016-06-13T09:16:25Z",
        "pdf_link": "http://arxiv.org/pdf/1606.03865v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.04366v3",
        "title": "Recursive nonlinear-system identification using latent variables",
        "summary": "  In this paper we develop a method for learning nonlinear systems with\nmultiple outputs and inputs. We begin by modelling the errors of a nominal\npredictor of the system using a latent variable framework. Then using the\nmaximum likelihood principle we derive a criterion for learning the model. The\nresulting optimization problem is tackled using a majorization-minimization\napproach. Finally, we develop a convex majorization technique and show that it\nenables a recursive identification method. The method learns parsimonious\npredictive models and is tested on both synthetic and real nonlinear systems.\n",
        "published": "2016-06-14T13:46:21Z",
        "pdf_link": "http://arxiv.org/pdf/1606.04366v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.04789v2",
        "title": "Network Maximal Correlation",
        "summary": "  We introduce Network Maximal Correlation (NMC) as a multivariate measure of\nnonlinear association among random variables. NMC is defined via an\noptimization that infers transformations of variables by maximizing aggregate\ninner products between transformed variables. For finite discrete and jointly\nGaussian random variables, we characterize a solution of the NMC optimization\nusing basis expansion of functions over appropriate basis functions. For finite\ndiscrete variables, we propose an algorithm based on alternating conditional\nexpectation to determine NMC. Moreover we propose a distributed algorithm to\ncompute an approximation of NMC for large and dense graphs using graph\npartitioning. For finite discrete variables, we show that the probability of\ndiscrepancy greater than any given level between NMC and NMC computed using\nempirical distributions decays exponentially fast as the sample size grows. For\njointly Gaussian variables, we show that under some conditions the NMC\noptimization is an instance of the Max-Cut problem. We then illustrate an\napplication of NMC in inference of graphical model for bijective functions of\njointly Gaussian variables. Finally, we show NMC's utility in a data\napplication of learning nonlinear dependencies among genes in a cancer dataset.\n",
        "published": "2016-06-15T14:45:16Z",
        "pdf_link": "http://arxiv.org/pdf/1606.04789v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.04820v2",
        "title": "Understanding Probabilistic Sparse Gaussian Process Approximations",
        "summary": "  Good sparse approximations are essential for practical inference in Gaussian\nProcesses as the computational cost of exact methods is prohibitive for large\ndatasets. The Fully Independent Training Conditional (FITC) and the Variational\nFree Energy (VFE) approximations are two recent popular methods. Despite\nsuperficial similarities, these approximations have surprisingly different\ntheoretical properties and behave differently in practice. We thoroughly\ninvestigate the two methods for regression both analytically and through\nillustrative examples, and draw conclusions to guide practical application.\n",
        "published": "2016-06-15T15:33:27Z",
        "pdf_link": "http://arxiv.org/pdf/1606.04820v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.05201v2",
        "title": "Assessing and tuning brain decoders: cross-validation, caveats, and\n  guidelines",
        "summary": "  Decoding, ie prediction from brain images or signals, calls for empirical\nevaluation of its predictive power. Such evaluation is achieved via\ncross-validation, a method also used to tune decoders' hyper-parameters. This\npaper is a review on cross-validation procedures for decoding in neuroimaging.\nIt includes a didactic overview of the relevant theoretical considerations.\nPractical aspects are highlighted with an extensive empirical study of the\ncommon decoders in within-and across-subject predictions, on multiple datasets\n--anatomical and functional MRI and MEG-- and simulations. Theory and\nexperiments outline that the popular \" leave-one-out \" strategy leads to\nunstable and biased estimates, and a repeated random splits method should be\npreferred. Experiments outline the large error bars of cross-validation in\nneuroimaging settings: typical confidence intervals of 10%. Nested\ncross-validation can tune decoders' parameters while avoiding circularity bias.\nHowever we find that it can be more favorable to use sane defaults, in\nparticular for non-sparse decoders.\n",
        "published": "2016-06-16T14:29:28Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05201v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.05241v1",
        "title": "The Mondrian Kernel",
        "summary": "  We introduce the Mondrian kernel, a fast random feature approximation to the\nLaplace kernel. It is suitable for both batch and online learning, and admits a\nfast kernel-width-selection procedure as the random features can be re-used\nefficiently for all kernel widths. The features are constructed by sampling\ntrees via a Mondrian process [Roy and Teh, 2009], and we highlight the\nconnection to Mondrian forests [Lakshminarayanan et al., 2014], where trees are\nalso sampled via a Mondrian process, but fit independently. This link provides\na new insight into the relationship between kernel methods and random forests.\n",
        "published": "2016-06-16T16:14:32Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05241v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.05273v1",
        "title": "The Effect of Heteroscedasticity on Regression Trees",
        "summary": "  Regression trees are becoming increasingly popular as omnibus predicting\ntools and as the basis of numerous modern statistical learning ensembles. Part\nof their popularity is their ability to create a regression prediction without\never specifying a structure for the mean model. However, the method implicitly\nassumes homogeneous variance across the entire explanatory-variable space. It\nis unknown how the algorithm behaves when faced with heteroscedastic data. In\nthis study, we assess the performance of the most popular regression-tree\nalgorithm in a single-variable setting under a very simple step-function model\nfor heteroscedasticity. We use simulation to show that the locations of splits,\nand hence the ability to accurately predict means, are both adversely\ninfluenced by the change in variance. We identify the pruning algorithm as the\nmain concern, although the effects on the splitting algorithm may be meaningful\nin some applications.\n",
        "published": "2016-06-16T17:11:27Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05273v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.05390v1",
        "title": "Making Tree Ensembles Interpretable",
        "summary": "  Tree ensembles, such as random forest and boosted trees, are renowned for\ntheir high prediction performance, whereas their interpretability is critically\nlimited. In this paper, we propose a post processing method that improves the\nmodel interpretability of tree ensembles. After learning a complex tree\nensembles in a standard way, we approximate it by a simpler model that is\ninterpretable for human. To obtain the simpler model, we derive the EM\nalgorithm minimizing the KL divergence from the complex ensemble. A synthetic\nexperiment showed that a complicated tree ensemble was approximated reasonably\nas interpretable.\n",
        "published": "2016-06-17T00:33:03Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05390v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.05492v3",
        "title": "PSF : Introduction to R Package for Pattern Sequence Based Forecasting\n  Algorithm",
        "summary": "  This paper discusses about an R package that implements the Pattern Sequence\nbased Forecasting (PSF) algorithm, which was developed for univariate time\nseries forecasting. This algorithm has been successfully applied to many\ndifferent fields. The PSF algorithm consists of two major parts: clustering and\nprediction. The clustering part includes selection of the optimum number of\nclusters. It labels time series data with reference to such clusters. The\nprediction part includes functions like optimum window size selection for\nspecific patterns and prediction of future values with reference to past\npattern sequences. The PSF package consists of various functions to implement\nthe PSF algorithm. It also contains a function which automates all other\nfunctions to obtain optimized prediction results. The aim of this package is to\npromote the PSF algorithm and to ease its implementation with minimum efforts.\nThis paper describes all the functions in the PSF package with their syntax. It\nalso provides a simple example of usage. Finally, the usefulness of this\npackage is discussed by comparing it to auto.arima and ets, well-known time\nseries forecasting functions available on CRAN repository.\n",
        "published": "2016-06-17T12:00:21Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05492v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.05672v1",
        "title": "Interpretability in Linear Brain Decoding",
        "summary": "  Improving the interpretability of brain decoding approaches is of primary\ninterest in many neuroimaging studies. Despite extensive studies of this type,\nat present, there is no formal definition for interpretability of brain\ndecoding models. As a consequence, there is no quantitative measure for\nevaluating the interpretability of different brain decoding methods. In this\npaper, we present a simple definition for interpretability of linear brain\ndecoding models. Then, we propose to combine the interpretability and the\nperformance of the brain decoding into a new multi-objective criterion for\nmodel selection. Our preliminary results on the toy data show that optimizing\nthe hyper-parameters of the regularized linear classifier based on the proposed\ncriterion results in more informative linear models. The presented definition\nprovides the theoretical background for quantitative evaluation of\ninterpretability in linear brain decoding.\n",
        "published": "2016-06-17T20:34:04Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05672v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.05889v2",
        "title": "Tight Performance Bounds for Compressed Sensing With Conventional and\n  Group Sparsity",
        "summary": "  In this paper, we study the problem of recovering a group sparse vector from\na small number of linear measurements. In the past the common approach has been\nto use various \"group sparsity-inducing\" norms such as the Group LASSO norm for\nthis purpose. By using the theory of convex relaxations, we show that it is\nalso possible to use $\\ell_1$-norm minimization for group sparse recovery. We\nintroduce a new concept called group robust null space property (GRNSP), and\nshow that, under suitable conditions, a group version of the restricted\nisometry property (GRIP) implies the GRNSP, and thus leads to group sparse\nrecovery. When all groups are of equal size, our bounds are less conservative\nthan known bounds. Moreover, our results apply even to situations where where\nthe groups have different sizes. When specialized to conventional sparsity, our\nbounds reduce to one of the well-known \"best possible\" conditions for sparse\nrecovery. This relationship between GRNSP and GRIP is new even for conventional\nsparsity, and substantially streamlines the proofs of some known results. Using\nthis relationship, we derive bounds on the $\\ell_p$-norm of the residual error\nvector for all $p \\in [1,2]$, and not just when $p = 2$. When the measurement\nmatrix consists of random samples of a sub-Gaussian random variable, we present\nbounds on the number of measurements, which are less conservative than\ncurrently known bounds.\n",
        "published": "2016-06-19T17:01:31Z",
        "pdf_link": "http://arxiv.org/pdf/1606.05889v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.06959v2",
        "title": "Dealing with a large number of classes -- Likelihood, Discrimination or\n  Ranking?",
        "summary": "  We consider training probabilistic classifiers in the case of a large number\nof classes. The number of classes is assumed too large to perform exact\nnormalisation over all classes. To account for this we consider a simple\napproach that directly approximates the likelihood. We show that this simple\napproach works well on toy problems and is competitive with recently introduced\nalternative non-likelihood based approximations. Furthermore, we relate this\napproach to a simple ranking objective. This leads us to suggest a specific\nsetting for the optimal threshold in the ranking objective.\n",
        "published": "2016-06-22T14:10:47Z",
        "pdf_link": "http://arxiv.org/pdf/1606.06959v2"
    },
    {
        "id": "http://arxiv.org/abs/1606.07240v3",
        "title": "PAC-Bayesian Analysis for a two-step Hierarchical Multiview Learning\n  Approach",
        "summary": "  We study a two-level multiview learning with more than two views under the\nPAC-Bayesian framework. This approach, sometimes referred as late fusion,\nconsists in learning sequentially multiple view-specific classifiers at the\nfirst level, and then combining these view-specific classifiers at the second\nlevel. Our main theoretical result is a generalization bound on the risk of the\nmajority vote which exhibits a term of diversity in the predictions of the\nview-specific classifiers. From this result it comes out that controlling the\ntrade-off between diversity and accuracy is a key element for multiview\nlearning, which complements other results in multiview learning. Finally, we\nexperiment our principle on multiview datasets extracted from the Reuters\nRCV1/RCV2 collection.\n",
        "published": "2016-06-23T09:29:53Z",
        "pdf_link": "http://arxiv.org/pdf/1606.07240v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.07840v1",
        "title": "Modeling Group Dynamics Using Probabilistic Tensor Decompositions",
        "summary": "  We propose a probabilistic modeling framework for learning the dynamic\npatterns in the collective behaviors of social agents and developing profiles\nfor different behavioral groups, using data collected from multiple information\nsources. The proposed model is based on a hierarchical Bayesian process, in\nwhich each observation is a finite mixture of an set of latent groups and the\nmixture proportions (i.e., group probabilities) are drawn randomly. Each group\nis associated with some distributions over a finite set of outcomes. Moreover,\nas time evolves, the structure of these groups also changes; we model the\nchange in the group structure by a hidden Markov model (HMM) with a fixed\ntransition probability. We present an efficient inference method based on\ntensor decompositions and the expectation-maximization (EM) algorithm for\nparameter estimation.\n",
        "published": "2016-06-24T21:51:24Z",
        "pdf_link": "http://arxiv.org/pdf/1606.07840v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08105v1",
        "title": "The Dependent Random Measures with Independent Increments in Mixture\n  Models",
        "summary": "  When observations are organized into groups where commonalties exist amongst\nthem, the dependent random measures can be an ideal choice for modeling. One of\nthe propositions of the dependent random measures is that the atoms of the\nposterior distribution are shared amongst groups, and hence groups can borrow\ninformation from each other. When normalized dependent random measures prior\nwith independent increments are applied, we can derive appropriate exchangeable\nprobability partition function (EPPF), and subsequently also deduce its\ninference algorithm given any mixture model likelihood. We provide all\nnecessary derivation and solution to this framework. For demonstration, we used\nmixture of Gaussians likelihood in combination with a dependent structure\nconstructed by linear combinations of CRMs. Our experiments show superior\nperformance when using this framework, where the inferred values including the\nmixing weights and the number of clusters both respond appropriately to the\nnumber of completely random measure used.\n",
        "published": "2016-06-27T01:24:02Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08105v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08455v1",
        "title": "Anomaly detection in video with Bayesian nonparametrics",
        "summary": "  A novel dynamic Bayesian nonparametric topic model for anomaly detection in\nvideo is proposed in this paper. Batch and online Gibbs samplers are developed\nfor inference. The paper introduces a new abnormality measure for decision\nmaking. The proposed method is evaluated on both synthetic and real data. The\ncomparison with a non-dynamic model shows the superiority of the proposed\ndynamic one in terms of the classification performance for anomaly detection.\n",
        "published": "2016-06-27T20:01:08Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08455v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08476v1",
        "title": "Dynamic Hierarchical Dirichlet Process for Abnormal Behaviour Detection\n  in Video",
        "summary": "  This paper proposes a novel dynamic Hierarchical Dirichlet Process topic\nmodel that considers the dependence between successive observations.\nConventional posterior inference algorithms for this kind of models require\nprocessing of the whole data through several passes. It is computationally\nintractable for massive or sequential data. We design the batch and online\ninference algorithms, based on the Gibbs sampling, for the proposed model. It\nallows to process sequential data, incrementally updating the model by a new\nobservation. The model is applied to abnormal behaviour detection in video\nsequences. A new abnormality measure is proposed for decision making. The\nproposed method is compared with the method based on the non- dynamic\nHierarchical Dirichlet Process, for which we also derive the online Gibbs\nsampler and the abnormality measure. The results with synthetic and real data\nshow that the consideration of the dynamics in a topic model improves the\nclassification performance for abnormal behaviour detection.\n",
        "published": "2016-06-27T20:29:42Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08476v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08549v1",
        "title": "Automatic Variational ABC",
        "summary": "  Approximate Bayesian Computation (ABC) is a framework for performing\nlikelihood-free posterior inference for simulation models. Stochastic\nVariational inference (SVI) is an appealing alternative to the inefficient\nsampling approaches commonly used in ABC. However, SVI is highly sensitive to\nthe variance of the gradient estimators, and this problem is exacerbated by\napproximating the likelihood. We draw upon recent advances in variance\nreduction for SV and likelihood-free inference using deterministic simulations\nto produce low variance gradient estimators of the variational lower-bound. By\nthen exploiting automatic differentiation libraries we can avoid nearly all\nmodel-specific derivations. We demonstrate performance on three problems and\ncompare to existing SVI algorithms. Our results demonstrate the correctness and\nefficiency of our algorithm.\n",
        "published": "2016-06-28T04:16:25Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08549v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08793v3",
        "title": "Modeling Industrial ADMET Data with Multitask Networks",
        "summary": "  Deep learning methods such as multitask neural networks have recently been\napplied to ligand-based virtual screening and other drug discovery\napplications. Using a set of industrial ADMET datasets, we compare neural\nnetworks to standard baseline models and analyze multitask learning effects\nwith both random cross-validation and a more relevant temporal validation\nscheme. We confirm that multitask learning can provide modest benefits over\nsingle-task models and show that smaller datasets tend to benefit more than\nlarger datasets from multitask learning. Additionally, we find that adding\nmassive amounts of side information is not guaranteed to improve performance\nrelative to simpler multitask learning. Our results emphasize that multitask\neffects are highly dataset-dependent, suggesting the use of dataset-specific\nmodels to maximize overall performance.\n",
        "published": "2016-06-28T17:22:29Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08793v3"
    },
    {
        "id": "http://arxiv.org/abs/1606.08882v1",
        "title": "Tracking Switched Dynamic Network Topologies from Information Cascades",
        "summary": "  Contagions such as the spread of popular news stories, or infectious\ndiseases, propagate in cascades over dynamic networks with unobservable\ntopologies. However, \"social signals\" such as product purchase time, or blog\nentry timestamps are measurable, and implicitly depend on the underlying\ntopology, making it possible to track it over time. Interestingly, network\ntopologies often \"jump\" between discrete states that may account for sudden\nchanges in the observed signals. The present paper advocates a switched dynamic\nstructural equation model to capture the topology-dependent cascade evolution,\nas well as the discrete states driving the underlying topologies. Conditions\nunder which the proposed switched model is identifiable are established.\nLeveraging the edge sparsity inherent to social networks, a recursive\n$\\ell_1$-norm regularized least-squares estimator is put forth to jointly track\nthe states and network topologies. An efficient first-order proximal-gradient\nalgorithm is developed to solve the resulting optimization problem. Numerical\nexperiments on both synthetic data and real cascades measured over the span of\none year are conducted, and test results corroborate the efficacy of the\nadvocated approach.\n",
        "published": "2016-06-28T20:49:00Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08882v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.08957v1",
        "title": "Alternating Estimation for Structured High-Dimensional Multi-Response\n  Models",
        "summary": "  We consider learning high-dimensional multi-response linear models with\nstructured parameters. By exploiting the noise correlations among responses, we\npropose an alternating estimation (AltEst) procedure to estimate the model\nparameters based on the generalized Dantzig selector. Under suitable sample\nsize and resampling assumptions, we show that the error of the estimates\ngenerated by AltEst, with high probability, converges linearly to certain\nminimum achievable level, which can be tersely expressed by a few geometric\nmeasures, such as Gaussian width of sets related to the parameter structure. To\nthe best of our knowledge, this is the first non-asymptotic statistical\nguarantee for such AltEst-type algorithm applied to estimation problem with\ngeneral structures.\n",
        "published": "2016-06-29T05:35:20Z",
        "pdf_link": "http://arxiv.org/pdf/1606.08957v1"
    },
    {
        "id": "http://arxiv.org/abs/1606.09066v3",
        "title": "Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach",
        "summary": "  Tree ensembles, such as random forests and boosted trees, are renowned for\ntheir high prediction performance. However, their interpretability is\ncritically limited due to the enormous complexity. In this study, we present a\nmethod to make a complex tree ensemble interpretable by simplifying the model.\nSpecifically, we formalize the simplification of tree ensembles as a model\nselection problem. Given a complex tree ensemble, we aim at obtaining the\nsimplest representation that is essentially equivalent to the original one. To\nthis end, we derive a Bayesian model selection algorithm that optimizes the\nsimplified model while maintaining the prediction performance. Our numerical\nexperiments on several datasets showed that complicated tree ensembles were\nreasonably approximated as interpretable.\n",
        "published": "2016-06-29T12:24:03Z",
        "pdf_link": "http://arxiv.org/pdf/1606.09066v3"
    },
    {
        "id": "http://arxiv.org/abs/1607.00084v2",
        "title": "On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations",
        "summary": "  The problem of finding overlapping communities in networks has gained much\nattention recently. Optimization-based approaches use non-negative matrix\nfactorization (NMF) or variants, but the global optimum cannot be provably\nattained in general. Model-based approaches, such as the popular\nmixed-membership stochastic blockmodel or MMSB (Airoldi et al., 2008), use\nparameters for each node to specify the overlapping communities, but standard\ninference techniques cannot guarantee consistency. We link the two approaches,\nby (a) establishing sufficient conditions for the symmetric NMF optimization to\nhave a unique solution under MMSB, and (b) proposing a computationally\nefficient algorithm called GeoNMF that is provably optimal and hence consistent\nfor a broad parameter regime. We demonstrate its accuracy on both simulated and\nreal-world datasets.\n",
        "published": "2016-07-01T00:17:01Z",
        "pdf_link": "http://arxiv.org/pdf/1607.00084v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.00706v1",
        "title": "A Semi-supervised learning approach to enhance health care\n  Community-based Question Answering: A case study in alcoholism",
        "summary": "  Community-based Question Answering (CQA) sites play an important role in\naddressing health information needs. However, a significant number of posted\nquestions remain unanswered. Automatically answering the posted questions can\nprovide a useful source of information for online health communities. In this\nstudy, we developed an algorithm to automatically answer health-related\nquestions based on past questions and answers (QA). We also aimed to understand\ninformation embedded within online health content that are good features in\nidentifying valid answers. Our proposed algorithm uses information retrieval\ntechniques to identify candidate answers from resolved QA. In order to rank\nthese candidates, we implemented a semi-supervised leaning algorithm that\nextracts the best answer to a question. We assessed this approach on a curated\ncorpus from Yahoo! Answers and compared against a rule-based string similarity\nbaseline. On our dataset, the semi-supervised learning algorithm has an\naccuracy of 86.2%. UMLS-based (health-related) features used in the model\nenhance the algorithm's performance by proximately 8 %. A reasonably high rate\nof accuracy is obtained given that the data is considerably noisy. Important\nfeatures distinguishing a valid answer from an invalid answer include text\nlength, number of stop words contained in a test question, a distance between\nthe test question and other questions in the corpus as well as a number of\noverlapping health-related terms between questions. Overall, our automated QA\nsystem based on historical QA pairs is shown to be effective according to the\ndata set in this case study. It is developed for general use in the health care\ndomain which can also be applied to other CQA sites.\n",
        "published": "2016-07-04T00:17:08Z",
        "pdf_link": "http://arxiv.org/pdf/1607.00706v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.00710v2",
        "title": "Automatic Generation of Probabilistic Programming from Time Series Data",
        "summary": "  Probabilistic programming languages represent complex data with intermingled\nmodels in a few lines of code. Efficient inference algorithms in probabilistic\nprogramming languages make possible to build unified frameworks to compute\ninteresting probabilities of various large, real-world problems. When the\nstructure of model is given, constructing a probabilistic program is rather\nstraightforward. Thus, main focus have been to learn the best model parameters\nand compute marginal probabilities. In this paper, we provide a new perspective\nto build expressive probabilistic program from continue time series data when\nthe structure of model is not given. The intuition behind of our method is to\nfind a descriptive covariance structure of time series data in nonparametric\nGaussian process regression. We report that such descriptive covariance\nstructure efficiently derives a probabilistic programming description\naccurately.\n",
        "published": "2016-07-04T00:50:30Z",
        "pdf_link": "http://arxiv.org/pdf/1607.00710v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.01369v3",
        "title": "On the Consistency of the Likelihood Maximization Vertex Nomination\n  Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph\n  Matching",
        "summary": "  Given a graph in which a few vertices are deemed interesting a priori, the\nvertex nomination task is to order the remaining vertices into a nomination\nlist such that there is a concentration of interesting vertices at the top of\nthe list. Previous work has yielded several approaches to this problem, with\ntheoretical results in the setting where the graph is drawn from a stochastic\nblock model (SBM), including a vertex nomination analogue of the Bayes optimal\nclassifier. In this paper, we prove that maximum likelihood (ML)-based vertex\nnomination is consistent, in the sense that the performance of the ML-based\nscheme asymptotically matches that of the Bayes optimal scheme. We prove\ntheorems of this form both when model parameters are known and unknown.\nAdditionally, we introduce and prove consistency of a related, more scalable\nrestricted-focus ML vertex nomination scheme. Finally, we incorporate vertex\nand edge features into ML-based vertex nomination and briefly explore the\nempirical effectiveness of this approach.\n",
        "published": "2016-07-05T19:08:48Z",
        "pdf_link": "http://arxiv.org/pdf/1607.01369v3"
    },
    {
        "id": "http://arxiv.org/abs/1607.01624v2",
        "title": "Bayesian Nonparametrics for Sparse Dynamic Networks",
        "summary": "  In this paper we propose a Bayesian nonparametric approach to modelling\nsparse time-varying networks. A positive parameter is associated to each node\nof a network, which models the sociability of that node. Sociabilities are\nassumed to evolve over time, and are modelled via a dynamic point process\nmodel. The model is able to capture long term evolution of the sociabilities.\nMoreover, it yields sparse graphs, where the number of edges grows\nsubquadratically with the number of nodes. The evolution of the sociabilities\nis described by a tractable time-varying generalised gamma process. We provide\nsome theoretical insights into the model and apply it to three datasets: a\nsimulated network, a network of hyperlinks between communities on Reddit, and a\nnetwork of co-occurences of words in Reuters news articles after the September\n11th attacks.\n",
        "published": "2016-07-06T14:02:43Z",
        "pdf_link": "http://arxiv.org/pdf/1607.01624v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.02011v2",
        "title": "Kernel Bayesian Inference with Posterior Regularization",
        "summary": "  We propose a vector-valued regression problem whose solution is equivalent to\nthe reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior\ndistribution. This equivalence provides a new understanding of kernel Bayesian\ninference. Moreover, the optimization problem induces a new regularization for\nthe posterior embedding estimator, which is faster and has comparable\nperformance to the squared regularization in kernel Bayes' rule. This\nregularization coincides with a former thresholding approach used in kernel\nPOMDPs whose consistency remains to be established. Our theoretical work solves\nthis open problem and provides consistency analysis in regression settings.\nBased on our optimizational formulation, we propose a flexible Bayesian\nposterior regularization framework which for the first time enables us to put\nregularization at the distribution level. We apply this method to nonparametric\nstate-space filtering tasks with extremely nonlinear dynamics and show\nperformance gains over all other baselines.\n",
        "published": "2016-07-07T13:44:44Z",
        "pdf_link": "http://arxiv.org/pdf/1607.02011v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.02085v1",
        "title": "A Classification Framework for Partially Observed Dynamical Systems",
        "summary": "  We present a general framework for classifying partially observed dynamical\nsystems based on the idea of learning in the model space. In contrast to the\nexisting approaches using model point estimates to represent individual data\nitems, we employ posterior distributions over models, thus taking into account\nin a principled manner the uncertainty due to both the generative\n(observational and/or dynamic noise) and observation (sampling in time)\nprocesses. We evaluate the framework on two testbeds - a biological pathway\nmodel and a stochastic double-well system. Crucially, we show that the\nclassifier performance is not impaired when the model class used for inferring\nposterior distributions is much more simple than the observation-generating\nmodel class, provided the reduced complexity inferential model class captures\nthe essential characteristics needed for the given classification task.\n",
        "published": "2016-07-07T17:17:04Z",
        "pdf_link": "http://arxiv.org/pdf/1607.02085v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.02670v1",
        "title": "Sparse additive Gaussian process with soft interactions",
        "summary": "  Additive nonparametric regression models provide an attractive tool for\nvariable selection in high dimensions when the relationship between the\nresponse and predictors is complex. They offer greater flexibility compared to\nparametric non-linear regression models and better interpretability and\nscalability than the non-parametric regression models. However, achieving\nsparsity simultaneously in the number of nonparametric components as well as in\nthe variables within each nonparametric component poses a stiff computational\nchallenge. In this article, we develop a novel Bayesian additive regression\nmodel using a combination of hard and soft shrinkages to separately control the\nnumber of additive components and the variables within each component. An\nefficient algorithm is developed to select the importance variables and\nestimate the interaction network. Excellent performance is obtained in\nsimulated and real data examples.\n",
        "published": "2016-07-09T22:29:32Z",
        "pdf_link": "http://arxiv.org/pdf/1607.02670v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.02676v1",
        "title": "Bayesian quantile additive regression trees",
        "summary": "  Ensemble of regression trees have become popular statistical tools for the\nestimation of conditional mean given a set of predictors. However, quantile\nregression trees and their ensembles have not yet garnered much attention\ndespite the increasing popularity of the linear quantile regression model. This\nwork proposes a Bayesian quantile additive regression trees model that shows\nvery good predictive performance illustrated using simulation studies and real\ndata applications. Further extension to tackle binary classification problems\nis also considered.\n",
        "published": "2016-07-10T01:20:57Z",
        "pdf_link": "http://arxiv.org/pdf/1607.02676v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.02738v2",
        "title": "Magnetic Hamiltonian Monte Carlo",
        "summary": "  Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct\nefficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we\npresent a generalization of HMC which exploits \\textit{non-canonical}\nHamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3\ndimensions a subset of the dynamics map onto the mechanics of a charged\nparticle coupled to a magnetic field. We establish a theoretical basis for the\nuse of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic,\nleapfrog-like integrator allowing for the implementation of magnetic HMC.\nFinally, we exhibit several examples where these non-canonical dynamics can\nlead to improved mixing of magnetic HMC relative to ordinary HMC.\n",
        "published": "2016-07-10T12:19:29Z",
        "pdf_link": "http://arxiv.org/pdf/1607.02738v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.03026v1",
        "title": "Retrospective Causal Inference with Machine Learning Ensembles: An\n  Application to Anti-Recidivism Policies in Colombia",
        "summary": "  We present new methods to estimate causal effects retrospectively from micro\ndata with the assistance of a machine learning ensemble. This approach\novercomes two important limitations in conventional methods like regression\nmodeling or matching: (i) ambiguity about the pertinent retrospective\ncounterfactuals and (ii) potential misspecification, overfitting, and otherwise\nbias-prone or inefficient use of a large identifying covariate set in the\nestimation of causal effects. Our method targets the analysis toward a well\ndefined ``retrospective intervention effect'' (RIE) based on hypothetical\npopulation interventions and applies a machine learning ensemble that allows\ndata to guide us, in a controlled fashion, on how to use a large identifying\ncovariate set. We illustrate with an analysis of policy options for reducing\nex-combatant recidivism in Colombia.\n",
        "published": "2016-07-11T16:47:47Z",
        "pdf_link": "http://arxiv.org/pdf/1607.03026v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.03300v1",
        "title": "From Dependence to Causation",
        "summary": "  Machine learning is the science of discovering statistical dependencies in\ndata, and the use of those dependencies to perform predictions. During the last\ndecade, machine learning has made spectacular progress, surpassing human\nperformance in complex tasks such as object recognition, car driving, and\ncomputer gaming. However, the central role of prediction in machine learning\navoids progress towards general-purpose artificial intelligence. As one way\nforward, we argue that causal inference is a fundamental component of human\nintelligence, yet ignored by learning algorithms.\n  Causal inference is the problem of uncovering the cause-effect relationships\nbetween the variables of a data generating system. Causal structures provide\nunderstanding about how these systems behave under changing, unseen\nenvironments. In turn, knowledge about these causal dynamics allows to answer\n\"what if\" questions, describing the potential responses of the system under\nhypothetical manipulations and interventions. Thus, understanding cause and\neffect is one step from machine learning towards machine reasoning and machine\nintelligence. But, currently available causal inference algorithms operate in\nspecific regimes, and rely on assumptions that are difficult to verify in\npractice.\n  This thesis advances the art of causal inference in three different ways.\nFirst, we develop a framework for the study of statistical dependence based on\ncopulas and random features. Second, we build on this framework to interpret\nthe problem of causal inference as the task of distribution classification,\nyielding a family of novel causal inference algorithms. Third, we discover\ncausal structures in convolutional neural network features using our\nalgorithms. The algorithms presented in this thesis are scalable, exhibit\nstrong theoretical guarantees, and achieve state-of-the-art performance in a\nvariety of real-world benchmarks.\n",
        "published": "2016-07-12T10:32:02Z",
        "pdf_link": "http://arxiv.org/pdf/1607.03300v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.03574v4",
        "title": "Effects of Additional Data on Bayesian Clustering",
        "summary": "  Hierarchical probabilistic models, such as mixture models, are used for\ncluster analysis. These models have two types of variables: observable and\nlatent. In cluster analysis, the latent variable is estimated, and it is\nexpected that additional information will improve the accuracy of the\nestimation of the latent variable. Many proposed learning methods are able to\nuse additional data; these include semi-supervised learning and transfer\nlearning. However, from a statistical point of view, a complex probabilistic\nmodel that encompasses both the initial and additional data might be less\naccurate due to having a higher-dimensional parameter. The present paper\npresents a theoretical analysis of the accuracy of such a model and clarifies\nwhich factor has the greatest effect on its accuracy, the advantages of\nobtaining additional data, and the disadvantages of increasing the complexity.\n",
        "published": "2016-07-13T02:41:24Z",
        "pdf_link": "http://arxiv.org/pdf/1607.03574v4"
    },
    {
        "id": "http://arxiv.org/abs/1607.03792v1",
        "title": "Kernel Density Estimation for Dynamical Systems",
        "summary": "  We study the density estimation problem with observations generated by\ncertain dynamical systems that admit a unique underlying invariant Lebesgue\ndensity. Observations drawn from dynamical systems are not independent and\nmoreover, usual mixing concepts may not be appropriate for measuring the\ndependence among these observations. By employing the $\\mathcal{C}$-mixing\nconcept to measure the dependence, we conduct statistical analysis on the\nconsistency and convergence of the kernel density estimator. Our main results\nare as follows: First, we show that with properly chosen bandwidth, the kernel\ndensity estimator is universally consistent under $L_1$-norm; Second, we\nestablish convergence rates for the estimator with respect to several classes\nof dynamical systems under $L_1$-norm. In the analysis, the density function\n$f$ is only assumed to be H\\\"{o}lder continuous which is a weak assumption in\nthe literature of nonparametric density estimation and also more realistic in\nthe dynamical system context. Last but not least, we prove that the same\nconvergence rates of the estimator under $L_\\infty$-norm and $L_1$-norm can be\nachieved when the density function is H\\\"{o}lder continuous, compactly\nsupported and bounded. The bandwidth selection problem of the kernel density\nestimator for dynamical system is also discussed in our study via numerical\nsimulations.\n",
        "published": "2016-07-13T15:32:35Z",
        "pdf_link": "http://arxiv.org/pdf/1607.03792v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.03842v1",
        "title": "Safe Policy Improvement by Minimizing Robust Baseline Regret",
        "summary": "  An important problem in sequential decision-making under uncertainty is to\nuse limited data to compute a safe policy, i.e., a policy that is guaranteed to\nperform at least as well as a given baseline strategy. In this paper, we\ndevelop and analyze a new model-based approach to compute a safe policy when we\nhave access to an inaccurate dynamics model of the system with known accuracy\nguarantees. Our proposed robust method uses this (inaccurate) model to directly\nminimize the (negative) regret w.r.t. the baseline policy. Contrary to the\nexisting approaches, minimizing the regret allows one to improve the baseline\npolicy in states with accurate dynamics and seamlessly fall back to the\nbaseline policy, otherwise. We show that our formulation is NP-hard and propose\nan approximate algorithm. Our empirical results on several domains show that\neven this relatively simple approximate algorithm can significantly outperform\nstandard approaches.\n",
        "published": "2016-07-13T17:55:45Z",
        "pdf_link": "http://arxiv.org/pdf/1607.03842v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.04566v1",
        "title": "Spectral Echolocation via the Wave Embedding",
        "summary": "  Spectral embedding uses eigenfunctions of the discrete Laplacian on a\nweighted graph to obtain coordinates for an embedding of an abstract data set\ninto Euclidean space. We propose a new pre-processing step of first using the\neigenfunctions to simulate a low-frequency wave moving over the data and using\nboth position as well as change in time of the wave to obtain a refined metric\nto which classical methods of dimensionality reduction can then applied. This\nis motivated by the behavior of waves, symmetries of the wave equation and the\nhunting technique of bats. It is shown to be effective in practice and also\nworks for other partial differential equations -- the method yields improved\nresults even for the classical heat equation.\n",
        "published": "2016-07-15T16:08:57Z",
        "pdf_link": "http://arxiv.org/pdf/1607.04566v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.05432v3",
        "title": "Nested Kriging predictions for datasets with large number of\n  observations",
        "summary": "  This work falls within the context of predicting the value of a real function\nat some input locations given a limited number of observations of this\nfunction. The Kriging interpolation technique (or Gaussian process regression)\nis often considered to tackle such a problem but the method suffers from its\ncomputational burden when the number of observation points is large. We\nintroduce in this article nested Kriging predictors which are constructed by\naggregating sub-models based on subsets of observation points. This approach is\nproven to have better theoretical properties than other aggregation methods\nthat can be found in the literature. Contrarily to some other methods it can be\nshown that the proposed aggregation method is consistent. Finally, the\npractical interest of the proposed method is illustrated on simulated datasets\nand on an industrial test case with $10^4$ observations in a 6-dimensional\nspace.\n",
        "published": "2016-07-19T07:27:02Z",
        "pdf_link": "http://arxiv.org/pdf/1607.05432v3"
    },
    {
        "id": "http://arxiv.org/abs/1607.05506v2",
        "title": "Distribution-dependent concentration inequalities for tighter\n  generalization bounds",
        "summary": "  Concentration inequalities are indispensable tools for studying the\ngeneralization capacity of learning models. Hoeffding's and McDiarmid's\ninequalities are commonly used, giving bounds independent of the data\ndistribution. Although this makes them widely applicable, a drawback is that\nthe bounds can be too loose in some specific cases. Although efforts have been\ndevoted to improving the bounds, we find that the bounds can be further\ntightened in some distribution-dependent scenarios and conditions for the\ninequalities can be relaxed. In particular, we propose four types of conditions\nfor probabilistic boundedness and bounded differences, and derive several\ndistribution-dependent extensions of Hoeffding's and McDiarmid's inequalities.\nThese extensions provide bounds for functions not satisfying the conditions of\nthe existing inequalities, and in some special cases, tighter bounds.\nFurthermore, we obtain generalization bounds for unbounded and\nhierarchy-bounded loss functions. Finally we discuss the potential applications\nof our extensions to learning theory.\n",
        "published": "2016-07-19T10:23:57Z",
        "pdf_link": "http://arxiv.org/pdf/1607.05506v2"
    },
    {
        "id": "http://arxiv.org/abs/1607.05974v1",
        "title": "Anomaly Detection and Localisation using Mixed Graphical Models",
        "summary": "  We propose a method that performs anomaly detection and localisation within\nheterogeneous data using a pairwise undirected mixed graphical model. The data\nare a mixture of categorical and quantitative variables, and the model is\nlearned over a dataset that is supposed not to contain any anomaly. We then use\nthe model over temporal data, potentially a data stream, using a version of the\ntwo-sided CUSUM algorithm. The proposed decision statistic is based on a\nconditional likelihood ratio computed for each variable given the others. Our\nresults show that this function allows to detect anomalies variable by\nvariable, and thus to localise the variables involved in the anomalies more\nprecisely than univariate methods based on simple marginals.\n",
        "published": "2016-07-20T14:31:30Z",
        "pdf_link": "http://arxiv.org/pdf/1607.05974v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.06534v3",
        "title": "The Landscape of Empirical Risk for Non-convex Losses",
        "summary": "  Most high-dimensional estimation and prediction methods propose to minimize a\ncost function (empirical risk) that is written as a sum of losses associated to\neach data point. In this paper we focus on the case of non-convex losses, which\nis practically important but still poorly understood. Classical empirical\nprocess theory implies uniform convergence of the empirical risk to the\npopulation risk. While uniform convergence implies consistency of the resulting\nM-estimator, it does not ensure that the latter can be computed efficiently.\n  In order to capture the complexity of computing M-estimators, we propose to\nstudy the landscape of the empirical risk, namely its stationary points and\ntheir properties. We establish uniform convergence of the gradient and Hessian\nof the empirical risk to their population counterparts, as soon as the number\nof samples becomes larger than the number of unknown parameters (modulo\nlogarithmic factors). Consequently, good properties of the population risk can\nbe carried to the empirical risk, and we can establish one-to-one\ncorrespondence of their stationary points. We demonstrate that in several\nproblems such as non-convex binary classification, robust regression, and\nGaussian mixture model, this result implies a complete characterization of the\nlandscape of the empirical risk, and of the convergence properties of descent\nalgorithms.\n  We extend our analysis to the very high-dimensional setting in which the\nnumber of parameters exceeds the number of samples, and provide a\ncharacterization of the empirical risk landscape under a nearly\ninformation-theoretically minimal condition. Namely, if the number of samples\nexceeds the sparsity of the unknown parameters vector (modulo logarithmic\nfactors), then a suitable uniform convergence result takes place. We apply this\nresult to non-convex binary classification and robust regression in very\nhigh-dimension.\n",
        "published": "2016-07-22T01:37:30Z",
        "pdf_link": "http://arxiv.org/pdf/1607.06534v3"
    },
    {
        "id": "http://arxiv.org/abs/1607.07573v1",
        "title": "Variational Mixture Models with Gamma or inverse-Gamma components",
        "summary": "  Mixture models with Gamma and or inverse-Gamma distributed mixture components\nare useful for medical image tissue segmentation or as post-hoc models for\nregression coefficients obtained from linear regression within a Generalised\nLinear Modeling framework (GLM), used in this case to separate stochastic\n(Gaussian) noise from some kind of positive or negative \"activation\" (modeled\nas Gamma or inverse-Gamma distributed). To date, the most common choice in this\ncontext it is Gaussian/Gamma mixture models learned through a maximum\nlikelihood (ML) approach; we recently extended such algorithm for mixture\nmodels with inverse-Gamma components. Here, we introduce a fully analytical\nVariational Bayes (VB) learning framework for both Gamma and/or inverse-Gamma\ncomponents. We use synthetic and resting state fMRI data to compare the\nperformance of the ML and VB algorithms in terms of area under the curve and\ncomputational cost. We observed that the ML Gaussian/Gamma model is very\nexpensive specially when considering high resolution images; furthermore, these\nsolutions are highly variable and they occasionally can overestimate the\nactivations severely. The Bayesian Gauss-Gamma is in general the fastest\nalgorithm but provides too dense solutions. The maximum likelihood\nGaussian/inverse-Gamma is also very fast but provides in general very sparse\nsolutions. The variational Gaussian/inverse-Gamma mixture model is the most\nrobust and its cost is acceptable even for high resolution images. Further, the\npresented methodology represents an essential building block that can be\ndirectly used in more complex inference tasks, specially designed to analyse\nMRI-fMRI data; such models include for example analytical variational mixture\nmodels with adaptive spatial regularization or better source models for new\nspatial blind source separation approaches.\n",
        "published": "2016-07-26T07:47:02Z",
        "pdf_link": "http://arxiv.org/pdf/1607.07573v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.08310v1",
        "title": "Preterm Birth Prediction: Deriving Stable and Interpretable Rules from\n  High Dimensional Data",
        "summary": "  Preterm births occur at an alarming rate of 10-15%. Preemies have a higher\nrisk of infant mortality, developmental retardation and long-term disabilities.\nPredicting preterm birth is difficult, even for the most experienced\nclinicians. The most well-designed clinical study thus far reaches a modest\nsensitivity of 18.2-24.2% at specificity of 28.6-33.3%. We take a different\napproach by exploiting databases of normal hospital operations. We aims are\ntwofold: (i) to derive an easy-to-use, interpretable prediction rule with\nquantified uncertainties, and (ii) to construct accurate classifiers for\npreterm birth prediction. Our approach is to automatically generate and select\nfrom hundreds (if not thousands) of possible predictors using stability-aware\ntechniques. Derived from a large database of 15,814 women, our simplified\nprediction rule with only 10 items has sensitivity of 62.3% at specificity of\n81.5%.\n",
        "published": "2016-07-28T04:25:30Z",
        "pdf_link": "http://arxiv.org/pdf/1607.08310v1"
    },
    {
        "id": "http://arxiv.org/abs/1607.08601v1",
        "title": "Limit theorems for eigenvectors of the normalized Laplacian for random\n  graphs",
        "summary": "  We prove a central limit theorem for the components of the eigenvectors\ncorresponding to the $d$ largest eigenvalues of the normalized Laplacian matrix\nof a finite dimensional random dot product graph. As a corollary, we show that\nfor stochastic blockmodel graphs, the rows of the spectral embedding of the\nnormalized Laplacian converge to multivariate normals and furthermore the mean\nand the covariance matrix of each row are functions of the associated vertex's\nblock membership. Together with prior results for the eigenvectors of the\nadjacency matrix, we then compare, via the Chernoff information between\nmultivariate normal distributions, how the choice of embedding method impacts\nsubsequent inference. We demonstrate that neither embedding method dominates\nwith respect to the inference task of recovering the latent block assignments.\n",
        "published": "2016-07-28T19:56:43Z",
        "pdf_link": "http://arxiv.org/pdf/1607.08601v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.00441v1",
        "title": "Kernel Risk-Sensitive Loss: Definition, Properties and Application to\n  Robust Adaptive Filtering",
        "summary": "  Nonlinear similarity measures defined in kernel space, such as correntropy,\ncan extract higher-order statistics of data and offer potentially significant\nperformance improvement over their linear counterparts especially in\nnon-Gaussian signal processing and machine learning. In this work, we propose a\nnew similarity measure in kernel space, called the kernel risk-sensitive loss\n(KRSL), and provide some important properties. We apply the KRSL to adaptive\nfiltering and investigate the robustness, and then develop the MKRSL algorithm\nand analyze the mean square convergence performance. Compared with correntropy,\nthe KRSL can offer a more efficient performance surface, thereby enabling a\ngradient based method to achieve faster convergence speed and higher accuracy\nwhile still maintaining the robustness to outliers. Theoretical analysis\nresults and superior performance of the new algorithm are confirmed by\nsimulation.\n",
        "published": "2016-08-01T14:19:32Z",
        "pdf_link": "http://arxiv.org/pdf/1608.00441v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.01008v3",
        "title": "Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and\n  Constrained Sampling",
        "summary": "  We study probability measures induced by set functions with constraints. Such\nmeasures arise in a variety of real-world settings, where prior knowledge,\nresource limitations, or other pragmatic considerations impose constraints. We\nconsider the task of rapidly sampling from such constrained measures, and\ndevelop fast Markov chain samplers for them. Our first main result is for MCMC\nsampling from Strongly Rayleigh (SR) measures, for which we present sharp\npolynomial bounds on the mixing time. As a corollary, this result yields a fast\nmixing sampler for Determinantal Point Processes (DPPs), yielding (to our\nknowledge) the first provably fast MCMC sampler for DPPs since their inception\nover four decades ago. Beyond SR measures, we develop MCMC samplers for\nprobabilistic models with hard constraints and identify sufficient conditions\nunder which their chains mix rapidly. We illustrate our claims by empirically\nverifying the dependence of mixing times on the key factors governing our\ntheoretical bounds.\n",
        "published": "2016-08-02T21:15:30Z",
        "pdf_link": "http://arxiv.org/pdf/1608.01008v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.01234v3",
        "title": "Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations",
        "summary": "  We study the problem of demixing a pair of sparse signals from noisy,\nnonlinear observations of their superposition. Mathematically, we consider a\nnonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \\ i=1,\\ldots,m$,\nwhere $x = \\Phi w+\\Psi z$ denotes the superposition signal, $\\Phi$ and $\\Psi$\nare orthonormal bases in $\\mathbb{R}^n$, and $w, z\\in\\mathbb{R}^n$ are sparse\ncoefficient vectors of the constituent signals, and $e_i$ represents the noise.\nMoreover, $g$ represents a nonlinear link function, and $a_i\\in\\mathbb{R}^n$ is\nthe $i$-th row of the measurement matrix, $A\\in\\mathbb{R}^{m\\times n}$.\nProblems of this nature arise in several applications ranging from astronomy,\ncomputer vision, and machine learning. In this paper, we make some concrete\nalgorithmic progress for the above demixing problem. Specifically, we consider\ntwo scenarios: (i) the case when the demixing procedure has no knowledge of the\nlink function, and (ii) the case when the demixing algorithm has perfect\nknowledge of the link function. In both cases, we provide fast algorithms for\nrecovery of the constituents $w$ and $z$ from the observations. Moreover, we\nsupport these algorithms with a rigorous theoretical analysis, and derive\n(nearly) tight upper bounds on the sample complexity of the proposed algorithms\nfor achieving stable recovery of the component signals. We also provide a range\nof numerical simulations to illustrate the performance of the proposed\nalgorithms on both real and synthetic signals and images.\n",
        "published": "2016-08-03T16:03:25Z",
        "pdf_link": "http://arxiv.org/pdf/1608.01234v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.01398v3",
        "title": "Iterative Hard Thresholding for Model Selection in Genome-Wide\n  Association Studies",
        "summary": "  A genome-wide association study (GWAS) correlates marker variation with trait\nvariation in a sample of individuals. Each study subject is genotyped at a\nmultitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here\nwe assume that subjects are unrelated and collected at random and that trait\nvalues are normally distributed or transformed to normality. Over the past\ndecade, researchers have been remarkably successful in applying GWAS analysis\nto hundreds of traits. The massive amount of data produced in these studies\npresent unique computational challenges. Penalized regression with LASSO or MCP\npenalties is capable of selecting a handful of associated SNPs from millions of\npotential SNPs. Unfortunately, model selection can be corrupted by false\npositives and false negatives, obscuring the genetic underpinning of a trait.\nThis paper introduces the iterative hard thresholding (IHT) algorithm to the\nGWAS analysis of continuous traits. Our parallel implementation of IHT\naccommodates SNP genotype compression and exploits multiple CPU cores and\ngraphics processing units (GPUs). This allows statistical geneticists to\nleverage commodity desktop computers in GWAS analysis and to avoid\nsupercomputing. We evaluate IHT performance on both simulated and real GWAS\ndata and conclude that it reduces false positive and false negative rates while\nremaining competitive in computational time with penalized regression. Source\ncode is freely available at https://github.com/klkeys/IHT.jl.\n",
        "published": "2016-08-04T00:05:24Z",
        "pdf_link": "http://arxiv.org/pdf/1608.01398v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.02280v1",
        "title": "Statistical Guarantees for Estimating the Centers of a Two-component\n  Gaussian Mixture by EM",
        "summary": "  Recently, a general method for analyzing the statistical accuracy of the EM\nalgorithm has been developed and applied to some simple latent variable models\n[Balakrishnan et al. 2016]. In that method, the basin of attraction for valid\ninitialization is required to be a ball around the truth. Using Stein's Lemma,\nwe extend these results in the case of estimating the centers of a\ntwo-component Gaussian mixture in $d$ dimensions. In particular, we\nsignificantly expand the basin of attraction to be the intersection of a half\nspace and a ball around the origin. If the signal-to-noise ratio is at least a\nconstant multiple of $ \\sqrt{d\\log d} $, we show that a random initialization\nstrategy is feasible.\n",
        "published": "2016-08-07T22:53:55Z",
        "pdf_link": "http://arxiv.org/pdf/1608.02280v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.02485v2",
        "title": "Boosting as a kernel-based method",
        "summary": "  Boosting combines weak (biased) learners to obtain effective learning\nalgorithms for classification and prediction. In this paper, we show a\nconnection between boosting and kernel-based methods, highlighting both\ntheoretical and practical applications. In the context of $\\ell_2$ boosting, we\nstart with a weak linear learner defined by a kernel $K$. We show that boosting\nwith this learner is equivalent to estimation with a special {\\it boosting\nkernel} that depends on $K$, as well as on the regression matrix, noise\nvariance, and hyperparameters. The number of boosting iterations is modeled as\na continuous hyperparameter, and fit along with other parameters using standard\ntechniques.\n  We then generalize the boosting kernel to a broad new class of boosting\napproaches for more general weak learners, including those based on the\n$\\ell_1$, hinge and Vapnik losses. The approach allows fast hyperparameter\ntuning for this general class, and has a wide range of applications, including\nrobust regression and classification. We illustrate some of these applications\nwith numerical examples on synthetic and real data.\n",
        "published": "2016-08-08T15:23:37Z",
        "pdf_link": "http://arxiv.org/pdf/1608.02485v2"
    },
    {
        "id": "http://arxiv.org/abs/1608.03817v3",
        "title": "Scaling Factorial Hidden Markov Models: Stochastic Variational Inference\n  without Messages",
        "summary": "  Factorial Hidden Markov Models (FHMMs) are powerful models for sequential\ndata but they do not scale well with long sequences. We propose a scalable\ninference and learning algorithm for FHMMs that draws on ideas from the\nstochastic variational inference, neural network and copula literatures. Unlike\nexisting approaches, the proposed algorithm requires no message passing\nprocedure among latent variables and can be distributed to a network of\ncomputers to speed up learning. Our experiments corroborate that the proposed\nalgorithm does not introduce further approximation bias compared to the proven\nstructured mean-field algorithm, and achieves better performance with long\nsequences and large FHMMs.\n",
        "published": "2016-08-12T15:02:10Z",
        "pdf_link": "http://arxiv.org/pdf/1608.03817v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.04048v1",
        "title": "Ultra High-Dimensional Nonlinear Feature Selection for Big Biological\n  Data",
        "summary": "  Machine learning methods are used to discover complex nonlinear relationships\nin biological and medical data. However, sophisticated learning models are\ncomputationally unfeasible for data with millions of features. Here we\nintroduce the first feature selection method for nonlinear learning problems\nthat can scale up to large, ultra-high dimensional biological data. More\nspecifically, we scale up the novel Hilbert-Schmidt Independence Criterion\nLasso (HSIC Lasso) to handle millions of features with tens of thousand\nsamples. The proposed method is guaranteed to find an optimal subset of\nmaximally predictive features with minimal redundancy, yielding higher\npredictive power and improved interpretability. Its effectiveness is\ndemonstrated through applications to classify phenotypes based on module\nexpression in human prostate cancer patients and to detect enzymes among\nprotein structures. We achieve high accuracy with as few as 20 out of one\nmillion features --- a dimensionality reduction of 99.998%. Our algorithm can\nbe implemented on commodity cloud computing platforms. The dramatic reduction\nof features may lead to the ubiquitous deployment of sophisticated prediction\nmodels in mobile health care applications.\n",
        "published": "2016-08-14T01:56:22Z",
        "pdf_link": "http://arxiv.org/pdf/1608.04048v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.04290v1",
        "title": "Robust Volume Minimization-Based Matrix Factorization for Remote Sensing\n  and Document Clustering",
        "summary": "  This paper considers \\emph{volume minimization} (VolMin)-based structured\nmatrix factorization (SMF). VolMin is a factorization criterion that decomposes\na given data matrix into a basis matrix times a structured coefficient matrix\nvia finding the minimum-volume simplex that encloses all the columns of the\ndata matrix. Recent work showed that VolMin guarantees the identifiability of\nthe factor matrices under mild conditions that are realistic in a wide variety\nof applications. This paper focuses on both theoretical and practical aspects\nof VolMin. On the theory side, exact equivalence of two independently developed\nsufficient conditions for VolMin identifiability is proven here, thereby\nproviding a more comprehensive understanding of this aspect of VolMin. On the\nalgorithm side, computational complexity and sensitivity to outliers are two\nkey challenges associated with real-world applications of VolMin. These are\naddressed here via a new VolMin algorithm that handles volume regularization in\na computationally simple way, and automatically detects and {iteratively\ndownweights} outliers, simultaneously. Simulations and real-data experiments\nusing a remotely sensed hyperspectral image and the Reuters document corpus are\nemployed to showcase the effectiveness of the proposed algorithm.\n",
        "published": "2016-08-15T14:51:10Z",
        "pdf_link": "http://arxiv.org/pdf/1608.04290v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.04961v3",
        "title": "Clustering Mixed Datasets Using Homogeneity Analysis with Applications\n  to Big Data",
        "summary": "  Datasets with a mixture of numerical and categorical attributes are routinely\nencountered in many application domains. In this work we examine an approach to\nclustering such datasets using homogeneity analysis. Homogeneity analysis\ndetermines a euclidean representation of the data. This can be analyzed by\nleveraging the large body of tools and techniques for data with a euclidean\nrepresentation. Experiments conducted as part of this study suggest that this\napproach can be useful in the analysis and exploration of big datasets with a\nmixture of numerical and categorical attributes.\n",
        "published": "2016-08-17T13:40:31Z",
        "pdf_link": "http://arxiv.org/pdf/1608.04961v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.05934v1",
        "title": "Spatial Modeling of Oil Exploration Areas Using Neural Networks and\n  ANFIS in GIS",
        "summary": "  Exploration of hydrocarbon resources is a highly complicated and expensive\nprocess where various geological, geochemical and geophysical factors are\ndeveloped then combined together. It is highly significant how to design the\nseismic data acquisition survey and locate the exploratory wells since\nincorrect or imprecise locations lead to waste of time and money during the\noperation. The objective of this study is to locate high-potential oil and gas\nfield in 1: 250,000 sheet of Ahwaz including 20 oil fields to reduce both time\nand costs in exploration and production processes. In this regard, 17 maps were\ndeveloped using GIS functions for factors including: minimum and maximum of\ntotal organic carbon (TOC), yield potential for hydrocarbons production (PP),\nTmax peak, production index (PI), oxygen index (OI), hydrogen index (HI) as\nwell as presence or proximity to high residual Bouguer gravity anomalies,\nproximity to anticline axis and faults, topography and curvature maps obtained\nfrom Asmari Formation subsurface contours. To model and to integrate maps, this\nstudy employed artificial neural network and adaptive neuro-fuzzy inference\nsystem (ANFIS) methods. The results obtained from model validation demonstrated\nthat the 17x10x5 neural network with R=0.8948, RMS=0.0267, and kappa=0.9079 can\nbe trained better than other models such as ANFIS and predicts the potential\nareas more accurately. However, this method failed to predict some oil fields\nand wrongly predict some areas as potential zones.\n",
        "published": "2016-08-21T13:00:45Z",
        "pdf_link": "http://arxiv.org/pdf/1608.05934v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.06622v2",
        "title": "The discriminative Kalman filter for nonlinear and non-Gaussian\n  sequential Bayesian filtering",
        "summary": "  The Kalman filter (KF) is used in a variety of applications for computing the\nposterior distribution of latent states in a state space model. The model\nrequires a linear relationship between states and observations. Extensions to\nthe Kalman filter have been proposed that incorporate linear approximations to\nnonlinear models, such as the extended Kalman filter (EKF) and the unscented\nKalman filter (UKF). However, we argue that in cases where the dimensionality\nof observed variables greatly exceeds the dimensionality of state variables, a\nmodel for $p(\\text{state}|\\text{observation})$ proves both easier to learn and\nmore accurate for latent space estimation. We derive and validate what we call\nthe discriminative Kalman filter (DKF): a closed-form discriminative version of\nBayesian filtering that readily incorporates off-the-shelf discriminative\nlearning techniques. Further, we demonstrate that given mild assumptions,\nhighly non-linear models for $p(\\text{state}|\\text{observation})$ can be\nspecified. We motivate and validate on synthetic datasets and in neural\ndecoding from non-human primates, showing substantial increases in decoding\nperformance versus the standard Kalman filter.\n",
        "published": "2016-08-23T19:53:20Z",
        "pdf_link": "http://arxiv.org/pdf/1608.06622v2"
    },
    {
        "id": "http://arxiv.org/abs/1608.07241v1",
        "title": "Formal Concept Analysis of Rodent Carriers of Zoonotic Disease",
        "summary": "  The technique of Formal Concept Analysis is applied to a dataset describing\nthe traits of rodents, with the goal of identifying zoonotic disease\ncarriers,or those species carrying infections that can spillover to cause human\ndisease. The concepts identified among these species together provide\nrules-of-thumb about the intrinsic biological features of rodents that carry\nzoonotic diseases, and offer utility for better targeting field surveillance\nefforts in the search for novel disease carriers in the wild.\n",
        "published": "2016-08-25T18:29:23Z",
        "pdf_link": "http://arxiv.org/pdf/1608.07241v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.07494v4",
        "title": "Estimating the Number of Clusters via Normalized Cluster Instability",
        "summary": "  We improve current instability-based methods for the selection of the number\nof clusters $k$ in cluster analysis by developing a normalized cluster\ninstability measure that corrects for the distribution of cluster sizes, a\npreviously unaccounted driver of cluster instability. We show that our\nnormalized instability measure outperforms current instability-based measures\nacross the whole sequence of possible $k$ and especially overcomes limitations\nin the context of large $k$. We also compare, for the first time, model-based\nand model-free approaches to determine cluster-instability and find their\nperformance to be comparable. We make our method available in the R-package\n\\verb+cstab+.\n",
        "published": "2016-08-26T15:26:01Z",
        "pdf_link": "http://arxiv.org/pdf/1608.07494v4"
    },
    {
        "id": "http://arxiv.org/abs/1608.07526v1",
        "title": "Maximum Correntropy Unscented Filter",
        "summary": "  The unscented transformation (UT) is an efficient method to solve the state\nestimation problem for a non-linear dynamic system, utilizing a derivative-free\nhigher-order approximation by approximating a Gaussian distribution rather than\napproximating a non-linear function. Applying the UT to a Kalman filter type\nestimator leads to the well-known unscented Kalman filter (UKF). Although the\nUKF works very well in Gaussian noises, its performance may deteriorate\nsignificantly when the noises are non-Gaussian, especially when the system is\ndisturbed by some heavy-tailed impulsive noises. To improve the robustness of\nthe UKF against impulsive noises, a new filter for nonlinear systems is\nproposed in this work, namely the maximum correntropy unscented filter (MCUF).\nIn MCUF, the UT is applied to obtain the prior estimates of the state and\ncovariance matrix, and a robust statistical linearization regression based on\nthe maximum correntropy criterion (MCC) is then used to obtain the posterior\nestimates of the state and covariance. The satisfying performance of the new\nalgorithm is confirmed by two illustrative examples.\n",
        "published": "2016-08-26T17:19:14Z",
        "pdf_link": "http://arxiv.org/pdf/1608.07526v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.07597v3",
        "title": "A Randomized Approach to Efficient Kernel Clustering",
        "summary": "  Kernel-based K-means clustering has gained popularity due to its simplicity\nand the power of its implicit non-linear representation of the data. A dominant\nconcern is the memory requirement since memory scales as the square of the\nnumber of data points. We provide a new analysis of a class of approximate\nkernel methods that have more modest memory requirements, and propose a\nspecific one-pass randomized kernel approximation followed by standard K-means\non the transformed data. The analysis and experiments suggest the method is\naccurate, while requiring drastically less memory than standard kernel K-means\nand significantly less memory than Nystrom based approximations.\n",
        "published": "2016-08-26T20:26:31Z",
        "pdf_link": "http://arxiv.org/pdf/1608.07597v3"
    },
    {
        "id": "http://arxiv.org/abs/1608.07986v4",
        "title": "Geometric adaptive Monte Carlo in random environment",
        "summary": "  Manifold Markov chain Monte Carlo algorithms have been introduced to sample\nmore effectively from challenging target densities exhibiting multiple modes or\nstrong correlations. Such algorithms exploit the local geometry of the\nparameter space, thus enabling chains to achieve a faster convergence rate when\nmeasured in number of steps. However, acquiring local geometric information can\noften increase computational complexity per step to the extent that sampling\nfrom high-dimensional targets becomes inefficient in terms of total\ncomputational time. This paper analyzes the computational complexity of\nmanifold Langevin Monte Carlo and proposes a geometric adaptive Monte Carlo\nsampler aimed at balancing the benefits of exploiting local geometry with\ncomputational cost to achieve a high effective sample size for a given\ncomputational cost. The suggested sampler is a discrete-time stochastic process\nin random environment. The random environment allows to switch between local\ngeometric and adaptive proposal kernels with the help of a schedule. An\nexponential schedule is put forward that enables more frequent use of geometric\ninformation in early transient phases of the chain, while saving computational\ntime in late stationary phases. The average complexity can be manually set\ndepending on the need for geometric exploitation posed by the underlying model.\n",
        "published": "2016-08-29T10:32:15Z",
        "pdf_link": "http://arxiv.org/pdf/1608.07986v4"
    },
    {
        "id": "http://arxiv.org/abs/1608.08362v1",
        "title": "Incremental Nonlinear System Identification and Adaptive Particle\n  Filtering Using Gaussian Process",
        "summary": "  An incremental/online state dynamic learning method is proposed for\nidentification of the nonlinear Gaussian state space models. The method embeds\nthe stochastic variational sparse Gaussian process as the probabilistic state\ndynamic model inside a particle filter framework. Model updating is done at\nmeasurement sample rate using stochastic gradient descent based optimization\nimplemented in the state estimation filtering loop. The performance of the\nproposed method is compared with state-of-the-art Gaussian process based batch\nlearning methods. Finally, it is shown that the state estimation performance\nsignificantly improves due to the online learning of state dynamics.\n",
        "published": "2016-08-30T08:32:13Z",
        "pdf_link": "http://arxiv.org/pdf/1608.08362v1"
    },
    {
        "id": "http://arxiv.org/abs/1608.08659v1",
        "title": "Joint Estimation of Multiple Dependent Gaussian Graphical Models with\n  Applications to Mouse Genomics",
        "summary": "  Gaussian graphical models are widely used to represent conditional dependence\namong random variables. In this paper, we propose a novel estimator for data\narising from a group of Gaussian graphical models that are themselves\ndependent. A motivating example is that of modeling gene expression collected\non multiple tissues from the same individual: here the multivariate outcome is\naffected by dependencies acting not only at the level of the specific tissues,\nbut also at the level of the whole body; existing methods that assume\nindependence among graphs are not applicable in this case. To estimate multiple\ndependent graphs, we decompose the problem into two graphical layers: the\nsystemic layer, which affects all outcomes and thereby induces cross- graph\ndependence, and the category-specific layer, which represents graph-specific\nvariation. We propose a graphical EM technique that estimates both layers\njointly, establish estimation consistency and selection sparsistency of the\nproposed estimator, and confirm by simulation that the EM method is superior to\na simple one-step method. We apply our technique to mouse genomics data and\nobtain biologically plausible results.\n",
        "published": "2016-08-30T21:12:07Z",
        "pdf_link": "http://arxiv.org/pdf/1608.08659v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.00189v1",
        "title": "A Birth and Death Process for Bayesian Network Structure Inference",
        "summary": "  Bayesian networks (BNs) are graphical models that are useful for representing\nhigh-dimensional probability distributions. There has been a great deal of\ninterest in recent years in the NP-hard problem of learning the structure of a\nBN from observed data. Typically, one assigns a score to various structures and\nthe search becomes an optimization problem that can be approached with either\ndeterministic or stochastic methods. In this paper, we walk through the space\nof graphs by modeling the appearance and disappearance of edges as a birth and\ndeath process and compare our novel approach to the popular Metropolis-Hastings\nsearch strategy. We give empirical evidence that the birth and death process\nhas superior mixing properties.\n",
        "published": "2016-10-01T20:57:13Z",
        "pdf_link": "http://arxiv.org/pdf/1610.00189v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.00907v1",
        "title": "Model Selection for Gaussian Process Regression by Approximation Set\n  Coding",
        "summary": "  Gaussian processes are powerful, yet analytically tractable models for\nsupervised learning. A Gaussian process is characterized by a mean function and\na covariance function (kernel), which are determined by a model selection\ncriterion. The functions to be compared do not just differ in their\nparametrization but in their fundamental structure. It is often not clear which\nfunction structure to choose, for instance to decide between a squared\nexponential and a rational quadratic kernel. Based on the principle of\napproximation set coding, we develop a framework for model selection to rank\nkernels for Gaussian process regression. In our experiments approximation set\ncoding shows promise to become a model selection criterion competitive with\nmaximum evidence (also called marginal likelihood) and leave-one-out\ncross-validation.\n",
        "published": "2016-10-04T09:20:08Z",
        "pdf_link": "http://arxiv.org/pdf/1610.00907v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.01492v1",
        "title": "Recovering Multiple Nonnegative Time Series From a Few Temporal\n  Aggregates",
        "summary": "  Motivated by electricity consumption metering, we extend existing nonnegative\nmatrix factorization (NMF) algorithms to use linear measurements as\nobservations, instead of matrix entries. The objective is to estimate multiple\ntime series at a fine temporal scale from temporal aggregates measured on each\nindividual series. Furthermore, our algorithm is extended to take into account\nindividual autocorrelation to provide better estimation, using a recent convex\nrelaxation of quadratically constrained quadratic program. Extensive\nexperiments on synthetic and real-world electricity consumption datasets\nillustrate the effectiveness of our matrix recovery algorithms.\n",
        "published": "2016-10-05T15:58:18Z",
        "pdf_link": "http://arxiv.org/pdf/1610.01492v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.01766v2",
        "title": "Constrained Maximum Correntropy Adaptive Filtering",
        "summary": "  Constrained adaptive filtering algorithms inculding constrained least mean\nsquare (CLMS), constrained affine projection (CAP) and constrained recursive\nleast squares (CRLS) have been extensively studied in many applications. Most\nexisting constrained adaptive filtering algorithms are developed under mean\nsquare error (MSE) criterion, which is an ideal optimality criterion under\nGaussian noises. This assumption however fails to model the behavior of\nnon-Gaussian noises found in practice. Motivated by the robustness and\nsimplicity of maximum correntropy criterion (MCC) in non-Gaussian impulsive\nnoises, this paper proposes a new adaptive filtering algorithm called\nconstrained maximum correntropy criterion (CMCC). Specifically, CMCC\nincorporates a linear constraint into a MCC filter to solve a constrained\noptimization problem explicitly. The proposed adaptive filtering algorithm is\neasy to implement and has low computational complexity, and in terms of\nconvergence accuracy (say lower mean square deviation) and stability, can\nsignificantly outperform those MSE based constrained adaptive algorithms in\npresence of heavy-tailed impulsive noises. Additionally, the mean square\nconvergence behaviors are studied under energy conservation relation, and a\nsufficient condition to ensure the mean square convergence and the steady-state\nmean square deviation (MSD) of the proposed algorithm are obtained. Simulation\nresults confirm the theoretical predictions under both Gaussian and non-\nGaussian noises, and demonstrate the excellent performance of the novel\nalgorithm by comparing it with other conventional methods.\n",
        "published": "2016-10-06T07:56:11Z",
        "pdf_link": "http://arxiv.org/pdf/1610.01766v2"
    },
    {
        "id": "http://arxiv.org/abs/1610.02287v3",
        "title": "The Generalized Reparameterization Gradient",
        "summary": "  The reparameterization gradient has become a widely used method to obtain\nMonte Carlo gradients to optimize the variational objective. However, this\ntechnique does not easily apply to commonly used distributions such as beta or\ngamma without further approximations, and most practical applications of the\nreparameterization gradient fit Gaussian distributions. In this paper, we\nintroduce the generalized reparameterization gradient, a method that extends\nthe reparameterization gradient to a wider class of variational distributions.\nGeneralized reparameterizations use invertible transformations of the latent\nvariables which lead to transformed distributions that weakly depend on the\nvariational parameters. This results in new Monte Carlo gradients that combine\nreparameterization gradients and score function gradients. We demonstrate our\napproach on variational inference for two complex probabilistic models. The\ngeneralized reparameterization is effective: even a single sample from the\nvariational distribution is enough to obtain a low-variance gradient.\n",
        "published": "2016-10-07T13:55:35Z",
        "pdf_link": "http://arxiv.org/pdf/1610.02287v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.02490v4",
        "title": "A nonparametric sequential test for online randomized experiments",
        "summary": "  We propose a nonparametric sequential test that aims to address two practical\nproblems pertinent to online randomized experiments: (i) how to do a hypothesis\ntest for complex metrics; (ii) how to prevent type $1$ error inflation under\ncontinuous monitoring. The proposed test does not require knowledge of the\nunderlying probability distribution generating the data. We use the bootstrap\nto estimate the likelihood for blocks of data followed by mixture sequential\nprobability ratio test. We validate this procedure on data from a major online\ne-commerce website. We show that the proposed test controls type $1$ error at\nany time, has good power, is robust to misspecification in the distribution\ngenerating the data, and allows quick inference in online randomized\nexperiments.\n",
        "published": "2016-10-08T06:13:50Z",
        "pdf_link": "http://arxiv.org/pdf/1610.02490v4"
    },
    {
        "id": "http://arxiv.org/abs/1610.02920v2",
        "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective",
        "summary": "  Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.\n",
        "published": "2016-10-10T14:02:30Z",
        "pdf_link": "http://arxiv.org/pdf/1610.02920v2"
    },
    {
        "id": "http://arxiv.org/abs/1610.03113v3",
        "title": "Truncated Variational Expectation Maximization",
        "summary": "  We derive a novel variational expectation maximization approach based on\ntruncated posterior distributions. Truncated distributions are proportional to\nexact posteriors within subsets of a discrete state space and equal zero\notherwise. The treatment of the distributions' subsets as variational\nparameters distinguishes the approach from previous variational approaches. The\nspecific structure of truncated distributions allows for deriving novel and\nmathematically grounded results, which in turn can be used to formulate novel\nefficient algorithms to optimize the parameters of probabilistic generative\nmodels. Most centrally, we find the variational lower bounds that correspond to\ntruncated distributions to be given by very concise and efficiently computable\nexpressions, while update equations for model parameters remain in their\nstandard form. Based on these findings, we show how efficient and easily\napplicable meta-algorithms can be formulated that guarantee a monotonic\nincrease of the variational bound. Example applications of the here derived\nframework provide novel theoretical results and learning procedures for latent\nvariable models as well as mixture models. Furthermore, we show that truncated\nvariation EM naturally interpolates between standard EM with full posteriors\nand EM based on the maximum a-posteriori state (MAP). The approach can,\ntherefore, be regarded as a generalization of the popular `hard EM' approach\ntowards a similarly efficient method which can capture more of the true\nposterior structure.\n",
        "published": "2016-10-10T22:11:43Z",
        "pdf_link": "http://arxiv.org/pdf/1610.03113v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.03276v1",
        "title": "Assisted Dictionary Learning for fMRI Data Analysis",
        "summary": "  Extracting information from functional magnetic resonance (fMRI) images has\nbeen a major area of research for more than two decades. The goal of this work\nis to present a new method for the analysis of fMRI data sets, that is capable\nto incorporate a priori available information, via an efficient optimization\nframework. Tests on synthetic data sets demonstrate significant performance\ngains over existing methods of this kind.\n",
        "published": "2016-10-11T11:06:28Z",
        "pdf_link": "http://arxiv.org/pdf/1610.03276v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.03425v3",
        "title": "Statistics of Robust Optimization: A Generalized Empirical Likelihood\n  Approach",
        "summary": "  We study statistical inference and distributionally robust solution methods\nfor stochastic optimization problems, focusing on confidence intervals for\noptimal values and solutions that achieve exact coverage asymptotically. We\ndevelop a generalized empirical likelihood framework---based on distributional\nuncertainty sets constructed from nonparametric $f$-divergence balls---for\nHadamard differentiable functionals, and in particular, stochastic optimization\nproblems. As consequences of this theory, we provide a principled method for\nchoosing the size of distributional uncertainty regions to provide one- and\ntwo-sided confidence intervals that achieve exact coverage. We also give an\nasymptotic expansion for our distributionally robust formulation, showing how\nrobustification regularizes problems by their variance. Finally, we show that\noptimizers of the distributionally robust formulations we study enjoy\n(essentially) the same consistency properties as those in classical sample\naverage approximations. Our general approach applies to quickly mixing\nstationary sequences, including geometrically ergodic Harris recurrent Markov\nchains.\n",
        "published": "2016-10-11T17:01:15Z",
        "pdf_link": "http://arxiv.org/pdf/1610.03425v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.04181v6",
        "title": "Removal of Batch Effects using Distribution-Matching Residual Networks",
        "summary": "  Sources of variability in experimentally derived data include measurement\nerror in addition to the physical phenomena of interest. This measurement error\nis a combination of systematic components, originating from the measuring\ninstrument, and random measurement errors. Several novel biological\ntechnologies, such as mass cytometry and single-cell RNA-seq, are plagued with\nsystematic errors that may severely affect statistical analysis if the data is\nnot properly calibrated. We propose a novel deep learning approach for removing\nsystematic batch effects. Our method is based on a residual network, trained to\nminimize the Maximum Mean Discrepancy (MMD) between the multivariate\ndistributions of two replicates, measured in different batches. We apply our\nmethod to mass cytometry and single-cell RNA-seq datasets, and demonstrate that\nit effectively attenuates batch effects.\n",
        "published": "2016-10-13T17:14:33Z",
        "pdf_link": "http://arxiv.org/pdf/1610.04181v6"
    },
    {
        "id": "http://arxiv.org/abs/1610.04751v1",
        "title": "Unsupervised clustering under the Union of Polyhedral Cones (UOPC) model",
        "summary": "  In this paper, we consider clustering data that is assumed to come from one\nof finitely many pointed convex polyhedral cones. This model is referred to as\nthe Union of Polyhedral Cones (UOPC) model. Similar to the Union of Subspaces\n(UOS) model where each data from each subspace is generated from a (unknown)\nbasis, in the UOPC model each data from each cone is assumed to be generated\nfrom a finite number of (unknown) \\emph{extreme rays}.To cluster data under\nthis model, we consider several algorithms - (a) Sparse Subspace Clustering by\nNon-negative constraints Lasso (NCL), (b) Least squares approximation (LSA),\nand (c) K-nearest neighbor (KNN) algorithm to arrive at affinity between data\npoints. Spectral Clustering (SC) is then applied on the resulting affinity\nmatrix to cluster data into different polyhedral cones. We show that on an\naverage KNN outperforms both NCL and LSA and for this algorithm we provide the\ndeterministic conditions for correct clustering. For an affinity measure\nbetween the cones it is shown that as long as the cones are not very coherent\nand as long as the density of data within each cone exceeds a threshold, KNN\nleads to accurate clustering. Finally, simulation results on real datasets\n(MNIST and YaleFace datasets) depict that the proposed algorithm works well on\nreal data indicating the utility of the UOPC model and the proposed algorithm.\n",
        "published": "2016-10-15T16:04:49Z",
        "pdf_link": "http://arxiv.org/pdf/1610.04751v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.04798v1",
        "title": "Communication-efficient Distributed Sparse Linear Discriminant Analysis",
        "summary": "  We propose a communication-efficient distributed estimation method for sparse\nlinear discriminant analysis (LDA) in the high dimensional regime. Our method\ndistributes the data of size $N$ into $m$ machines, and estimates a local\nsparse LDA estimator on each machine using the data subset of size $N/m$. After\nthe distributed estimation, our method aggregates the debiased local estimators\nfrom $m$ machines, and sparsifies the aggregated estimator. We show that the\naggregated estimator attains the same statistical rate as the centralized\nestimation method, as long as the number of machines $m$ is chosen\nappropriately. Moreover, we prove that our method can attain the model\nselection consistency under a milder condition than the centralized method.\nExperiments on both synthetic and real datasets corroborate our theory.\n",
        "published": "2016-10-15T23:39:45Z",
        "pdf_link": "http://arxiv.org/pdf/1610.04798v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.04811v2",
        "title": "Estimation of low rank density matrices by Pauli measurements",
        "summary": "  Density matrices are positively semi-definite Hermitian matrices with unit\ntrace that describe the states of quantum systems. Many quantum systems of\nphysical interest can be represented as high-dimensional low rank density\nmatrices. A popular problem in {\\it quantum state tomography} (QST) is to\nestimate the unknown low rank density matrix of a quantum system by conducting\nPauli measurements. Our main contribution is twofold. First, we establish the\nminimax lower bounds in Schatten $p$-norms with $1\\leq p\\leq +\\infty$ for low\nrank density matrices estimation by Pauli measurements. In our previous paper,\nthese minimax lower bounds are proved under the trace regression model with\nGaussian noise and the noise is assumed to have common variance. In this paper,\nwe prove these bounds under the Binomial observation model which meets the\nactual model in QST.\n  Second, we study the Dantzig estimator (DE) for estimating the unknown low\nrank density matrix under the Binomial observation model by using Pauli\nmeasurements. In our previous papers, we studied the least squares estimator\nand the projection estimator, where we proved the optimal convergence rates for\nthe least squares estimator in Schatten $p$-norms with $1\\leq p\\leq 2$ and,\nunder a stronger condition, the optimal convergence rates for the projection\nestimator in Schatten $p$-norms with $1\\leq p\\leq +\\infty$. In this paper, we\nshow that the results of these two distinct estimators can be simultaneously\nobtained by the Dantzig estimator. Moreover, better convergence rates in\nSchatten norm distances can be proved for Dantzig estimator under conditions\nweaker than those needed in previous papers. When the objective function of DE\nis replaced by the negative von Neumann entropy, we obtain sharp convergence\nrate in Kullback-Leibler divergence.\n",
        "published": "2016-10-16T02:28:08Z",
        "pdf_link": "http://arxiv.org/pdf/1610.04811v2"
    },
    {
        "id": "http://arxiv.org/abs/1610.05163v1",
        "title": "Spatio-temporal Gaussian processes modeling of dynamical systems in\n  systems biology",
        "summary": "  Quantitative modeling of post-transcriptional regulation process is a\nchallenging problem in systems biology. A mechanical model of the regulatory\nprocess needs to be able to describe the available spatio-temporal protein\nconcentration and mRNA expression data and recover the continuous\nspatio-temporal fields. Rigorous methods are required to identify model\nparameters. A promising approach to deal with these difficulties is proposed\nusing Gaussian process as a prior distribution over the latent function of\nprotein concentration and mRNA expression. In this study, we consider a partial\ndifferential equation mechanical model with differential operators and latent\nfunction. Since the operators at stake are linear, the information from the\nphysical model can be encoded into the kernel function. Hybrid Monte Carlo\nmethods are employed to carry out Bayesian inference of the partial\ndifferential equation parameters and Gaussian process kernel parameters. The\nspatio-temporal field of protein concentration and mRNA expression are\nreconstructed without explicitly solving the partial differential equation.\n",
        "published": "2016-10-17T15:25:56Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05163v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.05247v1",
        "title": "Black-box Importance Sampling",
        "summary": "  Importance sampling is widely used in machine learning and statistics, but\nits power is limited by the restriction of using simple proposals for which the\nimportance weights can be tractably calculated. We address this problem by\nstudying black-box importance sampling methods that calculate importance\nweights for samples generated from any unknown proposal or black-box mechanism.\nOur method allows us to use better and richer proposals to solve difficult\nproblems, and (somewhat counter-intuitively) also has the additional benefit of\nimproving the estimation accuracy beyond typical importance sampling. Both\ntheoretical and empirical analyses are provided.\n",
        "published": "2016-10-17T18:24:30Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05247v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.05275v1",
        "title": "A Unified Computational and Statistical Framework for Nonconvex Low-Rank\n  Matrix Estimation",
        "summary": "  We propose a unified framework for estimating low-rank matrices through\nnonconvex optimization based on gradient descent algorithm. Our framework is\nquite general and can be applied to both noisy and noiseless observations. In\nthe general case with noisy observations, we show that our algorithm is\nguaranteed to linearly converge to the unknown low-rank matrix up to minimax\noptimal statistical error, provided an appropriate initial estimator. While in\nthe generic noiseless setting, our algorithm converges to the unknown low-rank\nmatrix at a linear rate and enables exact recovery with optimal sample\ncomplexity. In addition, we develop a new initialization algorithm to provide a\ndesired initial estimator, which outperforms existing initialization algorithms\nfor nonconvex low-rank matrix estimation. We illustrate the superiority of our\nframework through three examples: matrix regression, matrix completion, and\none-bit matrix completion. We also corroborate our theory through extensive\nexperiments on synthetic data.\n",
        "published": "2016-10-17T19:16:39Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05275v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.05392v3",
        "title": "AutoGP: Exploring the Capabilities and Limitations of Gaussian Process\n  Models",
        "summary": "  We investigate the capabilities and limitations of Gaussian process models by\njointly exploring three complementary directions: (i) scalable and\nstatistically efficient inference; (ii) flexible kernels; and (iii) objective\nfunctions for hyperparameter learning alternative to the marginal likelihood.\nOur approach outperforms all previously reported GP methods on the standard\nMNIST dataset; performs comparatively to previous kernel-based methods using\nthe RECTANGLES-IMAGE dataset; and breaks the 1% error-rate barrier in GP models\nusing the MNIST8M dataset, showing along the way the scalability of our method\nat unprecedented scale for GP models (8 million observations) in classification\nproblems. Overall, our approach represents a significant breakthrough in kernel\nmethods and GP models, bridging the gap between deep learning approaches and\nkernel machines.\n",
        "published": "2016-10-18T01:09:19Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05392v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.05950v1",
        "title": "Consistent Kernel Mean Estimation for Functions of Random Variables",
        "summary": "  We provide a theoretical foundation for non-parametric estimation of\nfunctions of random variables using kernel mean embeddings. We show that for\nany continuous function $f$, consistent estimators of the mean embedding of a\nrandom variable $X$ lead to consistent estimators of the mean embedding of\n$f(X)$. For Mat\\'ern kernels and sufficiently smooth functions we also provide\nrates of convergence. Our results extend to functions of multiple random\nvariables. If the variables are dependent, we require an estimator of the mean\nembedding of their joint distribution as a starting point; if they are\nindependent, it is sufficient to have separate estimators of the mean\nembeddings of their marginal distributions. In either case, our results cover\nboth mean embeddings based on i.i.d. samples as well as \"reduced set\"\nexpansions in terms of dependent expansion points. The latter serves as a\njustification for using such expansions to limit memory resources when applying\nthe approach as a basis for probabilistic programming.\n",
        "published": "2016-10-19T10:23:55Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05950v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.05956v1",
        "title": "Clustering by connection center evolution",
        "summary": "  The determination of cluster centers generally depends on the scale that we\nuse to analyze the data to be clustered. Inappropriate scale usually leads to\nunreasonable cluster centers and thus unreasonable results. In this study, we\nfirst consider the similarity of elements in the data as the connectivity of\nnodes in an undirected graph, then present the concept of a connection center\nand regard it as the cluster center of the data. Based on this definition, the\ndetermination of cluster centers and the assignment of class are very simple,\nnatural and effective. One more crucial finding is that the cluster centers of\ndifferent scales can be obtained easily by the different powers of a similarity\nmatrix and the change of power from small to large leads to the dynamic\nevolution of cluster centers from local (microscopic) to global (microscopic).\nFurther, in this process of evolution, the number of categories changes\ndiscontinuously, which means that the presented method can automatically skip\nthe unreasonable number of clusters, suggest appropriate observation scales and\nprovide corresponding cluster results.\n",
        "published": "2016-10-19T10:52:07Z",
        "pdf_link": "http://arxiv.org/pdf/1610.05956v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.06194v3",
        "title": "Robust and Parallel Bayesian Model Selection",
        "summary": "  Effective and accurate model selection is an important problem in modern data\nanalysis. One of the major challenges is the computational burden required to\nhandle large data sets that cannot be stored or processed on one machine.\nAnother challenge one may encounter is the presence of outliers and\ncontaminations that damage the inference quality. The parallel \"divide and\nconquer\" model selection strategy divides the observations of the full data set\ninto roughly equal subsets and perform inference and model selection\nindependently on each subset. After local subset inference, this method\naggregates the posterior model probabilities or other model/variable selection\ncriteria to obtain a final model by using the notion of geometric median. This\napproach leads to improved concentration in finding the \"correct\" model and\nmodel parameters and also is provably robust to outliers and data\ncontamination.\n",
        "published": "2016-10-19T20:09:51Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06194v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.06235v1",
        "title": "Enhancing ICA Performance by Exploiting Sparsity: Application to FMRI\n  Analysis",
        "summary": "  Independent component analysis (ICA) is a powerful method for blind source\nseparation based on the assumption that sources are statistically independent.\nThough ICA has proven useful and has been employed in many applications,\ncomplete statistical independence can be too restrictive an assumption in\npractice. Additionally, important prior information about the data, such as\nsparsity, is usually available. Sparsity is a natural property of the data, a\nform of diversity, which, if incorporated into the ICA model, can relax the\nindependence assumption, resulting in an improvement in the overall separation\nperformance. In this work, we propose a new variant of ICA by entropy bound\nminimization (ICA-EBM)-a flexible, yet parameter-free algorithm-through the\ndirect exploitation of sparsity. Using this new SparseICA-EBM algorithm, we\nstudy the synergy of independence and sparsity through simulations on synthetic\nas well as functional magnetic resonance imaging (fMRI)-like data.\n",
        "published": "2016-10-19T21:53:07Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06235v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.06545v4",
        "title": "Revisiting Classifier Two-Sample Tests",
        "summary": "  The goal of two-sample tests is to assess whether two samples, $S_P \\sim P^n$\nand $S_Q \\sim Q^m$, are drawn from the same distribution. Perhaps intriguingly,\none relatively unexplored method to build two-sample tests is the use of binary\nclassifiers. In particular, construct a dataset by pairing the $n$ examples in\n$S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a\nnegative label. If the null hypothesis \"$P = Q$\" is true, then the\nclassification accuracy of a binary classifier on a held-out subset of this\ndataset should remain near chance-level. As we will show, such Classifier\nTwo-Sample Tests (C2ST) learn a suitable representation of the data on the fly,\nreturn test statistics in interpretable units, have a simple null distribution,\nand their predictive uncertainty allow to interpret where $P$ and $Q$ differ.\nThe goal of this paper is to establish the properties, performance, and uses of\nC2ST. First, we analyze their main theoretical properties. Second, we compare\ntheir performance against a variety of state-of-the-art alternatives. Third, we\npropose their use to evaluate the sample quality of generative models with\nintractable likelihoods, such as Generative Adversarial Networks (GANs).\nFourth, we showcase the novel application of GANs together with C2ST for causal\ndiscovery.\n",
        "published": "2016-10-20T19:16:10Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06545v4"
    },
    {
        "id": "http://arxiv.org/abs/1610.06665v1",
        "title": "On the Convergence of Stochastic Gradient MCMC Algorithms with\n  High-Order Integrators",
        "summary": "  Recent advances in Bayesian learning with large-scale data have witnessed\nemergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic\ngradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC\n(SGHMC), and the stochastic gradient thermostat. While finite-time convergence\nproperties of the SGLD with a 1st-order Euler integrator have recently been\nstudied, corresponding theory for general SG-MCMCs has not been explored. In\nthis paper we consider general SG-MCMCs with high-order integrators, and\ndevelop theory to analyze finite-time convergence properties and their\nasymptotic invariant measures. Our theoretical results show faster convergence\nrates and more accurate invariant measures for SG-MCMCs with higher-order\nintegrators. For example, with the proposed efficient 2nd-order symmetric\nsplitting integrator, the {\\em mean square error} (MSE) of the posterior\naverage for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$\niterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler\nintegrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs\nare also developed, with the same convergence rates as their fixed-step-size\ncounterparts for a specific decreasing sequence. Experiments on both synthetic\nand real datasets verify our theory, and show advantages of the proposed method\nin two large-scale real applications.\n",
        "published": "2016-10-21T04:28:15Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06665v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.06902v1",
        "title": "Dictionary Learning Strategies for Compressed Fiber Sensing Using a\n  Probabilistic Sparse Model",
        "summary": "  We present a sparse estimation and dictionary learning framework for\ncompressed fiber sensing based on a probabilistic hierarchical sparse model. To\nhandle severe dictionary coherence, selective shrinkage is achieved using a\nWeibull prior, which can be related to non-convex optimization with $p$-norm\nconstraints for $0 < p < 1$. In addition, we leverage the specific dictionary\nstructure to promote collective shrinkage based on a local similarity model.\nThis is incorporated in form of a kernel function in the joint prior density of\nthe sparse coefficients, thereby establishing a Markov random field-relation.\nApproximate inference is accomplished using a hybrid technique that combines\nHamilton Monte Carlo and Gibbs sampling. To estimate the dictionary parameter,\nwe pursue two strategies, relying on either a deterministic or a probabilistic\nmodel for the dictionary parameter. In the first strategy, the parameter is\nestimated based on alternating estimation. In the second strategy, it is\njointly estimated along with the sparse coefficients. The performance is\nevaluated in comparison to an existing method in various scenarios using\nsimulations and experimental data.\n",
        "published": "2016-10-21T19:27:48Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06902v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.06949v1",
        "title": "Mean-Field Variational Inference for Gradient Matching with Gaussian\n  Processes",
        "summary": "  Gradient matching with Gaussian processes is a promising tool for learning\nparameters of ordinary differential equations (ODE's). The essence of gradient\nmatching is to model the prior over state variables as a Gaussian process which\nimplies that the joint distribution given the ODE's and GP kernels is also\nGaussian distributed. The state-derivatives are integrated out analytically\nsince they are modelled as latent variables. However, the state variables\nthemselves are also latent variables because they are contaminated by noise.\nPrevious work sampled the state variables since integrating them out is\n\\textit{not} analytically tractable. In this paper we use mean-field\napproximation to establish tight variational lower bounds that decouple state\nvariables and are therefore, in contrast to the integral over state variables,\nanalytically tractable and even concave for a restricted family of ODE's,\nincluding nonlinear and periodic ODE's. Such variational lower bounds\nfacilitate \"hill climbing\" to determine the maximum a posteriori estimate of\nODE parameters. An additional advantage of our approach over sampling methods\nis the determination of a proxy to the intractable posterior distribution over\nstate variables given observations and the ODE's.\n",
        "published": "2016-10-21T20:50:47Z",
        "pdf_link": "http://arxiv.org/pdf/1610.06949v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.07104v1",
        "title": "Independent Component Analysis by Entropy Maximization with Kernels",
        "summary": "  Independent component analysis (ICA) is the most popular method for blind\nsource separation (BSS) with a diverse set of applications, such as biomedical\nsignal processing, video and image analysis, and communications. Maximum\nlikelihood (ML), an optimal theoretical framework for ICA, requires knowledge\nof the true underlying probability density function (PDF) of the latent\nsources, which, in many applications, is unknown. ICA algorithms cast in the ML\nframework often deviate from its theoretical optimality properties due to poor\nestimation of the source PDF. Therefore, accurate estimation of source PDFs is\ncritical in order to avoid model mismatch and poor ICA performance. In this\npaper, we propose a new and efficient ICA algorithm based on entropy\nmaximization with kernels, (ICA-EMK), which uses both global and local\nmeasuring functions as constraints to dynamically estimate the PDF of the\nsources with reasonable complexity. In addition, the new algorithm performs\noptimization with respect to each of the cost function gradient directions\nseparately, enabling parallel implementations on multi-core computers. We\ndemonstrate the superior performance of ICA-EMK over competing ICA algorithms\nusing simulated as well as real-world data.\n",
        "published": "2016-10-22T23:08:01Z",
        "pdf_link": "http://arxiv.org/pdf/1610.07104v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.07216v2",
        "title": "Inertial Regularization and Selection (IRS): Sequential Regression in\n  High-Dimension and Sparsity",
        "summary": "  In this paper, we develop a new sequential regression modeling approach for\ndata streams. Data streams are commonly found around us, e.g in a retail\nenterprise sales data is continuously collected every day. A demand forecasting\nmodel is an important outcome from the data that needs to be continuously\nupdated with the new incoming data. The main challenge in such modeling arises\nwhen there is a) high dimensional and sparsity, b) need for an adaptive use of\nprior knowledge, and/or c) structural changes in the system. The proposed\napproach addresses these challenges by incorporating an adaptive L1-penalty and\ninertia terms in the loss function, and thus called Inertial Regularization and\nSelection (IRS). The former term performs model selection to handle the first\nchallenge while the latter is shown to address the last two challenges. A\nrecursive estimation algorithm is developed, and shown to outperform the\ncommonly used state-space models, such as Kalman Filters, in experimental\nstudies and real data.\n",
        "published": "2016-10-23T18:43:44Z",
        "pdf_link": "http://arxiv.org/pdf/1610.07216v2"
    },
    {
        "id": "http://arxiv.org/abs/1610.07262v2",
        "title": "Bayesian Nonparametric Modeling of Heterogeneous Groups of Censored Data",
        "summary": "  Datasets containing large samples of time-to-event data arising from several\nsmall heterogeneous groups are commonly encountered in statistics. This\npresents problems as they cannot be pooled directly due to their heterogeneity\nor analyzed individually because of their small sample size. Bayesian\nnonparametric modelling approaches can be used to model such datasets given\ntheir ability to flexibly share information across groups. In this paper, we\nwill compare three popular Bayesian nonparametric methods for modelling the\nsurvival functions of heterogeneous groups. Specifically, we will first compare\nthe modelling accuracy of the Dirichlet process, the hierarchical Dirichlet\nprocess, and the nested Dirichlet process on simulated datasets of different\nsizes, where group survival curves differ in shape or in expectation. We, then,\nwill compare the models on a real-world injury dataset.\n",
        "published": "2016-10-24T01:57:59Z",
        "pdf_link": "http://arxiv.org/pdf/1610.07262v2"
    },
    {
        "id": "http://arxiv.org/abs/1610.07407v5",
        "title": "C-mix: a high dimensional mixture model for censored durations, with\n  applications to genetic data",
        "summary": "  We introduce a mixture model for censored durations (C-mix), and develop\nmaximum likelihood inference for the joint estimation of the time distributions\nand latent regression parameters of the model. We consider a high-dimensional\nsetting, with datasets containing a large number of biomedical covariates. We\ntherefore penalize the negative log-likelihood by the Elastic-Net, which leads\nto a sparse parameterization of the model. Inference is achieved using an\nefficient Quasi-Newton Expectation Maximization (QNEM) algorithm, for which we\nprovide convergence properties. We then propose a score by assessing the\npatients risk of early adverse event. The statistical performance of the method\nis examined on an extensive Monte Carlo simulation study, and finally\nillustrated on three genetic datasets with high-dimensional covariates. We show\nthat our approach outperforms the state-of-the-art, namely both the CURE and\nCox proportional hazards models for this task, both in terms of C-index and\nAUC(t).\n",
        "published": "2016-10-24T13:40:26Z",
        "pdf_link": "http://arxiv.org/pdf/1610.07407v5"
    },
    {
        "id": "http://arxiv.org/abs/1610.08035v4",
        "title": "Parallelizable sparse inverse formulation Gaussian processes (SpInGP)",
        "summary": "  We propose a parallelizable sparse inverse formulation Gaussian process\n(SpInGP) for temporal models. It uses a sparse precision GP formulation and\nsparse matrix routines to speed up the computations. Due to the state-space\nformulation used in the algorithm, the time complexity of the basic SpInGP is\nlinear, and because all the computations are parallelizable, the parallel form\nof the algorithm is sublinear in the number of data points. We provide example\nalgorithms to implement the sparse matrix routines and experimentally test the\nmethod using both simulated and real data.\n",
        "published": "2016-10-25T19:39:35Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08035v4"
    },
    {
        "id": "http://arxiv.org/abs/1610.08074v1",
        "title": "Gaussian Process Kernels for Popular State-Space Time Series Models",
        "summary": "  In this paper we investigate a link between state- space models and Gaussian\nProcesses (GP) for time series modeling and forecasting. In particular, several\nwidely used state- space models are transformed into continuous time form and\ncorresponding Gaussian Process kernels are derived. Experimen- tal results\ndemonstrate that the derived GP kernels are correct and appropriate for\nGaussian Process Regression. An experiment with a real world dataset shows that\nthe modeling is identical with state-space models and with the proposed GP\nkernels. The considered connection allows the researchers to look at their\nmodels from a different angle and facilitate sharing ideas between these two\ndifferent modeling approaches.\n",
        "published": "2016-10-25T20:09:42Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08074v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.08189v1",
        "title": "Tensor Decompositions for Identifying Directed Graph Topologies and\n  Tracking Dynamic Networks",
        "summary": "  Directed networks are pervasive both in nature and engineered systems, often\nunderlying the complex behavior observed in biological systems, microblogs and\nsocial interactions over the web, as well as global financial markets. Since\ntheir structures are often unobservable, in order to facilitate network\nanalytics, one generally resorts to approaches capitalizing on measurable nodal\nprocesses to infer the unknown topology. Structural equation models (SEMs) are\ncapable of incorporating exogenous inputs to resolve inherent directional\nambiguities. However, conventional SEMs assume full knowledge of exogenous\ninputs, which may not be readily available in some practical settings. The\npresent paper advocates a novel SEM-based topology inference approach that\nentails factorization of a three-way tensor, constructed from the observed\nnodal data, using the well-known parallel factor (PARAFAC) decomposition. It\nturns out that second-order piecewise stationary statistics of exogenous\nvariables suffice to identify the hidden topology. Capitalizing on the\nuniqueness properties inherent to high-order tensor factorizations, it is shown\nthat topology identification is possible under reasonably mild conditions. In\naddition, to facilitate real-time operation and inference of time-varying\nnetworks, an adaptive (PARAFAC) tensor decomposition scheme which tracks the\ntopology-revealing tensor factors is developed. Extensive tests on simulated\nand real stock quote data demonstrate the merits of the novel tensor-based\napproach.\n",
        "published": "2016-10-26T06:12:17Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08189v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.08466v1",
        "title": "Recurrent switching linear dynamical systems",
        "summary": "  Many natural systems, such as neurons firing in the brain or basketball teams\ntraversing a court, give rise to time series data with complex, nonlinear\ndynamics. We can gain insight into these systems by decomposing the data into\nsegments that are each explained by simpler dynamic units. Building on\nswitching linear dynamical systems (SLDS), we present a new model class that\nnot only discovers these dynamical units, but also explains how their switching\nbehavior depends on observations or continuous latent states. These \"recurrent\"\nswitching linear dynamical systems provide further insight by discovering the\nconditions under which each unit is deployed, something that traditional SLDS\nmodels fail to do. We leverage recent algorithmic advances in approximate\ninference to make Bayesian inference in these models easy, fast, and scalable.\n",
        "published": "2016-10-26T19:08:04Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08466v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.08623v3",
        "title": "Poisson intensity estimation with reproducing kernels",
        "summary": "  Despite the fundamental nature of the inhomogeneous Poisson process in the\ntheory and application of stochastic processes, and its attractive\ngeneralizations (e.g. Cox process), few tractable nonparametric modeling\napproaches of intensity functions exist, especially when observed points lie in\na high-dimensional space. In this paper we develop a new, computationally\ntractable Reproducing Kernel Hilbert Space (RKHS) formulation for the\ninhomogeneous Poisson process. We model the square root of the intensity as an\nRKHS function. Whereas RKHS models used in supervised learning rely on the\nso-called representer theorem, the form of the inhomogeneous Poisson process\nlikelihood means that the representer theorem does not apply. However, we prove\nthat the representer theorem does hold in an appropriately transformed RKHS,\nguaranteeing that the optimization of the penalized likelihood can be cast as a\ntractable finite-dimensional problem. The resulting approach is simple to\nimplement, and readily scales to high dimensions and large-scale datasets.\n",
        "published": "2016-10-27T05:39:03Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08623v3"
    },
    {
        "id": "http://arxiv.org/abs/1610.08637v4",
        "title": "Statistical Inference for Model Parameters in Stochastic Gradient\n  Descent",
        "summary": "  The stochastic gradient descent (SGD) algorithm has been widely used in\nstatistical estimation for large-scale data due to its computational and memory\nefficiency. While most existing works focus on the convergence of the objective\nfunction or the error of the obtained solution, we investigate the problem of\nstatistical inference of true model parameters based on SGD when the population\nloss function is strongly convex and satisfies certain smoothness conditions.\nOur main contributions are two-fold. First, in the fixed dimension setup, we\npropose two consistent estimators of the asymptotic covariance of the average\niterate from SGD: (1) a plug-in estimator, and (2) a batch-means estimator,\nwhich is computationally more efficient and only uses the iterates from SGD.\nBoth proposed estimators allow us to construct asymptotically exact confidence\nintervals and hypothesis tests. Second, for high-dimensional linear regression,\nusing a variant of the SGD algorithm, we construct a debiased estimator of each\nregression coefficient that is asymptotically normal. This gives a one-pass\nalgorithm for computing both the sparse regression coefficients and confidence\nintervals, which is computationally attractive and applicable to online data.\n",
        "published": "2016-10-27T07:04:21Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08637v4"
    },
    {
        "id": "http://arxiv.org/abs/1610.08733v1",
        "title": "GPflow: A Gaussian process library using TensorFlow",
        "summary": "  GPflow is a Gaussian process library that uses TensorFlow for its core\ncomputations and Python for its front end. The distinguishing features of\nGPflow are that it uses variational inference as the primary approximation\nmethod, provides concise code through the use of automatic differentiation, has\nbeen engineered with a particular emphasis on software testing and is able to\nexploit GPU hardware.\n",
        "published": "2016-10-27T12:08:10Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08733v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.08813v1",
        "title": "Sparse Signal Subspace Decomposition Based on Adaptive Over-complete\n  Dictionary",
        "summary": "  This paper proposes a subspace decomposition method based on an over-complete\ndictionary in sparse representation, called \"Sparse Signal Subspace\nDecomposition\" (or 3SD) method. This method makes use of a novel criterion\nbased on the occurrence frequency of atoms of the dictionary over the data set.\nThis criterion, well adapted to subspace-decomposition over a dependent basis\nset, adequately re ects the intrinsic characteristic of regularity of the\nsignal. The 3SD method combines variance, sparsity and component frequency\ncriteria into an unified framework. It takes benefits from using an\nover-complete dictionary which preserves details and from subspace\ndecomposition which rejects strong noise. The 3SD method is very simple with a\nlinear retrieval operation. It does not require any prior knowledge on\ndistributions or parameters. When applied to image denoising, it demonstrates\nhigh performances both at preserving fine details and suppressing strong noise.\n",
        "published": "2016-10-27T14:45:47Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08813v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.08928v1",
        "title": "Rapid Posterior Exploration in Bayesian Non-negative Matrix\n  Factorization",
        "summary": "  Non-negative Matrix Factorization (NMF) is a popular tool for data\nexploration. Bayesian NMF promises to also characterize uncertainty in the\nfactorization. Unfortunately, current inference approaches such as MCMC mix\nslowly and tend to get stuck on single modes. We introduce a novel approach\nusing rapidly-exploring random trees (RRTs) to asymptotically cover regions of\nhigh posterior density. These are placed in a principled Bayesian framework via\nan online extension to nonparametric variational inference. On experiments on\nreal and synthetic data, we obtain greater coverage of the posterior and higher\nELBO values than standard NMF inference approaches.\n",
        "published": "2016-10-27T19:00:22Z",
        "pdf_link": "http://arxiv.org/pdf/1610.08928v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.09034v1",
        "title": "Geometric Dirichlet Means algorithm for topic inference",
        "summary": "  We propose a geometric algorithm for topic learning and inference that is\nbuilt on the convex geometry of topics arising from the Latent Dirichlet\nAllocation (LDA) model and its nonparametric extensions. To this end we study\nthe optimization of a geometric loss function, which is a surrogate to the\nLDA's likelihood. Our method involves a fast optimization based weighted\nclustering procedure augmented with geometric corrections, which overcomes the\ncomputational and statistical inefficiencies encountered by other techniques\nbased on Gibbs sampling and variational inference, while achieving the accuracy\ncomparable to that of a Gibbs sampler. The topic estimates produced by our\nmethod are shown to be statistically consistent under some conditions. The\nalgorithm is evaluated with extensive experiments on simulated and real data.\n",
        "published": "2016-10-27T23:35:57Z",
        "pdf_link": "http://arxiv.org/pdf/1610.09034v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.09490v1",
        "title": "A general multiblock method for structured variable selection",
        "summary": "  Regularised canonical correlation analysis was recently extended to more than\ntwo sets of variables by the multiblock method Regularised generalised\ncanonical correlation analysis (RGCCA). Further, Sparse GCCA (SGCCA) was\nproposed to address the issue of variable selection. However, for technical\nreasons, the variable selection offered by SGCCA was restricted to a covariance\nlink between the blocks (i.e., with $\\tau=1$). One of the main contributions of\nthis paper is to go beyond the covariance link and to propose an extension of\nSGCCA for the full RGCCA model (i.e., with $\\tau\\in[0, 1]$). In addition, we\npropose an extension of SGCCA that exploits structural relationships between\nvariables within blocks. Specifically, we propose an algorithm that allows\nstructured and sparsity-inducing penalties to be included in the RGCCA\noptimisation problem. The proposed multiblock method is illustrated on a real\nthree-block high-grade glioma data set, where the aim is to predict the\nlocation of the brain tumours, and on a simulated data set, where the aim is to\nillustrate the method's ability to reconstruct the true underlying weight\nvectors.\n",
        "published": "2016-10-29T11:28:56Z",
        "pdf_link": "http://arxiv.org/pdf/1610.09490v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.09600v7",
        "title": "Super-resolution estimation of cyclic arrival rates",
        "summary": "  Exploiting the fact that most arrival processes exhibit cyclic behaviour, we\npropose a simple procedure for estimating the intensity of a nonhomogeneous\nPoisson process. The estimator is the super-resolution analogue to Shao 2010\nand Shao & Lii 2011, which is a sum of $p$ sinusoids where $p$ and the\nfrequency, amplitude, and phase of each wave are not known and need to be\nestimated. This results in an interpretable yet flexible specification that is\nsuitable for use in modelling as well as in high resolution simulations.\n  Our estimation procedure sits in between classic periodogram methods and\natomic/total variation norm thresholding. Through a novel use of window\nfunctions in the point process domain, our approach attains super-resolution\nwithout semidefinite programming. Under suitable conditions, finite sample\nguarantees can be derived for our procedure. These resolve some open questions\nand expand existing results in spectral estimation literature.\n",
        "published": "2016-10-30T03:21:05Z",
        "pdf_link": "http://arxiv.org/pdf/1610.09600v7"
    },
    {
        "id": "http://arxiv.org/abs/1610.09659v1",
        "title": "Exploring and measuring non-linear correlations: Copulas, Lightspeed\n  Transportation and Clustering",
        "summary": "  We propose a methodology to explore and measure the pairwise correlations\nthat exist between variables in a dataset. The methodology leverages copulas\nfor encoding dependence between two variables, state-of-the-art optimal\ntransport for providing a relevant geometry to the copulas, and clustering for\nsummarizing the main dependence patterns found between the variables. Some of\nthe clusters centers can be used to parameterize a novel dependence coefficient\nwhich can target or forget specific dependence patterns. Finally, we illustrate\nand benchmark the methodology on several datasets. Code and numerical\nexperiments are available online for reproducible research.\n",
        "published": "2016-10-30T15:03:51Z",
        "pdf_link": "http://arxiv.org/pdf/1610.09659v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.09838v1",
        "title": "Analysis of Nonstationary Time Series Using Locally Coupled Gaussian\n  Processes",
        "summary": "  The analysis of nonstationary time series is of great importance in many\nscientific fields such as physics and neuroscience. In recent years, Gaussian\nprocess regression has attracted substantial attention as a robust and powerful\nmethod for analyzing time series. In this paper, we introduce a new framework\nfor analyzing nonstationary time series using locally stationary Gaussian\nprocess analysis with parameters that are coupled through a hidden Markov\nmodel. The main advantage of this framework is that arbitrary complex\nnonstationary covariance functions can be obtained by combining simpler\nstationary building blocks whose hidden parameters can be estimated in\nclosed-form. We demonstrate the flexibility of the method by analyzing two\nexamples of synthetic nonstationary signals: oscillations with time varying\nfrequency and time series with two dynamical states. Finally, we report an\nexample application on real magnetoencephalographic measurements of brain\nactivity.\n",
        "published": "2016-10-31T09:24:28Z",
        "pdf_link": "http://arxiv.org/pdf/1610.09838v1"
    },
    {
        "id": "http://arxiv.org/abs/1610.10025v5",
        "title": "Function Driven Diffusion for Personalized Counterfactual Inference",
        "summary": "  We consider the problem of constructing diffusion operators high dimensional\ndata $X$ to address counterfactual functions $F$, such as individualized\ntreatment effectiveness. We propose and construct a new diffusion metric $K_F$\nthat captures both the local geometry of $X$ and the directions of variance of\n$F$. The resulting diffusion metric is then used to define a localized\nfiltration of $F$ and answer counterfactual questions pointwise, particularly\nin situations such as drug trials where an individual patient's outcomes cannot\nbe studied long term both taking and not taking a medication. We validate the\nmodel on synthetic and real world clinical trials, and create individualized\nnotions of benefit from treatment.\n",
        "published": "2016-10-31T17:26:27Z",
        "pdf_link": "http://arxiv.org/pdf/1610.10025v5"
    },
    {
        "id": "http://arxiv.org/abs/1701.00422v2",
        "title": "Towards multiple kernel principal component analysis for integrative\n  analysis of tumor samples",
        "summary": "  Personalized treatment of patients based on tissue-specific cancer subtypes\nhas strongly increased the efficacy of the chosen therapies. Even though the\namount of data measured for cancer patients has increased over the last years,\nmost cancer subtypes are still diagnosed based on individual data sources (e.g.\ngene expression data). We propose an unsupervised data integration method based\non kernel principal component analysis. Principal component analysis is one of\nthe most widely used techniques in data analysis. Unfortunately, the\nstraight-forward multiple-kernel extension of this method leads to the use of\nonly one of the input matrices, which does not fit the goal of gaining\ninformation from all data sources. Therefore, we present a scoring function to\ndetermine the impact of each input matrix. The approach enables visualizing the\nintegrated data and subsequent clustering for cancer subtype identification.\nDue to the nature of the method, no free parameters have to be set. We apply\nthe methodology to five different cancer data sets and demonstrate its\nadvantages in terms of results and usability.\n",
        "published": "2017-01-02T15:37:46Z",
        "pdf_link": "http://arxiv.org/pdf/1701.00422v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.00481v2",
        "title": "Stochastic Variance-reduced Gradient Descent for Low-rank Matrix\n  Recovery from Linear Measurements",
        "summary": "  We study the problem of estimating low-rank matrices from linear measurements\n(a.k.a., matrix sensing) through nonconvex optimization. We propose an\nefficient stochastic variance reduced gradient descent algorithm to solve a\nnonconvex optimization problem of matrix sensing. Our algorithm is applicable\nto both noisy and noiseless settings. In the case with noisy observations, we\nprove that our algorithm converges to the unknown low-rank matrix at a linear\nrate up to the minimax optimal statistical error. And in the noiseless setting,\nour algorithm is guaranteed to linearly converge to the unknown low-rank matrix\nand achieves exact recovery with optimal sample complexity. Most notably, the\noverall computational complexity of our proposed algorithm, which is defined as\nthe iteration complexity times per iteration time complexity, is lower than the\nstate-of-the-art algorithms based on gradient descent. Experiments on synthetic\ndata corroborate the superiority of the proposed algorithm over the\nstate-of-the-art algorithms.\n",
        "published": "2017-01-02T18:58:38Z",
        "pdf_link": "http://arxiv.org/pdf/1701.00481v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.01064v3",
        "title": "Optimal Low-Rank Dynamic Mode Decomposition",
        "summary": "  Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing\nthe dynamics of non-linear systems from experimental datasets. Recently,\nseveral attempts have extended DMD to the context of low-rank approximations.\nThis extension is of particular interest for reduced-order modeling in various\napplicative domains, e.g. for climate prediction, to study molecular dynamics\nor micro-electromechanical devices. This low-rank extension takes the form of a\nnon-convex optimization problem. To the best of our knowledge, only sub-optimal\nalgorithms have been proposed in the literature to compute the solution of this\nproblem. In this paper, we prove that there exists a closed-form optimal\nsolution to this problem and design an effective algorithm to compute it based\non Singular Value Decomposition (SVD). A toy-example illustrates the gain in\nperformance of the proposed algorithm compared to state-of-the-art techniques.\n",
        "published": "2017-01-04T16:24:12Z",
        "pdf_link": "http://arxiv.org/pdf/1701.01064v3"
    },
    {
        "id": "http://arxiv.org/abs/1701.01437v2",
        "title": "NIPS 2016 Workshop on Representation Learning in Artificial and\n  Biological Neural Networks (MLINI 2016)",
        "summary": "  This workshop explores the interface between cognitive neuroscience and\nrecent advances in AI fields that aim to reproduce human performance such as\nnatural language processing and computer vision, and specifically deep learning\napproaches to such problems.\n  When studying the cognitive capabilities of the brain, scientists follow a\nsystem identification approach in which they present different stimuli to the\nsubjects and try to model the response that different brain areas have of that\nstimulus. The goal is to understand the brain by trying to find the function\nthat expresses the activity of brain areas in terms of different properties of\nthe stimulus. Experimental stimuli are becoming increasingly complex with more\nand more people being interested in studying real life phenomena such as the\nperception of natural images or natural sentences. There is therefore a need\nfor a rich and adequate vector representation of the properties of the\nstimulus, that we can obtain using advances in machine learning.\n  In parallel, new ML approaches, many of which in deep learning, are inspired\nto a certain extent by human behavior or biological principles. Neural networks\nfor example were originally inspired by biological neurons. More recently,\nprocesses such as attention are being used which have are inspired by human\nbehavior. However, the large bulk of these methods are independent of findings\nabout brain function, and it is unclear whether it is at all beneficial for\nmachine learning to try to emulate brain function in order to achieve the same\ntasks that the brain achieves.\n",
        "published": "2017-01-06T14:58:34Z",
        "pdf_link": "http://arxiv.org/pdf/1701.01437v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.01582v2",
        "title": "Learning Sparse Structural Changes in High-dimensional Markov Networks:\n  A Review on Methodologies and Theories",
        "summary": "  Recent years have seen an increasing popularity of learning the sparse\n\\emph{changes} in Markov Networks. Changes in the structure of Markov Networks\nreflect alternations of interactions between random variables under different\nregimes and provide insights into the underlying system. While each individual\nnetwork structure can be complicated and difficult to learn, the overall change\nfrom one network to another can be simple. This intuition gave birth to an\napproach that \\emph{directly} learns the sparse changes without modelling and\nlearning the individual (possibly dense) networks. In this paper, we review\nsuch a direct learning method with some latest developments along this line of\nresearch.\n",
        "published": "2017-01-06T09:30:22Z",
        "pdf_link": "http://arxiv.org/pdf/1701.01582v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.02071v1",
        "title": "Optimal statistical decision for Gaussian graphical model selection",
        "summary": "  Gaussian graphical model is a graphical representation of the dependence\nstructure for a Gaussian random vector. It is recognized as a powerful tool in\ndifferent applied fields such as bioinformatics, error-control codes, speech\nlanguage, information retrieval and others. Gaussian graphical model selection\nis a statistical problem to identify the Gaussian graphical model from a sample\nof a given size. Different approaches for Gaussian graphical model selection\nare suggested in the literature. One of them is based on considering the family\nof individual conditional independence tests. The application of this approach\nleads to the construction of a variety of multiple testing statistical\nprocedures for Gaussian graphical model selection. An important characteristic\nof these procedures is its error rate for a given sample size. In existing\nliterature great attention is paid to the control of error rates for incorrect\nedge inclusion (Type I error). However, in graphical model selection it is also\nimportant to take into account error rates for incorrect edge exclusion (Type\nII error). To deal with this issue we consider the graphical model selection\nproblem in the framework of the multiple decision theory. The quality of\nstatistical procedures is measured by a risk function with additive losses.\nAdditive losses allow both types of errors to be taken into account. We\nconstruct the tests of a Neyman structure for individual hypotheses and combine\nthem to obtain a multiple decision statistical procedure. We show that the\nobtained procedure is optimal in the sense that it minimizes the linear\ncombination of expected numbers of Type I and Type II errors in the class of\nunbiased multiple decision procedures.\n",
        "published": "2017-01-09T06:34:00Z",
        "pdf_link": "http://arxiv.org/pdf/1701.02071v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.02301v2",
        "title": "A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank\n  Matrix Recovery",
        "summary": "  We propose a generic framework based on a new stochastic variance-reduced\ngradient descent algorithm for accelerating nonconvex low-rank matrix recovery.\nStarting from an appropriate initial estimator, our proposed algorithm performs\nprojected gradient descent based on a novel semi-stochastic gradient\nspecifically designed for low-rank matrix recovery. Based upon the mild\nrestricted strong convexity and smoothness conditions, we derive a projected\nnotion of the restricted Lipschitz continuous gradient property, and prove that\nour algorithm enjoys linear convergence rate to the unknown low-rank matrix\nwith an improved computational complexity. Moreover, our algorithm can be\nemployed to both noiseless and noisy observations, where the optimal sample\ncomplexity and the minimax optimal statistical rate can be attained\nrespectively. We further illustrate the superiority of our generic framework\nthrough several specific examples, both theoretically and experimentally.\n",
        "published": "2017-01-09T18:56:56Z",
        "pdf_link": "http://arxiv.org/pdf/1701.02301v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.02967v2",
        "title": "A Large Dimensional Analysis of Least Squares Support Vector Machines",
        "summary": "  In this article, a large dimensional performance analysis of kernel least\nsquares support vector machines (LS-SVMs) is provided under the assumption of a\ntwo-class Gaussian mixture model for the input data. Building upon recent\nadvances in random matrix theory, we show, when the dimension of data $p$ and\ntheir number $n$ are both large, that the LS-SVM decision function can be well\napproximated by a normally distributed random variable, the mean and variance\nof which depend explicitly on a local behavior of the kernel function. This\ntheoretical result is then applied to the MNIST and Fashion-MNIST datasets\nwhich, despite their non-Gaussianity, exhibit a convincingly close behavior.\nMost importantly, our analysis provides a deeper understanding of the mechanism\ninto play in SVM-type methods and in particular of the impact on the choice of\nthe kernel function as well as some of their theoretical limits in separating\nhigh dimensional Gaussian vectors.\n",
        "published": "2017-01-11T13:36:34Z",
        "pdf_link": "http://arxiv.org/pdf/1701.02967v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.03755v1",
        "title": "What Can I Do Now? Guiding Users in a World of Automated Decisions",
        "summary": "  More and more processes governing our lives use in some part an automatic\ndecision step, where -- based on a feature vector derived from an applicant --\nan algorithm has the decision power over the final outcome. Here we present a\nsimple idea which gives some of the power back to the applicant by providing\nher with alternatives which would make the decision algorithm decide\ndifferently. It is based on a formalization reminiscent of methods used for\nevasion attacks, and consists in enumerating the subspaces where the\nclassifiers decides the desired output. This has been implemented for the\nspecific case of decision forests (ensemble methods based on decision trees),\nmapping the problem to an iterative version of enumerating $k$-cliques.\n",
        "published": "2017-01-13T17:49:47Z",
        "pdf_link": "http://arxiv.org/pdf/1701.03755v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.04207v1",
        "title": "Sparse Kernel Canonical Correlation Analysis via $\\ell_1$-regularization",
        "summary": "  Canonical correlation analysis (CCA) is a multivariate statistical technique\nfor finding the linear relationship between two sets of variables. The kernel\ngeneralization of CCA named kernel CCA has been proposed to find nonlinear\nrelations between datasets. Despite their wide usage, they have one common\nlimitation that is the lack of sparsity in their solution. In this paper, we\nconsider sparse kernel CCA and propose a novel sparse kernel CCA algorithm\n(SKCCA). Our algorithm is based on a relationship between kernel CCA and least\nsquares. Sparsity of the dual transformations is introduced by penalizing the\n$\\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not\nonly performs well in computing sparse dual transformations but also can\nalleviate the over-fitting problem of kernel CCA.\n",
        "published": "2017-01-16T09:15:12Z",
        "pdf_link": "http://arxiv.org/pdf/1701.04207v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.04342v1",
        "title": "Datenqualität in Regressionsproblemen",
        "summary": "  Regression models are increasingly built using datasets which do not follow a\ndesign of experiment. Instead, the data is e.g. gathered by an automated\nmonitoring of a technical system. As a consequence, already the input data\nrepresents phenomena of the system and violates statistical assumptions of\ndistributions. The input data can show correlations, clusters or other\npatterns. Further, the distribution of input data influences the reliability of\nregression models. We propose criteria to quantify typical phenomena of input\ndata for regression and show their suitability with simulated benchmark\ndatasets.\n  -----\n  Regressionen werden zunehmend auf Datens\\\"atzen angewendet, deren\nEingangsvektoren nicht durch eine statistische Versuchsplanung festgelegt\nwurden. Stattdessen werden die Daten beispielsweise durch die passive\nBeobachtung technischer Systeme gesammelt. Damit bilden bereits die\nEingangsdaten Ph\\\"anomene des Systems ab und widersprechen statistischen\nVerteilungsannahmen. Die Verteilung der Eingangsdaten hat Einfluss auf die\nZuverl\\\"assigkeit eines Regressionsmodells. Wir stellen deshalb\nBewertungskriterien f\\\"ur einige typische Ph\\\"anomene in Eingangsdaten von\nRegressionen vor und zeigen ihre Funktionalit\\\"at anhand simulierter\nBenchmarkdatens\\\"atze.\n",
        "published": "2017-01-16T16:03:14Z",
        "pdf_link": "http://arxiv.org/pdf/1701.04342v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.04532v1",
        "title": "Multi-view Regularized Gaussian Processes",
        "summary": "  Gaussian processes (GPs) have been proven to be powerful tools in various\nareas of machine learning. However, there are very few applications of GPs in\nthe scenario of multi-view learning. In this paper, we present a new GP model\nfor multi-view learning. Unlike existing methods, it combines multiple views by\nregularizing marginal likelihood with the consistency among the posterior\ndistributions of latent functions from different views. Moreover, we give a\ngeneral point selection scheme for multi-view learning and improve the proposed\nmodel by this criterion. Experimental results on multiple real world data sets\nhave verified the effectiveness of the proposed model and witnessed the\nperformance improvement through employing this novel point selection scheme.\n",
        "published": "2017-01-17T05:20:38Z",
        "pdf_link": "http://arxiv.org/pdf/1701.04532v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.05305v2",
        "title": "Random Forest Missing Data Algorithms",
        "summary": "  Random forest (RF) missing data algorithms are an attractive approach for\ndealing with missing data. They have the desirable properties of being able to\nhandle mixed types of missing data, they are adaptive to interactions and\nnonlinearity, and they have the potential to scale to big data settings.\nCurrently there are many different RF imputation algorithms but relatively\nlittle guidance about their efficacy, which motivated us to study their\nperformance. Using a large, diverse collection of data sets, performance of\nvarious RF algorithms was assessed under different missing data mechanisms.\nAlgorithms included proximity imputation, on the fly imputation, and imputation\nutilizing multivariate unsupervised and supervised splitting---the latter class\nrepresenting a generalization of a new promising imputation algorithm called\nmissForest. Performance of algorithms was assessed by ability to impute data\naccurately. Our findings reveal RF imputation to be generally robust with\nperformance improving with increasing correlation. Performance was good under\nmoderate to high missingness, and even (in certain cases) when data was missing\nnot at random.\n",
        "published": "2017-01-19T05:58:05Z",
        "pdf_link": "http://arxiv.org/pdf/1701.05305v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.05306v2",
        "title": "Estimating Individual Treatment Effect in Observational Data Using\n  Random Forest Methods",
        "summary": "  Estimation of individual treatment effect in observational data is\ncomplicated due to the challenges of confounding and selection bias. A useful\ninferential framework to address this is the counterfactual (potential\noutcomes) model which takes the hypothetical stance of asking what if an\nindividual had received both treatments. Making use of random forests (RF)\nwithin the counterfactual framework we estimate individual treatment effects by\ndirectly modeling the response. We find accurate estimation of individual\ntreatment effects is possible even in complex heterogeneous settings but that\nthe type of RF approach plays an important role in accuracy. Methods designed\nto be adaptive to confounding, when used in parallel with out-of-sample\nestimation, do best. One method found to be especially promising is\ncounterfactual synthetic forests. We illustrate this new methodology by\napplying it to a large comparative effectiveness trial, Project Aware, in order\nto explore the role drug use plays in sexual risk. The analysis reveals\nimportant connections between risky behavior, drug usage, and sexual risk.\n",
        "published": "2017-01-19T06:11:14Z",
        "pdf_link": "http://arxiv.org/pdf/1701.05306v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.05672v1",
        "title": "Stability Enhanced Large-Margin Classifier Selection",
        "summary": "  Stability is an important aspect of a classification procedure because\nunstable predictions can potentially reduce users' trust in a classification\nsystem and also harm the reproducibility of scientific conclusions. The major\ngoal of our work is to introduce a novel concept of classification instability,\ni.e., decision boundary instability (DBI), and incorporate it with the\ngeneralization error (GE) as a standard for selecting the most accurate and\nstable classifier. Specifically, we implement a two-stage algorithm: (i)\ninitially select a subset of classifiers whose estimated GEs are not\nsignificantly different from the minimal estimated GE among all the candidate\nclassifiers; (ii) the optimal classifier is chosen as the one achieving the\nminimal DBI among the subset selected in stage (i). This general selection\nprinciple applies to both linear and nonlinear classifiers. Large-margin\nclassifiers are used as a prototypical example to illustrate the above idea.\nOur selection method is shown to be consistent in the sense that the optimal\nclassifier simultaneously achieves the minimal GE and the minimal DBI. Various\nsimulations and real examples further demonstrate the advantage of our method\nover several alternative approaches.\n",
        "published": "2017-01-20T03:38:57Z",
        "pdf_link": "http://arxiv.org/pdf/1701.05672v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.06508v2",
        "title": "The Impact of Random Models on Clustering Similarity",
        "summary": "  Clustering is a central approach for unsupervised learning. After clustering\nis applied, the most fundamental analysis is to quantitatively compare\nclusterings. Such comparisons are crucial for the evaluation of clustering\nmethods as well as other tasks such as consensus clustering. It is often argued\nthat, in order to establish a baseline, clustering similarity should be\nassessed in the context of a random ensemble of clusterings. The prevailing\nassumption for the random clustering ensemble is the permutation model in which\nthe number and sizes of clusters are fixed. However, this assumption does not\nnecessarily hold in practice; for example, multiple runs of K-means clustering\nreturns clusterings with a fixed number of clusters, while the cluster size\ndistribution varies greatly. Here, we derive corrected variants of two\nclustering similarity measures (the Rand index and Mutual Information) in the\ncontext of two random clustering ensembles in which the number and sizes of\nclusters vary. In addition, we study the impact of one-sided comparisons in the\nscenario with a reference clustering. The consequences of different random\nmodels are illustrated using synthetic examples, handwriting recognition, and\ngene expression data. We demonstrate that the choice of random model can have a\ndrastic impact on the ranking of similar clustering pairs, and the evaluation\nof a clustering method with respect to a random baseline; thus, the choice of\nrandom clustering model should be carefully justified.\n",
        "published": "2017-01-23T17:09:34Z",
        "pdf_link": "http://arxiv.org/pdf/1701.06508v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.06597v1",
        "title": "Iterative Thresholding for Demixing Structured Superpositions in High\n  Dimensions",
        "summary": "  We consider the demixing problem of two (or more) high-dimensional vectors\nfrom nonlinear observations when the number of such observations is far less\nthan the ambient dimension of the underlying vectors. Specifically, we\ndemonstrate an algorithm that stably estimate the underlying components under\ngeneral \\emph{structured sparsity} assumptions on these components.\nSpecifically, we show that for certain types of structured superposition\nmodels, our method provably recovers the components given merely $n =\n\\mathcal{O}(s)$ samples where $s$ denotes the number of nonzero entries in the\nunderlying components. Moreover, our method achieves a fast (linear)\nconvergence rate, and also exhibits fast (near-linear) per-iteration complexity\nfor certain types of structured models. We also provide a range of simulations\nto illustrate the performance of the proposed algorithm.\n",
        "published": "2017-01-23T19:28:30Z",
        "pdf_link": "http://arxiv.org/pdf/1701.06597v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.06607v2",
        "title": "Stable Recovery Of Sparse Vectors From Random Sinusoidal Feature Maps",
        "summary": "  Random sinusoidal features are a popular approach for speeding up\nkernel-based inference in large datasets. Prior to the inference stage, the\napproach suggests performing dimensionality reduction by first multiplying each\ndata vector by a random Gaussian matrix, and then computing an element-wise\nsinusoid. Theoretical analysis shows that collecting a sufficient number of\nsuch features can be reliably used for subsequent inference in kernel\nclassification and regression.\n  In this work, we demonstrate that with a mild increase in the dimension of\nthe embedding, it is also possible to reconstruct the data vector from such\nrandom sinusoidal features, provided that the underlying data is sparse enough.\nIn particular, we propose a numerically stable algorithm for reconstructing the\ndata vector given the nonlinear features, and analyze its sample complexity.\nOur algorithm can be extended to other types of structured inverse problems,\nsuch as demixing a pair of sparse (but incoherent) vectors. We support the\nefficacy of our approach via numerical experiments.\n",
        "published": "2017-01-23T19:55:50Z",
        "pdf_link": "http://arxiv.org/pdf/1701.06607v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.06749v1",
        "title": "Robust mixture modelling using sub-Gaussian stable distribution",
        "summary": "  Heavy-tailed distributions are widely used in robust mixture modelling due to\npossessing thick tails. As a computationally tractable subclass of the stable\ndistributions, sub-Gaussian $\\alpha$-stable distribution received much interest\nin the literature. Here, we introduce a type of expectation maximization\nalgorithm that estimates parameters of a mixture of sub-Gaussian stable\ndistributions. A comparative study, in the presence of some well-known mixture\nmodels, is performed to show the robustness and performance of the mixture of\nsub-Gaussian $\\alpha$-stable distributions for modelling, simulated, synthetic,\nand real data.\n",
        "published": "2017-01-24T06:59:26Z",
        "pdf_link": "http://arxiv.org/pdf/1701.06749v1"
    },
    {
        "id": "http://arxiv.org/abs/1701.07920v2",
        "title": "Subset Selection for Multiple Linear Regression via Optimization",
        "summary": "  Subset selection in multiple linear regression aims to choose a subset of\ncandidate explanatory variables that tradeoff fitting error (explanatory power)\nand model complexity (number of variables selected). We build mathematical\nprogramming models for regression subset selection based on mean square and\nabsolute errors, and minimal-redundancy-maximal-relevance criteria. The\nproposed models are tested using a linear-program-based branch-and-bound\nalgorithm with tailored valid inequalities and big M values and are compared\nagainst the algorithms in the literature. For high dimensional cases, an\niterative heuristic algorithm is proposed based on the mathematical programming\nmodels and a core set concept, and a randomized version of the algorithm is\nderived to guarantee convergence to the global optimum. From the computational\nexperiments, we find that our models quickly find a quality solution while the\nrest of the time is spent to prove optimality; the iterative algorithms find\nsolutions in a relatively short time and are competitive compared to\nstate-of-the-art algorithms; using ad-hoc big M values is not recommended.\n",
        "published": "2017-01-27T01:43:10Z",
        "pdf_link": "http://arxiv.org/pdf/1701.07920v2"
    },
    {
        "id": "http://arxiv.org/abs/1701.07926v9",
        "title": "Boosted nonparametric hazards with time-dependent covariates",
        "summary": "  Given functional data from a survival process with time-dependent covariates,\nwe derive a smooth convex representation for its nonparametric log-likelihood\nfunctional and obtain its functional gradient. From this, we devise a generic\ngradient boosting procedure for estimating the hazard function\nnonparametrically. An illustrative implementation of the procedure using\nregression trees is described to show how to recover the unknown hazard. The\ngeneric estimator is consistent if the model is correctly specified;\nalternatively, an oracle inequality can be demonstrated for tree-based models.\nTo avoid overfitting, boosting employs several regularization devices. One of\nthem is step-size restriction, but the rationale for this is somewhat\nmysterious from the viewpoint of consistency. Our work brings some clarity to\nthis issue by revealing that step-size restriction is a mechanism for\npreventing the curvature of the risk from derailing convergence.\n",
        "published": "2017-01-27T02:45:58Z",
        "pdf_link": "http://arxiv.org/pdf/1701.07926v9"
    },
    {
        "id": "http://arxiv.org/abs/1701.08916v2",
        "title": "Prototypal Analysis and Prototypal Regression",
        "summary": "  Prototypal analysis is introduced to overcome two shortcomings of archetypal\nanalysis: its sensitivity to outliers and its non-locality, which reduces its\napplicability as a learning tool. Same as archetypal analysis, prototypal\nanalysis finds prototypes through convex combination of the data points and\napproximates the data through convex combination of the archetypes, but it adds\na penalty for using prototypes distant from the data points for their\nreconstruction. Prototypal analysis can be extended---via kernel embedding---to\nprobability distributions, since the convexity of the prototypes makes them\ninterpretable as mixtures. Finally, prototypal regression is developed, a\nrobust supervised procedure which allows the use of distributions as either\nfeatures or labels.\n",
        "published": "2017-01-31T05:04:55Z",
        "pdf_link": "http://arxiv.org/pdf/1701.08916v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.01000v3",
        "title": "Sharp Convergence Rates for Forward Regression in High-Dimensional\n  Sparse Linear Models",
        "summary": "  Forward regression is a statistical model selection and estimation procedure\nwhich inductively selects covariates that add predictive power into a working\nstatistical regression model. Once a model is selected, unknown regression\nparameters are estimated by least squares. This paper analyzes forward\nregression in high-dimensional sparse linear models. Probabilistic bounds for\nprediction error norm and number of selected covariates are proved. The\nanalysis in this paper gives sharp rates and does not require beta-min or\nirrepresentability conditions.\n",
        "published": "2017-02-03T13:33:32Z",
        "pdf_link": "http://arxiv.org/pdf/1702.01000v3"
    },
    {
        "id": "http://arxiv.org/abs/1702.01125v1",
        "title": "Energy Prediction using Spatiotemporal Pattern Networks",
        "summary": "  This paper presents a novel data-driven technique based on the spatiotemporal\npattern network (STPN) for energy/power prediction for complex dynamical\nsystems. Built on symbolic dynamic filtering, the STPN framework is used to\ncapture not only the individual system characteristics but also the pair-wise\ncausal dependencies among different sub-systems. For quantifying the causal\ndependency, a mutual information based metric is presented. An energy\nprediction approach is subsequently proposed based on the STPN framework. For\nvalidating the proposed scheme, two case studies are presented, one involving\nwind turbine power prediction (supply side energy) using the Western Wind\nIntegration data set generated by the National Renewable Energy Laboratory\n(NREL) for identifying the spatiotemporal characteristics, and the other,\nresidential electric energy disaggregation (demand side energy) using the\nBuilding America 2010 data set from NREL for exploring the temporal features.\nIn the energy disaggregation context, convex programming techniques beyond the\nSTPN framework are developed and applied to achieve improved disaggregation\nperformance.\n",
        "published": "2017-02-03T19:13:38Z",
        "pdf_link": "http://arxiv.org/pdf/1702.01125v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.01145v1",
        "title": "Query Efficient Posterior Estimation in Scientific Experiments via\n  Bayesian Active Learning",
        "summary": "  A common problem in disciplines of applied Statistics research such as\nAstrostatistics is of estimating the posterior distribution of relevant\nparameters. Typically, the likelihoods for such models are computed via\nexpensive experiments such as cosmological simulations of the universe. An\nurgent challenge in these research domains is to develop methods that can\nestimate the posterior with few likelihood evaluations.\n  In this paper, we study active posterior estimation in a Bayesian setting\nwhen the likelihood is expensive to evaluate. Existing techniques for posterior\nestimation are based on generating samples representative of the posterior.\nSuch methods do not consider efficiency in terms of likelihood evaluations. In\norder to be query efficient we treat posterior estimation in an active\nregression framework. We propose two myopic query strategies to choose where to\nevaluate the likelihood and implement them using Gaussian processes. Via\nexperiments on a series of synthetic and real examples we demonstrate that our\napproach is significantly more query efficient than existing techniques and\nother heuristics for posterior estimation.\n",
        "published": "2017-02-03T20:10:24Z",
        "pdf_link": "http://arxiv.org/pdf/1702.01145v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.01414v1",
        "title": "Shape-Based Approach to Household Load Curve Clustering and Prediction",
        "summary": "  Consumer Demand Response (DR) is an important research and industry problem,\nwhich seeks to categorize, predict and modify consumer's energy consumption.\nUnfortunately, traditional clustering methods have resulted in many hundreds of\nclusters, with a given consumer often associated with several clusters, making\nit difficult to classify consumers into stable representative groups and to\npredict individual energy consumption patterns. In this paper, we present a\nshape-based approach that better classifies and predicts consumer energy\nconsumption behavior at the household level. The method is based on Dynamic\nTime Warping. DTW seeks an optimal alignment between energy consumption\npatterns reflecting the effect of hidden patterns of regular consumer behavior.\nUsing real consumer 24-hour load curves from Opower Corporation, our method\nresults in a 50% reduction in the number of representative groups and an\nimprovement in prediction accuracy measured under DTW distance. We extend the\napproach to estimate which electrical devices will be used and in which hours.\n",
        "published": "2017-02-05T15:36:13Z",
        "pdf_link": "http://arxiv.org/pdf/1702.01414v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.01811v1",
        "title": "Hierarchical Symbolic Dynamic Filtering of Streaming Non-stationary Time\n  Series Data",
        "summary": "  This paper proposes a hierarchical feature extractor for non-stationary\nstreaming time series based on the concept of switching observable Markov chain\nmodels. The slow time-scale non-stationary behaviors are considered to be a\nmixture of quasi-stationary fast time-scale segments that are exhibited by\ncomplex dynamical systems. The idea is to model each unique stationary\ncharacteristic without a priori knowledge (e.g., number of possible unique\ncharacteristics) at a lower logical level, and capture the transitions from one\nlow-level model to another at a higher level. In this context, the concepts in\nthe recently developed Symbolic Dynamic Filtering (SDF) is extended, to build\nan online algorithm suited for handling quasi-stationary data at a lower level\nand a non-stationary behavior at a higher level without a priori knowledge. A\nkey observation made in this study is that the rate of change of data\nlikelihood seems to be a better indicator of change in data characteristics\ncompared to the traditional methods that mostly consider data likelihood for\nchange detection. The algorithm minimizes model complexity and captures data\nlikelihood. Efficacy demonstration and comparative evaluation of the proposed\nalgorithm are performed using time series data simulated from systems that\nexhibit nonlinear dynamics. We discuss results that show that the proposed\nhierarchical SDF algorithm can identify underlying features with significantly\nhigh degree of accuracy, even under very noisy conditions. Algorithm is\ndemonstrated to perform better than the baseline Hierarchical Dirichlet\nProcess-Hidden Markov Models (HDP-HMM). The low computational complexity of\nalgorithm makes it suitable for on-board, real time operations.\n",
        "published": "2017-02-06T22:34:44Z",
        "pdf_link": "http://arxiv.org/pdf/1702.01811v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.02165v1",
        "title": "Robust Clustering for Time Series Using Spectral Densities and\n  Functional Data Analysis",
        "summary": "  In this work a robust clustering algorithm for stationary time series is\nproposed. The algorithm is based on the use of estimated spectral densities,\nwhich are considered as functional data, as the basic characteristic of\nstationary time series for clustering purposes. A robust algorithm for\nfunctional data is then applied to the set of spectral densities. Trimming\ntechniques and restrictions on the scatter within groups reduce the effect of\nnoise in the data and help to prevent the identification of spurious clusters.\nThe procedure is tested in a simulation study, and is also applied to a real\ndata set.\n",
        "published": "2017-02-07T19:08:41Z",
        "pdf_link": "http://arxiv.org/pdf/1702.02165v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.03537v2",
        "title": "An Efficient, Expressive and Local Minima-free Method for Learning\n  Controlled Dynamical Systems",
        "summary": "  We propose a framework for modeling and estimating the state of controlled\ndynamical systems, where an agent can affect the system through actions and\nreceives partial observations. Based on this framework, we propose the\nPredictive State Representation with Random Fourier Features (RFFPSR). A key\nproperty in RFF-PSRs is that the state estimate is represented by a conditional\ndistribution of future observations given future actions. RFF-PSRs combine this\nrepresentation with moment-matching, kernel embedding and local optimization to\nachieve a method that enjoys several favorable qualities: It can represent\ncontrolled environments which can be affected by actions; it has an efficient\nand theoretically justified learning algorithm; it uses a non-parametric\nrepresentation that has expressive power to represent continuous non-linear\ndynamics. We provide a detailed formulation, a theoretical analysis and an\nexperimental evaluation that demonstrates the effectiveness of our method.\n",
        "published": "2017-02-12T16:13:29Z",
        "pdf_link": "http://arxiv.org/pdf/1702.03537v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.03994v1",
        "title": "metboost: Exploratory regression analysis with hierarchically clustered\n  data",
        "summary": "  As data collections become larger, exploratory regression analysis becomes\nmore important but more challenging. When observations are hierarchically\nclustered the problem is even more challenging because model selection with\nmixed effect models can produce misleading results when nonlinear effects are\nnot included into the model (Bauer and Cai, 2009). A machine learning method\ncalled boosted decision trees (Friedman, 2001) is a good approach for\nexploratory regression analysis in real data sets because it can detect\npredictors with nonlinear and interaction effects while also accounting for\nmissing data. We propose an extension to boosted decision decision trees called\nmetboost for hierarchically clustered data. It works by constraining the\nstructure of each tree to be the same across groups, but allowing the terminal\nnode means to differ. This allows predictors and split points to lead to\ndifferent predictions within each group, and approximates nonlinear group\nspecific effects. Importantly, metboost remains computationally feasible for\nthousands of observations and hundreds of predictors that may contain missing\nvalues. We apply the method to predict math performance for 15,240 students\nfrom 751 schools in data collected in the Educational Longitudinal Study 2002\n(Ingels et al., 2007), allowing 76 predictors to have unique effects for each\nschool. When comparing results to boosted decision trees, metboost has 15%\nimproved prediction performance. Results of a large simulation study show that\nmetboost has up to 70% improved variable selection performance and up to 30%\nimproved prediction performance compared to boosted decision trees when group\nsizes are small\n",
        "published": "2017-02-13T21:47:56Z",
        "pdf_link": "http://arxiv.org/pdf/1702.03994v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.04018v1",
        "title": "Intercomparison of Machine Learning Methods for Statistical Downscaling:\n  The Case of Daily and Extreme Precipitation",
        "summary": "  Statistical downscaling of global climate models (GCMs) allows researchers to\nstudy local climate change effects decades into the future. A wide range of\nstatistical models have been applied to downscaling GCMs but recent advances in\nmachine learning have not been explored. In this paper, we compare four\nfundamental statistical methods, Bias Correction Spatial Disaggregation (BCSD),\nOrdinary Least Squares, Elastic-Net, and Support Vector Machine, with three\nmore advanced machine learning methods, Multi-task Sparse Structure Learning\n(MSSL), BCSD coupled with MSSL, and Convolutional Neural Networks to downscale\ndaily precipitation in the Northeast United States. Metrics to evaluate of each\nmethod's ability to capture daily anomalies, large scale climate shifts, and\nextremes are analyzed. We find that linear methods, led by BCSD, consistently\noutperform non-linear approaches. The direct application of state-of-the-art\nmachine learning methods to statistical downscaling does not provide\nimprovements over simpler, longstanding approaches.\n",
        "published": "2017-02-13T23:20:22Z",
        "pdf_link": "http://arxiv.org/pdf/1702.04018v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.04407v4",
        "title": "Sequential Dirichlet Process Mixtures of Multivariate Skew\n  t-distributions for Model-based Clustering of Flow Cytometry Data",
        "summary": "  Flow cytometry is a high-throughput technology used to quantify multiple\nsurface and intracellular markers at the level of a single cell. This enables\nto identify cell sub-types, and to determine their relative proportions.\nImprovements of this technology allow to describe millions of individual cells\nfrom a blood sample using multiple markers. This results in high-dimensional\ndatasets, whose manual analysis is highly time-consuming and poorly\nreproducible. While several methods have been developed to perform automatic\nrecognition of cell populations, most of them treat and analyze each sample\nindependently. However, in practice, individual samples are rarely independent\n(e.g. longitudinal studies). Here, we propose to use a Bayesian nonparametric\napproach with Dirichlet process mixture (DPM) of multivariate skew\n$t$-distributions to perform model based clustering of flow-cytometry data. DPM\nmodels directly estimate the number of cell populations from the data, avoiding\nmodel selection issues, and skew $t$-distributions provides robustness to\noutliers and non-elliptical shape of cell populations. To accommodate repeated\nmeasurements, we propose a sequential strategy relying on a parametric\napproximation of the posterior. We illustrate the good performance of our\nmethod on simulated data, on an experimental benchmark dataset, and on new\nlongitudinal data from the DALIA-1 trial which evaluates a therapeutic vaccine\nagainst HIV. On the benchmark dataset, the sequential strategy outperforms all\nother methods evaluated, and similarly, leads to improved performance on the\nDALIA-1 data. We have made the method available for the community in the R\npackage NPflow.\n",
        "published": "2017-02-14T22:32:01Z",
        "pdf_link": "http://arxiv.org/pdf/1702.04407v4"
    },
    {
        "id": "http://arxiv.org/abs/1702.04775v2",
        "title": "Bayesian Additive Adaptive Basis Tensor Product Models for Modeling High\n  Dimensional Surfaces: An application to high-throughput toxicity testing",
        "summary": "  Many modern data sets are sampled with error from complex high-dimensional\nsurfaces. Methods such as tensor product splines or Gaussian processes are\neffective/well suited for characterizing a surface in two or three dimensions\nbut may suffer from difficulties when representing higher dimensional surfaces.\nMotivated by high throughput toxicity testing where observed dose-response\ncurves are cross sections of a surface defined by a chemical's structural\nproperties, a model is developed to characterize this surface to predict\nuntested chemicals' dose-responses. This manuscript proposes a novel approach\nthat models the multidimensional surface as a sum of learned basis functions\nformed as the tensor product of lower dimensional functions, which are\nthemselves representable by a basis expansion learned from the data. The model\nis described, a Gibbs sampling algorithm proposed, and is investigated in a\nsimulation study as well as data taken from the US EPA's ToxCast high\nthroughput toxicity testing platform.\n",
        "published": "2017-02-15T21:11:36Z",
        "pdf_link": "http://arxiv.org/pdf/1702.04775v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.05037v4",
        "title": "Additive Models with Trend Filtering",
        "summary": "  We study additive models built with trend filtering, i.e., additive models\nwhose components are each regularized by the (discrete) total variation of\ntheir $k$th (discrete) derivative, for a chosen integer $k \\geq 0$. This\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\npiecewise quadratic, etc.). Analogous to its advantages in the univariate case,\nadditive trend filtering has favorable theoretical and computational\nproperties, thanks in large part to the localized nature of the (discrete)\ntotal variation regularizer that it uses. On the theory side, we derive fast\nerror rates for additive trend filtering estimates, and show these rates are\nminimax optimal when the underlying function is additive and has component\nfunctions whose derivatives are of bounded variation. We also show that these\nrates are unattainable by additive smoothing splines (and by additive models\nbuilt from linear smoothers, in general). On the computational side, as per the\nstandard in additive models, backfitting is an appealing method for\noptimization, but it is particularly appealing for additive trend filtering\nbecause we can leverage a few highly efficient univariate trend filtering\nsolvers. Going one step further, we describe a new backfitting algorithm whose\niterations can be run in parallel, which (as far as we know) is the first of\nits kind. Lastly, we present experiments to examine the empirical performance\nof additive trend filtering.\n",
        "published": "2017-02-16T16:19:56Z",
        "pdf_link": "http://arxiv.org/pdf/1702.05037v4"
    },
    {
        "id": "http://arxiv.org/abs/1702.05243v3",
        "title": "Estimating Nonlinear Dynamics with the ConvNet Smoother",
        "summary": "  Estimating the state of a dynamical system from a series of noise-corrupted\nobservations is fundamental in many areas of science and engineering. The most\nwell-known method, the Kalman smoother (and the related Kalman filter), relies\non assumptions of linearity and Gaussianity that are rarely met in practice. In\nthis paper, we introduced a new dynamical smoothing method that exploits the\nremarkable capabilities of convolutional neural networks to approximate complex\nnon-linear functions. The main idea is to generate a training set composed of\nboth latent states and observations from an ensemble of simulators and to train\nthe deep network to recover the former from the latter. Importantly, this\nmethod only requires the availability of the simulators and can therefore be\napplied in situations in which either the latent dynamical model or the\nobservation model cannot be easily expressed in closed form. In our simulation\nstudies, we show that the resulting ConvNet smoother has almost optimal\nperformance in the Gaussian case even when the parameters are unknown.\nFurthermore, the method can be successfully applied to extremely non-linear and\nnon-Gaussian systems. Finally, we empirically validate our approach via the\nanalysis of measured brain signals.\n",
        "published": "2017-02-17T07:37:46Z",
        "pdf_link": "http://arxiv.org/pdf/1702.05243v3"
    },
    {
        "id": "http://arxiv.org/abs/1702.05289v2",
        "title": "Observable dictionary learning for high-dimensional statistical\n  inference",
        "summary": "  This paper introduces a method for efficiently inferring a high-dimensional\ndistributed quantity from a few observations. The quantity of interest (QoI) is\napproximated in a basis (dictionary) learned from a training set. The\ncoefficients associated with the approximation of the QoI in the basis are\ndetermined by minimizing the misfit with the observations. To obtain a\nprobabilistic estimate of the quantity of interest, a Bayesian approach is\nemployed. The QoI is treated as a random field endowed with a hierarchical\nprior distribution so that closed-form expressions can be obtained for the\nposterior distribution. The main contribution of the present work lies in the\nderivation of \\emph{a representation basis consistent with the observation\nchain} used to infer the associated coefficients. The resulting dictionary is\nthen tailored to be both observable by the sensors and accurate in\napproximating the posterior mean. An algorithm for deriving such an observable\ndictionary is presented. The method is illustrated with the estimation of the\nvelocity field of an open cavity flow from a handful of wall-mounted point\nsensors. Comparison with standard estimation approaches relying on Principal\nComponent Analysis and K-SVD dictionaries is provided and illustrates the\nsuperior performance of the present approach.\n",
        "published": "2017-02-17T10:25:24Z",
        "pdf_link": "http://arxiv.org/pdf/1702.05289v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.05683v2",
        "title": "SAGA and Restricted Strong Convexity",
        "summary": "  SAGA is a fast incremental gradient method on the finite sum problem and its\neffectiveness has been tested on a vast of applications. In this paper, we\nanalyze SAGA on a class of non-strongly convex and non-convex statistical\nproblem such as Lasso, group Lasso, Logistic regression with $\\ell_1$\nregularization, linear regression with SCAD regularization and Correct Lasso.\nWe prove that SAGA enjoys the linear convergence rate up to the statistical\nestimation accuracy, under the assumption of restricted strong convexity (RSC).\nIt significantly extends the applicability of SAGA in convex and non-convex\noptimization.\n",
        "published": "2017-02-19T00:39:06Z",
        "pdf_link": "http://arxiv.org/pdf/1702.05683v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.05777v5",
        "title": "Exponentially vanishing sub-optimal local minima in multilayer neural\n  networks",
        "summary": "  Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska\net al. (2015)) suggest that local minima with high error are exponentially rare\nin high dimensions. However, to prove low error guarantees for Multilayer\nNeural Networks (MNNs), previous works so far required either a heavily\nmodified MNN model or training method, strong assumptions on the labels (e.g.,\n\"near\" linear separability), or an unrealistic hidden layer with\n$\\Omega\\left(N\\right)$ units.\n  Results: We examine a MNN with one hidden layer of piecewise linear units, a\nsingle output, and a quadratic loss. We prove that, with high probability in\nthe limit of $N\\rightarrow\\infty$ datapoints, the volume of differentiable\nregions of the empiric loss containing sub-optimal differentiable local minima\nis exponentially vanishing in comparison with the same volume of global minima,\ngiven standard normal input of dimension\n$d_{0}=\\tilde{\\Omega}\\left(\\sqrt{N}\\right)$, and a more realistic number of\n$d_{1}=\\tilde{\\Omega}\\left(N/d_{0}\\right)$ hidden units. We demonstrate our\nresults numerically: for example, $0\\%$ binary classification training error on\nCIFAR with only $N/d_{0}\\approx 16$ hidden neurons.\n",
        "published": "2017-02-19T18:12:51Z",
        "pdf_link": "http://arxiv.org/pdf/1702.05777v5"
    },
    {
        "id": "http://arxiv.org/abs/1702.06209v1",
        "title": "Uniform Inference for High-dimensional Quantile Regression: Linear\n  Functionals and Regression Rank Scores",
        "summary": "  Hypothesis tests in models whose dimension far exceeds the sample size can be\nformulated much like the classical studentized tests only after the initial\nbias of estimation is removed successfully. The theory of debiased estimators\ncan be developed in the context of quantile regression models for a fixed\nquantile value. However, it is frequently desirable to formulate tests based on\nthe quantile regression process, as this leads to more robust tests and more\nstable confidence sets. Additionally, inference in quantile regression requires\nestimation of the so called sparsity function, which depends on the unknown\ndensity of the error. In this paper we consider a debiasing approach for the\nuniform testing problem. We develop high-dimensional regression rank scores and\nshow how to use them to estimate the sparsity function, as well as how to adapt\nthem for inference involving the quantile regression process. Furthermore, we\ndevelop a Kolmogorov-Smirnov test in a location-shift high-dimensional models\nand confidence sets that are uniformly valid for many quantile values. The main\ntechnical result are the development of a Bahadur representation of the\ndebiasing estimator that is uniform over a range of quantiles and uniform\nconvergence of the quantile process to the Brownian bridge process, which are\nof independent interest. Simulation studies illustrate finite sample properties\nof our procedure.\n",
        "published": "2017-02-20T23:38:40Z",
        "pdf_link": "http://arxiv.org/pdf/1702.06209v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.06234v3",
        "title": "Easily parallelizable and distributable class of algorithms for\n  structured sparsity, with optimal acceleration",
        "summary": "  Many statistical learning problems can be posed as minimization of a sum of\ntwo convex functions, one typically a composition of non-smooth and linear\nfunctions. Examples include regression under structured sparsity assumptions.\nPopular algorithms for solving such problems, e.g., ADMM, often involve\nnon-trivial optimization subproblems or smoothing approximation. We consider\ntwo classes of primal-dual algorithms that do not incur these difficulties, and\nunify them from a perspective of monotone operator theory. From this\nunification we propose a continuum of preconditioned forward-backward operator\nsplitting algorithms amenable to parallel and distributed computing. For the\nentire region of convergence of the whole continuum of algorithms, we establish\nits rates of convergence. For some known instances of this continuum, our\nanalysis closes the gap in theory. We further exploit the unification to\npropose a continuum of accelerated algorithms. We show that the whole continuum\nattains the theoretically optimal rate of convergence. The scalability of the\nproposed algorithms, as well as their convergence behavior, is demonstrated up\nto 1.2 million variables with a distributed implementation.\n",
        "published": "2017-02-21T01:25:51Z",
        "pdf_link": "http://arxiv.org/pdf/1702.06234v3"
    },
    {
        "id": "http://arxiv.org/abs/1702.06278v1",
        "title": "Column normalization of a random measurement matrix",
        "summary": "  In this note we answer a question of G. Lecu\\'{e}, by showing that column\nnormalization of a random matrix with iid entries need not lead to good sparse\nrecovery properties, even if the generating random variable has a reasonable\nmoment growth. Specifically, for every $2 \\leq p \\leq c_1\\log d$ we construct a\nrandom vector $X \\in R^d$ with iid, mean-zero, variance $1$ coordinates, that\nsatisfies $\\sup_{t \\in S^{d-1}} \\|<X,t>\\|_{L_q} \\leq c_2\\sqrt{q}$ for every\n$2\\leq q \\leq p$.\n  We show that if $m \\leq c_3\\sqrt{p}d^{1/p}$ and $\\tilde{\\Gamma}:R^d \\to R^m$\nis the column-normalized matrix generated by $m$ independent copies of $X$,\nthen with probability at least $1-2\\exp(-c_4m)$, $\\tilde{\\Gamma}$ does not\nsatisfy the exact reconstruction property of order $2$.\n",
        "published": "2017-02-21T06:42:28Z",
        "pdf_link": "http://arxiv.org/pdf/1702.06278v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.06525v3",
        "title": "A Unified Framework for Low-Rank plus Sparse Matrix Recovery",
        "summary": "  We propose a unified framework to solve general low-rank plus sparse matrix\nrecovery problems based on matrix factorization, which covers a broad family of\nobjective functions satisfying the restricted strong convexity and smoothness\nconditions. Based on projected gradient descent and the double thresholding\noperator, our proposed generic algorithm is guaranteed to converge to the\nunknown low-rank and sparse matrices at a locally linear rate, while matching\nthe best-known robustness guarantee (i.e., tolerance for sparsity). At the core\nof our theory is a novel structural Lipschitz gradient condition for low-rank\nplus sparse matrices, which is essential for proving the linear convergence\nrate of our algorithm, and we believe is of independent interest to prove fast\nrates for general superposition-structured models. We illustrate the\napplication of our framework through two concrete examples: robust matrix\nsensing and robust PCA. Experiments on both synthetic and real datasets\ncorroborate our theory.\n",
        "published": "2017-02-21T18:56:42Z",
        "pdf_link": "http://arxiv.org/pdf/1702.06525v3"
    },
    {
        "id": "http://arxiv.org/abs/1702.07066v1",
        "title": "A Unified Parallel Algorithm for Regularized Group PLS Scalable to Big\n  Data",
        "summary": "  Partial Least Squares (PLS) methods have been heavily exploited to analyse\nthe association between two blocs of data. These powerful approaches can be\napplied to data sets where the number of variables is greater than the number\nof observations and in presence of high collinearity between variables.\nDifferent sparse versions of PLS have been developed to integrate multiple data\nsets while simultaneously selecting the contributing variables. Sparse\nmodelling is a key factor in obtaining better estimators and identifying\nassociations between multiple data sets. The cornerstone of the sparsity\nversion of PLS methods is the link between the SVD of a matrix (constructed\nfrom deflated versions of the original matrices of data) and least squares\nminimisation in linear regression. We present here an accurate description of\nthe most popular PLS methods, alongside their mathematical proofs. A unified\nalgorithm is proposed to perform all four types of PLS including their\nregularised versions. Various approaches to decrease the computation time are\noffered, and we show how the whole procedure can be scalable to big data sets.\n",
        "published": "2017-02-23T02:08:51Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07066v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.07190v1",
        "title": "Spectral Clustering using PCKID - A Probabilistic Cluster Kernel for\n  Incomplete Data",
        "summary": "  In this paper, we propose PCKID, a novel, robust, kernel function for\nspectral clustering, specifically designed to handle incomplete data. By\ncombining posterior distributions of Gaussian Mixture Models for incomplete\ndata on different scales, we are able to learn a kernel for incomplete data\nthat does not depend on any critical hyperparameters, unlike the commonly used\nRBF kernel. To evaluate our method, we perform experiments on two real\ndatasets. PCKID outperforms the baseline methods for all fractions of missing\nvalues and in some cases outperforms the baseline methods with up to 25\npercentage points.\n",
        "published": "2017-02-23T12:19:31Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07190v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.07254v3",
        "title": "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithm",
        "summary": "  Learning rates for least-squares regression are typically expressed in terms\nof $L_2$-norms. In this paper we extend these rates to norms stronger than the\n$L_2$-norm without requiring the regression function to be contained in the\nhypothesis space. In the special case of Sobolev reproducing kernel Hilbert\nspaces used as hypotheses spaces, these stronger norms coincide with fractional\nSobolev norms between the used Sobolev space and $L_2$. As a consequence, not\nonly the target function but also some of its derivatives can be estimated\nwithout changing the algorithm. From a technical point of view, we combine the\nwell-known integral operator techniques with an embedding property, which so\nfar has only been used in combination with empirical process arguments. This\ncombination results in new finite sample bounds with respect to the stronger\nnorms. From these finite sample bounds our rates easily follow. Finally, we\nprove the asymptotic optimality of our results in many cases.\n",
        "published": "2017-02-23T15:10:15Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07254v3"
    },
    {
        "id": "http://arxiv.org/abs/1702.07398v2",
        "title": "Deep Nonparametric Estimation of Discrete Conditional Distributions via\n  Smoothed Dyadic Partitioning",
        "summary": "  We present an approach to deep estimation of discrete conditional probability\ndistributions. Such models have several applications, including generative\nmodeling of audio, image, and video data. Our approach combines two main\ntechniques: dyadic partitioning and graph-based smoothing of the discrete\nspace. By recursively decomposing each dimension into a series of binary splits\nand smoothing over the resulting distribution using graph-based trend\nfiltering, we impose a strict structure to the model and achieve much higher\nsample efficiency. We demonstrate the advantages of our model through a series\nof benchmarks on both synthetic and real-world datasets, in some cases reducing\nthe error by nearly half in comparison to other popular methods in the\nliterature. All of our models are implemented in Tensorflow and publicly\navailable at https://github.com/tansey/sdp .\n",
        "published": "2017-02-23T21:29:13Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07398v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.07405v1",
        "title": "GapTV: Accurate and Interpretable Low-Dimensional Regression and\n  Classification",
        "summary": "  We consider the problem of estimating a regression function in the common\nsituation where the number of features is small, where interpretability of the\nmodel is a high priority, and where simple linear or additive models fail to\nprovide adequate performance. To address this problem, we present GapTV, an\napproach that is conceptually related both to CART and to the more recent CRISP\nalgorithm, a state-of-the-art alternative method for interpretable nonlinear\nregression. GapTV divides the feature space into blocks of constant value and\nfits the value of all blocks jointly via a convex optimization routine. Our\nmethod is fully data-adaptive, in that it incorporates highly robust routines\nfor tuning all hyperparameters automatically. We compare our approach against\nCART and CRISP and demonstrate that GapTV finds a much better trade-off between\naccuracy and interpretability.\n",
        "published": "2017-02-23T21:58:27Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07405v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.07608v1",
        "title": "Microwave breast cancer detection using Empirical Mode Decomposition\n  features",
        "summary": "  Microwave-based breast cancer detection has been proposed as a complementary\napproach to compensate for some drawbacks of existing breast cancer detection\ntechniques. Among the existing microwave breast cancer detection methods,\nmachine learning-type algorithms have recently become more popular. These focus\non detecting the existence of breast tumours rather than performing imaging to\nidentify the exact tumour position. A key step of the machine learning\napproaches is feature extraction. One of the most widely used feature\nextraction method is principle component analysis (PCA). However, it can be\nsensitive to signal misalignment. This paper presents an empirical mode\ndecomposition (EMD)-based feature extraction method, which is more robust to\nthe misalignment. Experimental results involving clinical data sets combined\nwith numerically simulated tumour responses show that combined features from\nEMD and PCA improve the detection performance with an ensemble selection-based\nclassifier.\n",
        "published": "2017-02-24T14:42:06Z",
        "pdf_link": "http://arxiv.org/pdf/1702.07608v1"
    },
    {
        "id": "http://arxiv.org/abs/1702.08239v2",
        "title": "Bayesian inference on random simple graphs with power law degree\n  distributions",
        "summary": "  We present a model for random simple graphs with a degree distribution that\nobeys a power law (i.e., is heavy-tailed). To attain this behavior, the edge\nprobabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor\n(BFRY) random variables, which have been recently utilized in Bayesian\nstatistics for the construction of power law models in several applications.\nOur construction readily extends to capture the structure of latent factors,\nsimilarly to stochastic blockmodels, while maintaining its power law degree\ndistribution. The BFRY random variables are well approximated by gamma random\nvariables in a variational Bayesian inference routine, which we apply to\nseveral network datasets for which power law degree distributions are a natural\nassumption. By learning the parameters of the BFRY distribution via\nprobabilistic inference, we are able to automatically select the appropriate\npower law behavior from the data. In order to further scale our inference\nprocedure, we adopt stochastic gradient ascent routines where the gradients are\ncomputed on minibatches (i.e., subsets) of the edges in the graph.\n",
        "published": "2017-02-27T11:26:33Z",
        "pdf_link": "http://arxiv.org/pdf/1702.08239v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.08320v2",
        "title": "An Efficient Pseudo-likelihood Method for Sparse Binary Pairwise Markov\n  Network Estimation",
        "summary": "  The pseudo-likelihood method is one of the most popular algorithms for\nlearning sparse binary pairwise Markov networks. In this paper, we formulate\nthe $L_1$ regularized pseudo-likelihood problem as a sparse multiple logistic\nregression problem. In this way, many insights and optimization procedures for\nsparse logistic regression can be applied to the learning of discrete Markov\nnetworks. Specifically, we use the coordinate descent algorithm for generalized\nlinear models with convex penalties, combined with strong screening rules, to\nsolve the pseudo-likelihood problem with $L_1$ regularization. Therefore a\nsubstantial speedup without losing any accuracy can be achieved. Furthermore,\nthis method is more stable than the node-wise logistic regression approach on\nunbalanced high-dimensional data when penalized by small regularization\nparameters. Thorough numerical experiments on simulated data and real world\ndata demonstrate the advantages of the proposed method.\n",
        "published": "2017-02-27T15:17:04Z",
        "pdf_link": "http://arxiv.org/pdf/1702.08320v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.08402v2",
        "title": "A Mutually-Dependent Hadamard Kernel for Modelling Latent Variable\n  Couplings",
        "summary": "  We introduce a novel kernel that models input-dependent couplings across\nmultiple latent processes. The pairwise joint kernel measures covariance along\ninputs and across different latent signals in a mutually-dependent fashion. A\nlatent correlation Gaussian process (LCGP) model combines these non-stationary\nlatent components into multiple outputs by an input-dependent mixing matrix.\nProbit classification and support for multiple observation sets are derived by\nVariational Bayesian inference. Results on several datasets indicate that the\nLCGP model can recover the correlations between latent signals while\nsimultaneously achieving state-of-the-art performance. We highlight the latent\ncovariances with an EEG classification dataset where latent brain processes and\ntheir couplings simultaneously emerge from the model.\n",
        "published": "2017-02-27T17:48:46Z",
        "pdf_link": "http://arxiv.org/pdf/1702.08402v2"
    },
    {
        "id": "http://arxiv.org/abs/1702.08420v9",
        "title": "Embarrassingly Parallel Inference for Gaussian Processes",
        "summary": "  Training Gaussian process-based models typically involves an $ O(N^3)$\ncomputational bottleneck due to inverting the covariance matrix. Popular\nmethods for overcoming this matrix inversion problem cannot adequately model\nall types of latent functions, and are often not parallelizable. However,\njudicious choice of model structure can ameliorate this problem. A\nmixture-of-experts model that uses a mixture of $K$ Gaussian processes offers\nmodeling flexibility and opportunities for scalable inference. Our\nembarrassingly parallel algorithm combines low-dimensional matrix inversions\nwith importance sampling to yield a flexible, scalable mixture-of-experts model\nthat offers comparable performance to Gaussian process regression at a much\nlower computational cost.\n",
        "published": "2017-02-27T18:26:45Z",
        "pdf_link": "http://arxiv.org/pdf/1702.08420v9"
    },
    {
        "id": "http://arxiv.org/abs/1702.08848v4",
        "title": "Semi-supervised Learning based on Distributionally Robust Optimization",
        "summary": "  We propose a novel method for semi-supervised learning (SSL) based on\ndata-driven distributionally robust optimization (DRO) using optimal transport\nmetrics. Our proposed method enhances generalization error by using the\nunlabeled data to restrict the support of the worst case distribution in our\nDRO formulation. We enable the implementation of our DRO formulation by\nproposing a stochastic gradient descent algorithm which allows to easily\nimplement the training procedure. We demonstrate that our Semi-supervised DRO\nmethod is able to improve the generalization error over natural supervised\nprocedures and state-of-the-art SSL estimators. Finally, we include a\ndiscussion on the large sample behavior of the optimal uncertainty region in\nthe DRO formulation. Our discussion exposes important aspects such as the role\nof dimension reduction in SSL.\n",
        "published": "2017-02-28T16:30:17Z",
        "pdf_link": "http://arxiv.org/pdf/1702.08848v4"
    },
    {
        "id": "http://arxiv.org/abs/1703.00598v3",
        "title": "The Second Order Linear Model",
        "summary": "  We study a fundamental class of regression models called the second order\nlinear model (SLM). The SLM extends the linear model to high order functional\nspace and has attracted considerable research interest recently. Yet how to\nefficiently learn the SLM under full generality using nonconvex solver still\nremains an open question due to several fundamental limitations of the\nconventional gradient descent learning framework. In this study, we try to\nattack this problem from a gradient-free approach which we call the\nmoment-estimation-sequence (MES) method. We show that the conventional gradient\ndescent heuristic is biased by the skewness of the distribution therefore is no\nlonger the best practice of learning the SLM. Based on the MES framework, we\ndesign a nonconvex alternating iteration process to train a $d$-dimension\nrank-$k$ SLM within $O(kd)$ memory and one-pass of the dataset. The proposed\nmethod converges globally and linearly, achieves $\\epsilon$ recovery error\nafter retrieving $O[k^{2}d\\cdot\\mathrm{polylog}(kd/\\epsilon)]$ samples.\nFurthermore, our theoretical analysis reveals that not all SLMs can be learned\non every sub-gaussian distribution. When the instances are sampled from a\nso-called $\\tau$-MIP distribution, the SLM can be learned by $O(p/\\tau^{2})$\nsamples where $p$ and $\\tau$ are positive constants depending on the skewness\nand kurtosis of the distribution. For non-MIP distribution, an addition\ndiagonal-free oracle is necessary and sufficient to guarantee the learnability\nof the SLM. Numerical simulations verify the sharpness of our bounds on the\nsampling complexity and the linear convergence rate of our algorithm.\n",
        "published": "2017-03-02T03:23:01Z",
        "pdf_link": "http://arxiv.org/pdf/1703.00598v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.00787v2",
        "title": "Linearly constrained Gaussian processes",
        "summary": "  We consider a modification of the covariance function in Gaussian processes\nto correctly account for known linear constraints. By modelling the target\nfunction as a transformation of an underlying function, the constraints are\nexplicitly incorporated in the model such that they are guaranteed to be\nfulfilled by any sample drawn or prediction made. We also propose a\nconstructive procedure for designing the transformation operator and illustrate\nthe result on both simulated and real-data examples.\n",
        "published": "2017-03-02T14:02:03Z",
        "pdf_link": "http://arxiv.org/pdf/1703.00787v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.01056v5",
        "title": "Gauging Variational Inference",
        "summary": "  Computing partition function is the most important statistical inference task\narising in applications of Graphical Models (GM). Since it is computationally\nintractable, approximate methods have been used to resolve the issue in\npractice, where mean-field (MF) and belief propagation (BP) are arguably the\nmost popular and successful approaches of a variational type. In this paper, we\npropose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP\n(G-BP), improving MF and BP, respectively. Both provide lower bounds for the\npartition function by utilizing the so-called gauge transformation which\nmodifies factors of GM while keeping the partition function invariant.\nMoreover, we prove that both G-MF and G-BP are exact for GMs with a single loop\nof a special structure, even though the bare MF and BP perform badly in this\ncase. Our extensive experiments, on complete GMs of relatively small size and\non large GM (up-to 300 variables) confirm that the newly proposed algorithms\noutperform and generalize MF and BP.\n",
        "published": "2017-03-03T06:54:50Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01056v5"
    },
    {
        "id": "http://arxiv.org/abs/1703.01444v1",
        "title": "An unsupervised bayesian approach for the joint reconstruction and\n  classification of cutaneous reflectance confocal microscopy images",
        "summary": "  This paper studies a new Bayesian algorithm for the joint reconstruction and\nclassification of reflectance confocal microscopy (RCM) images, with\napplication to the identification of human skin lentigo. The proposed Bayesian\napproach takes advantage of the distribution of the multiplicative speckle\nnoise affecting the true reflectivity of these images and of appropriate priors\nfor the unknown model parameters. A Markov chain Monte Carlo (MCMC) algorithm\nis proposed to jointly estimate the model parameters and the image of true\nreflectivity while classifying images according to the distribution of their\nreflectivity. Precisely, a Metropolis-whitin-Gibbs sampler is investigated to\nsample the posterior distribution of the Bayesian model associated with RCM\nimages and to build estimators of its parameters, including labels indicating\nthe class of each RCM image. The resulting algorithm is applied to synthetic\ndata and to real images from a clinical study containing healthy and lentigo\npatients.\n",
        "published": "2017-03-04T11:37:00Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01444v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.01488v1",
        "title": "Autoencoding Variational Inference For Topic Models",
        "summary": "  Topic models are one of the most popular methods for learning representations\nof text, but a major challenge is that any change to the topic model requires\nmathematically deriving a new inference algorithm. A promising approach to\naddress this problem is autoencoding variational Bayes (AEVB), but it has\nproven diffi- cult to apply to topic models in practice. We present what is to\nour knowledge the first effective AEVB based inference method for latent\nDirichlet allocation (LDA), which we call Autoencoded Variational Inference For\nTopic Model (AVITM). This model tackles the problems caused for AEVB by the\nDirichlet prior and by component collapsing. We find that AVITM matches\ntraditional methods in accuracy with much better inference time. Indeed,\nbecause of the inference network, we find that it is unnecessary to pay the\ncomputational cost of running variational optimization on test data. Because\nAVITM is black box, it is readily applied to new topic models. As a dramatic\nillustration of this, we present a new topic model called ProdLDA, that\nreplaces the mixture model in LDA with a product of experts. By changing only\none line of code from LDA, we find that ProdLDA yields much more interpretable\ntopics, even if LDA is trained via collapsed Gibbs sampling.\n",
        "published": "2017-03-04T16:28:15Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01488v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.01536v1",
        "title": "A Statistical Machine Learning Approach to Yield Curve Forecasting",
        "summary": "  Yield curve forecasting is an important problem in finance. In this work we\nexplore the use of Gaussian Processes in conjunction with a dynamic modeling\nstrategy, much like the Kalman Filter, to model the yield curve. Gaussian\nProcesses have been successfully applied to model functional data in a variety\nof applications. A Gaussian Process is used to model the yield curve. The\nhyper-parameters of the Gaussian Process model are updated as the algorithm\nreceives yield curve data. Yield curve data is typically available as a time\nseries with a frequency of one day. We compare existing methods to forecast the\nyield curve with the proposed method. The results of this study showed that\nwhile a competing method (a multivariate time series method) performed well in\nforecasting the yields at the short term structure region of the yield curve,\nGaussian Processes perform well in the medium and long term structure regions\nof the yield curve. Accuracy in the long term structure region of the yield\ncurve has important practical implications. The Gaussian Process framework\nyields uncertainty and probability estimates directly in contrast to other\ncompeting methods. Analysts are frequently interested in this information. In\nthis study the proposed method has been applied to yield curve forecasting,\nhowever it can be applied to model high frequency time series data or data\nstreams in other domains.\n",
        "published": "2017-03-04T23:43:36Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01536v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.01541v2",
        "title": "Soft-DTW: a Differentiable Loss Function for Time-Series",
        "summary": "  We propose in this paper a differentiable learning loss between time series,\nbuilding upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the\nEuclidean distance, DTW can compare time series of variable size and is robust\nto shifts or dilatations across the time dimension. To compute DTW, one\ntypically solves a minimal-cost alignment problem between two time series using\ndynamic programming. Our work takes advantage of a smoothed formulation of DTW,\ncalled soft-DTW, that computes the soft-minimum of all alignment costs. We show\nin this paper that soft-DTW is a differentiable loss function, and that both\nits value and gradient can be computed with quadratic time/space complexity\n(DTW has quadratic time but linear space complexity). We show that this\nregularization is particularly well suited to average and cluster time series\nunder the DTW geometry, a task for which our proposal significantly outperforms\nexisting baselines. Next, we propose to tune the parameters of a machine that\noutputs time series by minimizing its fit with ground-truth labels in a\nsoft-DTW sense.\n",
        "published": "2017-03-05T01:30:28Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01541v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.01785v3",
        "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization",
        "summary": "  We study two procedures (reverse-mode and forward-mode) for computing the\ngradient of the validation error with respect to the hyperparameters of any\niterative learning algorithm such as stochastic gradient descent. These\nprocedures mirror two methods of computing gradients for recurrent neural\nnetworks and have different trade-offs in terms of running time and space\nrequirements. Our formulation of the reverse-mode procedure is linked to\nprevious work by Maclaurin et al. [2015] but does not require reversible\ndynamics. The forward-mode procedure is suitable for real-time hyperparameter\nupdates, which may significantly speed up hyperparameter optimization on large\ndatasets. We present experiments on data cleaning and on learning task\ninteractions. We also present one large-scale experiment where the use of\nprevious gradient-based methods would be prohibitive.\n",
        "published": "2017-03-06T09:44:32Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01785v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.01872v2",
        "title": "Computational Eco-Systems for Handwritten Digits Recognition",
        "summary": "  Inspired by the importance of diversity in biological system, we built an\nheterogeneous system that could achieve this goal. Our architecture could be\nsummarized in two basic steps. First, we generate a diverse set of\nclassification hypothesis using both Convolutional Neural Networks, currently\nthe state-of-the-art technique for this task, among with other traditional and\ninnovative machine learning techniques. Then, we optimally combine them through\nMeta-Nets, a family of recently developed and performing ensemble methods.\n",
        "published": "2017-03-06T13:59:39Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01872v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.01925v1",
        "title": "Grammar Variational Autoencoder",
        "summary": "  Deep generative models have been wildly successful at learning coherent\nlatent representations for continuous data such as video and audio. However,\ngenerative modeling of discrete data such as arithmetic expressions and\nmolecular structures still poses significant challenges. Crucially,\nstate-of-the-art methods often produce outputs that are not valid. We make the\nkey observation that frequently, discrete data can be represented as a parse\ntree from a context-free grammar. We propose a variational autoencoder which\nencodes and decodes directly to and from these parse trees, ensuring the\ngenerated outputs are always valid. Surprisingly, we show that not only does\nour model more often generate valid outputs, it also learns a more coherent\nlatent space in which nearby points decode to similar discrete outputs. We\ndemonstrate the effectiveness of our learned models by showing their improved\nperformance in Bayesian optimization for symbolic regression and molecular\nsynthesis.\n",
        "published": "2017-03-06T15:36:37Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01925v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.01962v1",
        "title": "Probabilistic Reduced-Order Modeling for Stochastic Partial Differential\n  Equations",
        "summary": "  We discuss a Bayesian formulation to coarse-graining (CG) of PDEs where the\ncoefficients (e.g. material parameters) exhibit random, fine scale variability.\nThe direct solution to such problems requires grids that are small enough to\nresolve this fine scale variability which unavoidably requires the repeated\nsolution of very large systems of algebraic equations. We establish a\nphysically inspired, data-driven coarse-grained model which learns a low-\ndimensional set of microstructural features that are predictive of the\nfine-grained model (FG) response. Once learned, those features provide a sharp\ndistribution over the coarse scale effec- tive coefficients of the PDE that are\nmost suitable for prediction of the fine scale model output. This ultimately\nallows to replace the computationally expensive FG by a generative proba-\nbilistic model based on evaluating the much cheaper CG several times. Sparsity\nenforcing pri- ors further increase predictive efficiency and reveal\nmicrostructural features that are important in predicting the FG response.\nMoreover, the model yields probabilistic rather than single-point predictions,\nwhich enables the quantification of the unavoidable epistemic uncertainty that\nis present due to the information loss that occurs during the coarse-graining\nprocess.\n",
        "published": "2017-03-06T16:42:05Z",
        "pdf_link": "http://arxiv.org/pdf/1703.01962v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.02628v3",
        "title": "Global optimization of Lipschitz functions",
        "summary": "  The goal of the paper is to design sequential strategies which lead to\nefficient optimization of an unknown function under the only assumption that it\nhas a finite Lipschitz constant. We first identify sufficient conditions for\nthe consistency of generic sequential algorithms and formulate the expected\nminimax rate for their performance. We introduce and analyze a first algorithm\ncalled LIPO which assumes the Lipschitz constant to be known. Consistency,\nminimax rates for LIPO are proved, as well as fast rates under an additional\nH\\\"older like condition. An adaptive version of LIPO is also introduced for the\nmore realistic setup where the Lipschitz constant is unknown and has to be\nestimated along with the optimization. Similar theoretical guarantees are shown\nto hold for the adaptive LIPO algorithm and a numerical assessment is provided\nat the end of the paper to illustrate the potential of this strategy with\nrespect to state-of-the-art methods over typical benchmark problems for global\noptimization.\n",
        "published": "2017-03-07T22:29:54Z",
        "pdf_link": "http://arxiv.org/pdf/1703.02628v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.02674v3",
        "title": "Polynomial Time Algorithms for Dual Volume Sampling",
        "summary": "  We study dual volume sampling, a method for selecting k columns from an n x m\nshort and wide matrix (n <= k <= m) such that the probability of selection is\nproportional to the volume spanned by the rows of the induced submatrix. This\nmethod was proposed by Avron and Boutsidis (2013), who showed it to be a\npromising method for column subset selection and its multiple applications.\nHowever, its wider adoption has been hampered by the lack of polynomial time\nsampling algorithms. We remove this hindrance by developing an exact\n(randomized) polynomial time sampling algorithm as well as its derandomization.\nThereafter, we study dual volume sampling via the theory of real stable\npolynomials and prove that its distribution satisfies the \"Strong Rayleigh\"\nproperty. This result has numerous consequences, including a provably\nfast-mixing Markov chain sampler that makes dual volume sampling much more\nattractive to practitioners. This sampler is closely related to classical\nalgorithms for popular experimental design methods that are to date lacking\ntheoretical analysis but are known to empirically work well.\n",
        "published": "2017-03-08T02:22:30Z",
        "pdf_link": "http://arxiv.org/pdf/1703.02674v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.03216v3",
        "title": "Trimmed Density Ratio Estimation",
        "summary": "  Density ratio estimation is a vital tool in both machine learning and\nstatistical community. However, due to the unbounded nature of density ratio,\nthe estimation procedure can be vulnerable to corrupted data points, which\noften pushes the estimated ratio toward infinity. In this paper, we present a\nrobust estimator which automatically identifies and trims outliers. The\nproposed estimator has a convex formulation, and the global optimum can be\nobtained via subgradient descent. We analyze the parameter estimation error of\nthis estimator under high-dimensional settings. Experiments are conducted to\nverify the effectiveness of the estimator.\n",
        "published": "2017-03-09T10:24:27Z",
        "pdf_link": "http://arxiv.org/pdf/1703.03216v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.03373v3",
        "title": "mlrMBO: A Modular Framework for Model-Based Optimization of Expensive\n  Black-Box Functions",
        "summary": "  We present mlrMBO, a flexible and comprehensive R toolbox for model-based\noptimization (MBO), also known as Bayesian optimization, which addresses the\nproblem of expensive black-box optimization by approximating the given\nobjective function through a surrogate regression model. It is designed for\nboth single- and multi-objective optimization with mixed continuous,\ncategorical and conditional parameters. Additional features include multi-point\nbatch proposal, parallelization, visualization, logging and error-handling.\nmlrMBO is implemented in a modular fashion, such that single components can be\neasily replaced or adapted by the user for specific use cases, e.g., any\nregression learner from the mlr toolbox for machine learning can be used, and\ninfill criteria and infill optimizers are easily exchangeable. We empirically\ndemonstrate that mlrMBO provides state-of-the-art performance by comparing it\non different benchmark scenarios against a wide range of other optimizers,\nincluding DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and\nHyperopt.\n",
        "published": "2017-03-09T17:52:50Z",
        "pdf_link": "http://arxiv.org/pdf/1703.03373v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.03457v1",
        "title": "Parallel Markov Chain Monte Carlo for the Indian Buffet Process",
        "summary": "  Indian Buffet Process based models are an elegant way for discovering\nunderlying features within a data set, but inference in such models can be\nslow. Inferring underlying features using Markov chain Monte Carlo either\nrelies on an uncollapsed representation, which leads to poor mixing, or on a\ncollapsed representation, which leads to a quadratic increase in computational\ncomplexity. Existing attempts at distributing inference have introduced\nadditional approximation within the inference procedure. In this paper we\npresent a novel algorithm to perform asymptotically exact parallel Markov chain\nMonte Carlo inference for Indian Buffet Process models. We take advantage of\nthe fact that the features are conditionally independent under the\nbeta-Bernoulli process. Because of this conditional independence, we can\npartition the features into two parts: one part containing only the finitely\nmany instantiated features and the other part containing the infinite tail of\nuninstantiated features. For the finite partition, parallel inference is simple\ngiven the instantiation of features. But for the infinite tail, performing\nuncollapsed MCMC leads to poor mixing and hence we collapse out the features.\nThe resulting hybrid sampler, while being parallel, produces samples\nasymptotically from the true posterior.\n",
        "published": "2017-03-09T20:43:40Z",
        "pdf_link": "http://arxiv.org/pdf/1703.03457v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.03503v2",
        "title": "Density Level Set Estimation on Manifolds with DBSCAN",
        "summary": "  We show that DBSCAN can estimate the connected components of the\n$\\lambda$-density level set $\\{ x : f(x) \\ge \\lambda\\}$ given $n$ i.i.d.\nsamples from an unknown density $f$. We characterize the regularity of the\nlevel set boundaries using parameter $\\beta > 0$ and analyze the estimation\nerror under the Hausdorff metric. When the data lies in $\\mathbb{R}^D$ we\nobtain a rate of $\\widetilde{O}(n^{-1/(2\\beta + D)})$, which matches known\nlower bounds up to logarithmic factors. When the data lies on an embedded\nunknown $d$-dimensional manifold in $\\mathbb{R}^D$, then we obtain a rate of\n$\\widetilde{O}(n^{-1/(2\\beta + d\\cdot \\max\\{1, \\beta \\})})$. Finally, we\nprovide adaptive parameter tuning in order to attain these rates with no a\npriori knowledge of the intrinsic dimension, density, or $\\beta$.\n",
        "published": "2017-03-10T01:28:15Z",
        "pdf_link": "http://arxiv.org/pdf/1703.03503v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.04046v2",
        "title": "DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw\n  Single-Channel EEG",
        "summary": "  The present study proposes a deep learning model, named DeepSleepNet, for\nautomatic sleep stage scoring based on raw single-channel EEG. Most of the\nexisting methods rely on hand-engineered features which require prior knowledge\nof sleep analysis. Only a few of them encode the temporal information such as\ntransition rules, which is important for identifying the next sleep stages,\ninto the extracted features. In the proposed model, we utilize Convolutional\nNeural Networks to extract time-invariant features, and bidirectional-Long\nShort-Term Memory to learn transition rules among sleep stages automatically\nfrom EEG epochs. We implement a two-step training algorithm to train our model\nefficiently. We evaluated our model using different single-channel EEGs\n(F4-EOG(Left), Fpz-Cz and Pz-Oz) from two public sleep datasets, that have\ndifferent properties (e.g., sampling rate) and scoring standards (AASM and\nR&K). The results showed that our model achieved similar overall accuracy and\nmacro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared to the\nstate-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both\ndatasets. This demonstrated that, without changing the model architecture and\nthe training algorithm, our model could automatically learn features for sleep\nstage scoring from different raw single-channel EEGs from different datasets\nwithout utilizing any hand-engineered features.\n",
        "published": "2017-03-12T00:15:32Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04046v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.04335v2",
        "title": "Practical Bayesian Optimization for Variable Cost Objectives",
        "summary": "  We propose a novel Bayesian Optimization approach for black-box functions\nwith an environmental variable whose value determines the tradeoff between\nevaluation cost and the fidelity of the evaluations. Further, we use a novel\napproach to sampling support points, allowing faster construction of the\nacquisition function. This allows us to achieve optimization with lower\noverheads than previous approaches and is implemented for a more general class\nof problem. We show this approach to be effective on synthetic and real world\nbenchmark problems.\n",
        "published": "2017-03-13T11:19:52Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04335v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.04455v6",
        "title": "Multivariate Gaussian and Student$-t$ Process Regression for\n  Multi-output Prediction",
        "summary": "  Gaussian process model for vector-valued function has been shown to be useful\nfor multi-output prediction. The existing method for this model is to\nre-formulate the matrix-variate Gaussian distribution as a multivariate normal\ndistribution. Although it is effective in many cases, re-formulation is not\nalways workable and is difficult to apply to other distributions because not\nall matrix-variate distributions can be transformed to respective multivariate\ndistributions, such as the case for matrix-variate Student$-t$ distribution. In\nthis paper, we propose a unified framework which is used not only to introduce\na novel multivariate Student$-t$ process regression model (MV-TPR) for\nmulti-output prediction, but also to reformulate the multivariate Gaussian\nprocess regression (MV-GPR) that overcomes some limitations of the existing\nmethods. Both MV-GPR and MV-TPR have closed-form expressions for the marginal\nlikelihoods and predictive distributions under this unified framework and thus\ncan adopt the same optimization approaches as used in the conventional GPR. The\nusefulness of the proposed methods is illustrated through several simulated and\nreal data examples. In particular, we verify empirically that MV-TPR has\nsuperiority for the datasets considered, including air quality prediction and\nbike rent prediction. At last, the proposed methods are shown to produce\nprofitable investment strategies in the stock markets.\n",
        "published": "2017-03-13T15:43:00Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04455v6"
    },
    {
        "id": "http://arxiv.org/abs/1703.04691v5",
        "title": "Conditional Time Series Forecasting with Convolutional Neural Networks",
        "summary": "  We present a method for conditional time series forecasting based on an\nadaptation of the recent deep convolutional WaveNet architecture. The proposed\nnetwork contains stacks of dilated convolutions that allow it to access a broad\nrange of history when forecasting, a ReLU activation function and conditioning\nis performed by applying multiple convolutional filters in parallel to separate\ntime series which allows for the fast processing of data and the exploitation\nof the correlation structure between the multivariate time series. We test and\nanalyze the performance of the convolutional network both unconditionally as\nwell as conditionally for financial time series forecasting using the S&P500,\nthe volatility index, the CBOE interest rate and several exchange rates and\nextensively compare it to the performance of the well-known autoregressive\nmodel and a long-short term memory network. We show that a convolutional\nnetwork is well-suited for regression-type problems and is able to effectively\nlearn dependencies in and between the series without the need for long\nhistorical time series, is a time-efficient and easy to implement alternative\nto recurrent-type networks and tends to outperform linear and recurrent models.\n",
        "published": "2017-03-14T20:07:12Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04691v5"
    },
    {
        "id": "http://arxiv.org/abs/1703.04778v1",
        "title": "A statistical model for aggregating judgments by incorporating peer\n  predictions",
        "summary": "  We propose a probabilistic model to aggregate the answers of respondents\nanswering multiple-choice questions. The model does not assume that everyone\nhas access to the same information, and so does not assume that the consensus\nanswer is correct. Instead, it infers the most probable world state, even if\nonly a minority vote for it. Each respondent is modeled as receiving a signal\ncontingent on the actual world state, and as using this signal to both\ndetermine their own answer and predict the answers given by others. By\nincorporating respondent's predictions of others' answers, the model infers\nlatent parameters corresponding to the prior over world states and the\nprobability of different signals being received in all possible world states,\nincluding counterfactual ones. Unlike other probabilistic models for\naggregation, our model applies to both single and multiple questions, in which\ncase it estimates each respondent's expertise. The model shows good\nperformance, compared to a number of other probabilistic models, on data from\nseven studies covering different types of expertise.\n",
        "published": "2017-03-14T22:23:17Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04778v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.04832v1",
        "title": "A Random Finite Set Model for Data Clustering",
        "summary": "  The goal of data clustering is to partition data points into groups to\nminimize a given objective function. While most existing clustering algorithms\ntreat each data point as vector, in many applications each datum is not a\nvector but a point pattern or a set of points. Moreover, many existing\nclustering methods require the user to specify the number of clusters, which is\nnot available in advance. This paper proposes a new class of models for data\nclustering that addresses set-valued data as well as unknown number of\nclusters, using a Dirichlet Process mixture of Poisson random finite sets. We\nalso develop an efficient Markov Chain Monte Carlo posterior inference\ntechnique that can learn the number of clusters and mixture parameters\nautomatically from the data. Numerical studies are presented to demonstrate the\nsalient features of this new model, in particular its capacity to discover\nextremely unbalanced clusters in data.\n",
        "published": "2017-03-14T23:35:57Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04832v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.04864v3",
        "title": "Optimization for L1-Norm Error Fitting via Data Aggregation",
        "summary": "  We propose a data aggregation-based algorithm with monotonic convergence to a\nglobal optimum for a generalized version of the L1-norm error fitting model\nwith an assumption of the fitting function. The proposed algorithm generalizes\nthe recent algorithm in the literature, aggregate and iterative disaggregate\n(AID), which selectively solves three specific L1-norm error fitting problems.\nWith the proposed algorithm, any L1-norm error fitting model can be solved\noptimally if it follows the form of the L1-norm error fitting problem and if\nthe fitting function satisfies the assumption. The proposed algorithm can also\nsolve multi-dimensional fitting problems with arbitrary constraints on the\nfitting coefficients matrix. The generalized problem includes popular models\nsuch as regression and the orthogonal Procrustes problem. The results of the\ncomputational experiment show that the proposed algorithms are faster than the\nstate-of-the-art benchmarks for L1-norm regression subset selection and L1-norm\nregression over a sphere. Further, the relative performance of the proposed\nalgorithm improves as data size increases.\n",
        "published": "2017-03-15T01:16:09Z",
        "pdf_link": "http://arxiv.org/pdf/1703.04864v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.05841v1",
        "title": "Adaptivity to Noise Parameters in Nonparametric Active Learning",
        "summary": "  This work addresses various open questions in the theory of active learning\nfor nonparametric classification. Our contributions are both statistical and\nalgorithmic: -We establish new minimax-rates for active learning under common\n\\textit{noise conditions}. These rates display interesting transitions -- due\nto the interaction between noise \\textit{smoothness and margin} -- not present\nin the passive setting. Some such transitions were previously conjectured, but\nremained unconfirmed. -We present a generic algorithmic strategy for adaptivity\nto unknown noise smoothness and margin; our strategy achieves optimal rates in\nmany general situations; furthermore, unlike in previous work, we avoid the\nneed for \\textit{adaptive confidence sets}, resulting in strictly milder\ndistributional requirements.\n",
        "published": "2017-03-16T22:37:55Z",
        "pdf_link": "http://arxiv.org/pdf/1703.05841v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.06177v2",
        "title": "On Consistency of Graph-based Semi-supervised Learning",
        "summary": "  Graph-based semi-supervised learning is one of the most popular methods in\nmachine learning. Some of its theoretical properties such as bounds for the\ngeneralization error and the convergence of the graph Laplacian regularizer\nhave been studied in computer science and statistics literatures. However, a\nfundamental statistical property, the consistency of the estimator from this\nmethod has not been proved. In this article, we study the consistency problem\nunder a non-parametric framework. We prove the consistency of graph-based\nlearning in the case that the estimated scores are enforced to be equal to the\nobserved responses for the labeled data. The sample sizes of both labeled and\nunlabeled data are allowed to grow in this result. When the estimated scores\nare not required to be equal to the observed responses, a tuning parameter is\nused to balance the loss function and the graph Laplacian regularizer. We give\na counterexample demonstrating that the estimator for this case can be\ninconsistent. The theoretical findings are supported by numerical studies.\n",
        "published": "2017-03-17T19:24:09Z",
        "pdf_link": "http://arxiv.org/pdf/1703.06177v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.06240v1",
        "title": "Multi-fidelity Bayesian Optimisation with Continuous Approximations",
        "summary": "  Bandit methods for black-box optimisation, such as Bayesian optimisation, are\nused in a variety of applications including hyper-parameter tuning and\nexperiment design. Recently, \\emph{multi-fidelity} methods have garnered\nconsiderable attention since function evaluations have become increasingly\nexpensive in such applications. Multi-fidelity methods use cheap approximations\nto the function of interest to speed up the overall optimisation process.\nHowever, most multi-fidelity methods assume only a finite number of\napproximations. In many practical applications however, a continuous spectrum\nof approximations might be available. For instance, when tuning an expensive\nneural network, one might choose to approximate the cross validation\nperformance using less data $N$ and/or few training iterations $T$. Here, the\napproximations are best viewed as arising out of a continuous two dimensional\nspace $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA,\nfor this setting. We characterise its theoretical properties and show that it\nachieves better regret than than strategies which ignore the approximations.\nBOCA outperforms several other baselines in synthetic and real experiments.\n",
        "published": "2017-03-18T03:28:40Z",
        "pdf_link": "http://arxiv.org/pdf/1703.06240v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.06476v2",
        "title": "Practical Coreset Constructions for Machine Learning",
        "summary": "  We investigate coresets - succinct, small summaries of large data sets - so\nthat solutions found on the summary are provably competitive with solution\nfound on the full data set. We provide an overview over the state-of-the-art in\ncoreset construction for machine learning. In Section 2, we present both the\nintuition behind and a theoretically sound framework to construct coresets for\ngeneral problems and apply it to $k$-means clustering. In Section 3 we\nsummarize existing coreset construction algorithms for a variety of machine\nlearning problems such as maximum likelihood estimation of mixture models,\nBayesian non-parametric models, principal component analysis, regression and\ngeneral empirical risk minimization.\n",
        "published": "2017-03-19T17:45:29Z",
        "pdf_link": "http://arxiv.org/pdf/1703.06476v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.06528v1",
        "title": "Universal Consistency and Robustness of Localized Support Vector\n  Machines",
        "summary": "  The massive amount of available data potentially used to discover patters in\nmachine learning is a challenge for kernel based algorithms with respect to\nruntime and storage capacities. Local approaches might help to relieve these\nissues. From a statistical point of view local approaches allow additionally to\ndeal with different structures in the data in different ways. This paper\nanalyses properties of localized kernel based, non-parametric statistical\nmachine learning methods, in particular of support vector machines (SVMs) and\nmethods close to them. We will show there that locally learnt kernel methods\nare universal consistent. Furthermore, we give an upper bound for the maxbias\nin order to show statistical robustness of the proposed method.\n",
        "published": "2017-03-19T22:23:01Z",
        "pdf_link": "http://arxiv.org/pdf/1703.06528v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.07596v2",
        "title": "Testing and Learning on Distributions with Symmetric Noise Invariance",
        "summary": "  Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD),\nthe resulting distance between distributions, are useful tools for fully\nnonparametric two-sample testing and learning on distributions. However, it is\nrarely that all possible differences between samples are of interest --\ndiscovered differences can be due to different types of measurement noise, data\ncollection artefacts or other irrelevant sources of variability. We propose\ndistances between distributions which encode invariance to additive symmetric\nnoise, aimed at testing whether the assumed true underlying processes differ.\nMoreover, we construct invariant features of distributions, leading to learning\nalgorithms robust to the impairment of the input distributions with symmetric\nadditive noise.\n",
        "published": "2017-03-22T10:40:40Z",
        "pdf_link": "http://arxiv.org/pdf/1703.07596v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.08031v1",
        "title": "Distribution of Gaussian Process Arc Lengths",
        "summary": "  We present the first treatment of the arc length of the Gaussian Process (GP)\nwith more than a single output dimension. GPs are commonly used for tasks such\nas trajectory modelling, where path length is a crucial quantity of interest.\nPreviously, only paths in one dimension have been considered, with no\ntheoretical consideration of higher dimensional problems. We fill the gap in\nthe existing literature by deriving the moments of the arc length for a\nstationary GP with multiple output dimensions. A new method is used to derive\nthe mean of a one-dimensional GP over a finite interval, by considering the\ndistribution of the arc length integrand. This technique is used to derive an\napproximate distribution over the arc length of a vector valued GP in\n$\\mathbb{R}^n$ by moment matching the distribution. Numerical simulations\nconfirm our theoretical derivations.\n",
        "published": "2017-03-23T12:17:00Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08031v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.08065v2",
        "title": "Robustness of Maximum Correntropy Estimation Against Large Outliers",
        "summary": "  The maximum correntropy criterion (MCC) has recently been successfully\napplied in robust regression, classification and adaptive filtering, where the\ncorrentropy is maximized instead of minimizing the well-known mean square error\n(MSE) to improve the robustness with respect to outliers (or impulsive noises).\nConsiderable efforts have been devoted to develop various robust adaptive\nalgorithms under MCC, but so far little insight has been gained as to how the\noptimal solution will be affected by outliers. In this work, we study this\nproblem in the context of parameter estimation for a simple linear\nerrors-in-variables (EIV) model where all variables are scalar. Under certain\nconditions, we derive an upper bound on the absolute value of the estimation\nerror and show that the optimal solution under MCC can be very close to the\ntrue value of the unknown parameter even with outliers (whose values can be\narbitrarily large) in both input and output variables. Illustrative examples\nare presented to verify and clarify the theory.\n",
        "published": "2017-03-23T13:41:47Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08065v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.08085v4",
        "title": "Reducing Crowdsourcing to Graphon Estimation, Statistically",
        "summary": "  Inferring the correct answers to binary tasks based on multiple noisy answers\nin an unsupervised manner has emerged as the canonical question for micro-task\ncrowdsourcing or more generally aggregating opinions. In graphon estimation,\none is interested in estimating edge intensities or probabilities between nodes\nusing a single snapshot of a graph realization. In the recent literature, there\nhas been exciting development within both of these topics. In the context of\ncrowdsourcing, the key intellectual challenge is to understand whether a given\ntask can be more accurately denoised by aggregating answers collected from\nother different tasks. In the context of graphon estimation, precise\ninformation limits and estimation algorithms remain of interest. In this paper,\nwe utilize a statistical reduction from crowdsourcing to graphon estimation to\nadvance the state-of-art for both of these challenges. We use concepts from\ngraphon estimation to design an algorithm that achieves better performance than\nthe {\\em majority voting} scheme for a setup that goes beyond the {\\em rank\none} models considered in the literature. We use known explicit lower bounds\nfor crowdsourcing to provide refined lower bounds for graphon estimation.\n",
        "published": "2017-03-23T14:29:29Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08085v4"
    },
    {
        "id": "http://arxiv.org/abs/1703.08110v2",
        "title": "Training Gaussian Mixture Models at Scale via Coresets",
        "summary": "  How can we train a statistical mixture model on a massive data set? In this\nwork we show how to construct coresets for mixtures of Gaussians. A coreset is\na weighted subset of the data, which guarantees that models fitting the coreset\nalso provide a good fit for the original data set. We show that, perhaps\nsurprisingly, Gaussian mixtures admit coresets of size polynomial in dimension\nand the number of mixture components, while being independent of the data set\nsize. Hence, one can harness computationally intensive algorithms to compute a\ngood approximation on a significantly smaller data set. More importantly, such\ncoresets can be efficiently constructed both in distributed and streaming\nsettings and do not impose restrictions on the data generating process. Our\nresults rely on a novel reduction of statistical estimation to problems in\ncomputational geometry and new combinatorial complexity results for mixtures of\nGaussians. Empirical evaluation on several real-world datasets suggests that\nour coreset-based approach enables significant reduction in training-time with\nnegligible approximation error.\n",
        "published": "2017-03-23T15:35:33Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08110v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.08251v1",
        "title": "The Dependence of Machine Learning on Electronic Medical Record Quality",
        "summary": "  There is growing interest in applying machine learning methods to Electronic\nMedical Records (EMR). Across different institutions, however, EMR quality can\nvary widely. This work investigated the impact of this disparity on the\nperformance of three advanced machine learning algorithms: logistic regression,\nmultilayer perceptron, and recurrent neural network. The EMR disparity was\nemulated using different permutations of the EMR collected at Children's\nHospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and\nCardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using\npatients from the PICU to predict in-ICU mortality for patients in a held out\nset of PICU and CTICU patients. The disparate patient populations between the\nPICU and CTICU provide an estimate of generalization errors across different\nICUs. We quantified and evaluated the generalization of these algorithms on\nvarying EMR size, input types, and fidelity of data.\n",
        "published": "2017-03-23T23:27:12Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08251v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.08619v4",
        "title": "Binarsity: a penalization for one-hot encoded features in linear\n  supervised learning",
        "summary": "  This paper deals with the problem of large-scale linear supervised learning\nin settings where a large number of continuous features are available. We\npropose to combine the well-known trick of one-hot encoding of continuous\nfeatures with a new penalization called \\emph{binarsity}. In each group of\nbinary features coming from the one-hot encoding of a single raw continuous\nfeature, this penalization uses total-variation regularization together with an\nextra linear constraint. This induces two interesting properties on the model\nweights of the one-hot encoded features: they are piecewise constant, and are\neventually block sparse. Non-asymptotic oracle inequalities for generalized\nlinear models are proposed. Moreover, under a sparse additive model assumption,\nwe prove that our procedure matches the state-of-the-art in this setting.\nNumerical experiments illustrate the good performances of our approach on\nseveral datasets. It is also noteworthy that our method has a numerical\ncomplexity comparable to standard $\\ell_1$ penalization.\n",
        "published": "2017-03-24T22:54:17Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08619v4"
    },
    {
        "id": "http://arxiv.org/abs/1703.08737v1",
        "title": "Learning to Predict: A Fast Re-constructive Method to Generate\n  Multimodal Embeddings",
        "summary": "  Integrating visual and linguistic information into a single multimodal\nrepresentation is an unsolved problem with wide-reaching applications to both\nnatural language processing and computer vision. In this paper, we present a\nsimple method to build multimodal representations by learning a\nlanguage-to-vision mapping and using its output to build multimodal embeddings.\nIn this sense, our method provides a cognitively plausible way of building\nrepresentations, consistent with the inherently re-constructive and associative\nnature of human memory. Using seven benchmark concept similarity tests we show\nthat the mapped vectors not only implicitly encode multimodal information, but\nalso outperform strong unimodal baselines and state-of-the-art multimodal\nmethods, thus exhibiting more \"human-like\" judgments---particularly in\nzero-shot settings.\n",
        "published": "2017-03-25T20:06:10Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08737v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.08937v1",
        "title": "A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis",
        "summary": "  Existing strategies for finite-armed stochastic bandits mostly depend on a\nparameter of scale that must be known in advance. Sometimes this is in the form\nof a bound on the payoffs, or the knowledge of a variance or subgaussian\nparameter. The notable exceptions are the analysis of Gaussian bandits with\nunknown mean and variance by Cowan and Katehakis [2015] and of uniform\ndistributions with unknown support [Cowan and Katehakis, 2015]. The results\nderived in these specialised cases are generalised here to the non-parametric\nsetup, where the learner knows only a bound on the kurtosis of the noise, which\nis a scale free measure of the extremity of outliers.\n",
        "published": "2017-03-27T05:41:03Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08937v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.08972v1",
        "title": "Thompson Sampling for Linear-Quadratic Control Problems",
        "summary": "  We consider the exploration-exploitation tradeoff in linear quadratic (LQ)\ncontrol problems, where the state dynamics is linear and the cost function is\nquadratic in states and controls. We analyze the regret of Thompson sampling\n(TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist\nsetting, i.e., when the parameters characterizing the LQ dynamics are fixed.\nDespite the empirical and theoretical success in a wide range of problems from\nmulti-armed bandit to linear bandit, we show that when studying the frequentist\nregret TS in control problems, we need to trade-off the frequency of sampling\noptimistic parameters and the frequency of switches in the control policy. This\nresults in an overall regret of $O(T^{2/3})$, which is significantly worse than\nthe regret $O(\\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty\nalgorithm in LQ control problems.\n",
        "published": "2017-03-27T08:45:57Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08972v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.08991v2",
        "title": "Multilabel Classification with R Package mlr",
        "summary": "  We implemented several multilabel classification algorithms in the machine\nlearning package mlr. The implemented methods are binary relevance, classifier\nchains, nested stacking, dependent binary relevance and stacking, which can be\nused with any base learner that is accessible in mlr. Moreover, there is access\nto the multilabel classification versions of randomForestSRC and rFerns. All\nthese methods can be easily compared by different implemented multilabel\nperformance measures and resampling methods in the standardized mlr framework.\nIn a benchmark experiment with several multilabel datasets, the performance of\nthe different methods is evaluated.\n",
        "published": "2017-03-27T10:03:27Z",
        "pdf_link": "http://arxiv.org/pdf/1703.08991v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.09112v2",
        "title": "Sparse Multi-Output Gaussian Processes for Medical Time Series\n  Prediction",
        "summary": "  In the scenario of real-time monitoring of hospital patients, high-quality\ninference of patients' health status using all information available from\nclinical covariates and lab tests is essential to enable successful medical\ninterventions and improve patient outcomes. Developing a computational\nframework that can learn from observational large-scale electronic health\nrecords (EHRs) and make accurate real-time predictions is a critical step. In\nthis work, we develop and explore a Bayesian nonparametric model based on\nGaussian process (GP) regression for hospital patient monitoring. We propose\nMedGP, a statistical framework that incorporates 24 clinical and lab covariates\nand supports a rich reference data set from which relationships between\nobserved covariates may be inferred and exploited for high-quality inference of\npatient state over time. To do this, we develop a highly structured sparse GP\nkernel to enable tractable computation over tens of thousands of time points\nwhile estimating correlations among clinical covariates, patients, and\nperiodicity in patient observations. MedGP has a number of benefits over\ncurrent methods, including (i) not requiring an alignment of the time series\ndata, (ii) quantifying confidence regions in the predictions, (iii) exploiting\na vast and rich database of patients, and (iv) inferring interpretable\nrelationships among clinical covariates. We evaluate and compare results from\nMedGP on the task of online prediction for three patient subgroups from two\nmedical data sets across 8,043 patients. We found MedGP improves online\nprediction over baseline methods for nearly all covariates across different\ndisease subgroups and studies. The publicly available code is at\nhttps://github.com/bee-hive/MedGP.\n",
        "published": "2017-03-27T14:38:15Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09112v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.09165v3",
        "title": "PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for\n  Low-Dose 3D CT Image Reconstruction",
        "summary": "  The development of computed tomography (CT) image reconstruction methods that\nsignificantly reduce patient radiation exposure while maintaining high image\nquality is an important area of research in low-dose CT (LDCT) imaging. We\npropose a new penalized weighted least squares (PWLS) reconstruction method\nthat exploits regularization based on an efficient Union of Learned TRAnsforms\n(PWLS-ULTRA). The union of square transforms is pre-learned from numerous image\npatches extracted from a dataset of CT images or volumes. The proposed\nPWLS-based cost function is optimized by alternating between a CT image\nreconstruction step, and a sparse coding and clustering step. The CT image\nreconstruction step is accelerated by a relaxed linearized augmented Lagrangian\nmethod with ordered-subsets that reduces the number of forward and back\nprojections. Simulations with 2-D and 3-D axial CT scans of the extended\ncardiac-torso phantom and 3D helical chest and abdomen scans show that for both\nnormal-dose and low-dose levels, the proposed method significantly improves the\nquality of reconstructed images compared to PWLS reconstruction with a\nnonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization\nbased on a union of learned transforms leads to better image reconstructions\nthan using a single learned square transform. We also incorporate patch-based\nweights in PWLS-ULTRA that enhance image quality and help improve image\nresolution uniformity. The proposed approach achieves comparable or better\nimage quality compared to learned overcomplete synthesis dictionaries, but\nimportantly, is much faster (computationally more efficient).\n",
        "published": "2017-03-27T16:16:35Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09165v3"
    },
    {
        "id": "http://arxiv.org/abs/1703.09207v2",
        "title": "Fairness in Criminal Justice Risk Assessments: The State of the Art",
        "summary": "  Objectives: Discussions of fairness in criminal justice risk assessments\ntypically lack conceptual precision. Rhetoric too often substitutes for careful\nanalysis. In this paper, we seek to clarify the tradeoffs between different\nkinds of fairness and between fairness and accuracy.\n  Methods: We draw on the existing literatures in criminology, computer science\nand statistics to provide an integrated examination of fairness and accuracy in\ncriminal justice risk assessments. We also provide an empirical illustration\nusing data from arraignments.\n  Results: We show that there are at least six kinds of fairness, some of which\nare incompatible with one another and with accuracy.\n  Conclusions: Except in trivial cases, it is impossible to maximize accuracy\nand fairness at the same time, and impossible simultaneously to satisfy all\nkinds of fairness. In practice, a major complication is different base rates\nacross different legally protected groups. There is a need to consider\nchallenging tradeoffs.\n",
        "published": "2017-03-27T17:50:53Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09207v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.09528v4",
        "title": "Discovering Latent Covariance Structures for Multiple Time Series",
        "summary": "  Analyzing multivariate time series data is important to predict future events\nand changes of complex systems in finance, manufacturing, and administrative\ndecisions. The expressiveness power of Gaussian Process (GP) regression methods\nhas been significantly improved by compositional covariance structures. In this\npaper, we present a new GP model which naturally handles multiple time series\nby placing an Indian Buffet Process (IBP) prior on the presence of shared\nkernels. Our selective covariance structure decomposition allows exploiting\nshared parameters over a set of multiple, selected time series. We also\ninvestigate the well-definedness of the models when infinite latent components\nare introduced. We present a pragmatic search algorithm which explores a larger\nstructure space efficiently. Experiments conducted on five real-world data sets\ndemonstrate that our new model outperforms existing methods in term of\nstructure discoveries and predictive performances.\n",
        "published": "2017-03-28T12:10:45Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09528v4"
    },
    {
        "id": "http://arxiv.org/abs/1703.09631v1",
        "title": "Algebraic Variety Models for High-Rank Matrix Completion",
        "summary": "  We consider a generalization of low-rank matrix completion to the case where\nthe data belongs to an algebraic variety, i.e. each data point is a solution to\na system of polynomial equations. In this case the original matrix is possibly\nhigh-rank, but it becomes low-rank after mapping each column to a higher\ndimensional space of monomial features. Many well-studied extensions of linear\nmodels, including affine subspaces and their union, can be described by a\nvariety model. In addition, varieties can be used to model a richer class of\nnonlinear quadratic and higher degree curves and surfaces. We study the\nsampling requirements for matrix completion under a variety model with a focus\non a union of affine subspaces. We also propose an efficient matrix completion\nalgorithm that minimizes a convex or non-convex surrogate of the rank of the\nmatrix of monomial features. Our algorithm uses the well-known \"kernel trick\"\nto avoid working directly with the high-dimensional monomial matrix. We show\nthe proposed algorithm is able to recover synthetically generated data up to\nthe predicted sampling complexity bounds. The proposed algorithm also\noutperforms standard low rank matrix completion and subspace clustering\ntechniques in experiments with real data.\n",
        "published": "2017-03-28T15:28:28Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09631v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.09813v1",
        "title": "Gradient-based Regularization Parameter Selection for Problems with\n  Non-smooth Penalty Functions",
        "summary": "  In high-dimensional and/or non-parametric regression problems, regularization\n(or penalization) is used to control model complexity and induce desired\nstructure. Each penalty has a weight parameter that indicates how strongly the\nstructure corresponding to that penalty should be enforced. Typically the\nparameters are chosen to minimize the error on a separate validation set using\na simple grid search or a gradient-free optimization method. It is more\nefficient to tune parameters if the gradient can be determined, but this is\noften difficult for problems with non-smooth penalty functions. Here we show\nthat for many penalized regression problems, the validation loss is actually\nsmooth almost-everywhere with respect to the penalty parameters. We can\ntherefore apply a modified gradient descent algorithm to tune parameters.\nThrough simulation studies on example regression problems, we find that\nincreasing the number of penalty parameters and tuning them using our method\ncan decrease the generalization error.\n",
        "published": "2017-03-28T21:41:13Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09813v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.09975v2",
        "title": "Improving Spectral Clustering using the Asymptotic Value of the\n  Normalised Cut",
        "summary": "  Spectral clustering is a popular and versatile clustering method based on a\nrelaxation of the normalised graph cut objective. Despite its popularity,\nhowever, there is no single agreed upon method for tuning the important scaling\nparameter, nor for determining automatically the number of clusters to extract.\nPopular heuristics exist, but corresponding theoretical results are scarce. In\nthis paper we investigate the asymptotic value of the normalised cut for an\nincreasing sample assumed to arise from an underlying probability distribution,\nand based on this result provide recommendations for improving spectral\nclustering methodology. A corresponding algorithm is proposed with strong\nempirical performance.\n",
        "published": "2017-03-29T11:28:55Z",
        "pdf_link": "http://arxiv.org/pdf/1703.09975v2"
    },
    {
        "id": "http://arxiv.org/abs/1703.10010v1",
        "title": "Optimal Policies for Observing Time Series and Related Restless Bandit\n  Problems",
        "summary": "  The trade-off between the cost of acquiring and processing data, and\nuncertainty due to a lack of data is fundamental in machine learning. A basic\ninstance of this trade-off is the problem of deciding when to make noisy and\ncostly observations of a discrete-time Gaussian random walk, so as to minimise\nthe posterior variance plus observation costs. We present the first proof that\na simple policy, which observes when the posterior variance exceeds a\nthreshold, is optimal for this problem. The proof generalises to a wide range\nof cost functions other than the posterior variance.\n  This result implies that optimal policies for linear-quadratic-Gaussian\ncontrol with costly observations have a threshold structure. It also implies\nthat the restless bandit problem of observing multiple such time series, has a\nwell-defined Whittle index. We discuss computation of that index, give\nclosed-form formulae for it, and compare the performance of the associated\nindex policy with heuristic policies.\n  The proof is based on a new verification theorem that demonstrates threshold\nstructure for Markov decision processes, and on the relation between binary\nsequences known as mechanical words and the dynamics of discontinuous nonlinear\nmaps, which frequently arise in physics, control and biology.\n",
        "published": "2017-03-29T12:46:59Z",
        "pdf_link": "http://arxiv.org/pdf/1703.10010v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.10827v1",
        "title": "Intraoperative margin assessment of human breast tissue in optical\n  coherence tomography images using deep neural networks",
        "summary": "  Objective: In this work, we perform margin assessment of human breast tissue\nfrom optical coherence tomography (OCT) images using deep neural networks\n(DNNs). This work simulates an intraoperative setting for breast cancer\nlumpectomy. Methods: To train the DNNs, we use both the state-of-the-art\nmethods (Weight Decay and DropOut) and a newly introduced regularization method\nbased on function norms. Commonly used methods can fail when only a small\ndatabase is available. The use of a function norm introduces a direct control\nover the complexity of the function with the aim of diminishing the risk of\noverfitting. Results: As neither the code nor the data of previous results are\npublicly available, the obtained results are compared with reported results in\nthe literature for a conservative comparison. Moreover, our method is applied\nto locally collected data on several data configurations. The reported results\nare the average over the different trials. Conclusion: The experimental results\nshow that the use of DNNs yields significantly better results than other\ntechniques when evaluated in terms of sensitivity, specificity, F1 score,\nG-mean and Matthews correlation coefficient. Function norm regularization\nyielded higher and more robust results than competing methods. Significance: We\nhave demonstrated a system that shows high promise for (partially) automated\nmargin assessment of human breast tissue, Equal error rate (EER) is reduced\nfrom approximately 12\\% (the lowest reported in the literature) to 5\\%\\,--\\,a\n58\\% reduction. The method is computationally feasible for intraoperative\napplication (less than 2 seconds per image).\n",
        "published": "2017-03-31T10:05:00Z",
        "pdf_link": "http://arxiv.org/pdf/1703.10827v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.10935v1",
        "title": "The Risk of Machine Learning",
        "summary": "  Many applied settings in empirical economics involve simultaneous estimation\nof a large number of parameters. In particular, applied economists are often\ninterested in estimating the effects of many-valued treatments (like teacher\neffects or location effects), treatment effects for many groups, and prediction\nmodels with many regressors. In these settings, machine learning methods that\ncombine regularized estimation and data-driven choices of regularization\nparameters are useful to avoid over-fitting. In this article, we analyze the\nperformance of a class of machine learning estimators that includes ridge,\nlasso and pretest in contexts that require simultaneous estimation of many\nparameters. Our analysis aims to provide guidance to applied researchers on (i)\nthe choice between regularized estimators in practice and (ii) data-driven\nselection of regularization parameters. To address (i), we characterize the\nrisk (mean squared error) of regularized estimators and derive their relative\nperformance as a function of simple features of the data generating process. To\naddress (ii), we show that data-driven choices of regularization parameters,\nbased on Stein's unbiased risk estimate or on cross-validation, yield\nestimators with risk uniformly close to the risk attained under the optimal\n(unfeasible) choice of regularization parameters. We use data from recent\nexamples in the empirical economics literature to illustrate the practical\napplicability of our results.\n",
        "published": "2017-03-31T15:13:33Z",
        "pdf_link": "http://arxiv.org/pdf/1703.10935v1"
    },
    {
        "id": "http://arxiv.org/abs/1703.10936v1",
        "title": "Prediction of infectious disease epidemics via weighted density\n  ensembles",
        "summary": "  Accurate and reliable predictions of infectious disease dynamics can be\nvaluable to public health organizations that plan interventions to decrease or\nprevent disease transmission. A great variety of models have been developed for\nthis task, using different model structures, covariates, and targets for\nprediction. Experience has shown that the performance of these models varies;\nsome tend to do better or worse in different seasons or at different points\nwithin a season. Ensemble methods combine multiple models to obtain a single\nprediction that leverages the strengths of each model. We considered a range of\nensemble methods that each form a predictive density for a target of interest\nas a weighted sum of the predictive densities from component models. In the\nsimplest case, equal weight is assigned to each component model; in the most\ncomplex case, the weights vary with the region, prediction target, week of the\nseason when the predictions are made, a measure of component model uncertainty,\nand recent observations of disease incidence. We applied these methods to\npredict measures of influenza season timing and severity in the United States,\nboth at the national and regional levels, using three component models. We\ntrained the models on retrospective predictions from 14 seasons (1997/1998 -\n2010/2011) and evaluated each model's prospective, out-of-sample performance in\nthe five subsequent influenza seasons. In this test phase, the ensemble methods\nshowed overall performance that was similar to the best of the component\nmodels, but offered more consistent performance across seasons than the\ncomponent models. Ensemble methods offer the potential to deliver more reliable\npredictions to public health decision makers.\n",
        "published": "2017-03-31T15:13:54Z",
        "pdf_link": "http://arxiv.org/pdf/1703.10936v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.00003v3",
        "title": "Ensemble Sales Forecasting Study in Semiconductor Industry",
        "summary": "  Sales forecasting plays a prominent role in business planning and business\nstrategy. The value and importance of advance information is a cornerstone of\nplanning activity, and a well-set forecast goal can guide sale-force more\nefficiently. In this paper CPU sales forecasting of Intel Corporation, a\nmultinational semiconductor industry, was considered. Past sale, future\nbooking, exchange rates, Gross domestic product (GDP) forecasting, seasonality\nand other indicators were innovatively incorporated into the quantitative\nmodeling. Benefit from the recent advances in computation power and software\ndevelopment, millions of models built upon multiple regressions, time series\nanalysis, random forest and boosting tree were executed in parallel. The models\nwith smaller validation errors were selected to form the ensemble model. To\nbetter capture the distinct characteristics, forecasting models were\nimplemented at lead time and lines of business level. The moving windows\nvalidation process automatically selected the models which closely represent\ncurrent market condition. The weekly cadence forecasting schema allowed the\nmodel to response effectively to market fluctuation. Generic variable\nimportance analysis was also developed to increase the model interpretability.\nRather than assuming fixed distribution, this non-parametric permutation\nvariable importance analysis provided a general framework across methods to\nevaluate the variable importance. This variable importance framework can\nfurther extend to classification problem by modifying the mean absolute\npercentage error(MAPE) into misclassify error. Please find the demo code at :\nhttps://github.com/qx0731/ensemble_forecast_methods\n",
        "published": "2017-04-28T03:50:15Z",
        "pdf_link": "http://arxiv.org/pdf/1705.00003v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.00394v1",
        "title": "Stochastic Divergence Minimization for Biterm Topic Model",
        "summary": "  As the emergence and the thriving development of social networks, a huge\nnumber of short texts are accumulated and need to be processed. Inferring\nlatent topics of collected short texts is useful for understanding its hidden\nstructure and predicting new contents. Unlike conventional topic models such as\nlatent Dirichlet allocation (LDA), a biterm topic model (BTM) was recently\nproposed for short texts to overcome the sparseness of document-level word\nco-occurrences by directly modeling the generation process of word pairs.\nStochastic inference algorithms based on collapsed Gibbs sampling (CGS) and\ncollapsed variational inference have been proposed for BTM. However, they\neither require large computational complexity, or rely on very crude\nestimation. In this work, we develop a stochastic divergence minimization\ninference algorithm for BTM to estimate latent topics more accurately in a\nscalable way. Experiments demonstrate the superiority of our proposed algorithm\ncompared with existing inference algorithms.\n",
        "published": "2017-05-01T01:05:09Z",
        "pdf_link": "http://arxiv.org/pdf/1705.00394v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.00461v2",
        "title": "Optimal Projected Variance Group-Sparse Block PCA",
        "summary": "  We address the problem of defining a group sparse formulation for Principal\nComponents Analysis (PCA) - or its equivalent formulations as Low Rank\napproximation or Dictionary Learning problems - which achieves a compromise\nbetween maximizing the variance explained by the components and promoting\nsparsity of the loadings. So we propose first a new definition of the variance\nexplained by non necessarily orthogonal components, which is optimal in some\naspect and compatible with the principal components situation. Then we use a\nspecific regularization of this variance by the group-$\\ell_{1}$ norm to define\na Group Sparse Maximum Variance (GSMV) formulation of PCA. The GSMV formulation\nachieves our objective by construction, and has the nice property that the\ninner non smooth optimization problem can be solved analytically, thus reducing\nGSMV to the maximization of a smooth and convex function under unit norm and\northogonality constraints, which generalizes Journee et al. (2010) to group\nsparsity. Numerical comparison with deflation on synthetic data shows that GSMV\nproduces steadily slightly better and more robust results for the retrieval of\nhidden sparse structures, and is about three times faster on these examples.\nApplication to real data shows the interest of group sparsity for variables\nselection in PCA of mixed data (categorical/numerical) .\n",
        "published": "2017-05-01T10:30:05Z",
        "pdf_link": "http://arxiv.org/pdf/1705.00461v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.00674v5",
        "title": "Vertex Nomination Via Seeded Graph Matching",
        "summary": "  Consider two networks on overlapping, non-identical vertex sets. Given\nvertices of interest in the first network, we seek to identify the\ncorresponding vertices, if any exist, in the second network. While in\nmoderately sized networks graph matching methods can be applied directly to\nrecover the missing correspondences, herein we present a principled methodology\nappropriate for situations in which the networks are too large for brute-force\ngraph matching. Our methodology identifies vertices in a local neighborhood of\nthe vertices of interest in the first network that have verifiable\ncorresponding vertices in the second network. Leveraging these known\ncorrespondences, referred to as seeds, we match the induced subgraphs in each\nnetwork generated by the neighborhoods of these verified seeds, and rank the\nvertices of the second network in terms of the most likely matches to the\noriginal vertices of interest. We demonstrate the applicability of our\nmethodology through simulations and real data examples.\n",
        "published": "2017-05-01T19:21:27Z",
        "pdf_link": "http://arxiv.org/pdf/1705.00674v5"
    },
    {
        "id": "http://arxiv.org/abs/1705.00956v3",
        "title": "Experimental Design for Non-Parametric Correction of Misspecified\n  Dynamical Models",
        "summary": "  We consider a class of misspecified dynamical models where the governing term\nis only approximately known. Under the assumption that observations of the\nsystem's evolution are accessible for various initial conditions, our goal is\nto infer a non-parametric correction to the misspecified driving term such as\nto faithfully represent the system dynamics and devise system evolution\npredictions for unobserved initial conditions.\n  We model the unknown correction term as a Gaussian Process and analyze the\nproblem of efficient experimental design to find an optimal correction term\nunder constraints such as a limited experimental budget. We suggest a novel\nformulation for experimental design for this Gaussian Process and show that\napproximately optimal (up to a constant factor) designs may be efficiently\nderived by utilizing results from the literature on submodular optimization.\nOur numerical experiments exemplify the effectiveness of these techniques.\n",
        "published": "2017-05-02T13:23:53Z",
        "pdf_link": "http://arxiv.org/pdf/1705.00956v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.01305v2",
        "title": "Mass Volume Curves and Anomaly Ranking",
        "summary": "  This paper aims at formulating the issue of ranking multivariate unlabeled\nobservations depending on their degree of abnormality as an unsupervised\nstatistical learning task. In the 1-d situation, this problem is usually\ntackled by means of tail estimation techniques: univariate observations are\nviewed as all the more `abnormal' as they are located far in the tail(s) of the\nunderlying probability distribution. It would be desirable as well to dispose\nof a scalar valued `scoring' function allowing for comparing the degree of\nabnormality of multivariate observations. Here we formulate the issue of\nscoring anomalies as a M-estimation problem by means of a novel functional\nperformance criterion, referred to as the Mass Volume curve (MV curve in\nshort), whose optimal elements are strictly increasing transforms of the\ndensity almost everywhere on the support of the density. We first study the\nstatistical estimation of the MV curve of a given scoring function and we\nprovide a strategy to build confidence regions using a smoothed bootstrap\napproach. Optimization of this functional criterion over the set of piecewise\nconstant scoring functions is next tackled. This boils down to estimating a\nsequence of empirical minimum volume sets whose levels are chosen adaptively\nfrom the data, so as to adjust to the variations of the optimal MV curve, while\ncontroling the bias of its approximation by a stepwise curve. Generalization\nbounds are then established for the difference in sup norm between the MV curve\nof the empirical scoring function thus obtained and the optimal MV curve.\n",
        "published": "2017-05-03T08:44:32Z",
        "pdf_link": "http://arxiv.org/pdf/1705.01305v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.01342v2",
        "title": "Linear Regression with Shuffled Labels",
        "summary": "  Is it possible to perform linear regression on datasets whose labels are\nshuffled with respect to the inputs? We explore this question by proposing\nseveral estimators that recover the weights of a noisy linear model from labels\nthat are shuffled by an unknown permutation. We show that the analog of the\nclassical least-squares estimator produces inconsistent estimates in this\nsetting, and introduce an estimator based on the self-moments of the input\nfeatures and labels. We study the regimes in which each estimator excels, and\ngeneralize the estimators to the setting where partial ordering information is\navailable in the form of experiments replicated independently. The result is a\nframework that enables robust inference, as we demonstrate by experiments on\nboth synthetic and standard datasets, where we are able to recover approximate\nweights using only shuffled labels. Our work demonstrates that linear\nregression in the absence of complete ordering information is possible and can\nbe of practical interest, particularly in experiments that characterize\npopulations of particles, such as flow cytometry.\n",
        "published": "2017-05-03T10:12:12Z",
        "pdf_link": "http://arxiv.org/pdf/1705.01342v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.03297v1",
        "title": "Semiparametric spectral modeling of the Drosophila connectome",
        "summary": "  We present semiparametric spectral modeling of the complete larval Drosophila\nmushroom body connectome. Motivated by a thorough exploratory data analysis of\nthe network via Gaussian mixture modeling (GMM) in the adjacency spectral\nembedding (ASE) representation space, we introduce the latent structure model\n(LSM) for network modeling and inference. LSM is a generalization of the\nstochastic block model (SBM) and a special case of the random dot product graph\n(RDPG) latent position model, and is amenable to semiparametric GMM in the ASE\nrepresentation space. The resulting connectome code derived via semiparametric\nGMM composed with ASE captures latent connectome structure and elucidates\nbiologically relevant neuronal properties.\n",
        "published": "2017-05-09T12:51:28Z",
        "pdf_link": "http://arxiv.org/pdf/1705.03297v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.03536v3",
        "title": "SILVar: Single Index Latent Variable Models",
        "summary": "  A semi-parametric, non-linear regression model in the presence of latent\nvariables is introduced. These latent variables can correspond to unmodeled\nphenomena or unmeasured agents in a complex networked system. This new\nformulation allows joint estimation of certain non-linearities in the system,\nthe direct interactions between measured variables, and the effects of\nunmodeled elements on the observed system. The particular form of the model\nadopted is justified, and learning is posed as a regularized empirical risk\nminimization. This leads to classes of structured convex optimization problems\nwith a \"sparse plus low-rank\" flavor. Relations between the proposed model and\nseveral common model paradigms, such as those of Robust Principal Component\nAnalysis (PCA) and Vector Autoregression (VAR), are established. Particularly\nin the VAR setting, the low-rank contributions can come from broad trends\nexhibited in the time series. Details of the algorithm for learning the model\nare presented. Experiments demonstrate the performance of the model and the\nestimation algorithm on simulated and real data.\n",
        "published": "2017-05-09T20:46:19Z",
        "pdf_link": "http://arxiv.org/pdf/1705.03536v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.04194v1",
        "title": "Influence Function and Robust Variant of Kernel Canonical Correlation\n  Analysis",
        "summary": "  Many unsupervised kernel methods rely on the estimation of the kernel\ncovariance operator (kernel CO) or kernel cross-covariance operator (kernel\nCCO). Both kernel CO and kernel CCO are sensitive to contaminated data, even\nwhen bounded positive definite kernels are used. To the best of our knowledge,\nthere are few well-founded robust kernel methods for statistical unsupervised\nlearning. In addition, while the influence function (IF) of an estimator can\ncharacterize its robustness, asymptotic properties and standard error, the IF\nof a standard kernel canonical correlation analysis (standard kernel CCA) has\nnot been derived yet. To fill this gap, we first propose a robust kernel\ncovariance operator (robust kernel CO) and a robust kernel cross-covariance\noperator (robust kernel CCO) based on a generalized loss function instead of\nthe quadratic loss function. Second, we derive the IF for robust kernel CCO and\nstandard kernel CCA. Using the IF of the standard kernel CCA, we can detect\ninfluential observations from two sets of data. Finally, we propose a method\nbased on the robust kernel CO and the robust kernel CCO, called {\\bf robust\nkernel CCA}, which is less sensitive to noise than the standard kernel CCA. The\nintroduced principles can also be applied to many other kernel methods\ninvolving kernel CO or kernel CCO. Our experiments on synthesized data and\nimaging genetics analysis demonstrate that the proposed IF of standard kernel\nCCA can identify outliers. It is also seen that the proposed robust kernel CCA\nmethod performs better for ideal and contaminated data than the standard kernel\nCCA.\n",
        "published": "2017-05-09T18:45:38Z",
        "pdf_link": "http://arxiv.org/pdf/1705.04194v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.04790v2",
        "title": "ShortFuse: Biomedical Time Series Representations in the Presence of\n  Structured Information",
        "summary": "  In healthcare applications, temporal variables that encode movement, health\nstatus and longitudinal patient evolution are often accompanied by rich\nstructured information such as demographics, diagnostics and medical exam data.\nHowever, current methods do not jointly optimize over structured covariates and\ntime series in the feature extraction process. We present ShortFuse, a method\nthat boosts the accuracy of deep learning models for time series by explicitly\nmodeling temporal interactions and dependencies with structured covariates.\nShortFuse introduces hybrid convolutional and LSTM cells that incorporate the\ncovariates via weights that are shared across the temporal domain. ShortFuse\noutperforms competing models by 3% on two biomedical applications, forecasting\nosteoarthritis-related cartilage degeneration and predicting surgical outcomes\nfor cerebral palsy patients, matching or exceeding the accuracy of models that\nuse features engineered by domain experts.\n",
        "published": "2017-05-13T06:00:01Z",
        "pdf_link": "http://arxiv.org/pdf/1705.04790v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.04886v2",
        "title": "Learning task structure via sparsity grouped multitask learning",
        "summary": "  Sparse mapping has been a key methodology in many high-dimensional scientific\nproblems. When multiple tasks share the set of relevant features, learning them\njointly in a group drastically improves the quality of relevant feature\nselection. However, in practice this technique is used limitedly since such\ngrouping information is usually hidden. In this paper, our goal is to recover\nthe group structure on the sparsity patterns and leverage that information in\nthe sparse learning. Toward this, we formulate a joint optimization problem in\nthe task parameter and the group membership, by constructing an appropriate\nregularizer to encourage sparse learning as well as correct recovery of task\ngroups. We further demonstrate that our proposed method recovers groups and the\nsparsity patterns in the task parameters accurately by extensive experiments.\n",
        "published": "2017-05-13T21:16:13Z",
        "pdf_link": "http://arxiv.org/pdf/1705.04886v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.05197v2",
        "title": "Convex Coupled Matrix and Tensor Completion",
        "summary": "  We propose a set of convex low rank inducing norms for a coupled matrices and\ntensors (hereafter coupled tensors), which shares information between matrices\nand tensors through common modes. More specifically, we propose a mixture of\nthe overlapped trace norm and the latent norms with the matrix trace norm, and\nthen, we propose a new completion algorithm based on the proposed norms. A key\nadvantage of the proposed norms is that it is convex and can find a globally\noptimal solution, while existing methods for coupled learning are non-convex.\nFurthermore, we analyze the excess risk bounds of the completion model\nregularized by our proposed norms which show that our proposed norms can\nexploit the low rankness of coupled tensors leading to better bounds compared\nto uncoupled norms. Through synthetic and real-world data experiments, we show\nthat the proposed completion algorithm compares favorably with existing\ncompletion algorithms.\n",
        "published": "2017-05-15T12:52:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.05197v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.05278v2",
        "title": "Unimodal probability distributions for deep ordinal classification",
        "summary": "  Probability distributions produced by the cross-entropy loss for ordinal\nclassification problems can possess undesired properties. We propose a\nstraightforward technique to constrain discrete ordinal probability\ndistributions to be unimodal via the use of the Poisson and binomial\nprobability distributions. We evaluate this approach in the context of deep\nlearning on two large ordinal image datasets, obtaining promising results.\n",
        "published": "2017-05-15T14:59:26Z",
        "pdf_link": "http://arxiv.org/pdf/1705.05278v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.05355v2",
        "title": "Probabilistic Matrix Factorization for Automated Machine Learning",
        "summary": "  In order to achieve state-of-the-art performance, modern machine learning\ntechniques require careful data pre-processing and hyperparameter tuning.\nMoreover, given the ever increasing number of machine learning models being\ndeveloped, model selection is becoming increasingly important. Automating the\nselection and tuning of machine learning pipelines consisting of data\npre-processing methods and machine learning models, has long been one of the\ngoals of the machine learning community. In this paper, we tackle this\nmeta-learning task by combining ideas from collaborative filtering and Bayesian\noptimization. Using probabilistic matrix factorization techniques and\nacquisition functions from Bayesian optimization, we exploit experiments\nperformed in hundreds of different datasets to guide the exploration of the\nspace of possible pipelines. In our experiments, we show that our approach\nquickly identifies high-performing pipelines across a wide range of datasets,\nsignificantly outperforming the current state-of-the-art.\n",
        "published": "2017-05-15T17:47:26Z",
        "pdf_link": "http://arxiv.org/pdf/1705.05355v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.05950v5",
        "title": "Kernel clustering: density biases and solutions",
        "summary": "  Kernel methods are popular in clustering due to their generality and\ndiscriminating power. However, we show that many kernel clustering criteria\nhave density biases theoretically explaining some practically significant\nartifacts empirically observed in the past. For example, we provide conditions\nand formally prove the density mode isolation bias in kernel K-means for a\ncommon class of kernels. We call it Breiman's bias due to its similarity to the\nhistogram mode isolation previously discovered by Breiman in decision tree\nlearning with Gini impurity. We also extend our analysis to other popular\nkernel clustering methods, e.g. average/normalized cut or dominant sets, where\ndensity biases can take different forms. For example, splitting isolated points\nby cut-based criteria is essentially the sparsest subset bias, which is the\nopposite of the density mode bias. Our findings suggest that a principled\nsolution for density biases in kernel clustering should directly address data\ninhomogeneity. We show that density equalization can be implicitly achieved\nusing either locally adaptive weights or locally adaptive kernels. Moreover,\ndensity equalization makes many popular kernel clustering objectives\nequivalent. Our synthetic and real data experiments illustrate density biases\nand proposed solutions. We anticipate that theoretical understanding of kernel\nclustering limitations and their principled solutions will be important for a\nbroad spectrum of data analysis applications across the disciplines.\n",
        "published": "2017-05-16T23:07:59Z",
        "pdf_link": "http://arxiv.org/pdf/1705.05950v5"
    },
    {
        "id": "http://arxiv.org/abs/1705.06189v3",
        "title": "Co-clustering through Optimal Transport",
        "summary": "  In this paper, we present a novel method for co-clustering, an unsupervised\nlearning approach that aims at discovering homogeneous groups of data instances\nand features by grouping them simultaneously. The proposed method uses the\nentropy regularized optimal transport between empirical measures defined on\ndata instances and features in order to obtain an estimated joint probability\ndensity function represented by the optimal coupling matrix. This matrix is\nfurther factorized to obtain the induced row and columns partitions using\nmultiscale representations approach. To justify our method theoretically, we\nshow how the solution of the regularized optimal transport can be seen from the\nvariational inference perspective thus motivating its use for co-clustering.\nThe algorithm derived for the proposed method and its kernelized version based\non the notion of Gromov-Wasserstein distance are fast, accurate and can\ndetermine automatically the number of both row and column clusters. These\nfeatures are vividly demonstrated through extensive experimental evaluations.\n",
        "published": "2017-05-17T14:46:12Z",
        "pdf_link": "http://arxiv.org/pdf/1705.06189v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.06364v1",
        "title": "Learning Gaussian Graphical Models Using Discriminated Hub Graphical\n  Lasso",
        "summary": "  We develop a new method called Discriminated Hub Graphical Lasso (DHGL) based\non Hub Graphical Lasso (HGL) by providing prior information of hubs. We apply\nthis new method in two situations: with known hubs and without known hubs. Then\nwe compare DHGL with HGL using several measures of performance. When some hubs\nare known, we can always estimate the precision matrix better via DHGL than\nHGL. When no hubs are known, we use Graphical Lasso (GL) to provide information\nof hubs and find that the performance of DHGL will always be better than HGL if\ncorrect prior information is given and will seldom degenerate when the prior\ninformation is wrong.\n",
        "published": "2017-05-17T22:51:51Z",
        "pdf_link": "http://arxiv.org/pdf/1705.06364v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.06408v1",
        "title": "Linear Dimensionality Reduction in Linear Time:\n  Johnson-Lindenstrauss-type Guarantees for Random Subspace",
        "summary": "  We consider the problem of efficient randomized dimensionality reduction with\nnorm-preservation guarantees. Specifically we prove data-dependent\nJohnson-Lindenstrauss-type geometry preservation guarantees for Ho's random\nsubspace method: When data satisfy a mild regularity condition -- the extent of\nwhich can be estimated by sampling from the data -- then random subspace\napproximately preserves the Euclidean geometry of the data with high\nprobability. Our guarantees are of the same order as those for random\nprojection, namely the required dimension for projection is logarithmic in the\nnumber of data points, but have a larger constant term in the bound which\ndepends upon this regularity. A challenging situation is when the original data\nhave a sparse representation, since this implies a very large projection\ndimension is required: We show how this situation can be improved for sparse\nbinary data by applying an efficient `densifying' preprocessing, which neither\nchanges the Euclidean geometry of the data nor requires an explicit\nmatrix-matrix multiplication. We corroborate our theoretical findings with\nexperiments on both dense and sparse high-dimensional datasets from several\napplication domains.\n",
        "published": "2017-05-18T03:31:11Z",
        "pdf_link": "http://arxiv.org/pdf/1705.06408v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07006v5",
        "title": "Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence\n  Modeling",
        "summary": "  Analyzing the underlying structure of multiple time-sequences provides\ninsights into the understanding of social networks and human activities. In\nthis work, we present the \\emph{Bayesian nonparametric Poisson process\nallocation} (BaNPPA), a latent-function model for time-sequences, which\nautomatically infers the number of latent functions. We model the intensity of\neach sequence as an infinite mixture of latent functions, each of which is\nobtained using a function drawn from a Gaussian process. We show that a\ntechnical challenge for the inference of such mixture models is the\nunidentifiability of the weights of the latent functions. We propose to cope\nwith the issue by regulating the volume of each latent function within a\nvariational inference algorithm. Our algorithm is computationally efficient and\nscales well to large data sets. We demonstrate the usefulness of our proposed\nmodel through experiments on both synthetic and real-world data sets.\n",
        "published": "2017-05-19T14:15:13Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07006v5"
    },
    {
        "id": "http://arxiv.org/abs/1705.07079v2",
        "title": "Scalable Variational Inference for Dynamical Systems",
        "summary": "  Gradient matching is a promising tool for learning parameters and state\ndynamics of ordinary differential equations. It is a grid free inference\napproach, which, for fully observable systems is at times competitive with\nnumerical integration. However, for many real-world applications, only sparse\nobservations are available or even unobserved variables are included in the\nmodel description. In these cases most gradient matching methods are difficult\nto apply or simply do not provide satisfactory results. That is why, despite\nthe high computational cost, numerical integration is still the gold standard\nin many applications. Using an existing gradient matching approach, we propose\na scalable variational inference framework which can infer states and\nparameters simultaneously, offers computational speedups, improved accuracy and\nworks well even under model misspecifications in a partially observable system.\n",
        "published": "2017-05-19T16:29:00Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07079v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07111v1",
        "title": "The Kernel Mixture Network: A Nonparametric Method for Conditional\n  Density Estimation of Continuous Random Variables",
        "summary": "  This paper introduces the kernel mixture network, a new method for\nnonparametric estimation of conditional probability densities using neural\nnetworks. We model arbitrarily complex conditional densities as linear\ncombinations of a family of kernel functions centered at a subset of training\npoints. The weights are determined by the outer layer of a deep neural network,\ntrained by minimizing the negative log likelihood. This generalizes the popular\nquantized softmax approach, which can be seen as a kernel mixture network with\nsquare and non-overlapping kernels. We test the performance of our method on\ntwo important applications, namely Bayesian filtering and generative modeling.\nIn the Bayesian filtering example, we show that the method can be used to\nfilter complex nonlinear and non-Gaussian signals defined on manifolds. The\nresulting kernel mixture network filter outperforms both the quantized softmax\nfilter and the extended Kalman filter in terms of model likelihood. Finally,\nour experiments on generative models show that, given the same architecture,\nthe kernel mixture network leads to higher test set likelihood, less\noverfitting and more diversified and realistic generated samples than the\nquantized softmax approach.\n",
        "published": "2017-05-19T17:45:19Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07111v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07131v2",
        "title": "Streaming Sparse Gaussian Process Approximations",
        "summary": "  Sparse pseudo-point approximations for Gaussian process (GP) models provide a\nsuite of methods that support deployment of GPs in the large data regime and\nenable analytic intractabilities to be sidestepped. However, the field lacks a\nprincipled method to handle streaming data in which both the posterior\ndistribution over function values and the hyperparameter estimates are updated\nin an online fashion. The small number of existing approaches either use\nsuboptimal hand-crafted heuristics for hyperparameter learning, or suffer from\ncatastrophic forgetting or slow updating when new data arrive. This paper\ndevelops a new principled framework for deploying Gaussian process\nprobabilistic models in the streaming setting, providing methods for learning\nhyperparameters and optimising pseudo-input locations. The proposed framework\nis assessed using synthetic and real-world datasets.\n",
        "published": "2017-05-19T18:01:28Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07131v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07152v3",
        "title": "Data-driven Optimal Cost Selection for Distributionally Robust\n  Optimization",
        "summary": "  Recently, (Blanchet, Kang, and Murhy 2016, and Blanchet, and Kang 2017)\nshowed that several machine learning algorithms, such as square-root Lasso,\nSupport Vector Machines, and regularized logistic regression, among many\nothers, can be represented exactly as distributionally robust optimization\n(DRO) problems. The distributional uncertainty is defined as a neighborhood\ncentered at the empirical distribution. We propose a methodology which learns\nsuch neighborhood in a natural data-driven way. We show rigorously that our\nframework encompasses adaptive regularization as a particular case. Moreover,\nwe demonstrate empirically that our proposed methodology is able to improve\nupon a wide range of popular machine learning estimators.\n",
        "published": "2017-05-19T19:16:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07152v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.07168v1",
        "title": "Doubly Robust Data-Driven Distributionally Robust Optimization",
        "summary": "  Data-driven Distributionally Robust Optimization (DD-DRO) via optimal\ntransport has been shown to encompass a wide range of popular machine learning\nalgorithms. The distributional uncertainty size is often shown to correspond to\nthe regularization parameter. The type of regularization (e.g. the norm used to\nregularize) corresponds to the shape of the distributional uncertainty. We\npropose a data-driven robust optimization methodology to inform the\ntransportation cost underlying the definition of the distributional\nuncertainty. We show empirically that this additional layer of robustification,\nwhich produces a method we called doubly robust data-driven distributionally\nrobust optimization (DD-R-DRO), allows to enhance the generalization properties\nof regularized estimators while reducing testing error relative to\nstate-of-the-art classifiers in a wide range of data sets.\n",
        "published": "2017-05-19T20:05:53Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07168v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07178v7",
        "title": "Accelerated Parallel Non-conjugate Sampling for Bayesian Non-parametric\n  Models",
        "summary": "  Inference of latent feature models in the Bayesian nonparametric setting is\ngenerally difficult, especially in high dimensional settings, because it\nusually requires proposing features from some prior distribution. In special\ncases, where the integration is tractable, we can sample new feature\nassignments according to a predictive likelihood. We present a novel method to\naccelerate the mixing of latent variable model inference by proposing feature\nlocations based on the data, as opposed to the prior. First, we introduce an\naccelerated feature proposal mechanism that we show is a valid MCMC algorithm\nfor posterior inference. Next, we propose an approximate inference strategy to\nperform accelerated inference in parallel. A two-stage algorithm that combines\nthe two approaches provides a computationally attractive method that can\nquickly reach local convergence to the posterior distribution of our model,\nwhile allowing us to exploit parallelization.\n",
        "published": "2017-05-19T20:38:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07178v7"
    },
    {
        "id": "http://arxiv.org/abs/1705.07220v3",
        "title": "Data-adaptive Active Sampling for Efficient Graph-Cognizant\n  Classification",
        "summary": "  The present work deals with active sampling of graph nodes representing\ntraining data for binary classification. The graph may be given or constructed\nusing similarity measures among nodal features. Leveraging the graph for\nclassification builds on the premise that labels across neighboring nodes are\ncorrelated according to a categorical Markov random field (MRF). This model is\nfurther relaxed to a Gaussian (G)MRF with labels taking continuous values - an\napproximation that not only mitigates the combinatorial complexity of the\ncategorical model, but also offers optimal unbiased soft predictors of the\nunlabeled nodes. The proposed sampling strategy is based on querying the node\nwhose label disclosure is expected to inflict the largest change on the GMRF,\nand in this sense it is the most informative on average. Such a strategy\nsubsumes several measures of expected model change, including uncertainty\nsampling, variance minimization, and sampling based on the $\\Sigma-$optimality\ncriterion. A simple yet effective heuristic is also introduced for increasing\nthe exploration capabilities of the sampler, and reducing bias of the resultant\nclassifier, by taking into account the confidence on the model label\npredictions. The novel sampling strategies are based on quantities that are\nreadily available without the need for model retraining, rendering them\ncomputationally efficient and scalable to large graphs. Numerical tests using\nsynthetic and real data demonstrate that the proposed methods achieve accuracy\nthat is comparable or superior to the state-of-the-art even at reduced runtime.\n",
        "published": "2017-05-19T23:06:50Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07220v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.07283v2",
        "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise",
        "summary": "  Dropout-based regularization methods can be regarded as injecting random\nnoise with pre-defined magnitude to different parts of the neural network\nduring training. It was recently shown that Bayesian dropout procedure not only\nimproves generalization but also leads to extremely sparse neural architectures\nby automatically setting the individual noise magnitude per weight. However,\nthis sparsity can hardly be used for acceleration since it is unstructured. In\nthe paper, we propose a new Bayesian model that takes into account the\ncomputational structure of neural networks and provides structured sparsity,\ne.g. removes neurons and/or convolutional channels in CNNs. To do this we\ninject noise to the neurons outputs while keeping the weights unregularized. We\nestablish the probabilistic model with a proper truncated log-uniform prior\nover the noise and truncated log-normal variational approximation that ensures\nthat the KL-term in the evidence lower bound is computed in closed-form. The\nmodel leads to structured sparsity by removing elements with a low SNR from the\ncomputation graph and provides significant acceleration on a number of deep\nneural architectures. The model is easy to implement as it can be formulated as\na separate dropout-like layer.\n",
        "published": "2017-05-20T09:20:06Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07283v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07321v2",
        "title": "Accelerated Hierarchical Density Clustering",
        "summary": "  We present an accelerated algorithm for hierarchical density based\nclustering. Our new algorithm improves upon HDBSCAN*, which itself provided a\nsignificant qualitative improvement over the popular DBSCAN algorithm. The\naccelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while\nsupporting variable density clusters, and eliminating the need for the\ndifficult to tune distance scale parameter. This makes accelerated HDBSCAN* the\ndefault choice for density based clustering.\n  Library available at: https://github.com/scikit-learn-contrib/hdbscan\n",
        "published": "2017-05-20T15:32:50Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07321v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07348v2",
        "title": "Calibrating Black Box Classification Models through the Thresholding\n  Method",
        "summary": "  In high-dimensional classification settings, we wish to seek a balance\nbetween high power and ensuring control over a desired loss function. In many\nsettings, the points most likely to be misclassified are those who lie near the\ndecision boundary of the given classification method. Often, these\nuninformative points should not be classified as they are noisy and do not\nexhibit strong signals. In this paper, we introduce the Thresholding Method to\nparameterize the problem of determining which points exhibit strong signals and\nshould be classified. We demonstrate the empirical performance of this novel\ncalibration method in providing loss function control at a desired level, as\nwell as explore how the method assuages the effect of overfitting. We explore\nthe benefits of error control through the Thresholding Method in difficult,\nhigh-dimensional, simulated settings. Finally, we show the flexibility of the\nThresholding Method through applying the method in a variety of real data\nsettings.\n",
        "published": "2017-05-20T19:38:36Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07348v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07362v1",
        "title": "Honey Bee Dance Modeling in Real-time using Machine Learning",
        "summary": "  The waggle dance that honeybees perform is an astonishing way of\ncommunicating the location of food source. After over 60 years of its\ndiscovery, researchers still use manual labeling by watching hours of dance\nvideos to detect different transitions between dance components thus extracting\ninformation regarding the distance and direction to the food source. We propose\nan automated process to monitor and segment different components of honeybee\nwaggle dance. The process is highly accurate, runs in real-time, and can use\nshared information between multiple dances.\n",
        "published": "2017-05-20T22:25:05Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07362v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07469v1",
        "title": "Improved Algorithms for Matrix Recovery from Rank-One Projections",
        "summary": "  We consider the problem of estimation of a low-rank matrix from a limited\nnumber of noisy rank-one projections. In particular, we propose two fast,\nnon-convex \\emph{proper} algorithms for matrix recovery and support them with\nrigorous theoretical analysis. We show that the proposed algorithms enjoy\nlinear convergence and that their sample complexity is independent of the\ncondition number of the unknown true low-rank matrix. By leveraging recent\nadvances in low-rank matrix approximation techniques, we show that our\nalgorithms achieve computational speed-ups over existing methods. Finally, we\ncomplement our theory with some numerical experiments.\n",
        "published": "2017-05-21T16:20:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07469v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07585v2",
        "title": "Union of Intersections (UoI) for Interpretable Data Driven Discovery and\n  Prediction",
        "summary": "  The increasing size and complexity of scientific data could dramatically\nenhance discovery and prediction for basic scientific applications. Realizing\nthis potential, however, requires novel statistical analysis methods that are\nboth interpretable and predictive. We introduce Union of Intersections (UoI), a\nflexible, modular, and scalable framework for enhanced model selection and\nestimation. Methods based on UoI perform model selection and model estimation\nthrough intersection and union operations, respectively. We show that UoI-based\nmethods achieve low-variance and nearly unbiased estimation of a small number\nof interpretable features, while maintaining high-quality prediction accuracy.\nWe perform extensive numerical investigation to evaluate a UoI algorithm\n($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the\nextraction of interpretable functional networks from human electrophysiology\nrecordings as well as accurate prediction of phenotypes from genotype-phenotype\ndata with reduced features. We also show (with the $UoI_{L1Logistic}$ and\n$UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for\nclassification and matrix factorization on several benchmark biomedical data\nsets. These results suggest that methods based on the UoI framework could\nimprove interpretation and prediction in data-driven discovery across\nscientific fields.\n",
        "published": "2017-05-22T07:28:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07585v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07592v1",
        "title": "Improved Clustering with Augmented k-means",
        "summary": "  Identifying a set of homogeneous clusters in a heterogeneous dataset is one\nof the most important classes of problems in statistical modeling. In the realm\nof unsupervised partitional clustering, k-means is a very important algorithm\nfor this. In this technical report, we develop a new k-means variant called\nAugmented k-means, which is a hybrid of k-means and logistic regression. During\neach iteration, logistic regression is used to predict the current cluster\nlabels, and the cluster belonging probabilities are used to control the\nsubsequent re-estimation of cluster means. Observations which can't be firmly\nidentified into clusters are excluded from the re-estimation step. This can be\nvaluable when the data exhibit many characteristics of real datasets such as\nheterogeneity, non-sphericity, substantial overlap, and high scatter. Augmented\nk-means frequently outperforms k-means by more accurately classifying\nobservations into known clusters and / or converging in fewer iterations. We\ndemonstrate this on both simulated and real datasets. Our algorithm is\nimplemented in Python and will be available with this report.\n",
        "published": "2017-05-22T07:47:24Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07592v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07606v2",
        "title": "Guide Actor-Critic for Continuous Control",
        "summary": "  Actor-critic methods solve reinforcement learning problems by updating a\nparameterized policy known as an actor in a direction that increases an\nestimate of the expected return known as a critic. However, existing\nactor-critic methods only use values or gradients of the critic to update the\npolicy parameter. In this paper, we propose a novel actor-critic method called\nthe guide actor-critic (GAC). GAC firstly learns a guide actor that locally\nmaximizes the critic and then it updates the policy parameter based on the\nguide actor by supervised learning. Our main theoretical contributions are two\nfolds. First, we show that GAC updates the guide actor by performing\nsecond-order optimization in the action space where the curvature matrix is\nbased on the Hessians of the critic. Second, we show that the deterministic\npolicy gradient method is a special case of GAC when the Hessians are ignored.\nThrough experiments, we show that our method is a promising reinforcement\nlearning method for continuous controls.\n",
        "published": "2017-05-22T08:32:10Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07606v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.07642v1",
        "title": "From optimal transport to generative modeling: the VEGAN cookbook",
        "summary": "  We study unsupervised generative modeling in terms of the optimal transport\n(OT) problem between true (but unknown) data distribution $P_X$ and the latent\nvariable model distribution $P_G$. We show that the OT problem can be\nequivalently written in terms of probabilistic encoders, which are constrained\nto match the posterior and prior distributions over the latent space. When\nrelaxed, this constrained optimization problem leads to a penalized optimal\ntransport (POT) objective, which can be efficiently minimized using stochastic\ngradient descent by sampling from $P_X$ and $P_G$. We show that POT for the\n2-Wasserstein distance coincides with the objective heuristically employed in\nadversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the\nfirst theoretical justification for AAEs known to the authors. We also compare\nPOT to other popular techniques like variational auto-encoders (VAE) (Kingma\nand Welling, 2014). Our theoretical results include (a) a better understanding\nof the commonly observed blurriness of images generated by VAEs, and (b)\nestablishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and\nPOT for the 1-Wasserstein distance.\n",
        "published": "2017-05-22T10:14:05Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07642v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07654v1",
        "title": "ReFACTor: Practical Low-Rank Matrix Estimation Under Column-Sparsity",
        "summary": "  Various problems in data analysis and statistical genetics call for recovery\nof a column-sparse, low-rank matrix from noisy observations. We propose\nReFACTor, a simple variation of the classical Truncated Singular Value\nDecomposition (TSVD) algorithm. In contrast to previous sparse principal\ncomponent analysis (PCA) algorithms, our algorithm can provably reveal a\nlow-rank signal matrix better, and often significantly better, than the widely\nused TSVD, making it the algorithm of choice whenever column-sparsity is\nsuspected. Empirically, we observe that ReFACTor consistently outperforms TSVD\neven when the underlying signal is not sparse, suggesting that it is generally\nsafe to use ReFACTor instead of TSVD and PCA. The algorithm is extremely simple\nto implement and its running time is dominated by the runtime of PCA, making it\nas practical as standard principal component analysis.\n",
        "published": "2017-05-22T10:43:09Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07654v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07751v1",
        "title": "An Asynchronous Distributed Framework for Large-scale Learning Based on\n  Parameter Exchanges",
        "summary": "  In many distributed learning problems, the heterogeneous loading of computing\nmachines may harm the overall performance of synchronous strategies. In this\npaper, we propose an effective asynchronous distributed framework for the\nminimization of a sum of smooth functions, where each machine performs\niterations in parallel on its local function and updates a shared parameter\nasynchronously. In this way, all machines can continuously work even though\nthey do not have the latest version of the shared parameter. We prove the\nconvergence of the consistency of this general distributed asynchronous method\nfor gradient iterations then show its efficiency on the matrix factorization\nproblem for recommender systems and on binary classification.\n",
        "published": "2017-05-22T13:58:58Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07751v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07761v3",
        "title": "VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational\n  Learning",
        "summary": "  Deep generative models provide powerful tools for distributions over\ncomplicated manifolds, such as those of natural images. But many of these\nmethods, including generative adversarial networks (GANs), can be difficult to\ntrain, in part because they are prone to mode collapse, which means that they\ncharacterize only a few modes of the true distribution. To address this, we\nintroduce VEEGAN, which features a reconstructor network, reversing the action\nof the generator by mapping from data to noise. Our training objective retains\nthe original asymptotic consistency guarantee of GANs, and can be interpreted\nas a novel autoencoder loss over the noise. In sharp contrast to a traditional\nautoencoder over data points, VEEGAN does not require specifying a loss\nfunction over the data, but rather only over the representations, which are\nstandard normal by assumption. On an extensive set of synthetic and real world\nimage datasets, VEEGAN indeed resists mode collapsing to a far greater extent\nthan other recent GAN variants, and produces more realistic samples.\n",
        "published": "2017-05-22T14:17:57Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07761v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.07832v1",
        "title": "Concrete Dropout",
        "summary": "  Dropout is used as a practical tool to obtain uncertainty estimates in large\nvision models and reinforcement learning (RL) tasks. But to obtain\nwell-calibrated uncertainty estimates, a grid-search over the dropout\nprobabilities is necessary - a prohibitive operation with large models, and an\nimpossible one with RL. We propose a new dropout variant which gives improved\nperformance and better calibrated uncertainties. Relying on recent developments\nin Bayesian deep learning, we use a continuous relaxation of dropout's discrete\nmasks. Together with a principled optimisation objective, this allows for\nautomatic tuning of the dropout probability in large models, and as a result\nfaster experimentation cycles. In RL this allows the agent to adapt its\nuncertainty dynamically as more data is observed. We analyse the proposed\nvariant extensively on a range of tasks, and give insights into common practice\nin the field where larger dropout probabilities are often used in deeper model\nlayers.\n",
        "published": "2017-05-22T16:25:02Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07832v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.07857v1",
        "title": "Real Time Image Saliency for Black Box Classifiers",
        "summary": "  In this work we develop a fast saliency detection method that can be applied\nto any differentiable image classifier. We train a masking model to manipulate\nthe scores of the classifier by masking salient parts of the input image. Our\nmodel generalises well to unseen images and requires a single forward pass to\nperform saliency detection, therefore suitable for use in real-time systems. We\ntest our approach on CIFAR-10 and ImageNet datasets and show that the produced\nsaliency maps are easily interpretable, sharp, and free of artifacts. We\nsuggest a new metric for saliency and test our method on the ImageNet object\nlocalisation task. We achieve results outperforming other weakly supervised\nmethods.\n",
        "published": "2017-05-22T17:00:55Z",
        "pdf_link": "http://arxiv.org/pdf/1705.07857v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.08236v1",
        "title": "3D Convolutional Neural Networks for Brain Tumor Segmentation: A\n  Comparison of Multi-resolution Architectures",
        "summary": "  This paper analyzes the use of 3D Convolutional Neural Networks for brain\ntumor segmentation in MR images. We address the problem using three different\narchitectures that combine fine and coarse features to obtain the final\nsegmentation. We compare three different networks that use multi-resolution\nfeatures in terms of both design and performance and we show that they improve\ntheir single-resolution counterparts.\n",
        "published": "2017-05-23T13:28:59Z",
        "pdf_link": "http://arxiv.org/pdf/1705.08236v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.08415v6",
        "title": "Supervised Community Detection with Line Graph Neural Networks",
        "summary": "  Traditionally, community detection in graphs can be solved using spectral\nmethods or posterior inference under probabilistic graphical models. Focusing\non random graph families such as the stochastic block model, recent research\nhas unified both approaches and identified both statistical and computational\ndetection thresholds in terms of the signal-to-noise ratio. By recasting\ncommunity detection as a node-wise classification problem on graphs, we can\nalso study it from a learning perspective. We present a novel family of Graph\nNeural Networks (GNNs) for solving community detection problems in a supervised\nlearning setting. We show that, in a data-driven manner and without access to\nthe underlying generative models, they can match or even surpass the\nperformance of the belief propagation algorithm on binary and multi-class\nstochastic block models, which is believed to reach the computational\nthreshold. In particular, we propose to augment GNNs with the non-backtracking\noperator defined on the line graph of edge adjacencies. Our models also achieve\ngood performance on real-world datasets. In addition, we perform the first\nanalysis of the optimization landscape of training linear GNNs for community\ndetection problems, demonstrating that under certain simplifications and\nassumptions, the loss values at local and global minima are not far apart.\n",
        "published": "2017-05-23T17:03:33Z",
        "pdf_link": "http://arxiv.org/pdf/1705.08415v6"
    },
    {
        "id": "http://arxiv.org/abs/1705.08716v1",
        "title": "An experimental study of graph-based semi-supervised classification with\n  additional node information",
        "summary": "  The volume of data generated by internet and social networks is increasing\nevery day, and there is a clear need for efficient ways of extracting useful\ninformation from them. As those data can take different forms, it is important\nto use all the available data representations for prediction.\n  In this paper, we focus our attention on supervised classification using both\nregular plain, tabular, data and structural information coming from a network\nstructure. 14 techniques are investigated and compared in this study and can be\ndivided in three classes: the first one uses only the plain data to build a\nclassification model, the second uses only the graph structure and the last\nuses both information sources. The relative performances in these three cases\nare investigated. Furthermore, the effect of using a graph embedding and\nwell-known indicators in spatial statistics is also studied.\n  Possible applications are automatic classification of web pages or other\nlinked documents, of people in a social network or of proteins in a biological\ncomplex system, to name a few.\n  Based on our comparison, we draw some general conclusions and advices to\ntackle this particular classification task: some datasets can be better\nexplained by their graph structure (graph-driven), or by their feature set\n(features-driven). The most efficient methods are discussed in both cases.\n",
        "published": "2017-05-24T12:05:44Z",
        "pdf_link": "http://arxiv.org/pdf/1705.08716v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.08814v1",
        "title": "Boundary Crossing Probabilities for General Exponential Families",
        "summary": "  We consider parametric exponential families of dimension $K$ on the real\nline. We study a variant of \\textit{boundary crossing probabilities} coming\nfrom the multi-armed bandit literature, in the case when the real-valued\ndistributions form an exponential family of dimension $K$. Formally, our result\nis a concentration inequality that bounds the probability that\n$\\mathcal{B}^\\psi(\\hat \\theta_n,\\theta^\\star)\\geq f(t/n)/n$, where\n$\\theta^\\star$ is the parameter of an unknown target distribution, $\\hat\n\\theta_n$ is the empirical parameter estimate built from $n$ observations,\n$\\psi$ is the log-partition function of the exponential family and\n$\\mathcal{B}^\\psi$ is the corresponding Bregman divergence. From the\nperspective of stochastic multi-armed bandits, we pay special attention to the\ncase when the boundary function $f$ is logarithmic, as it is enables to analyze\nthe regret of the state-of-the-art \\KLUCB\\ and \\KLUCBp\\ strategies, whose\nanalysis was left open in such generality. Indeed, previous results only hold\nfor the case when $K=1$, while we provide results for arbitrary finite\ndimension $K$, thus considerably extending the existing results. Perhaps\nsurprisingly, we highlight that the proof techniques to achieve these strong\nresults already existed three decades ago in the work of T.L. Lai, and were\napparently forgotten in the bandit community. We provide a modern rewriting of\nthese beautiful techniques that we believe are useful beyond the application to\nstochastic multi-armed bandits.\n",
        "published": "2017-05-24T15:19:05Z",
        "pdf_link": "http://arxiv.org/pdf/1705.08814v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.08933v2",
        "title": "Doubly Stochastic Variational Inference for Deep Gaussian Processes",
        "summary": "  Gaussian processes (GPs) are a good choice for function approximation as they\nare flexible, robust to over-fitting, and provide well-calibrated predictive\nuncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of\nGPs, but inference in these models has proved challenging. Existing approaches\nto inference in DGP models assume approximate posteriors that force\nindependence between the layers, and do not work well in practice. We present a\ndoubly stochastic variational inference algorithm, which does not force\nindependence between layers. With our method of inference we demonstrate that a\nDGP model can be used effectively on data ranging in size from hundreds to a\nbillion points. We provide strong empirical evidence that our inference scheme\nfor DGPs works well in practice in both classification and regression.\n",
        "published": "2017-05-24T19:10:17Z",
        "pdf_link": "http://arxiv.org/pdf/1705.08933v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.09046v2",
        "title": "Expectation Propagation for t-Exponential Family Using Q-Algebra",
        "summary": "  Exponential family distributions are highly useful in machine learning since\ntheir calculation can be performed efficiently through natural parameters. The\nexponential family has recently been extended to the t-exponential family,\nwhich contains Student-t distributions as family members and thus allows us to\nhandle noisy data well. However, since the t-exponential family is denied by\nthe deformed exponential, we cannot derive an efficient learning algorithm for\nthe t-exponential family such as expectation propagation (EP). In this paper,\nwe borrow the mathematical tools of q-algebra from statistical physics and show\nthat the pseudo additivity of distributions allows us to perform calculation of\nt-exponential family distributions through natural parameters. We then develop\nan expectation propagation (EP) algorithm for the t-exponential family, which\nprovides a deterministic approximation to the posterior or predictive\ndistribution with simple moment matching. We finally apply the proposed EP\nalgorithm to the Bayes point machine and Student-t process classication, and\ndemonstrate their performance numerically.\n",
        "published": "2017-05-25T04:56:09Z",
        "pdf_link": "http://arxiv.org/pdf/1705.09046v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.09048v2",
        "title": "Convergence of Langevin MCMC in KL-divergence",
        "summary": "  Langevin diffusion is a commonly used tool for sampling from a given\ndistribution. In this work, we establish that when the target density $p^*$ is\nsuch that $\\log p^*$ is $L$ smooth and $m$ strongly convex, discrete Langevin\ndiffusion produces a distribution $p$ with $KL(p||p^*)\\leq \\epsilon$ in\n$\\tilde{O}(\\frac{d}{\\epsilon})$ steps, where $d$ is the dimension of the sample\nspace. We also study the convergence rate when the strong-convexity assumption\nis absent. By considering the Langevin diffusion as a gradient flow in the\nspace of probability distributions, we obtain an elegant analysis that applies\nto the stronger property of convergence in KL-divergence and gives a\nconceptually simpler proof of the best-known convergence results in weaker\nmetrics.\n",
        "published": "2017-05-25T05:08:26Z",
        "pdf_link": "http://arxiv.org/pdf/1705.09048v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.09199v3",
        "title": "Non-parametric estimation of Jensen-Shannon Divergence in Generative\n  Adversarial Network training",
        "summary": "  Generative Adversarial Networks (GANs) have become a widely popular framework\nfor generative modelling of high-dimensional datasets. However their training\nis well-known to be difficult. This work presents a rigorous statistical\nanalysis of GANs providing straight-forward explanations for common training\npathologies such as vanishing gradients. Furthermore, it proposes a new\ntraining objective, Kernel GANs, and demonstrates its practical effectiveness\non large-scale real-world data sets. A key element in the analysis is the\ndistinction between training with respect to the (unknown) data distribution,\nand its empirical counterpart. To overcome issues in GAN training, we pursue\nthe idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating\nnoise in the input distributions of the discriminator. As we show, this\neffectively leads to an empirical version of the JSD in which the true and the\ngenerator densities are replaced by kernel density estimates, which leads to\nKernel GANs.\n",
        "published": "2017-05-25T14:32:26Z",
        "pdf_link": "http://arxiv.org/pdf/1705.09199v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.09353v2",
        "title": "Predictive State Recurrent Neural Networks",
        "summary": "  We present a new model, Predictive State Recurrent Neural Networks (PSRNNs),\nfor filtering and prediction in dynamical systems. PSRNNs draw on insights from\nboth Recurrent Neural Networks (RNNs) and Predictive State Representations\n(PSRs), and inherit advantages from both types of models. Like many successful\nRNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer\nfunctions to combine information from multiple sources. We show that such\nbilinear functions arise naturally from state updates in Bayes filters like\nPSRs, in which observations can be viewed as gating belief states. We also show\nthat PSRNNs can be learned effectively by combining Backpropogation Through\nTime (BPTT) with an initialization derived from a statistically consistent\nlearning algorithm for PSRs called two-stage regression (2SR). Finally, we show\nthat PSRNNs can be factorized using tensor decomposition, reducing model size\nand suggesting interesting connections to existing multiplicative architectures\nsuch as LSTMs. We applied PSRNNs to 4 datasets, and showed that we outperform\nseveral popular alternative approaches to modeling dynamical systems in all\ncases.\n",
        "published": "2017-05-25T20:40:13Z",
        "pdf_link": "http://arxiv.org/pdf/1705.09353v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.09851v2",
        "title": "Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and\n  High Frequency Trading",
        "summary": "  Deep learning applies hierarchical layers of hidden variables to construct\nnonlinear high dimensional predictors. Our goal is to develop and train deep\nlearning architectures for spatio-temporal modeling. Training a deep\narchitecture is achieved by stochastic gradient descent (SGD) and drop-out (DO)\nfor parameter regularization with a goal of minimizing out-of-sample predictive\nmean squared error. To illustrate our methodology, we predict the sharp\ndiscontinuities in traffic flow data, and secondly, we develop a classification\nrule to predict short-term futures market prices as a function of the order\nbook depth. Finally, we conclude with directions for future research.\n",
        "published": "2017-05-27T18:17:58Z",
        "pdf_link": "http://arxiv.org/pdf/1705.09851v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.10306v2",
        "title": "Auto-Encoding Sequential Monte Carlo",
        "summary": "  We build on auto-encoding sequential Monte Carlo (AESMC): a method for model\nand proposal learning based on maximizing the lower bound to the log marginal\nlikelihood in a broad family of structured probabilistic models. Our approach\nrelies on the efficiency of sequential Monte Carlo (SMC) for performing\ninference in structured probabilistic models and the flexibility of deep neural\nnetworks to model complex conditional probability distributions. We develop\nadditional theoretical insights and introduce a new training procedure which\nimproves both model and proposal learning. We demonstrate that our approach\nprovides a fast, easy-to-implement and scalable means for simultaneous model\nlearning and proposal adaptation in deep generative models.\n",
        "published": "2017-05-29T17:54:11Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10306v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.10378v4",
        "title": "Fair Inference On Outcomes",
        "summary": "  In this paper, we consider the problem of fair statistical inference\ninvolving outcome variables. Examples include classification and regression\nproblems, and estimating treatment effects in randomized trials or\nobservational data. The issue of fairness arises in such problems where some\ncovariates or treatments are \"sensitive,\" in the sense of having potential of\ncreating discrimination. In this paper, we argue that the presence of\ndiscrimination can be formalized in a sensible way as the presence of an effect\nof a sensitive covariate on the outcome along certain causal pathways, a view\nwhich generalizes (Pearl, 2009). A fair outcome model can then be learned by\nsolving a constrained optimization problem. We discuss a number of\ncomplications that arise in classical statistical inference due to this view\nand provide workarounds based on recent work in causal and semi-parametric\ninference.\n",
        "published": "2017-05-29T19:51:38Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10378v4"
    },
    {
        "id": "http://arxiv.org/abs/1705.10388v1",
        "title": "Model Selection in Bayesian Neural Networks via Horseshoe Priors",
        "summary": "  Bayesian Neural Networks (BNNs) have recently received increasing attention\nfor their ability to provide well-calibrated posterior uncertainties. However,\nmodel selection---even choosing the number of nodes---remains an open question.\nIn this work, we apply a horseshoe prior over node pre-activations of a\nBayesian neural network, which effectively turns off nodes that do not help\nexplain the data. We demonstrate that our prior prevents the BNN from\nunder-fitting even when the number of nodes required is grossly over-estimated.\nMoreover, this model selection over the number of nodes doesn't come at the\nexpense of predictive or computational performance; in fact, we learn smaller\nnetworks with comparable predictive performance to current approaches.\n",
        "published": "2017-05-29T20:35:42Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10388v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.10813v3",
        "title": "Large Linear Multi-output Gaussian Process Learning",
        "summary": "  Gaussian processes (GPs), or distributions over arbitrary functions in a\ncontinuous domain, can be generalized to the multi-output case: a linear model\nof coregionalization (LMC) is one approach. LMCs estimate and exploit\ncorrelations across the multiple outputs. While model estimation can be\nperformed efficiently for single-output GPs, these assume stationarity, but in\nthe multi-output case the cross-covariance interaction is not stationary. We\npropose Large Linear GP (LLGP), which circumvents the need for stationarity by\ninducing structure in the LMC kernel through a common grid of inputs shared\nbetween outputs, enabling optimization of GP hyperparameters for\nmulti-dimensional outputs and low-dimensional inputs. When applied to synthetic\ntwo-dimensional and real time series data, we find our theoretical improvement\nrelative to the current solutions for multi-output GPs is realized with LLGP\nreducing training time while improving or maintaining predictive mean accuracy.\nMoreover, by using a direct likelihood approximation rather than a variational\none, model confidence estimates are significantly improved.\n",
        "published": "2017-05-30T18:19:16Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10813v3"
    },
    {
        "id": "http://arxiv.org/abs/1705.10817v1",
        "title": "Dynamics Based Features For Graph Classification",
        "summary": "  Numerous social, medical, engineering and biological challenges can be framed\nas graph-based learning tasks. Here, we propose a new feature based approach to\nnetwork classification. We show how dynamics on a network can be useful to\nreveal patterns about the organization of the components of the underlying\ngraph where the process takes place. We define generalized assortativities on\nnetworks and use them as generalized features across multiple time scales.\nThese features turn out to be suitable signatures for discriminating between\ndifferent classes of networks. Our method is evaluated empirically on\nestablished network benchmarks. We also introduce a new dataset of human brain\nnetworks (connectomes) and use it to evaluate our method. Results reveal that\nour dynamics based features are competitive and often outperform state of the\nart accuracies.\n",
        "published": "2017-05-30T18:33:44Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10817v1"
    },
    {
        "id": "http://arxiv.org/abs/1705.10888v2",
        "title": "Identification of Gaussian Process State Space Models",
        "summary": "  The Gaussian process state space model (GPSSM) is a non-linear dynamical\nsystem, where unknown transition and/or measurement mappings are described by\nGPs. Most research in GPSSMs has focussed on the state estimation problem,\ni.e., computing a posterior of the latent state given the model. However, the\nkey challenge in GPSSMs has not been satisfactorily addressed yet: system\nidentification, i.e., learning the model. To address this challenge, we impose\na structured Gaussian variational posterior distribution over the latent\nstates, which is parameterised by a recognition model in the form of a\nbi-directional recurrent neural network. Inference with this structure allows\nus to recover a posterior smoothed over sequences of data. We provide a\npractical algorithm for efficiently computing a lower bound on the marginal\nlikelihood using the reparameterisation trick. This further allows for the use\nof arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can\nefficiently generate plausible future trajectories of the identified system\nafter only observing a small number of episodes from the true system.\n",
        "published": "2017-05-30T23:27:44Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10888v2"
    },
    {
        "id": "http://arxiv.org/abs/1705.10934v4",
        "title": "Learning Graphs with Monotone Topology Properties and Multiple Connected\n  Components",
        "summary": "  Recent papers have formulated the problem of learning graphs from data as an\ninverse covariance estimation with graph Laplacian constraints. While such\nproblems are convex, existing methods cannot guarantee that solutions will have\nspecific graph topology properties (e.g., being $k$-partite), which are\ndesirable for some applications. In fact, the problem of learning a graph with\ngiven topology properties, e.g., finding the $k$-partite graph that best\nmatches the data, is in general non-convex. In this paper, we develop novel\ntheoretical results that provide performance guarantees for an approach to\nsolve these problems. Our solution decomposes this problem into two\nsub-problems, for which efficient solutions are known. Specifically, a graph\ntopology inference (GTI) step is employed to select a feasible graph topology,\ni.e., one having the desired property. Then, a graph weight estimation (GWE)\nstep is performed by solving a generalized graph Laplacian estimation problem,\nwhere edges are constrained by the topology found in the GTI step. Our main\nresult is a bound on the error of the GWE step as a function of the error in\nthe GTI step. This error bound indicates that the GTI step should be solved\nusing an algorithm that approximates the similarity matrix by another matrix\nwhose entries have been thresholded to zero to have the desired type of graph\ntopology. The GTI stage can leverage existing methods (e.g., state of the art\napproaches for graph coloring) which are typically based on minimizing the\ntotal weight of removed edges. Since the GWE stage is formulated as an inverse\ncovariance estimation problem with linear constraints, it can be solved using\nexisting convex optimization methods. We demonstrate that our two step approach\ncan achieve good results for both synthetic and texture image data.\n",
        "published": "2017-05-31T03:38:14Z",
        "pdf_link": "http://arxiv.org/pdf/1705.10934v4"
    },
    {
        "id": "http://arxiv.org/abs/1706.00182v3",
        "title": "Efficient learning with robust gradient descent",
        "summary": "  Minimizing the empirical risk is a popular training strategy, but for\nlearning tasks where the data may be noisy or heavy-tailed, one may require\nmany observations in order to generalize well. To achieve better performance\nunder less stringent requirements, we introduce a procedure which constructs a\nrobust approximation of the risk gradient for use in an iterative learning\nroutine. Using high-probability bounds on the excess risk of this algorithm, we\nshow that our update does not deviate far from the ideal gradient-based update.\nEmpirical tests using both controlled simulations and real-world benchmark data\nshow that in diverse settings, the proposed procedure can learn more\nefficiently, using less resources (iterations and observations) while\ngeneralizing better.\n",
        "published": "2017-06-01T07:00:56Z",
        "pdf_link": "http://arxiv.org/pdf/1706.00182v3"
    },
    {
        "id": "http://arxiv.org/abs/1706.00292v3",
        "title": "Learning Generative Models with Sinkhorn Divergences",
        "summary": "  The ability to compare two degenerate probability distributions (i.e. two\nprobability distributions supported on two distinct low-dimensional manifolds\nliving in a much higher-dimensional space) is a crucial problem arising in the\nestimation of generative models for high-dimensional observations such as those\narising in computer vision or natural language. It is known that optimal\ntransport metrics can represent a cure for this problem, since they were\nspecifically designed as an alternative to information divergences to handle\nsuch problematic scenarios. Unfortunately, training generative machines using\nOT raises formidable computational and statistical challenges, because of (i)\nthe computational burden of evaluating OT losses, (ii) the instability and lack\nof smoothness of these losses, (iii) the difficulty to estimate robustly these\nlosses and their gradients in high dimension. This paper presents the first\ntractable computational method to train large scale generative models using an\noptimal transport loss, and tackles these three issues by relying on two key\nideas: (a) entropic smoothing, which turns the original OT loss into one that\ncan be computed using Sinkhorn fixed point iterations; (b) algorithmic\n(automatic) differentiation of these iterations. These two approximations\nresult in a robust and differentiable approximation of the OT loss with\nstreamlined GPU execution. Entropic smoothing generates a family of losses\ninterpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus\nallowing to find a sweet spot leveraging the geometry of OT and the favorable\nhigh-dimensional sample complexity of MMD which comes with unbiased gradient\nestimates. The resulting computational architecture complements nicely standard\ndeep network generative models by a stack of extra layers implementing the loss\nfunction.\n",
        "published": "2017-06-01T13:37:37Z",
        "pdf_link": "http://arxiv.org/pdf/1706.00292v3"
    },
    {
        "id": "http://arxiv.org/abs/1706.00514v3",
        "title": "Selective Inference for Change Point Detection in Multi-dimensional\n  Sequences",
        "summary": "  We study the problem of detecting change points (CPs) that are characterized\nby a subset of dimensions in a multi-dimensional sequence. A method for\ndetecting those CPs can be formulated as a two-stage method: one for selecting\nrelevant dimensions, and another for selecting CPs. It has been difficult to\nproperly control the false detection probability of these CP detection methods\nbecause selection bias in each stage must be properly corrected. Our main\ncontribution in this paper is to formulate a CP detection problem as a\nselective inference problem, and show that exact (non-asymptotic) inference is\npossible for a class of CP detection methods. We demonstrate the performances\nof the proposed selective inference framework through numerical simulations and\nits application to our motivating medical data analysis problem.\n",
        "published": "2017-06-01T22:31:37Z",
        "pdf_link": "http://arxiv.org/pdf/1706.00514v3"
    },
    {
        "id": "http://arxiv.org/abs/1706.01338v1",
        "title": "Understanding the Learned Iterative Soft Thresholding Algorithm with\n  matrix factorization",
        "summary": "  Sparse coding is a core building block in many data analysis and machine\nlearning pipelines. Typically it is solved by relying on generic optimization\ntechniques, such as the Iterative Soft Thresholding Algorithm and its\naccelerated version (ISTA, FISTA). These methods are optimal in the class of\nfirst-order methods for non-smooth, convex functions. However, they do not\nexploit the particular structure of the problem at hand nor the input data\ndistribution. An acceleration using neural networks, coined LISTA, was proposed\nin Gregor and Le Cun (2010), which showed empirically that one could achieve\nhigh quality estimates with few iterations by modifying the parameters of the\nproximal splitting appropriately.\n  In this paper we study the reasons for such acceleration. Our mathematical\nanalysis reveals that it is related to a specific matrix factorization of the\nGram kernel of the dictionary, which attempts to nearly diagonalise the kernel\nwith a basis that produces a small perturbation of the $\\ell_1$ ball. When this\nfactorization succeeds, we prove that the resulting splitting algorithm enjoys\nan improved convergence bound with respect to the non-adaptive version.\nMoreover, our analysis also shows that conditions for acceleration occur mostly\nat the beginning of the iterative process, consistent with numerical\nexperiments. We further validate our analysis by showing that on dictionaries\nwhere this factorization does not exist, adaptive acceleration fails.\n",
        "published": "2017-06-02T07:24:10Z",
        "pdf_link": "http://arxiv.org/pdf/1706.01338v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.01807v1",
        "title": "GAN and VAE from an Optimal Transport Point of View",
        "summary": "  This short article revisits some of the ideas introduced in arXiv:1701.07875\nand arXiv:1705.07642 in a simple setup. This sheds some lights on the\nconnexions between Variational Autoencoders (VAE), Generative Adversarial\nNetworks (GAN) and Minimum Kantorovitch Estimators (MKE).\n",
        "published": "2017-06-06T15:03:15Z",
        "pdf_link": "http://arxiv.org/pdf/1706.01807v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.01825v1",
        "title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated\n  Exploration of Chemical Space",
        "summary": "  Chemical space is so large that brute force searches for new interesting\nmolecules are infeasible. High-throughput virtual screening via computer\ncluster simulations can speed up the discovery process by collecting very large\namounts of data in parallel, e.g., up to hundreds or thousands of parallel\nmeasurements. Bayesian optimization (BO) can produce additional acceleration by\nsequentially identifying the most useful simulations or experiments to be\nperformed next. However, current BO methods cannot scale to the large numbers\nof parallel measurements and the massive libraries of molecules currently used\nin high-throughput screening. Here, we propose a scalable solution based on a\nparallel and distributed implementation of Thompson sampling (PDTS). We show\nthat, in small scale problems, PDTS performs similarly as parallel expected\nimprovement (EI), a batch version of the most widely used BO heuristic.\nAdditionally, in settings where parallel EI does not scale, PDTS outperforms\nother scalable baselines such as a greedy search, $\\epsilon$-greedy approaches\nand a random search method. These results show that PDTS is a successful\nsolution for large-scale parallel BO.\n",
        "published": "2017-06-06T15:57:17Z",
        "pdf_link": "http://arxiv.org/pdf/1706.01825v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.01865v2",
        "title": "Estimating Shape Parameters of Piecewise Linear-Quadratic Problems",
        "summary": "  Piecewise Linear-Quadratic (PLQ) penalties are widely used to develop models\nin statistical inference, signal processing, and machine learning. Common\nexamples of PLQ penalties include least squares, Huber, Vapnik, 1-norm, and\ntheir asymmetric generalizations. Properties of these estimators depend on the\nchoice of penalty and its shape parameters, such as degree of asymmetry for the\nquantile loss, and transition point between linear and quadratic pieces for the\nHuber function. In this paper, we develop a statistical framework that can help\nthe modeler to automatically tune shape parameters once the shape of the\npenalty has been chosen. The choice of the parameter is informed by the basic\nnotion that each QS penalty should correspond to a true statistical density.\nThe normalization constant inherent in this requirement helps to inform the\noptimization over shape parameters, giving a joint optimization problem over\nthese as well as primary parameters of interest. A second contribution is to\nconsider optimization methods for these joint problems. We show that basic\nfirst-order methods can be immediately brought to bear, and design specialized\nextensions of interior point (IP) methods for PLQ problems that can quickly and\nefficiently solve the joint problem. Synthetic problems and larger-scale\npractical examples illustrate the potential of the approach.\n",
        "published": "2017-06-06T17:27:48Z",
        "pdf_link": "http://arxiv.org/pdf/1706.01865v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.02326v2",
        "title": "Improving Variational Auto-Encoders using convex combination linear\n  Inverse Autoregressive Flow",
        "summary": "  In this paper, we propose a new volume-preserving flow and show that it\nperforms similarly to the linear general normalizing flow. The idea is to\nenrich a linear Inverse Autoregressive Flow by introducing multiple\nlower-triangular matrices with ones on the diagonal and combining them using a\nconvex combination. In the experimental studies on MNIST and Histopathology\ndata we show that the proposed approach outperforms other volume-preserving\nflows and is competitive with current state-of-the-art linear normalizing flow.\n",
        "published": "2017-06-07T18:25:12Z",
        "pdf_link": "http://arxiv.org/pdf/1706.02326v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.02412v2",
        "title": "A Robust Learning Algorithm for Regression Models Using Distributionally\n  Robust Optimization under the Wasserstein Metric",
        "summary": "  We present a Distributionally Robust Optimization (DRO) approach to estimate\na robustified regression plane in a linear regression setting, when the\nobserved samples are potentially contaminated with adversarially corrupted\noutliers. Our approach mitigates the impact of outliers through hedging against\na family of distributions on the observed data, some of which assign very low\nprobabilities to the outliers. The set of distributions under consideration are\nclose to the empirical distribution in the sense of the Wasserstein metric. We\nshow that this DRO formulation can be relaxed to a convex optimization problem\nwhich encompasses a class of models. By selecting proper norm spaces for the\nWasserstein metric, we are able to recover several commonly used regularized\nregression models. We provide new insights into the regularization term and\ngive guidance on the selection of the regularization coefficient from the\nstandpoint of a confidence region. We establish two types of performance\nguarantees for the solution to our formulation under mild conditions. One is\nrelated to its out-of-sample behavior (prediction bias), and the other concerns\nthe discrepancy between the estimated and true regression planes (estimation\nbias). Extensive numerical results demonstrate the superiority of our approach\nto a host of regression models, in terms of the prediction and estimation\naccuracies. We also consider the application of our robust learning procedure\nto outlier detection, and show that our approach achieves a much higher AUC\n(Area Under the ROC Curve) than M-estimation.\n",
        "published": "2017-06-07T23:35:13Z",
        "pdf_link": "http://arxiv.org/pdf/1706.02412v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.02492v1",
        "title": "Consistency Results for Stationary Autoregressive Processes with\n  Constrained Coefficients",
        "summary": "  We consider stationary autoregressive processes with coefficients restricted\nto an ellipsoid, which includes autoregressive processes with absolutely\nsummable coefficients. We provide consistency results under different norms for\nthe estimation of such processes using constrained and penalized estimators. As\nan application we show some weak form of universal consistency. Simulations\nshow that directly including the constraint in the estimation can lead to more\nrobust results.\n",
        "published": "2017-06-08T09:34:31Z",
        "pdf_link": "http://arxiv.org/pdf/1706.02492v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.02829v4",
        "title": "Time Series Using Exponential Smoothing Cells",
        "summary": "  Time series analysis is used to understand and predict dynamic processes,\nincluding evolving demands in business, weather, markets, and biological\nrhythms. Exponential smoothing is used in all these domains to obtain simple\ninterpretable models of time series and to forecast future values. Despite its\npopularity, exponential smoothing fails dramatically in the presence of\noutliers, large amounts of noise, or when the underlying time series changes.\n  We propose a flexible model for time series analysis, using exponential\nsmoothing cells for overlapping time windows. The approach can detect and\nremove outliers, denoise data, fill in missing observations, and provide\nmeaningful forecasts in challenging situations. In contrast to classic\nexponential smoothing, which solves a nonconvex optimization problem over the\nsmoothing parameters and initial state, the proposed approach requires solving\na single structured convex optimization problem. Recent developments in\nefficient convex optimization of large-scale dynamic models make the approach\ntractable. We illustrate new capabilities using synthetic examples, and then\nuse the approach to analyze and forecast noisy real-world time series. Code for\nthe approach and experiments is publicly available.\n",
        "published": "2017-06-09T04:05:01Z",
        "pdf_link": "http://arxiv.org/pdf/1706.02829v4"
    },
    {
        "id": "http://arxiv.org/abs/1706.03373v2",
        "title": "Multiple Instance Dictionary Learning for Beat-to-Beat Heart Rate\n  Monitoring from Ballistocardiograms",
        "summary": "  A multiple instance dictionary learning approach, Dictionary Learning using\nFunctions of Multiple Instances (DL-FUMI), is used to perform beat-to-beat\nheart rate estimation and to characterize heartbeat signatures from\nballistocardiogram (BCG) signals collected with a hydraulic bed sensor. DL-FUMI\nestimates a \"heartbeat concept\" that represents an individual's personal\nballistocardiogram heartbeat pattern. DL-FUMI formulates heartbeat detection\nand heartbeat characterization as a multiple instance learning problem to\naddress the uncertainty inherent in aligning BCG signals with ground truth\nduring training. Experimental results show that the estimated heartbeat concept\nfound by DL-FUMI is an effective heartbeat prototype and achieves superior\nperformance over comparison algorithms.\n",
        "published": "2017-06-11T16:21:08Z",
        "pdf_link": "http://arxiv.org/pdf/1706.03373v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.03662v2",
        "title": "Practical Gauss-Newton Optimisation for Deep Learning",
        "summary": "  We present an efficient block-diagonal ap- proximation to the Gauss-Newton\nmatrix for feedforward neural networks. Our result- ing algorithm is\ncompetitive against state- of-the-art first order optimisation methods, with\nsometimes significant improvement in optimisation performance. Unlike\nfirst-order methods, for which hyperparameter tuning of the optimisation\nparameters is often a labo- rious process, our approach can provide good\nperformance even when used with default set- tings. A side result of our work\nis that for piecewise linear transfer functions, the net- work objective\nfunction can have no differ- entiable local maxima, which may partially explain\nwhy such transfer functions facilitate effective optimisation.\n",
        "published": "2017-06-12T14:39:48Z",
        "pdf_link": "http://arxiv.org/pdf/1706.03662v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.03673v2",
        "title": "Dealing with Integer-valued Variables in Bayesian Optimization with\n  Gaussian Processes",
        "summary": "  Bayesian optimization (BO) methods are useful for optimizing functions that\nare expensive to evaluate, lack an analytical expression and whose evaluations\ncan be contaminated by noise. These methods rely on a probabilistic model of\nthe objective function, typically a Gaussian process (GP), upon which an\nacquisition function is built. This function guides the optimization process\nand measures the expected utility of performing an evaluation of the objective\nat a new point. GPs assume continous input variables. When this is not the\ncase, such as when some of the input variables take integer values, one has to\nintroduce extra approximations. A common approach is to round the suggested\nvariable value to the closest integer before doing the evaluation of the\nobjective. We show that this can lead to problems in the optimization process\nand describe a more principled approach to account for input variables that are\ninteger-valued. We illustrate in both synthetic and a real experiments the\nutility of our approach, which significantly improves the results of standard\nBO methods on problems involving integer-valued variables.\n",
        "published": "2017-06-12T14:52:41Z",
        "pdf_link": "http://arxiv.org/pdf/1706.03673v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.03779v2",
        "title": "General Latent Feature Models for Heterogeneous Datasets",
        "summary": "  Latent feature modeling allows capturing the latent structure responsible for\ngenerating the observed properties of a set of objects. It is often used to\nmake predictions either for new values of interest or missing information in\nthe original data, as well as to perform data exploratory analysis. However,\nalthough there is an extensive literature on latent feature models for\nhomogeneous datasets, where all the attributes that describe each object are of\nthe same (continuous or discrete) nature, there is a lack of work on latent\nfeature modeling for heterogeneous databases. In this paper, we introduce a\ngeneral Bayesian nonparametric latent feature model suitable for heterogeneous\ndatasets, where the attributes describing each object can be either discrete,\ncontinuous or mixed variables. The proposed model presents several important\nproperties. First, it accounts for heterogeneous data while keeping the\nproperties of conjugate models, which allow us to infer the model in linear\ntime with respect to the number of objects and attributes. Second, its Bayesian\nnonparametric nature allows us to automatically infer the model complexity from\nthe data, i.e., the number of features necessary to capture the latent\nstructure in the data. Third, the latent features in the model are\nbinary-valued variables, easing the interpretability of the obtained latent\nfeatures in data exploratory analysis. We show the flexibility of the proposed\nmodel by solving both prediction and data analysis tasks on several real-world\ndatasets. Moreover, a software package of the GLFM is publicly available for\nother researcher to use and improve it.\n",
        "published": "2017-06-12T18:00:03Z",
        "pdf_link": "http://arxiv.org/pdf/1706.03779v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.04632v1",
        "title": "Stochastic Gradient MCMC Methods for Hidden Markov Models",
        "summary": "  Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling\nBayesian inference to large datasets under an assumption of i.i.d data. We\ninstead develop an SG-MCMC algorithm to learn the parameters of hidden Markov\nmodels (HMMs) for time-dependent data. There are two challenges to applying\nSG-MCMC in this setting: The latent discrete states, and needing to break\ndependencies when considering minibatches. We consider a marginal likelihood\nrepresentation of the HMM and propose an algorithm that harnesses the inherent\nmemory decay of the process. We demonstrate the effectiveness of our algorithm\non synthetic experiments and an ion channel recording data, with runtimes\nsignificantly outperforming batch MCMC.\n",
        "published": "2017-06-14T18:44:29Z",
        "pdf_link": "http://arxiv.org/pdf/1706.04632v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.05136v1",
        "title": "Deep Generative Models for Relational Data with Side Information",
        "summary": "  We present a probabilistic framework for overlapping community discovery and\nlink prediction for relational data, given as a graph. The proposed framework\nhas: (1) a deep architecture which enables us to infer multiple layers of\nlatent features/communities for each node, providing superior link prediction\nperformance on more complex networks and better interpretability of the latent\nfeatures; and (2) a regression model which allows directly conditioning the\nnode latent features on the side information available in form of node\nattributes. Our framework handles both (1) and (2) via a clean, unified model,\nwhich enjoys full local conjugacy via data augmentation, and facilitates\nefficient inference via closed form Gibbs sampling. Moreover, inference cost\nscales in the number of edges which is attractive for massive but sparse\nnetworks. Our framework is also easily extendable to model weighted networks\nwith count-valued edges. We compare with various state-of-the-art methods and\nreport results, both quantitative and qualitative, on several benchmark data\nsets.\n",
        "published": "2017-06-16T02:52:38Z",
        "pdf_link": "http://arxiv.org/pdf/1706.05136v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.05612v2",
        "title": "Kernel Two-Sample Hypothesis Testing Using Kernel Set Classification",
        "summary": "  The two-sample hypothesis testing problem is studied for the challenging\nscenario of high dimensional data sets with small sample sizes. We show that\nthe two-sample hypothesis testing problem can be posed as a one-class set\nclassification problem. In the set classification problem the goal is to\nclassify a set of data points that are assumed to have a common class. We prove\nthat the average probability of error given a set is less than or equal to the\nBayes error and decreases as a power of $n$ number of sample data points in the\nset. We use the positive definite Set Kernel for directly mapping sets of data\nto an associated Reproducing Kernel Hilbert Space, without the need to learn a\nprobability distribution. We specifically solve the two-sample hypothesis\ntesting problem using a one-class SVM in conjunction with the proposed Set\nKernel. We compare the proposed method with the Maximum Mean Discrepancy,\nF-Test and T-Test methods on a number of challenging simulated high dimensional\nand small sample size data. We also perform two-sample hypothesis testing\nexperiments on six cancer gene expression data sets and achieve zero type-I and\ntype-II error results on all data sets.\n",
        "published": "2017-06-18T06:51:06Z",
        "pdf_link": "http://arxiv.org/pdf/1706.05612v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.06150v2",
        "title": "A Comparison of Resampling and Recursive Partitioning Methods in Random\n  Forest for Estimating the Asymptotic Variance Using the Infinitesimal\n  Jackknife",
        "summary": "  The infinitesimal jackknife (IJ) has recently been applied to the random\nforest to estimate its prediction variance. These theorems were verified under\na traditional random forest framework which uses classification and regression\ntrees (CART) and bootstrap resampling. However, random forests using\nconditional inference (CI) trees and subsampling have been found to be not\nprone to variable selection bias. Here, we conduct simulation experiments using\na novel approach to explore the applicability of the IJ to random forests using\nvariations on the resampling method and base learner. Test data points were\nsimulated and each trained using random forest on one hundred simulated\ntraining data sets using different combinations of resampling and base\nlearners. Using CI trees instead of traditional CART trees as well as using\nsubsampling instead of bootstrap sampling resulted in a much more accurate\nestimation of prediction variance when using the IJ. The random forest\nvariations here have been incorporated into an open source software package for\nthe R programming language.\n",
        "published": "2017-06-19T19:32:05Z",
        "pdf_link": "http://arxiv.org/pdf/1706.06150v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.06178v1",
        "title": "Infinite Mixture Model of Markov Chains",
        "summary": "  We propose a Bayesian nonparametric mixture model for prediction- and\ninformation extraction tasks with an efficient inference scheme. It models\ncategorical-valued time series that exhibit dynamics from multiple underlying\npatterns (e.g. user behavior traces). We simplify the idea of capturing these\npatterns by hierarchical hidden Markov models (HHMMs) - and extend the existing\napproaches by the additional representation of structural information. Our\nempirical results are based on both synthetic- and real world data. They\nindicate that the results are easily interpretable, and that the model excels\nat segmentation and prediction performance: it successfully identifies the\ngenerating patterns and can be used for effective prediction of future\nobservations.\n",
        "published": "2017-06-19T21:08:51Z",
        "pdf_link": "http://arxiv.org/pdf/1706.06178v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.06691v1",
        "title": "Interpretable Predictions of Tree-based Ensembles via Actionable Feature\n  Tweaking",
        "summary": "  Machine-learned models are often described as \"black boxes\". In many\nreal-world applications however, models may have to sacrifice predictive power\nin favour of human-interpretability. When this is the case, feature engineering\nbecomes a crucial task, which requires significant and time-consuming human\neffort. Whilst some features are inherently static, representing properties\nthat cannot be influenced (e.g., the age of an individual), others capture\ncharacteristics that could be adjusted (e.g., the daily amount of carbohydrates\ntaken). Nonetheless, once a model is learned from the data, each prediction it\nmakes on new instances is irreversible - assuming every instance to be a static\npoint located in the chosen feature space. There are many circumstances however\nwhere it is important to understand (i) why a model outputs a certain\nprediction on a given instance, (ii) which adjustable features of that instance\nshould be modified, and finally (iii) how to alter such a prediction when the\nmutated instance is input back to the model. In this paper, we present a\ntechnique that exploits the internals of a tree-based ensemble classifier to\noffer recommendations for transforming true negative instances into positively\npredicted ones. We demonstrate the validity of our approach using an online\nadvertising application. First, we design a Random Forest classifier that\neffectively separates between two types of ads: low (negative) and high\n(positive) quality ads (instances). Then, we introduce an algorithm that\nprovides recommendations that aim to transform a low quality ad (negative\ninstance) into a high quality one (positive instance). Finally, we evaluate our\napproach on a subset of the active inventory of a large ad network, Yahoo\nGemini.\n",
        "published": "2017-06-20T22:32:02Z",
        "pdf_link": "http://arxiv.org/pdf/1706.06691v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.06878v3",
        "title": "An Unsupervised Method for Estimating the Global Horizontal Irradiance\n  from Photovoltaic Power Measurements",
        "summary": "  In this paper, we present a method to determine the global horizontal\nirradiance (GHI) from the power measurements of one or more PV systems, located\nin the same neighborhood. The method is completely unsupervised and is based on\na physical model of a PV plant. The precise assessment of solar irradiance is\npivotal for the forecast of the electric power generated by photovoltaic (PV)\nplants. However, on-ground measurements are expensive and are generally not\nperformed for small and medium-sized PV plants. Satellite-based services\nrepresent a valid alternative to on site measurements, but their space-time\nresolution is limited. Results from two case studies located in Switzerland are\npresented. The performance of the proposed method at assessing GHI is compared\nwith that of free and commercial satellite services. Our results show that the\npresented method is generally better than satellite-based services, especially\nat high temporal resolutions.\n",
        "published": "2017-06-21T13:06:57Z",
        "pdf_link": "http://arxiv.org/pdf/1706.06878v3"
    },
    {
        "id": "http://arxiv.org/abs/1706.06971v2",
        "title": "Ensembles of phalanxes across assessment metrics for robust ranking of\n  homologous proteins",
        "summary": "  Two proteins are homologous if they have a common evolutionary origin, and\nthe binary classification problem is to identify proteins in a candidate set\nthat are homologous to a particular native protein. The feature (explanatory)\nvariables available for classification are various measures of similarity of\nproteins. There are multiple classification problems of this type for different\nnative proteins and their respective candidate sets. Homologous proteins are\nrare in a single candidate set, giving a highly unbalanced two-class problem.\nThe goal is to rank proteins in a candidate set according to the probability of\nbeing homologous to the set's native protein. An ideal classifier will place\nall the homologous proteins at the head of such a list. Our approach uses an\nensemble of models in a classifier and an ensemble of assessment metrics. For a\ngiven metric a classifier combines models, each based on a subset of the\navailable feature variables which we call phalanxes. The proposed ensemble of\nphalanxes identifies strong and diverse subsets of feature variables. A second\nphase of ensembling aggregates classifiers based on diverse evaluation metrics.\nThe overall result is called an ensemble of phalanxes and metrics. It provide\nrobustness against both close and distant homologues.\n",
        "published": "2017-06-21T15:49:14Z",
        "pdf_link": "http://arxiv.org/pdf/1706.06971v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.07258v1",
        "title": "Scalable Multi-Class Gaussian Process Classification using Expectation\n  Propagation",
        "summary": "  This paper describes an expectation propagation (EP) method for multi-class\nclassification with Gaussian processes that scales well to very large datasets.\nIn such a method the estimate of the log-marginal-likelihood involves a sum\nacross the data instances. This enables efficient training using stochastic\ngradients and mini-batches. When this type of training is used, the\ncomputational cost does not depend on the number of data instances $N$.\nFurthermore, extra assumptions in the approximate inference process make the\nmemory cost independent of $N$. The consequence is that the proposed EP method\ncan be used on datasets with millions of instances. We compare empirically this\nmethod with alternative approaches that approximate the required computations\nusing variational inference. The results show that it performs similar or even\nbetter than these techniques, which sometimes give significantly worse\npredictive distributions in terms of the test log-likelihood. Besides this, the\ntraining process of the proposed approach also seems to converge in a smaller\nnumber of iterations.\n",
        "published": "2017-06-22T11:18:15Z",
        "pdf_link": "http://arxiv.org/pdf/1706.07258v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.07834v2",
        "title": "Cover Tree Compressed Sensing for Fast MR Fingerprint Recovery",
        "summary": "  We adopt data structure in the form of cover trees and iteratively apply\napproximate nearest neighbour (ANN) searches for fast compressed sensing\nreconstruction of signals living on discrete smooth manifolds. Levering on the\nrecent stability results for the inexact Iterative Projected Gradient (IPG)\nalgorithm and by using the cover tree's ANN searches, we decrease the\nprojection cost of the IPG algorithm to be logarithmically growing with data\npopulation for low dimensional smooth manifolds. We apply our results to\nquantitative MRI compressed sensing and in particular within the Magnetic\nResonance Fingerprinting (MRF) framework. For a similar (or sometimes better)\nreconstruction accuracy, we report 2-3 orders of magnitude reduction in\ncomputations compared to the standard iterative method which uses brute-force\nsearches.\n",
        "published": "2017-06-23T18:57:17Z",
        "pdf_link": "http://arxiv.org/pdf/1706.07834v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.08203v2",
        "title": "Dr.VAE: Drug Response Variational Autoencoder",
        "summary": "  We present two deep generative models based on Variational Autoencoders to\nimprove the accuracy of drug response prediction. Our models, Perturbation\nVariational Autoencoder and its semi-supervised extension, Drug Response\nVariational Autoencoder (Dr.VAE), learn latent representation of the underlying\ngene states before and after drug application that depend on: (i) drug-induced\nbiological change of each gene and (ii) overall treatment response outcome. Our\nVAE-based models outperform the current published benchmarks in the field by\nanywhere from 3 to 11% AUROC and 2 to 30% AUPR. In addition, we found that\nbetter reconstruction accuracy does not necessarily lead to improvement in\nclassification accuracy and that jointly trained models perform better than\nmodels that minimize reconstruction error independently.\n",
        "published": "2017-06-26T01:41:04Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08203v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.08222v1",
        "title": "YouTube-8M Video Understanding Challenge Approach and Applications",
        "summary": "  This paper introduces the YouTube-8M Video Understanding Challenge hosted as\na Kaggle competition and also describes my approach to experimenting with\nvarious models. For each of my experiments, I provide the score result as well\nas possible improvements to be made. Towards the end of the paper, I discuss\nthe various ensemble learning techniques that I applied on the dataset which\nsignificantly boosted my overall competition score. At last, I discuss the\nexciting future of video understanding research and also the many applications\nthat such research could significantly improve.\n",
        "published": "2017-06-26T04:15:55Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08222v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.08263v4",
        "title": "Efficient Manifold and Subspace Approximations with Spherelets",
        "summary": "  In statistical dimensionality reduction, it is common to rely on the\nassumption that high dimensional data tend to concentrate near a lower\ndimensional manifold. There is a rich literature on approximating the unknown\nmanifold, and on exploiting such approximations in clustering, data\ncompression, and prediction. Most of the literature relies on linear or locally\nlinear approximations. In this article, we propose a simple and general\nalternative, which instead uses spheres, an approach we refer to as spherelets.\nWe develop spherical principal components analysis (SPCA), and provide theory\non the convergence rate for global and local SPCA, while showing that\nspherelets can provide lower covering numbers and MSEs for many manifolds.\nResults relative to state-of-the-art competitors show gains in ability to\naccurately approximate manifolds with fewer components. Unlike most\ncompetitors, which simply output lower-dimensional features, our approach\nprojects data onto the estimated manifold to produce fitted values that can be\nused for model assessment and cross validation. The methods are illustrated\nwith applications to multiple data sets.\n",
        "published": "2017-06-26T07:45:55Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08263v4"
    },
    {
        "id": "http://arxiv.org/abs/1706.08495v2",
        "title": "Uncertainty Decomposition in Bayesian Neural Networks with Latent\n  Variables",
        "summary": "  Bayesian neural networks (BNNs) with latent variables are probabilistic\nmodels which can automatically identify complex stochastic patterns in the\ndata. We describe and study in these models a decomposition of predictive\nuncertainty into its epistemic and aleatoric components. First, we show how\nsuch a decomposition arises naturally in a Bayesian active learning scenario by\nfollowing an information theoretic approach. Second, we use a similar\ndecomposition to develop a novel risk sensitive objective for safe\nreinforcement learning (RL). This objective minimizes the effect of model bias\nin environments whose stochastic dynamics are described by BNNs with latent\nvariables. Our experiments illustrate the usefulness of the resulting\ndecomposition in active learning and safe RL settings.\n",
        "published": "2017-06-26T17:36:28Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08495v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.08699v1",
        "title": "Two-Stage Hybrid Day-Ahead Solar Forecasting",
        "summary": "  Power supply from renewable resources is on a global rise where it is\nforecasted that renewable generation will surpass other types of generation in\na foreseeable future. Increased generation from renewable resources, mainly\nsolar and wind, exposes the power grid to more vulnerabilities, conceivably due\nto their variable generation, thus highlighting the importance of accurate\nforecasting methods. This paper proposes a two-stage day-ahead solar\nforecasting method that breaks down the forecasting into linear and nonlinear\nparts, determines subsequent forecasts, and accordingly, improves accuracy of\nthe obtained results. To further reduce the error resulted from nonstationarity\nof the historical solar radiation data, a data processing approach, including\npre-process and post-process levels, is integrated with the proposed method.\nNumerical simulations on three test days with different weather conditions\nexhibit the effectiveness of the proposed two-stage model.\n",
        "published": "2017-06-27T07:37:27Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08699v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.08936v2",
        "title": "Fast Algorithms for Learning Latent Variables in Graphical Models",
        "summary": "  We study the problem of learning latent variables in Gaussian graphical\nmodels. Existing methods for this problem assume that the precision matrix of\nthe observed variables is the superposition of a sparse and a low-rank\ncomponent. In this paper, we focus on the estimation of the low-rank component,\nwhich encodes the effect of marginalization over the latent variables. We\nintroduce fast, proper learning algorithms for this problem. In contrast with\nexisting approaches, our algorithms are manifestly non-convex. We support their\nefficacy via a rigorous theoretical analysis, and show that our algorithms\nmatch the best possible in terms of sample complexity, while achieving\ncomputational speed-ups over existing methods. We complement our theory with\nseveral numerical experiments.\n",
        "published": "2017-06-27T16:44:56Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08936v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.08984v1",
        "title": "Unsupervised Learning via Total Correlation Explanation",
        "summary": "  Learning by children and animals occurs effortlessly and largely without\nobvious supervision. Successes in automating supervised learning have not\ntranslated to the more ambiguous realm of unsupervised learning where goals and\nlabels are not provided. Barlow (1961) suggested that the signal that brains\nleverage for unsupervised learning is dependence, or redundancy, in the sensory\nenvironment. Dependence can be characterized using the information-theoretic\nmultivariate mutual information measure called total correlation. The principle\nof Total Cor-relation Ex-planation (CorEx) is to learn representations of data\nthat \"explain\" as much dependence in the data as possible. We review some\nmanifestations of this principle along with successes in unsupervised learning\nproblems across diverse domains including human behavior, biology, and\nlanguage.\n",
        "published": "2017-06-27T18:02:34Z",
        "pdf_link": "http://arxiv.org/pdf/1706.08984v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.09411v2",
        "title": "Generalized notions of sparsity and restricted isometry property. Part\n  II: Applications",
        "summary": "  The restricted isometry property (RIP) is a universal tool for data recovery.\nWe explore the implication of the RIP in the framework of generalized sparsity\nand group measurements introduced in the Part I paper. It turns out that for a\ngiven measurement instrument the number of measurements for RIP can be improved\nby optimizing over families of Banach spaces. Second, we investigate the\npreservation of difference of two sparse vectors, which is not trivial in\ngeneralized models. Third, we extend the RIP of partial Fourier measurements at\noptimal scaling of number of measurements with random sign to far more general\ngroup structured measurements. Lastly, we also obtain RIP in infinite dimension\nin the context of Fourier measurement concepts with sparsity naturally replaced\nby smoothness assumptions.\n",
        "published": "2017-06-28T15:48:00Z",
        "pdf_link": "http://arxiv.org/pdf/1706.09411v2"
    },
    {
        "id": "http://arxiv.org/abs/1706.09751v1",
        "title": "Bayesian Semisupervised Learning with Deep Generative Models",
        "summary": "  Neural network based generative models with discriminative components are a\npowerful approach for semi-supervised learning. However, these techniques a)\ncannot account for model uncertainty in the estimation of the model's\ndiscriminative component and b) lack flexibility to capture complex stochastic\npatterns in the label generation process. To avoid these problems, we first\npropose to use a discriminative component with stochastic inputs for increased\nnoise flexibility. We show how an efficient Gibbs sampling procedure can\nmarginalize the stochastic inputs when inferring missing labels in this model.\nFollowing this, we extend the discriminative component to be fully Bayesian and\nproduce estimates of uncertainty in its parameter values. This opens the door\nfor semi-supervised Bayesian active learning.\n",
        "published": "2017-06-29T13:41:05Z",
        "pdf_link": "http://arxiv.org/pdf/1706.09751v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.09880v4",
        "title": "A Fixed-Point of View on Gradient Methods for Big Data",
        "summary": "  Interpreting gradient methods as fixed-point iterations, we provide a\ndetailed analysis of those methods for minimizing convex objective functions.\nDue to their conceptual and algorithmic simplicity, gradient methods are widely\nused in machine learning for massive data sets (big data). In particular,\nstochastic gradient methods are considered the de- facto standard for training\ndeep neural networks. Studying gradient methods within the realm of fixed-point\ntheory provides us with powerful tools to analyze their convergence properties.\nIn particular, gradient methods using inexact or noisy gradients, such as\nstochastic gradient descent, can be studied conveniently using well-known\nresults on inexact fixed-point iterations. Moreover, as we demonstrate in this\npaper, the fixed-point approach allows an elegant derivation of accelerations\nfor basic gradient methods. In particular, we will show how gradient descent\ncan be accelerated by a fixed-point preserving transformation of an operator\nassociated with the objective function.\n",
        "published": "2017-06-29T17:46:50Z",
        "pdf_link": "http://arxiv.org/pdf/1706.09880v4"
    },
    {
        "id": "http://arxiv.org/abs/1706.09985v1",
        "title": "Towards Bursting Filter Bubble via Contextual Risks and Uncertainties",
        "summary": "  A rising topic in computational journalism is how to enhance the diversity in\nnews served to subscribers to foster exploration behavior in news reading.\nDespite the success of preference learning in personalized news recommendation,\ntheir over-exploitation causes filter bubble that isolates readers from\nopposing viewpoints and hurts long-term user experiences with lack of\nserendipity. Since news providers can recommend neither opposite nor\ndiversified opinions if unpopularity of these articles is surely predicted,\nthey can only bet on the articles whose forecasts of click-through rate involve\nhigh variability (risks) or high estimation errors (uncertainties). We propose\na novel Bayesian model of uncertainty-aware scoring and ranking for news\narticles. The Bayesian binary classifier models probability of success (defined\nas a news click) as a Beta-distributed random variable conditional on a vector\nof the context (user features, article features, and other contextual\nfeatures). The posterior of the contextual coefficients can be computed\nefficiently using a low-rank version of Laplace's method via thin Singular\nValue Decomposition. Efficiencies in personalized targeting of exceptional\narticles, which are chosen by each subscriber in test period, are evaluated on\nreal-world news datasets. The proposed estimator slightly outperformed existing\ntraining and scoring algorithms, in terms of efficiency in identifying\nsuccessful outliers.\n",
        "published": "2017-06-30T00:33:35Z",
        "pdf_link": "http://arxiv.org/pdf/1706.09985v1"
    },
    {
        "id": "http://arxiv.org/abs/1706.10272v1",
        "title": "Nuclear penalized multinomial regression with an application to\n  predicting at bat outcomes in baseball",
        "summary": "  We propose the nuclear norm penalty as an alternative to the ridge penalty\nfor regularized multinomial regression. This convex relaxation of reduced-rank\nmultinomial regression has the advantage of leveraging underlying structure\namong the response categories to make better predictions. We apply our method,\nnuclear penalized multinomial regression (NPMR), to Major League Baseball\nplay-by-play data to predict outcome probabilities based on batter-pitcher\nmatchups. The interpretation of the results meshes well with subject-area\nexpertise and also suggests a novel understanding of what differentiates\nplayers.\n",
        "published": "2017-06-30T16:58:25Z",
        "pdf_link": "http://arxiv.org/pdf/1706.10272v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.00379v2",
        "title": "Sparse Regularization in Marketing and Economics",
        "summary": "  Sparse alpha-norm regularization has many data-rich applications in Marketing\nand Economics. Alpha-norm, in contrast to lasso and ridge regularization, jumps\nto a sparse solution. This feature is attractive for ultra high-dimensional\nproblems that occur in demand estimation and forecasting. The alpha-norm\nobjective is nonconvex and requires coordinate descent and proximal operators\nto find the sparse solution. We study a typical marketing demand forecasting\nproblem, grocery store sales for salty snacks, that has many dummy variables as\ncontrols. The key predictors of demand include price, equivalized volume,\npromotion, flavor, scent, and brand effects. By comparing with many commonly\nused machine learning methods, alpha-norm regularization achieves its goal of\nproviding accurate out-of-sample estimates for the promotion lift effects.\nFinally, we conclude with directions for future research.\n",
        "published": "2017-09-01T16:01:25Z",
        "pdf_link": "http://arxiv.org/pdf/1709.00379v2"
    },
    {
        "id": "http://arxiv.org/abs/1709.00401v3",
        "title": "Statistical Inference for Data-adaptive Doubly Robust Estimators with\n  Survival Outcomes",
        "summary": "  The consistency of doubly robust estimators relies on consistent estimation\nof at least one of two nuisance regression parameters. In moderate to large\ndimensions, the use of flexible data-adaptive regression estimators may aid in\nachieving this consistency. However, $n^{1/2}$-consistency of doubly robust\nestimators is not guaranteed if one of the nuisance estimators is inconsistent.\nIn this paper we present a doubly robust estimator for survival analysis with\nthe novel property that it converges to a Gaussian variable at $n^{1/2}$-rate\nfor a large class of data-adaptive estimators of the nuisance parameters, under\nthe only assumption that at least one of them is consistently estimated at a\n$n^{1/4}$-rate. This result is achieved through adaptation of recent ideas in\nsemiparametric inference, which amount to: (i) Gaussianizing (i.e., making\nasymptotically linear) a drift term that arises in the asymptotic analysis of\nthe doubly robust estimator, and (ii) using cross-fitting to avoid entropy\nconditions on the nuisance estimators. We present the formula of the asymptotic\nvariance of the estimator, which allows computation of doubly robust confidence\nintervals and p-values. We illustrate the finite-sample properties of the\nestimator in simulation studies, and demonstrate its use in a phase III\nclinical trial for estimating the effect of a novel therapy for the treatment\nof HER2 positive breast cancer.\n",
        "published": "2017-09-01T17:46:07Z",
        "pdf_link": "http://arxiv.org/pdf/1709.00401v3"
    },
    {
        "id": "http://arxiv.org/abs/1709.00776v1",
        "title": "Estimation of interventional effects of features on prediction",
        "summary": "  The interpretability of prediction mechanisms with respect to the underlying\nprediction problem is often unclear. While several studies have focused on\ndeveloping prediction models with meaningful parameters, the causal\nrelationships between the predictors and the actual prediction have not been\nconsidered. Here, we connect the underlying causal structure of a data\ngeneration process and the causal structure of a prediction mechanism. To\nachieve this, we propose a framework that identifies the feature with the\ngreatest causal influence on the prediction and estimates the necessary causal\nintervention of a feature such that a desired prediction is obtained. The\ngeneral concept of the framework has no restrictions regarding data linearity;\nhowever, we focus on an implementation for linear data here. The framework\napplicability is evaluated using artificial data and demonstrated using\nreal-world data.\n",
        "published": "2017-09-03T23:23:40Z",
        "pdf_link": "http://arxiv.org/pdf/1709.00776v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.00843v2",
        "title": "Extending the scope of the small-ball method",
        "summary": "  The small-ball method was introduced as a way of obtaining a high\nprobability, isomorphic lower bound on the quadratic empirical process, under\nweak assumptions on the indexing class. The key assumption was that class\nmembers satisfy a uniform small-ball estimate: that $Pr(|f| \\geq\n\\kappa\\|f\\|_{L_2}) \\geq \\delta$ for given constants $\\kappa$ and $\\delta$.\n  Here we extend the small-ball method and obtain a high probability,\nalmost-isometric (rather than isomorphic) lower bound on the quadratic\nempirical process. The scope of the result is considerably wider than the\nsmall-ball method: there is no need for class members to satisfy a uniform\nsmall-ball condition, and moreover, motivated by the notion of tournament\nlearning procedures, the result is stable under a `majority vote'.\n",
        "published": "2017-09-04T07:38:44Z",
        "pdf_link": "http://arxiv.org/pdf/1709.00843v2"
    },
    {
        "id": "http://arxiv.org/abs/1709.01179v4",
        "title": "Continuous-Time Flows for Efficient Inference and Density Estimation",
        "summary": "  Two fundamental problems in unsupervised learning are efficient inference for\nlatent-variable models and robust density estimation based on large amounts of\nunlabeled data. Algorithms for the two tasks, such as normalizing flows and\ngenerative adversarial networks (GANs), are often developed independently. In\nthis paper, we propose the concept of {\\em continuous-time flows} (CTFs), a\nfamily of diffusion-based methods that are able to asymptotically approach a\ntarget distribution. Distinct from normalizing flows and GANs, CTFs can be\nadopted to achieve the above two goals in one framework, with theoretical\nguarantees. Our framework includes distilling knowledge from a CTF for\nefficient inference, and learning an explicit energy-based distribution with\nCTFs for density estimation. Both tasks rely on a new technique for\ndistribution matching within amortized learning. Experiments on various tasks\ndemonstrate promising performance of the proposed CTF framework, compared to\nrelated techniques.\n",
        "published": "2017-09-04T22:00:16Z",
        "pdf_link": "http://arxiv.org/pdf/1709.01179v4"
    },
    {
        "id": "http://arxiv.org/abs/1709.01180v1",
        "title": "A Convergence Analysis for A Class of Practical Variance-Reduction\n  Stochastic Gradient MCMC",
        "summary": "  Stochastic gradient Markov Chain Monte Carlo (SG-MCMC) has been developed as\na flexible family of scalable Bayesian sampling algorithms. However, there has\nbeen little theoretical analysis of the impact of minibatch size to the\nalgorithm's convergence rate. In this paper, we prove that under a limited\ncomputational budget/time, a larger minibatch size leads to a faster decrease\nof the mean squared error bound (thus the fastest one corresponds to using full\ngradients), which motivates the necessity of variance reduction in SG-MCMC.\nConsequently, by borrowing ideas from stochastic optimization, we propose a\npractical variance-reduction technique for SG-MCMC, that is efficient in both\ncomputation and storage. We develop theory to prove that our algorithm induces\na faster convergence rate than standard SG-MCMC. A number of large-scale\nexperiments, ranging from Bayesian learning of logistic regression to deep\nneural networks, validate the theory and demonstrate the superiority of the\nproposed variance-reduction SG-MCMC framework.\n",
        "published": "2017-09-04T22:11:24Z",
        "pdf_link": "http://arxiv.org/pdf/1709.01180v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.01233v9",
        "title": "Supervised Dimensionality Reduction for Big Data",
        "summary": "  To solve key biomedical problems, experimentalists now routinely measure\nmillions or billions of features (dimensions) per sample, with the hope that\ndata science techniques will be able to build accurate data-driven inferences.\nBecause sample sizes are typically orders of magnitude smaller than the\ndimensionality of these data, valid inferences require finding a\nlow-dimensional representation that preserves the discriminating information\n(e.g., whether the individual suffers from a particular disease). There is a\nlack of interpretable supervised dimensionality reduction methods that scale to\nmillions of dimensions with strong statistical theoretical guarantees.We\nintroduce an approach, XOX, to extending principal components analysis by\nincorporating class-conditional moment estimates into the low-dimensional\nprojection. The simplest ver-sion, \"Linear Optimal Low-rank\" projection (LOL),\nincorporates the class-conditional means. We prove, and substantiate with both\nsynthetic and real data benchmarks, that LOL and its generalizations in the XOX\nframework lead to improved data representations for subsequent classification,\nwhile maintaining computational efficiency and scalability. Using multiple\nbrain imaging datasets consisting of >150 million features, and several\ngenomics datasets with>500,000 features, LOL outperforms other scalable linear\ndimensionality reduction techniques in terms of accuracy, while only requiring\na few minutes on a standard desktop computer.\n",
        "published": "2017-09-05T04:19:38Z",
        "pdf_link": "http://arxiv.org/pdf/1709.01233v9"
    },
    {
        "id": "http://arxiv.org/abs/1709.01907v1",
        "title": "Deep and Confident Prediction for Time Series at Uber",
        "summary": "  Reliable uncertainty estimation for time series prediction is critical in\nmany fields, including physics, biology, and manufacturing. At Uber,\nprobabilistic time series forecasting is used for robust prediction of number\nof trips during special events, driver incentive allocation, as well as\nreal-time anomaly detection across millions of metrics. Classical time series\nmodels are often used in conjunction with a probabilistic formulation for\nuncertainty estimation. However, such models are hard to tune, scale, and add\nexogenous variables to. Motivated by the recent resurgence of Long Short Term\nMemory networks, we propose a novel end-to-end Bayesian deep model that\nprovides time series prediction along with uncertainty estimation. We provide\ndetailed experiments of the proposed solution on completed trips data, and\nsuccessfully apply it to large-scale time series anomaly detection at Uber.\n",
        "published": "2017-09-06T17:29:50Z",
        "pdf_link": "http://arxiv.org/pdf/1709.01907v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.02702v1",
        "title": "Entropic Determinants",
        "summary": "  The ability of many powerful machine learning algorithms to deal with large\ndata sets without compromise is often hampered by computationally expensive\nlinear algebra tasks, of which calculating the log determinant is a canonical\nexample. In this paper we demonstrate the optimality of Maximum Entropy methods\nin approximating such calculations. We prove the equivalence between mean value\nconstraints and sample expectations in the big data limit, that Covariance\nmatrix eigenvalue distributions can be completely defined by moment information\nand that the reduction of the self entropy of a maximum entropy proposal\ndistribution, achieved by adding more moments reduces the KL divergence between\nthe proposal and true eigenvalue distribution. We empirically verify our\nresults on a variety of SparseSuite matrices and establish best practices.\n",
        "published": "2017-09-08T13:41:26Z",
        "pdf_link": "http://arxiv.org/pdf/1709.02702v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.02855v1",
        "title": "Roll-back Hamiltonian Monte Carlo",
        "summary": "  We propose a new framework for Hamiltonian Monte Carlo (HMC) on truncated\nprobability distributions with smooth underlying density functions. Traditional\nHMC requires computing the gradient of potential function associated with the\ntarget distribution, and therefore does not perform its full power on truncated\ndistributions due to lack of continuity and differentiability. In our\nframework, we introduce a sharp sigmoid factor in the density function to\napproximate the probability drop at the truncation boundary. The target\npotential function is approximated by a new potential which smoothly extends to\nthe entire sample space. HMC is then performed on the approximate potential.\nWhile our method is easy to implement and applies to a wide range of problems,\nit also achieves comparable computational efficiency on various sampling tasks\ncompared to other baseline methods. RBHMC also gives rise to a new approach for\nBayesian inference on constrained spaces.\n",
        "published": "2017-09-08T20:47:08Z",
        "pdf_link": "http://arxiv.org/pdf/1709.02855v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.04024v3",
        "title": "Discovering Potential Correlations via Hypercontractivity",
        "summary": "  Discovering a correlation from one variable to another variable is of\nfundamental scientific and practical interest. While existing correlation\nmeasures are suitable for discovering average correlation, they fail to\ndiscover hidden or potential correlations. To bridge this gap, (i) we postulate\na set of natural axioms that we expect a measure of potential correlation to\nsatisfy; (ii) we show that the rate of information bottleneck, i.e., the\nhypercontractivity coefficient, satisfies all the proposed axioms; (iii) we\nprovide a novel estimator to estimate the hypercontractivity coefficient from\nsamples; and (iv) we provide numerical experiments demonstrating that this\nproposed estimator discovers potential correlations among various indicators of\nWHO datasets, is robust in discovering gene interactions from gene expression\ntime series data, and is statistically more powerful than the estimators for\nother correlation measures in binary hypothesis testing of canonical examples\nof potential correlations.\n",
        "published": "2017-09-12T19:07:32Z",
        "pdf_link": "http://arxiv.org/pdf/1709.04024v3"
    },
    {
        "id": "http://arxiv.org/abs/1709.04135v2",
        "title": "Weighted Orthogonal Components Regression Analysis",
        "summary": "  In the multiple linear regression setting, we propose a general framework,\ntermed weighted orthogonal components regression (WOCR), which encompasses many\nknown methods as special cases, including ridge regression and principal\ncomponents regression. WOCR makes use of the monotonicity inherent in\northogonal components to parameterize the weight function. The formulation\nallows for efficient determination of tuning parameters and hence is\ncomputationally advantageous. Moreover, WOCR offers insights for deriving new\nbetter variants. Specifically, we advocate weighting components based on their\ncorrelations with the response, which leads to enhanced predictive performance.\nBoth simulated studies and real data examples are provided to assess and\nillustrate the advantages of the proposed methods.\n",
        "published": "2017-09-13T05:01:35Z",
        "pdf_link": "http://arxiv.org/pdf/1709.04135v2"
    },
    {
        "id": "http://arxiv.org/abs/1709.04862v1",
        "title": "Random Forests of Interaction Trees for Estimating Individualized\n  Treatment Effects in Randomized Trials",
        "summary": "  Assessing heterogeneous treatment effects has become a growing interest in\nadvancing precision medicine. Individualized treatment effects (ITE) play a\ncritical role in such an endeavor. Concerning experimental data collected from\nrandomized trials, we put forward a method, termed random forests of\ninteraction trees (RFIT), for estimating ITE on the basis of interaction trees\n(Su et al., 2009). To this end, we first propose a smooth sigmoid surrogate\n(SSS) method, as an alternative to greedy search, to speed up tree\nconstruction. RFIT outperforms the traditional `separate regression' approach\nin estimating ITE. Furthermore, standard errors for the estimated ITE via RFIT\ncan be obtained with the infinitesimal jackknife method. We assess and\nillustrate the use of RFIT via both simulation and the analysis of data from an\nacupuncture headache trial.\n",
        "published": "2017-09-14T16:34:02Z",
        "pdf_link": "http://arxiv.org/pdf/1709.04862v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.05119v1",
        "title": "Dependence Modeling in Ultra High Dimensions with Vine Copulas and the\n  Graphical Lasso",
        "summary": "  To model high dimensional data, Gaussian methods are widely used since they\nremain tractable and yield parsimonious models by imposing strong assumptions\non the data. Vine copulas are more flexible by combining arbitrary marginal\ndistributions and (conditional) bivariate copulas. Yet, this adaptability is\naccompanied by sharply increasing computational effort as the dimension\nincreases. The approach proposed in this paper overcomes this burden and makes\nthe first step into ultra high dimensional non-Gaussian dependence modeling by\nusing a divide-and-conquer approach. First, we apply Gaussian methods to split\ndatasets into feasibly small subsets and second, apply parsimonious and\nflexible vine copulas thereon. Finally, we reconcile them into one joint model.\nWe provide numerical results demonstrating the feasibility of our approach in\nmoderate dimensions and showcase its ability to estimate ultra high dimensional\nnon-Gaussian dependence models in thousands of dimensions.\n",
        "published": "2017-09-15T09:13:58Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05119v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.05216v1",
        "title": "Optimal Learning for Sequential Decision Making for Expensive Cost\n  Functions with Stochastic Binary Feedbacks",
        "summary": "  We consider the problem of sequentially making decisions that are rewarded by\n\"successes\" and \"failures\" which can be predicted through an unknown\nrelationship that depends on a partially controllable vector of attributes for\neach instance. The learner takes an active role in selecting samples from the\ninstance pool. The goal is to maximize the probability of success in either\noffline (training) or online (testing) phases. Our problem is motivated by\nreal-world applications where observations are time-consuming and/or expensive.\nWe develop a knowledge gradient policy using an online Bayesian linear\nclassifier to guide the experiment by maximizing the expected value of\ninformation of labeling each alternative. We provide a finite-time analysis of\nthe estimated error and show that the maximum likelihood estimator based\nproduced by the KG policy is consistent and asymptotically normal. We also show\nthat the knowledge gradient policy is asymptotically optimal in an offline\nsetting. This work further extends the knowledge gradient to the setting of\ncontextual bandits. We report the results of a series of experiments that\ndemonstrate its efficiency.\n",
        "published": "2017-09-13T22:01:20Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05216v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.05276v1",
        "title": "Mixtures and products in two graphical models",
        "summary": "  We compare two statistical models of three binary random variables. One is a\nmixture model and the other is a product of mixtures model called a restricted\nBoltzmann machine. Although the two models we study look different from their\nparametrizations, we show that they represent the same set of distributions on\nthe interior of the probability simplex, and are equal up to closure. We give a\nsemi-algebraic description of the model in terms of six binomial inequalities\nand obtain closed form expressions for the maximum likelihood estimates. We\nbriefly discuss extensions to larger models.\n",
        "published": "2017-09-15T15:42:04Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05276v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.05321v3",
        "title": "Learning Functional Causal Models with Generative Neural Networks",
        "summary": "  We introduce a new approach to functional causal modeling from observational\ndata, called Causal Generative Neural Networks (CGNN). CGNN leverages the power\nof neural networks to learn a generative model of the joint distribution of the\nobserved variables, by minimizing the Maximum Mean Discrepancy between\ngenerated and observed data. An approximate learning criterion is proposed to\nscale the computational cost of the approach to linear complexity in the number\nof observations. The performance of CGNN is studied throughout three\nexperiments. Firstly, CGNN is applied to cause-effect inference, where the task\nis to identify the best causal hypothesis out of $X\\rightarrow Y$ and\n$Y\\rightarrow X$. Secondly, CGNN is applied to the problem of identifying\nv-structures and conditional independences. Thirdly, CGNN is applied to\nmultivariate functional causal modeling: given a skeleton describing the direct\ndependences in a set of random variables $\\textbf{X} = [X_1, \\ldots, X_d]$,\nCGNN orients the edges in the skeleton to uncover the directed acyclic causal\ngraph describing the causal structure of the random variables. On all three\ntasks, CGNN is extensively assessed on both artificial and real-world data,\ncomparing favorably to the state-of-the-art. Finally, CGNN is extended to\nhandle the case of confounders, where latent variables are involved in the\noverall causal model.\n",
        "published": "2017-09-15T17:16:21Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05321v3"
    },
    {
        "id": "http://arxiv.org/abs/1709.05501v6",
        "title": "Constrained Bayesian Optimization for Automatic Chemical Design",
        "summary": "  Automatic Chemical Design is a framework for generating novel molecules with\noptimized properties. The original scheme, featuring Bayesian optimization over\nthe latent space of a variational autoencoder, suffers from the pathology that\nit tends to produce invalid molecular structures. First, we demonstrate\nempirically that this pathology arises when the Bayesian optimization scheme\nqueries latent points far away from the data on which the variational\nautoencoder has been trained. Secondly, by reformulating the search procedure\nas a constrained Bayesian optimization problem, we show that the effects of\nthis pathology can be mitigated, yielding marked improvements in the validity\nof the generated molecules. We posit that constrained Bayesian optimization is\na good approach for solving this class of training set mismatch in many\ngenerative tasks involving Bayesian optimization over the latent space of a\nvariational autoencoder.\n",
        "published": "2017-09-16T11:38:35Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05501v6"
    },
    {
        "id": "http://arxiv.org/abs/1709.05552v1",
        "title": "Multivariate Gaussian Network Structure Learning",
        "summary": "  We consider a graphical model where a multivariate normal vector is\nassociated with each node of the underlying graph and estimate the graphical\nstructure. We minimize a loss function obtained by regressing the vector at\neach node on those at the remaining ones under a group penalty. We show that\nthe proposed estimator can be computed by a fast convex optimization algorithm.\nWe show that as the sample size increases, the estimated regression\ncoefficients and the correct graphical structure are correctly estimated with\nprobability tending to one. By extensive simulations, we show the superiority\nof the proposed method over comparable procedures. We apply the technique on\ntwo real datasets. The first one is to identify gene and protein networks\nshowing up in cancer cell lines, and the second one is to reveal the\nconnections among different industries in the US.\n",
        "published": "2017-09-16T18:58:33Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05552v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.05667v1",
        "title": "Bayesian nonparametric Principal Component Analysis",
        "summary": "  Principal component analysis (PCA) is very popular to perform dimension\nreduction. The selection of the number of significant components is essential\nbut often based on some practical heuristics depending on the application. Only\nfew works have proposed a probabilistic approach able to infer the number of\nsignificant components. To this purpose, this paper introduces a Bayesian\nnonparametric principal component analysis (BNP-PCA). The proposed model\nprojects observations onto a random orthogonal basis which is assigned a prior\ndistribution defined on the Stiefel manifold. The prior on factor scores\ninvolves an Indian buffet process to model the uncertainty related to the\nnumber of components. The parameters of interest as well as the nuisance\nparameters are finally inferred within a fully Bayesian framework via Monte\nCarlo sampling. A study of the (in-)consistence of the marginal maximum a\nposteriori estimator of the latent dimension is carried out. A new estimator of\nthe subspace dimension is proposed. Moreover, for sake of statistical\nsignificance, a Kolmogorov-Smirnov test based on the posterior distribution of\nthe principal components is used to refine this estimate.\n  The behaviour of the algorithm is first studied on various synthetic\nexamples. Finally, the proposed BNP dimension reduction approach is shown to be\neasily yet efficiently coupled with clustering or latent factor models within a\nunique framework.\n",
        "published": "2017-09-17T14:24:05Z",
        "pdf_link": "http://arxiv.org/pdf/1709.05667v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.06171v2",
        "title": "Learning Low-Dimensional Metrics",
        "summary": "  This paper investigates the theoretical foundations of metric learning,\nfocused on three key questions that are not fully addressed in prior work: 1)\nwe consider learning general low-dimensional (low-rank) metrics as well as\nsparse metrics; 2) we develop upper and lower (minimax)bounds on the\ngeneralization error; 3) we quantify the sample complexity of metric learning\nin terms of the dimension of the feature space and the dimension/rank of the\nunderlying metric;4) we also bound the accuracy of the learned metric relative\nto the underlying true generative metric. All the results involve novel\nmathematical approaches to the metric learning problem, and lso shed new light\non the special case of ordinal embedding (aka non-metric multidimensional\nscaling).\n",
        "published": "2017-09-18T21:26:43Z",
        "pdf_link": "http://arxiv.org/pdf/1709.06171v2"
    },
    {
        "id": "http://arxiv.org/abs/1709.06557v1",
        "title": "A Summary Of The Kernel Matrix, And How To Learn It Effectively Using\n  Semidefinite Programming",
        "summary": "  Kernel-based learning algorithms are widely used in machine learning for\nproblems that make use of the similarity between object pairs. Such algorithms\nfirst embed all data points into an alternative space, where the inner product\nbetween object pairs specifies their distance in the embedding space. Applying\nkernel methods to partially labeled datasets is a classical challenge in this\nregard, requiring that the distances between unlabeled pairs must somehow be\nlearnt using the labeled data. In this independent study, I will summarize the\nwork of G. Lanckriet et al.'s work on \"Learning the Kernel Matrix with\nSemidefinite Programming\" used in support vector machines (SVM) algorithms for\nthe transduction problem. Throughout the report, I have provide alternative\nexplanations / derivations / analysis related to this work which is designed to\nease the understanding of the original article.\n",
        "published": "2017-09-18T23:17:58Z",
        "pdf_link": "http://arxiv.org/pdf/1709.06557v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.06970v3",
        "title": "An Expectation Conditional Maximization approach for Gaussian graphical\n  models",
        "summary": "  Bayesian graphical models are a useful tool for understanding dependence\nrelationships among many variables, particularly in situations with external\nprior information. In high-dimensional settings, the space of possible graphs\nbecomes enormous, rendering even state-of-the-art Bayesian stochastic search\ncomputationally infeasible. We propose a deterministic alternative to estimate\nGaussian and Gaussian copula graphical models using an Expectation Conditional\nMaximization (ECM) algorithm, extending the EM approach from Bayesian variable\nselection to graphical model estimation. We show that the ECM approach enables\nfast posterior exploration under a sequence of mixture priors, and can\nincorporate multiple sources of information.\n",
        "published": "2017-09-20T17:05:19Z",
        "pdf_link": "http://arxiv.org/pdf/1709.06970v3"
    },
    {
        "id": "http://arxiv.org/abs/1709.07175v1",
        "title": "Lazy stochastic principal component analysis",
        "summary": "  Stochastic principal component analysis (SPCA) has become a popular\ndimensionality reduction strategy for large, high-dimensional datasets. We\nderive a simplified algorithm, called Lazy SPCA, which has reduced\ncomputational complexity and is better suited for large-scale distributed\ncomputation. We prove that SPCA and Lazy SPCA find the same approximations to\nthe principal subspace, and that the pairwise distances between samples in the\nlower-dimensional space is invariant to whether SPCA is executed lazily or not.\nEmpirical studies find downstream predictive performance to be identical for\nboth methods, and superior to random projections, across a range of predictive\nmodels (linear regression, logistic lasso, and random forests). In our largest\nexperiment with 4.6 million samples, Lazy SPCA reduced 43.7 hours of\ncomputation to 9.9 hours. Overall, Lazy SPCA relies exclusively on matrix\nmultiplications, besides an operation on a small square matrix whose size\ndepends only on the target dimensionality.\n",
        "published": "2017-09-21T06:43:49Z",
        "pdf_link": "http://arxiv.org/pdf/1709.07175v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.08135v1",
        "title": "Weather Forecasting Error in Solar Energy Forecasting",
        "summary": "  As renewable distributed energy resources (DERs) penetrate the power grid at\nan accelerating speed, it is essential for operators to have accurate solar\nphotovoltaic (PV) energy forecasting for efficient operations and planning.\nGenerally, observed weather data are applied in the solar PV generation\nforecasting model while in practice the energy forecasting is based on\nforecasted weather data. In this paper, a study on the uncertainty in weather\nforecasting for the most commonly used weather variables is presented. The\nforecasted weather data for six days ahead is compared with the observed data\nand the results of analysis are quantified by statistical metrics. In addition,\nthe most influential weather predictors in energy forecasting model are\nselected. The performance of historical and observed weather data errors is\nassessed using a solar PV generation forecasting model. Finally, a sensitivity\ntest is performed to identify the influential weather variables whose accurate\nvalues can significantly improve the results of energy forecasting.\n",
        "published": "2017-09-24T01:37:31Z",
        "pdf_link": "http://arxiv.org/pdf/1709.08135v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.08770v1",
        "title": "On the Model Shrinkage Effect of Gamma Process Edge Partition Models",
        "summary": "  The edge partition model (EPM) is a fundamental Bayesian nonparametric model\nfor extracting an overlapping structure from binary matrix. The EPM adopts a\ngamma process ($\\Gamma$P) prior to automatically shrink the number of active\natoms. However, we empirically found that the model shrinkage of the EPM does\nnot typically work appropriately and leads to an overfitted solution. An\nanalysis of the expectation of the EPM's intensity function suggested that the\ngamma priors for the EPM hyperparameters disturb the model shrinkage effect of\nthe internal $\\Gamma$P. In order to ensure that the model shrinkage effect of\nthe EPM works in an appropriate manner, we proposed two novel generative\nconstructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM\nincorporating Dirichlet priors instead of the gamma priors. Furthermore, all\nDEPM's model parameters including the infinite atoms of the $\\Gamma$P prior\ncould be marginalized out, and thus it was possible to derive a truly infinite\nDEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler.\nWe experimentally confirmed that the model shrinkage of the proposed models\nworks well and that the IDEPM indicated state-of-the-art performance in\ngeneralization ability, link prediction accuracy, mixing efficiency, and\nconvergence speed.\n",
        "published": "2017-09-26T01:00:13Z",
        "pdf_link": "http://arxiv.org/pdf/1709.08770v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.08915v1",
        "title": "Telling Cause from Effect using MDL-based Local and Global Regression",
        "summary": "  We consider the fundamental problem of inferring the causal direction between\ntwo univariate numeric random variables $X$ and $Y$ from observational data.\nThe two-variable case is especially difficult to solve since it is not possible\nto use standard conditional independence tests between the variables.\n  To tackle this problem, we follow an information theoretic approach based on\nKolmogorov complexity and use the Minimum Description Length (MDL) principle to\nprovide a practical solution. In particular, we propose a compression scheme to\nencode local and global functional relations using MDL-based regression. We\ninfer $X$ causes $Y$ in case it is shorter to describe $Y$ as a function of $X$\nthan the inverse direction. In addition, we introduce Slope, an efficient\nlinear-time algorithm that through thorough empirical evaluation on both\nsynthetic and real world data we show outperforms the state of the art by a\nwide margin.\n",
        "published": "2017-09-26T09:49:18Z",
        "pdf_link": "http://arxiv.org/pdf/1709.08915v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.09102v1",
        "title": "Adaptive Nonparametric Clustering",
        "summary": "  This paper presents a new approach to non-parametric cluster analysis called\nAdaptive Weights Clustering (AWC). The idea is to identify the clustering\nstructure by checking at different points and for different scales on departure\nfrom local homogeneity. The proposed procedure describes the clustering\nstructure in terms of weights \\( w_{ij} \\) each of them measures the degree of\nlocal inhomogeneity for two neighbor local clusters using statistical tests of\n\"no gap\" between them. % The procedure starts from very local scale, then the\nparameter of locality grows by some factor at each step. The method is fully\nadaptive and does not require to specify the number of clusters or their\nstructure. The clustering results are not sensitive to noise and outliers, the\nprocedure is able to recover different clusters with sharp edges or manifold\nstructure. The method is scalable and computationally feasible. An intensive\nnumerical study shows a state-of-the-art performance of the method in various\nartificial examples and applications to text data. Our theoretical study states\noptimal sensitivity of AWC to local inhomogeneity.\n",
        "published": "2017-09-26T15:59:59Z",
        "pdf_link": "http://arxiv.org/pdf/1709.09102v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.09274v1",
        "title": "Symbolic Analysis-based Reduced Order Markov Modeling of Time Series\n  Data",
        "summary": "  This paper presents a technique for reduced-order Markov modeling for compact\nrepresentation of time-series data. In this work, symbolic dynamics-based tools\nhave been used to infer an approximate generative Markov model. The time-series\ndata are first symbolized by partitioning the continuous measurement space of\nthe signal and then, the discrete sequential data are modeled using symbolic\ndynamics. In the proposed approach, the size of temporal memory of the symbol\nsequence is estimated from spectral properties of the resulting stochastic\nmatrix corresponding to a first-order Markov model of the symbol sequence.\nThen, hierarchical clustering is used to represent the states of the\ncorresponding full-state Markov model to construct a reduced-order or size\nMarkov model with a non-deterministic algebraic structure. Subsequently, the\nparameters of the reduced-order Markov model are identified from the original\nmodel by making use of a Bayesian inference rule. The final model is selected\nusing information-theoretic criteria. The proposed concept is elucidated and\nvalidated on two different data sets as examples. The first example analyzes a\nset of pressure data from a swirl-stabilized combustor, where controlled\nprotocols are used to induce flame instabilities. Variations in the complexity\nof the derived Markov model represent how the system operating condition\nchanges from a stable to an unstable combustion regime. In the second example,\nthe data set is taken from NASA's data repository for prognostics of bearings\non rotating shafts. We show that, even with a very small state-space, the\nreduced-order models are able to achieve comparable performance and that the\nproposed approach provides flexibility in the selection of a final model for\nrepresentation and learning.\n",
        "published": "2017-09-26T22:11:26Z",
        "pdf_link": "http://arxiv.org/pdf/1709.09274v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.09301v1",
        "title": "Multi-way Interacting Regression via Factorization Machines",
        "summary": "  We propose a Bayesian regression method that accounts for multi-way\ninteractions of arbitrary orders among the predictor variables. Our model makes\nuse of a factorization mechanism for representing the regression coefficients\nof interactions among the predictors, while the interaction selection is guided\nby a prior distribution on random hypergraphs, a construction which generalizes\nthe Finite Feature Model. We present a posterior inference algorithm based on\nGibbs sampling, and establish posterior consistency of our regression model.\nOur method is evaluated with extensive experiments on simulated data and\ndemonstrated to be able to identify meaningful interactions in applications in\ngenetics and retail demand forecasting.\n",
        "published": "2017-09-27T01:51:19Z",
        "pdf_link": "http://arxiv.org/pdf/1709.09301v1"
    },
    {
        "id": "http://arxiv.org/abs/1709.10041v1",
        "title": "Bayesian Multi Plate High Throughput Screening of Compounds",
        "summary": "  High throughput screening of compounds (chemicals) is an essential part of\ndrug discovery [7], involving thousands to millions of compounds, with the\npurpose of identifying candidate hits. Most statistical tools, including the\nindustry standard B-score method, work on individual compound plates and do not\nexploit cross-plate correlation or statistical strength among plates. We\npresent a new statistical framework for high throughput screening of compounds\nbased on Bayesian nonparametric modeling. The proposed approach is able to\nidentify candidate hits from multiple plates simultaneously, sharing\nstatistical strength among plates and providing more robust estimates of\ncompound activity. It can flexibly accommodate arbitrary distributions of\ncompound activities and is applicable to any plate geometry. The algorithm\nprovides a principled statistical approach for hit identification and false\ndiscovery rate control. Experiments demonstrate significant improvements in hit\nidentification sensitivity and specificity over the B-score method, which is\nhighly sensitive to threshold choice. The framework is implemented as an\nefficient R extension package BHTSpack and is suitable for large scale data\nsets.\n",
        "published": "2017-09-28T16:17:25Z",
        "pdf_link": "http://arxiv.org/pdf/1709.10041v1"
    },
    {
        "id": "http://arxiv.org/abs/1711.00083v1",
        "title": "Synth-Validation: Selecting the Best Causal Inference Method for a Given\n  Dataset",
        "summary": "  Many decisions in healthcare, business, and other policy domains are made\nwithout the support of rigorous evidence due to the cost and complexity of\nperforming randomized experiments. Using observational data to answer causal\nquestions is risky: subjects who receive different treatments also differ in\nother ways that affect outcomes. Many causal inference methods have been\ndeveloped to mitigate these biases. However, there is no way to know which\nmethod might produce the best estimate of a treatment effect in a given study.\nIn analogy to cross-validation, which estimates the prediction error of\npredictive models applied to a given dataset, we propose synth-validation, a\nprocedure that estimates the estimation error of causal inference methods\napplied to a given dataset. In synth-validation, we use the observed data to\nestimate generative distributions with known treatment effects. We apply each\ncausal inference method to datasets sampled from these distributions and\ncompare the effect estimates with the known effects to estimate error. Using\nsimulations, we show that using synth-validation to select a causal inference\nmethod for each study lowers the expected estimation error relative to\nconsistently using any single method.\n",
        "published": "2017-10-31T20:05:27Z",
        "pdf_link": "http://arxiv.org/pdf/1711.00083v1"
    },
    {
        "id": "http://arxiv.org/abs/1711.00382v4",
        "title": "A Large Dimensional Study of Regularized Discriminant Analysis\n  Classifiers",
        "summary": "  This article carries out a large dimensional analysis of standard regularized\ndiscriminant analysis classifiers designed on the assumption that data arise\nfrom a Gaussian mixture model with different means and covariances. The\nanalysis relies on fundamental results from random matrix theory (RMT) when\nboth the number of features and the cardinality of the training data within\neach class grow large at the same pace. Under mild assumptions, we show that\nthe asymptotic classification error approaches a deterministic quantity that\ndepends only on the means and covariances associated with each class as well as\nthe problem dimensions. Such a result permits a better understanding of the\nperformance of regularized discriminant analsysis, in practical large but\nfinite dimensions, and can be used to determine and pre-estimate the optimal\nregularization parameter that minimizes the misclassification error\nprobability. Despite being theoretically valid only for Gaussian data, our\nfindings are shown to yield a high accuracy in predicting the performances\nachieved with real data sets drawn from the popular USPS data base, thereby\nmaking an interesting connection between theory and practice.\n",
        "published": "2017-11-01T14:52:06Z",
        "pdf_link": "http://arxiv.org/pdf/1711.00382v4"
    },
    {
        "id": "http://arxiv.org/abs/1711.00673v5",
        "title": "Fast Information-theoretic Bayesian Optimisation",
        "summary": "  Information-theoretic Bayesian optimisation techniques have demonstrated\nstate-of-the-art performance in tackling important global optimisation\nproblems. However, current information-theoretic approaches require many\napproximations in implementation, introduce often-prohibitive computational\noverhead and limit the choice of kernels available to model the objective. We\ndevelop a fast information-theoretic Bayesian Optimisation method, FITBO, that\navoids the need for sampling the global minimiser, thus significantly reducing\ncomputational overhead. Moreover, in comparison with existing approaches, our\nmethod faces fewer constraints on kernel choice and enjoys the merits of\ndealing with the output space. We demonstrate empirically that FITBO inherits\nthe performance associated with information-theoretic Bayesian optimisation,\nwhile being even faster than simpler Bayesian optimisation approaches, such as\nExpected Improvement.\n",
        "published": "2017-11-02T10:09:09Z",
        "pdf_link": "http://arxiv.org/pdf/1711.00673v5"
    },
    {
        "id": "http://arxiv.org/abs/1711.00799v2",
        "title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum\n  Approximation",
        "summary": "  Modeling sequential data has become more and more important in practice. Some\napplications are autonomous driving, virtual sensors and weather forecasting.\nTo model such systems so called recurrent models are used. In this article we\nintroduce two new Deep Recurrent Gaussian Process (DRGP) models based on the\nSparse Spectrum Gaussian Process (SSGP) and the improved variational version\ncalled Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the\nrecurrent structure given by an existing DRGP based on a specific sparse\nNystr\\\"om approximation. Therefore, we also variationally integrate out the\ninput-space and hence can propagate uncertainty through the layers. We can show\nthat for the resulting lower bound an optimal variational distribution exists.\nTraining is realized through optimizing the variational lower bound. Using\nDistributed Variational Inference (DVI), we can reduce the computational\ncomplexity. We improve over current state of the art methods in prediction\naccuracy for experimental data-sets used for their evaluation and introduce a\nnew data-set for engine control, named Emission. Furthermore, our method can\neasily be adapted for unsupervised learning, e.g. the latent variable model and\nits deep version.\n",
        "published": "2017-11-02T16:18:43Z",
        "pdf_link": "http://arxiv.org/pdf/1711.00799v2"
    },
    {
        "id": "http://arxiv.org/abs/1711.00882v2",
        "title": "Correcting Nuisance Variation using Wasserstein Distance",
        "summary": "  Profiling cellular phenotypes from microscopic imaging can provide meaningful\nbiological information resulting from various factors affecting the cells. One\nmotivating application is drug development: morphological cell features can be\ncaptured from images, from which similarities between different drug compounds\napplied at different doses can be quantified. The general approach is to find a\nfunction mapping the images to an embedding space of manageable dimensionality\nwhose geometry captures relevant features of the input images. An important\nknown issue for such methods is separating relevant biological signal from\nnuisance variation. For example, the embedding vectors tend to be more\ncorrelated for cells that were cultured and imaged during the same week than\nfor those from different weeks, despite having identical drug compounds applied\nin both cases. In this case, the particular batch in which a set of experiments\nwere conducted constitutes the domain of the data; an ideal set of image\nembeddings should contain only the relevant biological information (e.g. drug\neffects). We develop a general framework for adjusting the image embeddings in\norder to `forget' domain-specific information while preserving relevant\nbiological information. To achieve this, we minimize a loss function based on\ndistances between marginal distributions (such as the Wasserstein distance) of\nembeddings across domains for each replicated treatment. For the dataset we\npresent results with, the only replicated treatment happens to be the negative\ncontrol treatment, for which we do not expect any treatment-induced cell\nmorphology changes. We find that for our transformed embeddings (i) the\nunderlying geometric structure is not only preserved but the embeddings also\ncarry improved biological signal; and (ii) less domain-specific information is\npresent.\n",
        "published": "2017-11-02T18:42:40Z",
        "pdf_link": "http://arxiv.org/pdf/1711.00882v2"
    },
    {
        "id": "http://arxiv.org/abs/1711.01796v2",
        "title": "Independently Interpretable Lasso: A New Regularizer for Sparse\n  Regression with Uncorrelated Variables",
        "summary": "  Sparse regularization such as $\\ell_1$ regularization is a quite powerful and\nwidely used strategy for high dimensional learning problems. The effectiveness\nof sparse regularization has been supported practically and theoretically by\nseveral studies. However, one of the biggest issues in sparse regularization is\nthat its performance is quite sensitive to correlations between features.\nOrdinary $\\ell_1$ regularization can select variables correlated with each\nother, which results in deterioration of not only its generalization error but\nalso interpretability. In this paper, we propose a new regularization method,\n\"Independently Interpretable Lasso\" (IILasso). Our proposed regularizer\nsuppresses selecting correlated variables, and thus each active variable\nindependently affects the objective variable in the model. Hence, we can\ninterpret regression coefficients intuitively and also improve the performance\nby avoiding overfitting. We analyze theoretical property of IILasso and show\nthat the proposed method is much advantageous for its sign recovery and\nachieves almost minimax optimal convergence rate. Synthetic and real data\nanalyses also indicate the effectiveness of IILasso.\n",
        "published": "2017-11-06T09:22:21Z",
        "pdf_link": "http://arxiv.org/pdf/1711.01796v2"
    },
    {
        "id": "http://arxiv.org/abs/1711.01847v1",
        "title": "Extracting low-dimensional dynamics from multiple large-scale neural\n  population recordings by learning to predict correlations",
        "summary": "  A powerful approach for understanding neural population dynamics is to\nextract low-dimensional trajectories from population recordings using\ndimensionality reduction methods. Current approaches for dimensionality\nreduction on neural data are limited to single population recordings, and can\nnot identify dynamics embedded across multiple measurements. We propose an\napproach for extracting low-dimensional dynamics from multiple, sequential\nrecordings. Our algorithm scales to data comprising millions of observed\ndimensions, making it possible to access dynamics distributed across large\npopulations or multiple brain areas. Building on subspace-identification\napproaches for dynamical systems, we perform parameter estimation by minimizing\na moment-matching objective using a scalable stochastic gradient descent\nalgorithm: The model is optimized to predict temporal covariations across\nneurons and across time. We show how this approach naturally handles missing\ndata and multiple partial recordings, and can identify dynamics and predict\ncorrelations even in the presence of severe subsampling and small overlap\nbetween recordings. We demonstrate the effectiveness of the approach both on\nsimulated data and a whole-brain larval zebrafish imaging dataset.\n",
        "published": "2017-11-06T11:59:19Z",
        "pdf_link": "http://arxiv.org/pdf/1711.01847v1"
    },
    {
        "id": "http://arxiv.org/abs/1711.01861v1",
        "title": "Flexible statistical inference for mechanistic models of neural dynamics",
        "summary": "  Mechanistic models of single-neuron dynamics have been extensively studied in\ncomputational neuroscience. However, identifying which models can\nquantitatively reproduce empirically measured data has been challenging. We\npropose to overcome this limitation by using likelihood-free inference\napproaches (also known as Approximate Bayesian Computation, ABC) to perform\nfull Bayesian inference on single-neuron models. Our approach builds on recent\nadvances in ABC by learning a neural network which maps features of the\nobserved data to the posterior distribution over parameters. We learn a\nBayesian mixture-density network approximating the posterior over multiple\nrounds of adaptively chosen simulations. Furthermore, we propose an efficient\napproach for handling missing features and parameter settings for which the\nsimulator fails, as well as a strategy for automatically learning relevant\nfeatures using recurrent neural networks. On synthetic data, our approach\nefficiently estimates posterior distributions and recovers ground-truth\nparameters. On in-vitro recordings of membrane voltages, we recover\nmultivariate posteriors over biophysical parameters, which yield\nmodel-predicted voltage traces that accurately match empirical data. Our\napproach will enable neuroscientists to perform Bayesian inference on complex\nneuron models without having to design model-specific algorithms, closing the\ngap between mechanistic and statistical approaches to single-neuron modelling.\n",
        "published": "2017-11-06T12:36:07Z",
        "pdf_link": "http://arxiv.org/pdf/1711.01861v1"
    },
    {
        "id": "http://arxiv.org/abs/1711.02226v1",
        "title": "Unsupervised Transformation Learning via Convex Relaxations",
        "summary": "  Our goal is to extract meaningful transformations from raw images, such as\nvarying the thickness of lines in handwriting or the lighting in a portrait. We\npropose an unsupervised approach to learn such transformations by attempting to\nreconstruct an image from a linear combination of transformations of its\nnearest neighbors. On handwritten digits and celebrity portraits, we show that\neven with linear transformations, our method generates visually high-quality\nmodified images. Moreover, since our method is semiparametric and does not\nmodel the data distribution, the learned transformations extrapolate off the\ntraining data and can be applied to new types of images.\n",
        "published": "2017-11-06T23:56:41Z",
        "pdf_link": "http://arxiv.org/pdf/1711.02226v1"
    },
    {
        "id": "http://arxiv.org/abs/1711.02283v2",
        "title": "Large-Scale Optimal Transport and Mapping Estimation",
        "summary": "  This paper presents a novel two-step approach for the fundamental problem of\nlearning an optimal map from one distribution to another. First, we learn an\noptimal transport (OT) plan, which can be thought as a one-to-many map between\nthe two distributions. To that end, we propose a stochastic dual approach of\nregularized OT, and show empirically that it scales better than a recent\nrelated approach when the amount of samples is very large. Second, we estimate\na \\textit{Monge map} as a deep neural network learned by approximating the\nbarycentric projection of the previously-obtained OT plan. This\nparameterization allows generalization of the mapping outside the support of\nthe input measure. We prove two theoretical stability results of regularized OT\nwhich show that our estimations converge to the OT plan and Monge map between\nthe underlying continuous measures. We showcase our proposed approach on two\napplications: domain adaptation and generative modeling.\n",
        "published": "2017-11-07T04:53:07Z",
        "pdf_link": "http://arxiv.org/pdf/1711.02283v2"
    },
    {
        "id": "http://arxiv.org/abs/1711.02475v2",
        "title": "Bayesian model and dimension reduction for uncertainty propagation:\n  applications in random media",
        "summary": "  Well-established methods for the solution of stochastic partial differential\nequations (SPDEs) typically struggle in problems with high-dimensional\ninputs/outputs. Such difficulties are only amplified in large-scale\napplications where even a few tens of full-order model runs are impracticable.\nWhile dimensionality reduction can alleviate some of these issues, it is not\nknown which and how many features of the (high-dimensional) input are actually\npredictive of the (high-dimensional) output. In this paper, we advocate a\nBayesian formulation that is capable of performing simultaneous dimension and\nmodel-order reduction. It consists of a component that encodes the\nhigh-dimensional input into a low-dimensional set of feature functions by\nemploying sparsity-enforcing priors and a decoding component that makes use of\nthe solution of a coarse-grained model in order to reconstruct that of the\nfull-order model. Both components are represented with latent variables in a\nprobabilistic graphical model and are simultaneously trained using Stochastic\nVariational Inference methods. The model is capable of quantifying the\npredictive uncertainty due to the information loss that unavoidably takes place\nin any model-order/dimension reduction as well as the uncertainty arising from\nfinite-sized training datasets. We demonstrate its capabilities in the context\nof random media where fine-scale fluctuations can give rise to random inputs\nwith tens of thousands of variables. With a few tens of full-order model\nsimulations, the proposed model is capable of identifying salient physical\nfeatures and produce sharp predictions under different boundary conditions of\nthe full output which itself consists of thousands of components.\n",
        "published": "2017-11-07T14:19:24Z",
        "pdf_link": "http://arxiv.org/pdf/1711.02475v2"
    },
    {
        "id": "http://arxiv.org/abs/1711.02887v1",
        "title": "Universal consistency and minimax rates for online Mondrian Forests",
        "summary": "  We establish the consistency of an algorithm of Mondrian Forests, a\nrandomized classification algorithm that can be implemented online. First, we\namend the original Mondrian Forest algorithm, that considers a fixed lifetime\nparameter. Indeed, the fact that this parameter is fixed hinders the\nstatistical consistency of the original procedure. Our modified Mondrian Forest\nalgorithm grows trees with increasing lifetime parameters $\\lambda_n$, and uses\nan alternative updating rule, allowing to work also in an online fashion.\nSecond, we provide a theoretical analysis establishing simple conditions for\nconsistency. Our theoretical analysis also exhibits a surprising fact: our\nalgorithm achieves the minimax rate (optimal rate) for the estimation of a\nLipschitz regression function, which is a strong extension of previous results\nto an arbitrary dimension.\n",
        "published": "2017-11-08T09:47:42Z",
        "pdf_link": "http://arxiv.org/pdf/1711.02887v1"
    }
]