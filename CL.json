[
    {
        "id": "http://arxiv.org/abs/cs/9809020v1",
        "title": "Linear Segmentation and Segment Significance",
        "summary": "  We present a new method for discovering a segmental discourse structure of a\ndocument while categorizing segment function. We demonstrate how retrieval of\nnoun phrases and pronominal forms, along with a zero-sum weighting scheme,\ndetermines topicalized segmentation. Futhermore, we use term distribution to\naid in identifying the role that the segment performs in the document. Finally,\nwe present results of evaluation in terms of precision and recall which surpass\nearlier approaches.\n",
        "published": "1998-09-15T23:49:32Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809022v1",
        "title": "Modelling Users, Intentions, and Structure in Spoken Dialog",
        "summary": "  We outline how utterances in dialogs can be interpreted using a partial first\norder logic. We exploit the capability of this logic to talk about the truth\nstatus of formulae to define a notion of coherence between utterances and\nexplain how this coherence relation can serve for the construction of AND/OR\ntrees that represent the segmentation of the dialog. In a BDI model we\nformalize basic assumptions about dialog and cooperative behaviour of\nparticipants. These assumptions provide a basis for inferring speech acts from\ncoherence relations between utterances and attitudes of dialog participants.\nSpeech acts prove to be useful for determining dialog segments defined on the\nnotion of completing expectations of dialog participants. Finally, we sketch\nhow explicit segmentation signalled by cue phrases and performatives is covered\nby our dialog model.\n",
        "published": "1998-09-17T11:10:14Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809024v2",
        "title": "A Lexicalized Tree Adjoining Grammar for English",
        "summary": "  This document describes a sizable grammar of English written in the TAG\nformalism and implemented for use with the XTAG system. This report and the\ngrammar described herein supersedes the TAG grammar described in an earlier\n1995 XTAG technical report. The English grammar described in this report is\nbased on the TAG formalism which has been extended to include lexicalization,\nand unification-based feature structures. The range of syntactic phenomena that\ncan be handled is large and includes auxiliaries (including inversion), copula,\nraising and small clause constructions, topicalization, relative clauses,\ninfinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO\nconstructions, noun-noun modifications, extraposition, determiner sequences,\ngenitives, negation, noun-verb contractions, sentential adjuncts and\nimperatives. This technical report corresponds to the XTAG Release 8/31/98. The\nXTAG grammar is continuously updated with the addition of new analyses and\nmodification of old ones, and an online version of this report can be found at\nthe XTAG web page at http://www.cis.upenn.edu/~xtag/\n",
        "published": "1998-09-18T00:33:47Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809024v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809026v1",
        "title": "Prefix Probabilities from Stochastic Tree Adjoining Grammars",
        "summary": "  Language models for speech recognition typically use a probability model of\nthe form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other\nhand, are typically used to assign structure to utterances. A language model of\nthe above form is constructed from such grammars by computing the prefix\nprobability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all\npossible terminations of the prefix a_1 ... a_n. The main result in this paper\nis an algorithm to compute such prefix probabilities given a stochastic Tree\nAdjoining Grammar (TAG). The algorithm achieves the required computation in\nO(n^6) time. The probability of subderivations that do not derive any words in\nthe prefix, but contribute structurally to its derivation, are precomputed to\nachieve termination. This algorithm enables existing corpus-based estimation\ntechniques for stochastic TAGs to be used for language modelling.\n",
        "published": "1998-09-18T03:45:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809027v1",
        "title": "Conditions on Consistency of Probabilistic Tree Adjoining Grammars",
        "summary": "  Much of the power of probabilistic methods in modelling language comes from\ntheir ability to compare several derivations for the same string in the\nlanguage. An important starting point for the study of such cross-derivational\nproperties is the notion of _consistency_. The probability model defined by a\nprobabilistic grammar is said to be _consistent_ if the probabilities assigned\nto all the strings in the language sum to one. From the literature on\nprobabilistic context-free grammars (CFGs), we know precisely the conditions\nwhich ensure that consistency is true for a given CFG. This paper derives the\nconditions under which a given probabilistic Tree Adjoining Grammar (TAG) can\nbe shown to be consistent. It gives a simple algorithm for checking consistency\nand gives the formal justification for its correctness. The conditions derived\nhere can be used to ensure that probability models that use TAGs can be checked\nfor _deficiency_ (i.e. whether any probability mass is assigned to strings that\ncannot be generated).\n",
        "published": "1998-09-18T03:58:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809028v1",
        "title": "Separating Dependency from Constituency in a Tree Rewriting System",
        "summary": "  In this paper we present a new tree-rewriting formalism called Link-Sharing\nTree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using\nLSTAG we define an approach towards coordination where linguistic dependency is\ndistinguished from the notion of constituency. Such an approach towards\ncoordination that explicitly distinguishes dependencies from constituency gives\na better formal understanding of its representation when compared to previous\napproaches that use tree-rewriting systems which conflate the two issues.\n",
        "published": "1998-09-18T04:44:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809029v1",
        "title": "Incremental Parser Generation for Tree Adjoining Grammars",
        "summary": "  This paper describes the incremental generation of parse tables for the\nLR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented\nhandles modifications to the input grammar by updating the parser generated so\nfar. In this paper, a lazy generation of LR-type parsers for TALs is defined in\nwhich parse tables are created by need while parsing. We then describe an\nincremental parser generator for TALs which responds to modification of the\ninput grammar by updating parse tables built so far.\n",
        "published": "1998-09-18T05:03:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809050v1",
        "title": "A Freely Available Morphological Analyzer, Disambiguator and Context\n  Sensitive Lemmatizer for German",
        "summary": "  In this paper we present Morphy, an integrated tool for German morphology,\npart-of-speech tagging and context-sensitive lemmatization. Its large lexicon\nof more than 320,000 word forms plus its ability to process German compound\nnouns guarantee a wide morphological coverage. Syntactic ambiguities can be\nresolved with a standard statistical part-of-speech tagger. By using the output\nof the tagger, the lemmatizer can determine the correct root even for ambiguous\nword forms. The complete package is freely available and can be downloaded from\nthe World Wide Web.\n",
        "published": "1998-09-23T12:59:39Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809050v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809106v1",
        "title": "Processing Unknown Words in HPSG",
        "summary": "  The lexical acquisition system presented in this paper incrementally updates\nlinguistic properties of unknown words inferred from their surrounding context\nby parsing sentences with an HPSG grammar for German. We employ a gradual,\ninformation-based concept of ``unknownness'' providing a uniform treatment for\nthe range of completely known to maximally unknown lexical entries. ``Unknown''\ninformation is viewed as revisable information, which is either generalizable\nor specializable. Updating takes place after parsing, which only requires a\nmodified lexical lookup. Revisable pieces of information are identified by\ngrammar-specified declarations which provide access paths into the parse\nfeature structure. The updating mechanism revises the corresponding places in\nthe lexical feature structures iff the context actually provides new\ninformation. For revising generalizable information, type union is required. A\nworked-out example demonstrates the inferential capacity of our implemented\nsystem.\n",
        "published": "1998-09-25T11:02:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809106v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809107v1",
        "title": "Computing Declarative Prosodic Morphology",
        "summary": "  This paper describes a computational, declarative approach to prosodic\nmorphology that uses inviolable constraints to denote small finite candidate\nsets which are filtered by a restrictive incremental optimization mechanism.\nThe new approach is illustrated with an implemented fragment of Modern Hebrew\nverbs couched in MicroCUF, an expressive constraint logic formalism. For\ngeneration and parsing of word forms, I propose a novel off-line technique to\neliminate run-time optimization. It produces a finite-state oracle that\nefficiently restricts the constraint interpreter's search space. As a\nbyproduct, unknown words can be analyzed without special mechanisms. Unlike\npure finite-state transducer approaches, this hybrid setup allows for more\nexpressivity in constraints to specify e.g. token identity for reduplication or\narithmetic constraints for phonetics.\n",
        "published": "1998-09-25T14:32:38Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809107v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809112v1",
        "title": "On the Evaluation and Comparison of Taggers: The Effect of Noise in\n  Testing Corpora",
        "summary": "  This paper addresses the issue of {\\sc pos} tagger evaluation. Such\nevaluation is usually performed by comparing the tagger output with a reference\ntest corpus, which is assumed to be error-free. Currently used corpora contain\nnoise which causes the obtained performance to be a distortion of the real\nvalue. We analyze to what extent this distortion may invalidate the comparison\nbetween taggers or the measure of the improvement given by a new system. The\nmain conclusion is that a more rigorous testing experimentation\nsetting/designing is needed to reliably evaluate and compare tagger accuracies.\n",
        "published": "1998-09-28T07:49:11Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809112v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9809113v1",
        "title": "Improving Tagging Performance by Using Voting Taggers",
        "summary": "  We present a bootstrapping method to develop an annotated corpus, which is\nspecially useful for languages with few available resources. The method is\nbeing applied to develop a corpus of Spanish of over 5Mw. The method consists\non taking advantage of the collaboration of two different POS taggers. The\ncases in which both taggers agree present a higher accuracy and are used to\nretrain the taggers.\n",
        "published": "1998-09-28T07:50:55Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9809113v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9810014v1",
        "title": "Resources for Evaluation of Summarization Techniques",
        "summary": "  We report on two corpora to be used in the evaluation of component systems\nfor the tasks of (1) linear segmentation of text and (2) summary-directed\nsentence extraction. We present characteristics of the corpora, methods used in\nthe collection of user judgments, and an overview of the application of the\ncorpora to evaluating the component system. Finally, we discuss the problems\nand issues with construction of the test set which apply broadly to the\nconstruction of evaluation resources for language technologies.\n",
        "published": "1998-10-13T20:33:05Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9810014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9810015v1",
        "title": "Restrictions on Tree Adjoining Languages",
        "summary": "  Several methods are known for parsing languages generated by Tree Adjoining\nGrammars (TAGs) in O(n^6) worst case running time. In this paper we investigate\nwhich restrictions on TAGs and TAG derivations are needed in order to lower\nthis O(n^6) time complexity, without introducing large runtime constants, and\nwithout losing any of the generative power needed to capture the syntactic\nconstructions in natural language that can be handled by unrestricted TAGs. In\nparticular, we describe an algorithm for parsing a strict subclass of TAG in\nO(n^5), and attempt to show that this subclass retains enough generative power\nto make it useful in the general case.\n",
        "published": "1998-10-13T21:17:13Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9810015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9811008v1",
        "title": "Translating near-synonyms: Possibilities and preferences in the\n  interlingua",
        "summary": "  This paper argues that an interlingual representation must explicitly\nrepresent some parts of the meaning of a situation as possibilities (or\npreferences), not as necessary or definite components of meaning (or\nconstraints). Possibilities enable the analysis and generation of nuance,\nsomething required for faithful translation. Furthermore, the representation of\nthe meaning of words, especially of near-synonyms, is crucial, because it\nspecifies which nuances words can convey in which contexts.\n",
        "published": "1998-11-02T21:29:41Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9811008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9811009v1",
        "title": "Choosing the Word Most Typical in Context Using a Lexical Co-occurrence\n  Network",
        "summary": "  This paper presents a partial solution to a component of the problem of\nlexical choice: choosing the synonym most typical, or expected, in context. We\napply a new statistical approach to representing the context of a word through\nlexical co-occurrence networks. The implementation was trained and evaluated on\na large corpus, and results show that the inclusion of second-order\nco-occurrence relations improves the performance of our implemented lexical\nchoice program.\n",
        "published": "1998-11-02T23:06:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9811009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9811016v1",
        "title": "Comparing a statistical and a rule-based tagger for German",
        "summary": "  In this paper we present the results of comparing a statistical tagger for\nGerman based on decision trees and a rule-based Brill-Tagger for German. We\nused the same training corpus (and therefore the same tag-set) to train both\ntaggers. We then applied the taggers to the same test corpus and compared their\nrespective behavior and in particular their error rates. Both taggers perform\nsimilarly with an error rate of around 5%. From the detailed error analysis it\ncan be seen that the rule-based tagger has more problems with unknown words\nthan the statistical tagger. But the results are opposite for tokens that are\nmany-ways ambiguous. If the unknown words are fed into the taggers with the\nhelp of an external lexicon (such as the Gertwol system) the error rate of the\nrule-based tagger drops to 4.7%, and the respective rate of the statistical\ntaggers drops to around 3.7%. Combining the taggers by using the output of one\ntagger to help the other did not lead to any further improvement.\n",
        "published": "1998-11-11T11:06:34Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9811016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9811022v2",
        "title": "Expoiting Syntactic Structure for Language Modeling",
        "summary": "  The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words--binary-parse-structure with headword annotation and\noperates in a left-to-right manner --- therefore usable for automatic speech\nrecognition. The model, its probabilistic parameterization, and a set of\nexperiments meant to evaluate its predictive power are presented; an\nimprovement over standard trigram modeling is achieved.\n",
        "published": "1998-11-12T17:31:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9811022v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/9811025v2",
        "title": "A Structured Language Model",
        "summary": "  The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words - binary-parse-structure with headword annotation. The\nmodel, its probabilistic parametrization, and a set of experiments meant to\nevaluate its predictive power are presented.\n",
        "published": "1998-11-13T16:53:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9811025v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/9812001v3",
        "title": "A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S\n  tructural Disambiguation",
        "summary": "  In this thesis, I address the problem of automatically acquiring lexical\nsemantic knowledge, especially that of case frame patterns, from large corpus\ndata and using the acquired knowledge in structural disambiguation. The\napproach I adopt has the following characteristics: (1) dividing the problem\ninto three subproblems: case slot generalization, case dependency learning, and\nword clustering (thesaurus construction). (2) viewing each subproblem as that\nof statistical estimation and defining probability models for each subproblem,\n(3) adopting the Minimum Description Length (MDL) principle as learning\nstrategy, (4) employing efficient learning algorithms, and (5) viewing the\ndisambiguation problem as that of statistical prediction. Major contributions\nof this thesis include: (1) formalization of the lexical knowledge acquisition\nproblem, (2) development of a number of learning methods for lexical knowledge\nacquisition, and (3) development of a high-performance disambiguation method.\n",
        "published": "1998-12-01T11:43:32Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9812001v3"
    },
    {
        "id": "http://arxiv.org/abs/cs/9812005v1",
        "title": "Optimal Multi-Paragraph Text Segmentation by Dynamic Programming",
        "summary": "  There exist several methods of calculating a similarity curve, or a sequence\nof similarity values, representing the lexical cohesion of successive text\nconstituents, e.g., paragraphs. Methods for deciding the locations of fragment\nboundaries are, however, scarce. We propose a fragmentation method based on\ndynamic programming. The method is theoretically sound and guaranteed to\nprovide an optimal splitting on the basis of a similarity curve, a preferred\nfragment length, and a cost function defined. The method is especially useful\nwhen control on fragment size is of importance.\n",
        "published": "1998-12-04T16:16:35Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9812005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9812018v1",
        "title": "A Flexible Shallow Approach to Text Generation",
        "summary": "  In order to support the efficient development of NL generation systems, two\northogonal methods are currently pursued with emphasis: (1) reusable, general,\nand linguistically motivated surface realization components, and (2) simple,\ntask-oriented template-based techniques. In this paper we argue that, from an\napplication-oriented perspective, the benefits of both are still limited. In\norder to improve this situation, we suggest and evaluate shallow generation\nmethods associated with increased flexibility. We advise a close connection\nbetween domain-motivated and linguistic ontologies that supports the quick\nadaptation to new tasks and domains, rather than the reuse of general\nresources. Our method is especially designed for generating reports with\nlimited linguistic variations.\n",
        "published": "1998-12-16T16:37:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9812018v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9901005v1",
        "title": "An Empirical Approach to Temporal Reference Resolution (journal version)",
        "summary": "  Scheduling dialogs, during which people negotiate the times of appointments,\nare common in everyday life. This paper reports the results of an in-depth\nempirical investigation of resolving explicit temporal references in scheduling\ndialogs. There are four phases of this work: data annotation and evaluation,\nmodel development, system implementation and evaluation, and model evaluation\nand analysis. The system and model were developed primarily on one set of data,\nand then applied later to a much more complex data set, to assess the\ngeneralizability of the model for the task being performed. Many different\ntypes of empirical methods are applied to pinpoint the strengths and weaknesses\nof the approach. Detailed annotation instructions were developed and an\nintercoder reliability study was performed, showing that naive annotators can\nreliably perform the targeted annotations. A fully automatic system has been\ndeveloped and evaluated on unseen test data, with good results on both data\nsets. We adopt a pure realization of a recency-based focus model to identify\nprecisely when it is and is not adequate for the task being addressed. In\naddition to system results, an in-depth evaluation of the model itself is\npresented, based on detailed manual annotations. The results are that few\nerrors occur specifically due to the model of focus being used, and the set of\nanaphoric relations defined in the model are low in ambiguity for both data\nsets.\n",
        "published": "1999-01-13T17:37:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9901005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9902001v1",
        "title": "Compacting the Penn Treebank Grammar",
        "summary": "  Treebanks, such as the Penn Treebank (PTB), offer a simple approach to\nobtaining a broad coverage grammar: one can simply read the grammar off the\nparse trees in the treebank. While such a grammar is easy to obtain, a\nsquare-root rate of growth of the rule set with corpus size suggests that the\nderived grammar is far from complete and that much more treebanked text would\nbe required to obtain a complete grammar, if one exists at some limit. However,\nwe offer an alternative explanation in terms of the underspecification of\nstructures within the treebank. This hypothesis is explored by applying an\nalgorithm to compact the derived grammar by eliminating redundant rules --\nrules whose right hand sides can be parsed by other rules. The size of the\nresulting compacted grammar, which is significantly less than that of the full\ntreebank grammar, is shown to approach a limit. However, such a compacted\ngrammar does not yield very good performance figures. A version of the\ncompaction algorithm taking rule probabilities into account is proposed, which\nis argued to be more linguistically motivated. Combined with simple\nthresholding, this method can be used to give a 58% reduction in grammar size\nwithout significant change in parsing performance, and can produce a 69%\nreduction with some gain in recall, but a loss in precision.\n",
        "published": "1999-01-31T18:57:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9902001v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9902029v1",
        "title": "The \"Fodor\"-FODOR fallacy bites back",
        "summary": "  The paper argues that Fodor and Lepore are misguided in their attack on\nPustejovsky's Generative Lexicon, largely because their argument rests on a\ntraditional, but implausible and discredited, view of the lexicon on which it\nis effectively empty of content, a view that stands in the long line of\nexplaining word meaning (a) by ostension and then (b) explaining it by means of\na vacuous symbol in a lexicon, often the word itself after typographic\ntransmogrification. (a) and (b) both share the wrong belief that to a word must\ncorrespond a simple entity that is its meaning. I then turn to the semantic\nrules that Pustejovsky uses and argue first that, although they have novel\nfeatures, they are in a well-established Artificial Intelligence tradition of\nexplaining meaning by reference to structures that mention other structures\nassigned to words that may occur in close proximity to the first. It is argued\nthat Fodor and Lepore's view that there cannot be such rules is without\nfoundation, and indeed systems using such rules have proved their practical\nworth in computational systems. Their justification descends from line of\nargument, whose high points were probably Wittgenstein and Quine that meaning\nis not to be understood by simple links to the world, ostensive or otherwise,\nbut by the relationship of whole cultural representational structures to each\nother and to the world as a whole.\n",
        "published": "1999-02-25T14:41:24Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9902029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9902030v1",
        "title": "Is Word Sense Disambiguation just one more NLP task?",
        "summary": "  This paper compares the tasks of part-of-speech (POS) tagging and\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\nrelated by fineness of grain or anything like that, but are quite different\nkinds of task, particularly becuase there is nothing in POS corresponding to\nsense novelty. The paper also argues for the reintegration of sub-tasks that\nare being separated for evaluation\n",
        "published": "1999-02-25T14:41:32Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9902030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9903003v1",
        "title": "A Formal Framework for Linguistic Annotation",
        "summary": "  `Linguistic annotation' covers any descriptive or analytic notations applied\nto raw language data. The basic data may be in the form of time functions --\naudio, video and/or physiological recordings -- or it may be textual. The added\nnotations may include transcriptions of all sorts (from phonetic features to\ndiscourse structures), part-of-speech and sense tagging, syntactic analysis,\n`named entity' identification, co-reference annotation, and so on. While there\nare several ongoing efforts to provide formats and tools for such annotations\nand to publish annotated linguistic databases, the lack of widely accepted\nstandards is becoming a critical problem. Proposed standards, to the extent\nthey exist, have focussed on file formats. This paper focuses instead on the\nlogical structure of linguistic annotations. We survey a wide variety of\nexisting annotation formats and demonstrate a common conceptual core, the\nannotation graph. This provides a formal framework for constructing,\nmaintaining and searching linguistic annotations, while remaining consistent\nwith many alternative data structures and file formats.\n",
        "published": "1999-03-02T12:30:55Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9903003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9903008v1",
        "title": "Empirically Evaluating an Adaptable Spoken Dialogue System",
        "summary": "  Recent technological advances have made it possible to build real-time,\ninteractive spoken dialogue systems for a wide variety of applications.\nHowever, when users do not respect the limitations of such systems, performance\ntypically degrades. Although users differ with respect to their knowledge of\nsystem limitations, and although different dialogue strategies make system\nlimitations more apparent to users, most current systems do not try to improve\nperformance by adapting dialogue behavior to individual users. This paper\npresents an empirical evaluation of TOOT, an adaptable spoken dialogue system\nfor retrieving train schedules on the web. We conduct an experiment in which 20\nusers carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,\nresulting in a corpus of 80 dialogues. The values for a wide range of\nevaluation measures are then extracted from this corpus. Our results show that\nadaptable TOOT generally outperforms non-adaptable TOOT, and that the utility\nof adaptation depends on TOOT's initial dialogue strategies.\n",
        "published": "1999-03-05T22:03:13Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9903008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9904008v1",
        "title": "Transducers from Rewrite Rules with Backreferences",
        "summary": "  Context sensitive rewrite rules have been widely used in several areas of\nnatural language processing, including syntax, morphology, phonology and speech\nprocessing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various\nalgorithms to compile such rewrite rules into finite-state transducers. The\npresent paper extends this work by allowing a limited form of backreferencing\nin such rules. The explicit use of backreferencing leads to more elegant and\ngeneral solutions.\n",
        "published": "1999-04-15T14:00:41Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9904008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9904009v1",
        "title": "An ascription-based approach to speech acts",
        "summary": "  The two principal areas of natural language processing research in pragmatics\nare belief modelling and speech act processing. Belief modelling is the\ndevelopment of techniques to represent the mental attitudes of a dialogue\nparticipant. The latter approach, speech act processing, based on speech act\ntheory, involves viewing dialogue in planning terms. Utterances in a dialogue\nare modelled as steps in a plan where understanding an utterance involves\nderiving the complete plan a speaker is attempting to achieve. However,\nprevious speech act based approaches have been limited by a reliance upon\nrelatively simplistic belief modelling techniques and their relationship to\nplanning and plan recognition. In particular, such techniques assume\nprecomputed nested belief structures. In this paper, we will present an\napproach to speech act processing based on novel belief modelling techniques\nwhere nested beliefs are propagated on demand.\n",
        "published": "1999-04-15T16:03:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9904009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9904018v1",
        "title": "A Computational Memory and Processing Model for Processing for Prosody",
        "summary": "  This paper links prosody to the information in a text and how it is processed\nby the speaker. It describes the operation and output of LOQ, a text-to-speech\nimplementation that includes a model of limited attention and working memory.\nAttentional limitations are key. Varying the attentional parameter in the\nsimulations varies in turn what counts as given and new in a text, and\ntherefore, the intonational contours with which it is uttered. Currently, the\nsystem produces prosody in three different styles: child-like, adult\nexpressive, and knowledgeable. This prosody also exhibits differences within\neach style -- no two simulations are alike. The limited resource approach\ncaptures some of the stylistic and individual variety found in natural prosody.\n",
        "published": "1999-04-24T23:45:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9904018v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9905001v1",
        "title": "Supervised Grammar Induction Using Training Data with Limited\n  Constituent Information",
        "summary": "  Corpus-based grammar induction generally relies on hand-parsed training data\nto learn the structure of the language. Unfortunately, the cost of building\nlarge annotated corpora is prohibitively expensive. This work aims to improve\nthe induction strategy when there are few labels in the training data. We show\nthat the most informative linguistic constituents are the higher nodes in the\nparse trees, typically denoting complex noun phrases and sentential clauses.\nThey account for only 20% of all constituents. For inducing grammars from\nsparsely labeled training data (e.g., only higher-level constituent labels), we\npropose an adaptation strategy, which produces grammars that parse almost as\nwell as grammars induced from fully labeled corpora. Our results suggest that\nfor a partial parser to replace human annotators, it must be able to\nautomatically extract higher-level constituents rather than base noun phrases.\n",
        "published": "1999-05-02T20:48:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9905001v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906003v1",
        "title": "The syntactic processing of particles in Japanese spoken language",
        "summary": "  Particles fullfill several distinct central roles in the Japanese language.\nThey can mark arguments as well as adjuncts, can be functional or have semantic\nfuntions. There is, however, no straightforward matching from particles to\nfunctions, as, e.g., GA can mark the subject, the object or an adjunct of a\nsentence. Particles can cooccur. Verbal arguments that could be identified by\nparticles can be eliminated in the Japanese sentence. And finally, in spoken\nlanguage particles are often omitted. A proper treatment of particles is thus\nnecessary to make an analysis of Japanese sentences possible. Our treatment is\nbased on an empirical investigation of 800 dialogues. We set up a type\nhierarchy of particles motivated by their subcategorizational and\nmodificational behaviour. This type hierarchy is part of the Japanese syntax in\nVERBMOBIL.\n",
        "published": "1999-06-02T12:03:14Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906009v1",
        "title": "Cascaded Markov Models",
        "summary": "  This paper presents a new approach to partial parsing of context-free\nstructures. The approach is based on Markov Models. Each layer of the resulting\nstructure is represented by its own Markov Model, and output of a lower layer\nis passed as input to the next higher layer. An empirical evaluation of the\nmethod yields very good results for NP/PP chunking of German newspaper texts.\n",
        "published": "1999-06-06T17:36:34Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906014v1",
        "title": "Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System",
        "summary": "  The NWO Priority Programme Language and Speech Technology is a 5-year\nresearch programme aiming at the development of spoken language information\nsystems. In the Programme, two alternative natural language processing (NLP)\nmodules are developed in parallel: a grammar-based (conventional, rule-based)\nmodule and a data-oriented (memory-based, stochastic, DOP) module. In order to\ncompare the NLP modules, a formal evaluation has been carried out three years\nafter the start of the Programme. This paper describes the evaluation procedure\nand the evaluation results. The grammar-based component performs much better\nthan the data-oriented one in this comparison.\n",
        "published": "1999-06-14T10:06:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906015v1",
        "title": "Learning Transformation Rules to Find Grammatical Relations",
        "summary": "  Grammatical relationships are an important level of natural language\nprocessing. We present a trainable approach to find these relationships through\ntransformation sequences and error-driven learning. Our approach finds\ngrammatical relationships between core syntax groups and bypasses much of the\nparsing phase. On our training and test set, our procedure achieves 63.6%\nrecall and 77.3% precision (f-score = 69.8).\n",
        "published": "1999-06-14T22:06:24Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906020v1",
        "title": "Temporal Meaning Representations in a Natural Language Front-End",
        "summary": "  Previous work in the context of natural language querying of temporal\ndatabases has established a method to map automatically from a large subset of\nEnglish time-related questions to suitable expressions of a temporal logic-like\nlanguage, called TOP. An algorithm to translate from TOP to the TSQL2 temporal\ndatabase language has also been defined. This paper shows how TOP expressions\ncould be translated into a simpler logic-like language, called BOT. BOT is very\nclose to traditional first-order predicate logic (FOPL), and hence existing\nmethods to manipulate FOPL expressions can be exploited to interface to\ntime-sensitive applications other than TSQL2 databases, maintaining the\nexisting English-to-TOP mapping.\n",
        "published": "1999-06-22T08:28:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906025v1",
        "title": "Mapping Multilingual Hierarchies Using Relaxation Labeling",
        "summary": "  This paper explores the automatic construction of a multilingual Lexical\nKnowledge Base from pre-existing lexical resources. We present a new and robust\napproach for linking already existing lexical/semantic hierarchies. We used a\nconstraint satisfaction algorithm (relaxation labeling) to select --among all\nthe candidate translations proposed by a bilingual dictionary-- the right\nEnglish WordNet synset for each sense in a taxonomy automatically derived from\na Spanish monolingual dictionary. Although on average, there are 15 possible\nWordNet connections for each sense in the taxonomy, the method achieves an\naccuracy over 80%. Finally, we also propose several ways in which this\ntechnique could be applied to enrich and improve existing lexical databases.\n",
        "published": "1999-06-24T16:56:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906026v1",
        "title": "Robust Grammatical Analysis for Spoken Dialogue Systems",
        "summary": "  We argue that grammatical analysis is a viable alternative to concept\nspotting for processing spoken input in a practical spoken dialogue system. We\ndiscuss the structure of the grammar, and a model for robust parsing which\ncombines linguistic sources of information and statistical sources of\ninformation. We discuss test results suggesting that grammatical processing\nallows fast and accurate processing of spoken input.\n",
        "published": "1999-06-25T08:16:23Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9906034v1",
        "title": "A Unified Example-Based and Lexicalist Approach to Machine Translation",
        "summary": "  We present an approach to Machine Translation that combines the ideas and\nmethodologies of the Example-Based and Lexicalist theoretical frameworks. The\napproach has been implemented in a multilingual Machine Translation system.\n",
        "published": "1999-06-30T23:06:09Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9906034v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907003v1",
        "title": "Annotation graphs as a framework for multidimensional linguistic data\n  analysis",
        "summary": "  In recent work we have presented a formal framework for linguistic annotation\nbased on labeled acyclic digraphs. These `annotation graphs' offer a simple yet\npowerful method for representing complex annotation structures incorporating\nhierarchy and overlap. Here, we motivate and illustrate our approach using\ndiscourse-level annotations of text and speech data drawn from the CALLHOME,\nCOCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain\nspecialists, we have constructed a hybrid multi-level annotation for a fragment\nof the Boston University Radio Speech Corpus which includes the following\nlevels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named\nentity. We show how annotation graphs can represent hybrid multi-level\nstructures which derive from a diverse set of file formats. We also show how\nthe approach facilitates substantive comparison of multiple annotations of a\nsingle signal based on different theoretical models. The discussion shows how\nannotation graphs open the door to wide-ranging integration of tools, formats\nand corpora.\n",
        "published": "1999-07-05T14:51:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907006v1",
        "title": "Representing Text Chunks",
        "summary": "  Dividing sentences in chunks of words is a useful preprocessing step for\nparsing, information extraction and information retrieval. (Ramshaw and Marcus,\n1995) have introduced a \"convenient\" data representation for chunking by\nconverting it to a tagging task. In this paper we will examine seven different\ndata representations for the problem of recognizing noun phrase chunks. We will\nshow that the the data representation choice has a minor influence on chunking\nperformance. However, equipped with the most suitable data representation, our\nmemory-based learning chunker was able to improve the best published chunking\nresults for a standard data set.\n",
        "published": "1999-07-06T12:44:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907007v2",
        "title": "Cross-Language Information Retrieval for Technical Documents",
        "summary": "  This paper proposes a Japanese/English cross-language information retrieval\n(CLIR) system targeting technical documents. Our system first translates a\ngiven query containing technical terms into the target language, and then\nretrieves documents relevant to the translated query. The translation of\ntechnical terms is still problematic in that technical terms are often compound\nwords, and thus new terms can be progressively created simply by combining\nexisting base words. In addition, Japanese often represents loanwords based on\nits phonogram. Consequently, existing dictionaries find it difficult to achieve\nsufficient coverage. To counter the first problem, we use a compound word\ntranslation method, which uses a bilingual dictionary for base words and\ncollocational statistics to resolve translation ambiguity. For the second\nproblem, we propose a transliteration method, which identifies phonetic\nequivalents in the target language. We also show the effectiveness of our\nsystem using a test collection for CLIR.\n",
        "published": "1999-07-06T16:25:46Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907007v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907008v1",
        "title": "Explanation-based Learning for Machine Translation",
        "summary": "  In this paper we present an application of explanation-based learning (EBL)\nin the parsing module of a real-time English-Spanish machine translation system\ndesigned to translate closed captions. We discuss the efficiency/coverage\ntrade-offs available in EBL and introduce the techniques we use to increase\ncoverage while maintaining a high level of space and time efficiency. Our\nperformance results indicate that this approach is effective.\n",
        "published": "1999-07-06T18:35:41Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907010v1",
        "title": "Language Identification With Confidence Limits",
        "summary": "  A statistical classification algorithm and its application to language\nidentification from noisy input are described. The main innovation is to\ncompute confidence limits on the classification, so that the algorithm\nterminates when enough evidence to make a clear decision has been made, and so\navoiding problems with categories that have similar characteristics. A second\napplication, to genre identification, is briefly examined. The results show\nthat some of the problems of other language identification techniques can be\navoided, and illustrate a more important point: that a statistical language\nprocess can be used to provide feedback about its own success rate.\n",
        "published": "1999-07-07T09:28:40Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907010v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907012v1",
        "title": "Selective Magic HPSG Parsing",
        "summary": "  We propose a parser for constraint-logic grammars implementing HPSG that\ncombines the advantages of dynamic bottom-up and advanced top-down control. The\nparser allows the user to apply magic compilation to specific constraints in a\ngrammar which as a result can be processed dynamically in a bottom-up and\ngoal-directed fashion. State of the art top-down processing techniques are used\nto deal with the remaining constraints. We discuss various aspects concerning\nthe implementation of the parser as part of a grammar development system.\n",
        "published": "1999-07-08T09:46:37Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907013v1",
        "title": "Corpus Annotation for Parser Evaluation",
        "summary": "  We describe a recently developed corpus annotation scheme for evaluating\nparsers that avoids shortcomings of current methods. The scheme encodes\ngrammatical relations between heads and dependents, and has been used to mark\nup a new public-domain corpus of naturally occurring English text. We show how\nthe corpus can be used to evaluate the accuracy of a robust parser, and relate\nthe corpus to extant resources.\n",
        "published": "1999-07-08T10:08:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907013v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907017v1",
        "title": "A Bootstrap Approach to Automatically Generating Lexical Transfer Rules",
        "summary": "  We describe a method for automatically generating Lexical Transfer Rules\n(LTRs) from word equivalences using transfer rule templates. Templates are\nskeletal LTRs, unspecified for words. New LTRs are created by instantiating a\ntemplate with words, provided that the words belong to the appropriate lexical\ncategories required by the template. We define two methods for creating an\ninventory of templates and using them to generate new LTRs. A simpler method\nconsists of extracting a finite set of templates from a sample of hand coded\nLTRs and directly using them in the generation process. A further method\nconsists of abstracting over the initial finite set of templates to define\nhigher level templates, where bilingual equivalences are defined in terms of\ncorrespondences involving phrasal categories. Phrasal templates are then mapped\nonto sets of lexical templates with the aid of grammars. In this way an\ninfinite set of lexical templates is recursively defined. New LTRs are created\nby parsing input words, matching a template at the phrasal level and using the\ncorresponding lexical categories to instantiate the lexical template. The\ndefinition of an infinite set of templates enables the automatic creation of\nLTRs for multi-word, non-compositional word equivalences of any cardinality.\n",
        "published": "1999-07-09T22:39:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9907021v1",
        "title": "Architectural Considerations for Conversational Systems -- The\n  Verbmobil/INTARC Experience",
        "summary": "  The paper describes the speech to speech translation system INTARC, developed\nduring the first phase of the Verbmobil project. The general design goals of\nthe INTARC system architecture were time synchronous processing as well as\nincrementality and interactivity as a means to achieve a higher degree of\nrobustness and scalability. Interactivity means that in addition to the\nbottom-up (in terms of processing levels) data flow the ability to process\ntop-down restrictions considering the same signal segment for all processing\nlevels. The construction of INTARC 2.0, which has been operational since fall\n1996, followed an engineering approach focussing on the integration of symbolic\n(linguistic) and stochastic (recognition) techniques which led to a\ngeneralization of the concept of a ``one pass'' beam search.\n",
        "published": "1999-07-14T09:21:16Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9907021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9908001v1",
        "title": "Detecting Sub-Topic Correspondence through Bipartite Term Clustering",
        "summary": "  This paper addresses a novel task of detecting sub-topic correspondence in a\npair of text fragments, enhancing common notions of text similarity. This task\nis addressed by coupling corresponding term subsets through bipartite\nclustering. The paper presents a cost-based clustering scheme and compares it\nwith a bipartite version of the single-link method, providing illustrating\nresults.\n",
        "published": "1999-08-01T14:02:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9908001v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9909002v1",
        "title": "Semantic robust parsing for noun extraction from natural language\n  queries",
        "summary": "  This paper describes how robust parsing techniques can be fruitful applied\nfor building a query generation module which is part of a pipelined NLP\narchitecture aimed at process natural language queries in a restricted domain.\nWe want to show that semantic robustness represents a key issue in those NLP\nsystems where it is more likely to have partial and ill-formed utterances due\nto various factors (e.g. noisy environments, low quality of speech recognition\nmodules, etc...) and where it is necessary to succeed, even if partially, in\nextracting some meaningful information.\n",
        "published": "1999-09-02T15:53:07Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9909002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9910020v1",
        "title": "Selective Sampling for Example-based Word Sense Disambiguation",
        "summary": "  This paper proposes an efficient example sampling method for example-based\nword sense disambiguation systems. To construct a database of practical size, a\nconsiderable overhead for manual sense disambiguation (overhead for\nsupervision) is required. In addition, the time complexity of searching a\nlarge-sized database poses a considerable problem (overhead for search). To\ncounter these problems, our method selectively samples a smaller-sized\neffective subset from a given example set for use in word sense disambiguation.\nOur method is characterized by the reliance on the notion of training utility:\nthe degree to which each example is informative for future example sampling\nwhen used for the training of the system. The system progressively collects\nexamples by selecting those with greatest utility. The paper reports the\neffectiveness of our method through experiments on about one thousand\nsentences. Compared to experiments with other example sampling methods, our\nmethod reduced both the overhead for supervision and the overhead for search,\nwithout the degeneration of the performance of the system.\n",
        "published": "1999-10-23T11:19:35Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9910020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9910022v1",
        "title": "Practical experiments with regular approximation of context-free\n  languages",
        "summary": "  Several methods are discussed that construct a finite automaton given a\ncontext-free grammar, including both methods that lead to subsets and those\nthat lead to supersets of the original context-free language. Some of these\nmethods of regular approximation are new, and some others are presented here in\na more refined form with respect to existing literature. Practical experiments\nwith the different methods of regular approximation are performed for\nspoken-language input: hypotheses from a speech recognizer are filtered through\na finite automaton.\n",
        "published": "1999-10-25T15:00:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9910022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9911006v1",
        "title": "Question Answering System Using Syntactic Information",
        "summary": "  Question answering task is now being done in TREC8 using English documents.\nWe examined question answering task in Japanese sentences. Our method selects\nthe answer by matching the question sentence with knowledge-based data written\nin natural language. We use syntactic information to obtain highly accurate\nanswers.\n",
        "published": "1999-11-15T05:48:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9911006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9911011v1",
        "title": "One-Level Prosodic Morphology",
        "summary": "  Recent developments in theoretical linguistics have lead to a widespread\nacceptance of constraint-based analyses of prosodic morphology phenomena such\nas truncation, infixation, floating morphemes and reduplication. Of these,\nreduplication is particularly challenging for state-of-the-art computational\nmorphology, since it involves copying of some part of a phonological string. In\nthis paper I argue for certain extensions to the one-level model of phonology\nand morphology (Bird & Ellison 1994) to cover the computational aspects of\nprosodic morphology using finite-state methods. In a nutshell, enriched lexical\nrepresentations provide additional automaton arcs to repeat or skip sounds and\nalso to allow insertion of additional material. A kind of resource\nconsciousness is introduced to control this additional freedom, distinguishing\nbetween producer and consumer arcs. The non-finite-state copying aspect of\nreduplication is mapped to automata intersection, itself a non-finite-state\noperation. Bounded local optimization prunes certain automaton arcs that fail\nto contribute to linguistic optimisation criteria. The paper then presents\nimplemented case studies of Ulwa construct state infixation, German\nhypocoristic truncation and Tagalog over-applying reduplication that illustrate\nthe expressive power of this approach, before its merits and limitations are\ndiscussed and possible extensions are sketched. I conclude that the one-level\napproach to prosodic morphology presents an attractive way of extending\nfinite-state techniques to difficult phenomena that hitherto resisted elegant\ncomputational analyses.\n",
        "published": "1999-11-19T16:10:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9911011v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912003v1",
        "title": "Resolution of Indirect Anaphora in Japanese Sentences Using Examples 'X\n  no Y (Y of X)'",
        "summary": "  A noun phrase can indirectly refer to an entity that has already been\nmentioned. For example, ``I went into an old house last night. The roof was\nleaking badly and ...'' indicates that ``the roof'' is associated with `` an\nold house}'', which was mentioned in the previous sentence. This kind of\nreference (indirect anaphora) has not been studied well in natural language\nprocessing, but is important for coherence resolution, language understanding,\nand machine translation. In order to analyze indirect anaphora, we need a case\nframe dictionary for nouns that contains knowledge of the relationships between\ntwo nouns but no such dictionary presently exists. Therefore, we are forced to\nuse examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead.\nWe tried estimating indirect anaphora using this information and obtained a\nrecall rate of 63% and a precision rate of 68% on test sentences. This\nindicates that the information of ``X no Y'' is useful to a certain extent when\nwe cannot make use of a noun case frame dictionary. We estimated the results\nthat would be given by a noun case frame dictionary, and obtained recall and\nprecision rates of 71% and 82% respectively. Finally, we proposed a way to\nconstruct a noun case frame dictionary by using examples of ``X no Y.''\n",
        "published": "1999-12-13T04:42:25Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912004v1",
        "title": "Pronoun Resolution in Japanese Sentences Using Surface Expressions and\n  Examples",
        "summary": "  In this paper, we present a method of estimating referents of demonstrative\npronouns, personal pronouns, and zero pronouns in Japanese sentences using\nexamples, surface expressions, topics and foci. Unlike conventional work which\nwas semantic markers for semantic constraints, we used examples for semantic\nconstraints and showed in our experiments that examples are as useful as\nsemantic markers. We also propose many new methods for estimating referents of\npronouns. For example, we use the form ``X of Y'' for estimating referents of\ndemonstrative adjectives. In addition to our new methods, we used many\nconventional methods. As a result, experiments using these methods obtained a\nprecision rate of 87% in estimating referents of demonstrative pronouns,\npersonal pronouns, and zero pronouns for training sentences, and obtained a\nprecision rate of 78% for test sentences.\n",
        "published": "1999-12-13T04:46:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912004v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912005v1",
        "title": "An Estimate of Referent of Noun Phrases in Japanese Sentences",
        "summary": "  In machine translation and man-machine dialogue, it is important to clarify\nreferents of noun phrases. We present a method for determining the referents of\nnoun phrases in Japanese sentences by using the referential properties,\nmodifiers, and possessors of noun phrases. Since the Japanese language has no\narticles, it is difficult to decide whether a noun phrase has an antecedent or\nnot. We had previously estimated the referential properties of noun phrases\nthat correspond to articles by using clue words in the sentences. By using\nthese referential properties, our system determined the referents of noun\nphrases in Japanese sentences. Furthermore we used the modifiers and possessors\nof noun phrases in determining the referents of noun phrases. As a result, on\ntraining sentences we obtained a precision rate of 82% and a recall rate of 85%\nin the determination of the referents of noun phrases that have antecedents. On\ntest sentences, we obtained a precision rate of 79% and a recall rate of 77%.\n",
        "published": "1999-12-13T05:20:40Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912006v1",
        "title": "Resolution of Verb Ellipsis in Japanese Sentence using Surface\n  Expressions and Examples",
        "summary": "  Verbs are sometimes omitted in Japanese sentences. It is necessary to recover\nomitted verbs for purposes of language understanding, machine translation, and\nconversational processing. This paper describes a practical way to recover\nomitted verbs by using surface expressions and examples. We experimented the\nresolution of verb ellipses by using this information, and obtained a recall\nrate of 73% and a precision rate of 66% on test sentences.\n",
        "published": "1999-12-13T05:19:46Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912007v1",
        "title": "An Example-Based Approach to Japanese-to-English Translation of Tense,\n  Aspect, and Modality",
        "summary": "  We have developed a new method for Japanese-to-English translation of tense,\naspect, and modality that uses an example-based method. In this method the\nsimilarity between input and example sentences is defined as the degree of\nsemantic matching between the expressions at the ends of the sentences. Our\nmethod also uses the k-nearest neighbor method in order to exclude the effects\nof noise; for example, wrongly tagged data in the bilingual corpora.\nExperiments show that our method can translate tenses, aspects, and modalities\nmore accurately than the top-level MT software currently available on the\nmarket can. Moreover, it does not require hand-craft rules.\n",
        "published": "1999-12-13T06:01:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912009v1",
        "title": "Deduction over Mixed-Level Logic Representations for Text Passage\n  Retrieval",
        "summary": "  A system is described that uses a mixed-level representation of (part of)\nmeaning of natural language documents (based on standard Horn Clause Logic) and\na variable-depth search strategy that distinguishes between the different\nlevels of abstraction in the knowledge representation to locate specific\npassages in the documents. Mixed-level representations as well as\nvariable-depth search strategies are applicable in fields outside that of NLP.\n",
        "published": "1999-12-15T11:02:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/9912017v1",
        "title": "Mixed-Level Knowledge Representation and Variable-Depth Inference in\n  Natural Language Processing",
        "summary": "  A system is described that uses a mixed-level knowledge representation based\non standard Horn Clause Logic to represent (part of) the meaning of natural\nlanguage documents. A variable-depth search strategy is outlined that\ndistinguishes between the different levels of abstraction in the knowledge\nrepresentation to locate specific passages in the documents. A detailed\ndescription of the linguistic aspects of the system is given. Mixed-level\nrepresentations as well as variable-depth search strategies are applicable in\nfields outside that of NLP.\n",
        "published": "1999-12-23T15:48:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/9912017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001010v1",
        "title": "A Real World Implementation of Answer Extraction",
        "summary": "  In this paper we describe ExtrAns, an answer extraction system. Answer\nextraction (AE) aims at retrieving those exact passages of a document that\ndirectly answer a given user question. AE is more ambitious than information\nretrieval and information extraction in that the retrieval results are phrases,\nnot entire documents, and in that the queries may be arbitrarily specific. It\nis less ambitious than full-fledged question answering in that the answers are\nnot generated from a knowledge base but looked up in the text of documents. The\ncurrent version of ExtrAns is able to parse unedited Unix \"man pages\", and\nderive the logical form of their sentences. User queries are also translated\ninto logical forms. A theorem prover then retrieves the relevant phrases, which\nare presented through selective highlighting in their context.\n",
        "published": "2000-01-14T16:01:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001010v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001012v1",
        "title": "Measures of Distributional Similarity",
        "summary": "  We study distributional similarity measures for the purpose of improving\nprobability estimation for unseen cooccurrences. Our contributions are\nthree-fold: an empirical comparison of a broad range of measures; a\nclassification of similarity functions based on the information that they\nincorporate; and the introduction of a novel function that is superior at\nevaluating potential proxy distributions.\n",
        "published": "2000-01-18T23:19:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001020v1",
        "title": "Exploiting Syntactic Structure for Natural Language Modeling",
        "summary": "  The thesis presents an attempt at using the syntactic structure in natural\nlanguage for improved language models for speech recognition. The structured\nlanguage model merges techniques in automatic parsing and language modeling\nusing an original probabilistic parameterization of a shift-reduce parser. A\nmaximum likelihood reestimation procedure belonging to the class of\nexpectation-maximization algorithms is employed for training the model.\nExperiments on the Wall Street Journal, Switchboard and Broadcast News corpora\nshow improvement in both perplexity and word error rate - word lattice\nrescoring - over the standard 3-gram language model. The significance of the\nthesis lies in presenting an original approach to language modeling that uses\nthe hierarchical - syntactic - structure in natural language to improve on\ncurrent 3-gram modeling techniques for large vocabulary speech recognition.\n",
        "published": "2000-01-24T20:18:16Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001021v1",
        "title": "Refinement of a Structured Language Model",
        "summary": "  A new language model for speech recognition inspired by linguistic analysis\nis presented. The model develops hidden hierarchical structure incrementally\nand uses it to extract meaningful information from the word history - thus\nenabling the use of extended distance dependencies - in an attempt to\ncomplement the locality of currently used n-gram Markov models. The model, its\nprobabilistic parametrization, a reestimation algorithm for the model\nparameters and a set of experiments meant to evaluate its potential for speech\nrecognition are presented.\n",
        "published": "2000-01-24T20:55:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001022v1",
        "title": "Recognition Performance of a Structured Language Model",
        "summary": "  A new language model for speech recognition inspired by linguistic analysis\nis presented. The model develops hidden hierarchical structure incrementally\nand uses it to extract meaningful information from the word history - thus\nenabling the use of extended distance dependencies - in an attempt to\ncomplement the locality of currently used trigram models. The structured\nlanguage model, its probabilistic parameterization and performance in a\ntwo-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus\nshow an improvement in both perplexity and word error rate over conventional\ntrigram models.\n",
        "published": "2000-01-24T21:18:37Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0001023v1",
        "title": "Structured Language Modeling for Speech Recognition",
        "summary": "  A new language model for speech recognition is presented. The model develops\nhidden hierarchical syntactic-like structure incrementally and uses it to\nextract meaningful information from the word history, thus complementing the\nlocality of currently used trigram models. The structured language model (SLM)\nand its performance in a two-pass speech recognizer --- lattice decoding ---\nare presented. Experiments on the WSJ corpus show an improvement in both\nperplexity (PPL) and word error rate (WER) over conventional trigram models.\n",
        "published": "2000-01-25T19:35:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0001023v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0002007v1",
        "title": "Requirements of Text Processing Lexicons",
        "summary": "  As text processing systems expand in scope, they will require ever larger\nlexicons along with a parsing capability for discriminating among many senses\nof a word. Existing systems do not incorporate such subtleties in meaning for\ntheir lexicons. Ordinary dictionaries contain such information, but are largely\nuntapped. When the contents of dictionaries are scrutinized, they reveal many\nrequirements that must be satisfied in representing meaning and in developing\nsemantic parsers. These requirements were identified in research designed to\nfind primitive verb concepts. The requirements are outlined and general\nprocedures for satisfying them through the use of ordinary dictionaries are\ndescribed, illustrated by building frames for and examining the definitions of\n\"change\" and its uses as a hypernym in other definitions.\n",
        "published": "2000-02-11T19:17:18Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0002007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0002017v1",
        "title": "An Usage Measure Based on Psychophysical Relations",
        "summary": "  A new word usage measure is proposed. It is based on psychophysical relations\nand allows to reveal words by its degree of \"importance\" for making basic\ndictionaries of sublanguages.\n",
        "published": "2000-02-27T04:09:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0002017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003055v1",
        "title": "TnT - A Statistical Part-of-Speech Tagger",
        "summary": "  Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.\nContrary to claims found elsewhere in the literature, we argue that a tagger\nbased on Markov models performs at least as well as other current approaches,\nincluding the Maximum Entropy framework. A recent comparison has even shown\nthat TnT performs significantly better for the tested corpora. We describe the\nbasic model of TnT, the techniques used for smoothing and for handling unknown\nwords. Furthermore, we present evaluations on two corpora.\n",
        "published": "2000-03-13T09:55:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003055v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003060v1",
        "title": "Message Classification in the Call Center",
        "summary": "  Customer care in technical domains is increasingly based on e-mail\ncommunication, allowing for the reproduction of approved solutions. Identifying\nthe customer's problem is often time-consuming, as the problem space changes if\nnew products are launched. This paper describes a new approach to the\nclassification of e-mail requests based on shallow text processing and machine\nlearning techniques. It is implemented within an assistance system for call\ncenter agents that is used in a commercial setting.\n",
        "published": "2000-03-14T13:09:28Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003060v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003074v1",
        "title": "A Finite State and Data-Oriented Method for Grapheme to Phoneme\n  Conversion",
        "summary": "  A finite-state method, based on leftmost longest-match replacement, is\npresented for segmenting words into graphemes, and for converting graphemes\ninto phonemes. A small set of hand-crafted conversion rules for Dutch achieves\na phoneme accuracy of over 93%. The accuracy of the system is further improved\nby using transformation-based learning. The phoneme accuracy of the best system\n(using a large set of rule templates and a `lazy' variant of Brill's algoritm),\ntrained on only 40K words, reaches 99% accuracy.\n",
        "published": "2000-03-23T11:29:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003074v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003081v1",
        "title": "Variable Word Rate N-grams",
        "summary": "  The rate of occurrence of words is not uniform but varies from document to\ndocument. Despite this observation, parameters for conventional n-gram language\nmodels are usually derived using the assumption of a constant word rate. In\nthis paper we investigate the use of variable word rate assumption, modelled by\na Poisson distribution or a continuous mixture of Poissons. We present an\napproach to estimating the relative frequencies of words or n-grams taking\nprior information of their occurrences into account. Discounting and smoothing\nschemes are also considered. Using the Broadcast News task, the approach\ndemonstrates a reduction of perplexity up to 10%.\n",
        "published": "2000-03-29T16:35:58Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003081v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003083v1",
        "title": "Advances in domain independent linear text segmentation",
        "summary": "  This paper describes a method for linear text segmentation which is twice as\naccurate and over seven times as fast as the state-of-the-art (Reynar, 1998).\nInter-sentence similarity is replaced by rank in the local context. Boundary\nlocations are discovered by divisive clustering.\n",
        "published": "2000-03-30T16:56:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003083v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0003084v1",
        "title": "Information Extraction from Broadcast News",
        "summary": "  This paper discusses the development of trainable statistical models for\nextracting content from television and radio news broadcasts. In particular we\nconcentrate on statistical finite state models for identifying proper names and\nother named entities in broadcast speech. Two models are presented: the first\nrepresents name class information as a word attribute; the second represents\nboth word-word and class-class transitions explicitly. A common n-gram based\nformulation is used for both models. The task of named entity identification is\ncharacterized by relatively sparse training data and issues related to\nsmoothing are discussed. Experiments are reported using the DARPA/NIST Hub-4E\nevaluation for North American Broadcast News.\n",
        "published": "2000-03-30T16:52:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0003084v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0004016v1",
        "title": "Looking at discourse in a corpus: The role of lexical cohesion",
        "summary": "  This paper is aimed at reporting on the development and application of a\ncomputer model for discourse analysis through segmentation. Segmentation refers\nto the principled division of texts into contiguous constituents. Other studies\nhave looked at the application of a number of models to the analysis of\ndiscourse by computer. The segmentation procedure developed for the present\ninvestigation is called LSM ('Link Set Median'). It was applied to three corpus\nof 300 texts from three different genres. The results obtained by application\nof the LSM procedure on the corpus were then compared to segmentation carried\nout at random. Statistical analyses suggested that LSM significantly\noutperformed random segmentation, thus indicating that the segmentation was\nmeaningful.\n",
        "published": "2000-04-28T03:18:09Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0004016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005006v1",
        "title": "A Simple Approach to Building Ensembles of Naive Bayesian Classifiers\n  for Word Sense Disambiguation",
        "summary": "  This paper presents a corpus-based approach to word sense disambiguation that\nbuilds an ensemble of Naive Bayesian classifiers, each of which is based on\nlexical features that represent co--occurring words in varying sized windows of\ncontext. Despite the simplicity of this approach, empirical results\ndisambiguating the widely studied nouns line and interest show that such an\nensemble achieves accuracy rivaling the best previously published results.\n",
        "published": "2000-05-07T00:15:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005015v1",
        "title": "Noun Phrase Recognition by System Combination",
        "summary": "  The performance of machine learning algorithms can be improved by combining\nthe output of different systems. In this paper we apply this idea to the\nrecognition of noun phrases.We generate different classifiers by using\ndifferent representations of the data. By combining the results with voting\ntechniques described in (Van Halteren et.al. 1998) we manage to improve the\nbest reported performances on standard data sets for base noun phrases and\narbitrary noun phrases.\n",
        "published": "2000-05-10T11:58:12Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005016v1",
        "title": "Improving Testsuites via Instrumentation",
        "summary": "  This paper explores the usefulness of a technique from software engineering,\nnamely code instrumentation, for the development of large-scale natural\nlanguage grammars. Information about the usage of grammar rules in test\nsentences is used to detect untested rules, redundant test sentences, and\nlikely causes of overgeneration. Results show that less than half of a\nlarge-coverage grammar for German is actually tested by two large testsuites,\nand that 10-30% of testing time is redundant. The methodology applied can be\nseen as a re-use of grammar writing knowledge for testsuite compilation.\n",
        "published": "2000-05-10T14:51:14Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005019v1",
        "title": "On the Scalability of the Answer Extraction System \"ExtrAns\"",
        "summary": "  This paper reports on the scalability of the answer extraction system\nExtrAns. An answer extraction system locates the exact phrases in the documents\nthat contain the explicit answers to the user queries. Answer extraction\nsystems are therefore more convenient than document retrieval systems in\nsituations where the user wants to find specific information in limited time.\n  ExtrAns performs answer extraction over UNIX manpages. It has been\nconstructed by combining available linguistic resources and implementing only a\nfew modules from scratch. A resolution procedure between the minimal logical\nform of the user query and the minimal logical forms of the manpage sentences\nfinds the answers to the queries. These answers are displayed to the user,\ntogether with pointers to the respective manpages, and the exact phrases that\ncontribute to the answer are highlighted.\n  This paper shows that the increase in response times is not a big issue when\nscaling the system up from 30 to 500 documents, and that the response times for\n500 documents are still acceptable for a real-time answer extraction system.\n",
        "published": "2000-05-12T14:12:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005025v1",
        "title": "Finite-State Reduplication in One-Level Prosodic Morphology",
        "summary": "  Reduplication, a central instance of prosodic morphology, is particularly\nchallenging for state-of-the-art computational morphology, since it involves\ncopying of some part of a phonological string. In this paper I advocate a\nfinite-state method that combines enriched lexical representations via\nintersection to implement the copying. The proposal includes a\nresource-conscious variant of automata and can benefit from the existence of\nlazy algorithms. Finally, the implementation of a complex case from Koasati is\npresented.\n",
        "published": "2000-05-22T14:07:28Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0005029v1",
        "title": "Ranking suspected answers to natural language questions using predictive\n  annotation",
        "summary": "  In this paper, we describe a system to rank suspected answers to natural\nlanguage questions. We process both corpus and query using a new technique,\npredictive annotation, which augments phrases in texts with labels anticipating\ntheir being targets of certain kinds of questions. Given a natural language\nquestion, an IR system returns a set of matching passages, which are then\nanalyzed and ranked according to various criteria described in this paper. We\nprovide an evaluation of the techniques based on results from the TREC Q&A\nevaluation in which our system participated.\n",
        "published": "2000-05-30T17:10:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0005029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006003v1",
        "title": "Exploiting Diversity in Natural Language Processing: Combining Parsers",
        "summary": "  Three state-of-the-art statistical parsers are combined to produce more\naccurate parses, as well as new bounds on achievable Treebank parsing accuracy.\nTwo general approaches are presented and two combination techniques are\ndescribed for each approach. Both parametric and non-parametric models are\nexplored. The resulting parsers surpass the best previously published\nperformance results for the Penn Treebank.\n",
        "published": "2000-06-01T18:42:24Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006011v1",
        "title": "Bagging and Boosting a Treebank Parser",
        "summary": "  Bagging and boosting, two effective machine learning techniques, are applied\nto natural language parsing. Experiments using these techniques with a\ntrainable statistical parser are described. The best resulting system provides\nroughly as large of a gain in F-measure as doubling the corpus size. Error\nanalysis of the result of the boosting technique reveals some inconsistent\nannotations in the Penn Treebank, suggesting a semi-automatic method for\nfinding inconsistent treebank annotations.\n",
        "published": "2000-06-05T18:04:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006011v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006012v1",
        "title": "Exploiting Diversity for Natural Language Parsing",
        "summary": "  The popularity of applying machine learning methods to computational\nlinguistics problems has produced a large supply of trainable natural language\nprocessing systems. Most problems of interest have an array of off-the-shelf\nproducts or downloadable code implementing solutions using various techniques.\nWhere these solutions are developed independently, it is observed that their\nerrors tend to be independently distributed. This thesis is concerned with\napproaches for capitalizing on this situation in a sample problem domain, Penn\nTreebank-style parsing.\n  The machine learning community provides techniques for combining outputs of\nclassifiers, but parser output is more structured and interdependent than\nclassifications. To address this discrepancy, two novel strategies for\ncombining parsers are used: learning to control a switch between parsers and\nconstructing a hybrid parse from multiple parsers' outputs.\n  Off-the-shelf parsers are not developed with an intention to perform well in\na collaborative ensemble. Two techniques are presented for producing an\nensemble of parsers that collaborate. All of the ensemble members are created\nusing the same underlying parser induction algorithm, and the method for\nproducing complementary parsers is only loosely constrained by that chosen\nalgorithm.\n",
        "published": "2000-06-05T21:33:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006017v1",
        "title": "Turning Speech Into Scripts",
        "summary": "  We describe an architecture for implementing spoken natural language dialogue\ninterfaces to semi-autonomous systems, in which the central idea is to\ntransform the input speech signal through successive levels of representation\ncorresponding roughly to linguistic knowledge, dialogue knowledge, and domain\nknowledge. The final representation is an executable program in a simple\nscripting language equivalent to a subset of Cshell. At each stage of the\ntranslation process, an input is transformed into an output, producing as a\nbyproduct a \"meta-output\" which describes the nature of the transformation\nperformed. We show how consistent use of the output/meta-output distinction\npermits a simple and perspicuous treatment of apparently diverse topics\nincluding resolution of pronouns, correction of user misconceptions, and\noptimization of scripts. The methods described have been concretely realized in\na prototype speech interface to a simulation of the Personal Satellite\nAssistant.\n",
        "published": "2000-06-09T17:28:40Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006019v1",
        "title": "A Compact Architecture for Dialogue Management Based on Scripts and\n  Meta-Outputs",
        "summary": "  We describe an architecture for spoken dialogue interfaces to semi-autonomous\nsystems that transforms speech signals through successive representations of\nlinguistic, dialogue, and domain knowledge. Each step produces an output, and a\nmeta-output describing the transformation, with an executable program in a\nsimple scripting language as the final result. The output/meta-output\ndistinction permits perspicuous treatment of diverse tasks such as resolving\npronouns, correcting user misconceptions, and optimizing scripts.\n",
        "published": "2000-06-09T21:41:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006020v1",
        "title": "A Comparison of the XTAG and CLE Grammars for English",
        "summary": "  When people develop something intended as a large broad-coverage grammar,\nthey usually have a more specific goal in mind. Sometimes this goal is covering\na corpus; sometimes the developers have theoretical ideas they wish to\ninvestigate; most often, work is driven by a combination of these two main\ntypes of goal. What tends to happen after a while is that the community of\npeople working with the grammar starts thinking of some phenomena as\n``central'', and makes serious efforts to deal with them; other phenomena are\nlabelled ``marginal'', and ignored. Before long, the distinction between\n``central'' and ``marginal'' becomes so ingrained that it is automatic, and\npeople virtually stop thinking about the ``marginal'' phenomena. In practice,\nthe only way to bring the marginal things back into focus is to look at what\nother people are doing and compare it with one's own work. In this paper, we\nwill take two large grammars, XTAG and the CLE, and examine each of them from\nthe other's point of view. We will find in both cases not only that important\nthings are missing, but that the perspective offered by the other grammar\nsuggests simple and practical ways of filling in the holes. It turns out that\nthere is a pleasing symmetry to the picture. XTAG has a very good treatment of\ncomplement structure, which the CLE to some extent lacks; conversely, the CLE\noffers a powerful and general account of adjuncts, which the XTAG grammar does\nnot fully duplicate. If we examine the way in which each grammar does the thing\nit is good at, we find that the relevant methods are quite easy to port to the\nother framework, and in fact only involve generalization and systematization of\nexisting mechanisms.\n",
        "published": "2000-06-09T20:49:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006021v1",
        "title": "Compiling Language Models from a Linguistically Motivated Unification\n  Grammar",
        "summary": "  Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task.\n",
        "published": "2000-06-09T22:03:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006023v2",
        "title": "Dialogue Act Modeling for Automatic Tagging and Recognition of\n  Conversational Speech",
        "summary": "  We describe a statistical approach for modeling dialogue acts in\nconversational speech, i.e., speech-act-like units such as Statement, Question,\nBackchannel, Agreement, Disagreement, and Apology. Our model detects and\npredicts dialogue acts based on lexical, collocational, and prosodic cues, as\nwell as on the discourse coherence of the dialogue act sequence. The dialogue\nmodel is based on treating the discourse structure of a conversation as a\nhidden Markov model and the individual dialogue acts as observations emanating\nfrom the model states. Constraints on the likely sequence of dialogue acts are\nmodeled via a dialogue act n-gram. The statistical dialogue grammar is combined\nwith word n-grams, decision trees, and neural networks modeling the\nidiosyncratic lexical and prosodic manifestations of each dialogue act. We\ndevelop a probabilistic integration of speech recognition with dialogue\nmodeling, to improve both speech recognition and dialogue act classification\naccuracy. Models are trained and evaluated using a large hand-labeled database\nof 1,155 conversations from the Switchboard corpus of spontaneous\nhuman-to-human telephone speech. We achieved good dialogue act labeling\naccuracy (65% based on errorful, automatically recognized words and prosody,\nand 71% based on word transcripts, compared to a chance baseline accuracy of\n35% and human accuracy of 84%) and a small reduction in word recognition error.\n",
        "published": "2000-06-11T06:06:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006023v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006024v1",
        "title": "Can Prosody Aid the Automatic Classification of Dialog Acts in\n  Conversational Speech?",
        "summary": "  Identifying whether an utterance is a statement, question, greeting, and so\nforth is integral to effective automatic understanding of natural dialog.\nLittle is known, however, about how such dialog acts (DAs) can be automatically\nclassified in truly natural conversation. This study asks whether current\napproaches, which use mainly word information, could be improved by adding\nprosodic information. The study is based on more than 1000 conversations from\nthe Switchboard corpus. DAs were hand-annotated, and prosodic features\n(duration, pause, F0, energy, and speaking rate) were automatically extracted\nfor each DA. In training, decision trees based on these features were inferred;\ntrees were then applied to unseen test data to evaluate performance.\nPerformance was evaluated for prosody models alone, and after combining the\nprosody models with word information -- either from true words or from the\noutput of an automatic speech recognizer. For an overall classification task,\nas well as three subtasks, prosody made significant contributions to\nclassification. Feature-specific analyses further revealed that although\ncanonical features (such as F0 for questions) were important, less obvious\nfeatures could compensate if canonical features were removed. Finally, in each\ntask, integrating the prosodic model with a DA-specific statistical language\nmodel improved performance over that of the language model alone, especially\nfor the case of recognized words. Results suggest that DAs are redundantly\nmarked in natural conversation, and that a variety of automatically extractable\nprosodic features could aid dialog processing in speech applications.\n",
        "published": "2000-06-11T06:00:11Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006024v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006025v1",
        "title": "Entropy-based Pruning of Backoff Language Models",
        "summary": "  A criterion for pruning parameters from N-gram backoff language models is\ndeveloped, based on the relative entropy between the original and the pruned\nmodel. It is shown that the relative entropy resulting from pruning a single\nN-gram can be computed exactly and efficiently for backoff models. The relative\nentropy measure can be expressed as a relative change in training set\nperplexity. This leads to a simple pruning criterion whereby all N-grams that\nchange perplexity by less than a threshold are removed from the model.\nExperiments show that a production-quality Hub4 LM can be reduced to 26% its\noriginal size without increasing recognition error. We also compare the\napproach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and\nshow that their approach can be interpreted as an approximation to the relative\nentropy criterion. Experimentally, both approaches select similar sets of\nN-grams (about 85% overlap), with the exact relative entropy criterion giving\nmarginally better performance.\n",
        "published": "2000-06-11T18:20:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006028v1",
        "title": "Trainable Methods for Surface Natural Language Generation",
        "summary": "  We present three systems for surface natural language generation that are\ntrainable from annotated corpora. The first two systems, called NLG1 and NLG2,\nrequire a corpus marked only with domain-specific semantic attributes, while\nthe last system, called NLG3, requires a corpus marked with both semantic\nattributes and syntactic dependency information. All systems attempt to produce\na grammatical natural language phrase from a domain-specific semantic\nrepresentation. NLG1 serves a baseline system and uses phrase frequencies to\ngenerate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy\nprobability models to individually generate each word in the phrase. The\nsystems NLG2 and NLG3 learn to determine both the word choice and the word\norder of the phrase. We present experiments in which we generate phrases to\ndescribe flights in the air travel domain.\n",
        "published": "2000-06-13T21:06:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006036v1",
        "title": "Prosody-Based Automatic Segmentation of Speech into Sentences and Topics",
        "summary": "  A crucial step in processing speech audio data for information extraction,\ntopic detection, or browsing/playback is to segment the input into sentence and\ntopic units. Speech segmentation is challenging, since the cues typically\npresent for segmenting text (headers, paragraphs, punctuation) are absent in\nspoken language. We investigate the use of prosody (information gleaned from\nthe timing and melody of speech) for these tasks. Using decision tree and\nhidden Markov modeling techniques, we combine prosodic cues with word-based\napproaches, and evaluate performance on two speech corpora, Broadcast News and\nSwitchboard. Results show that the prosodic model alone performs on par with,\nor better than, word-based statistical language models -- for both true and\nautomatically recognized words in news speech. The prosodic model achieves\ncomparable performance with significantly less training data, and requires no\nhand-labeling of prosodic events. Across tasks and corpora, we obtain a\nsignificant improvement over word-only models using a probabilistic combination\nof prosodic and lexical information. Inspection reveals that the prosodic\nmodels capture language-independent boundary indicators described in the\nliterature. Finally, cue usage is task and corpus dependent. For example, pause\nand pitch features are highly informative for segmenting news speech, whereas\npause, duration and word-based cues dominate for natural conversation.\n",
        "published": "2000-06-27T04:39:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006038v1",
        "title": "Approximation and Exactness in Finite State Optimality Theory",
        "summary": "  Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that\nOptimality Theory with gradient constraints generally is not finite state. A\nnew finite-state treatment of gradient constraints is presented which improves\nupon the approximation of Karttunen (1998). The method turns out to be exact,\nand very compact, for the syllabification analysis of Prince and Smolensky\n(1993).\n",
        "published": "2000-06-28T10:06:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006038v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0006044v1",
        "title": "Finite-State Non-Concatenative Morphotactics",
        "summary": "  Finite-state morphology in the general tradition of the Two-Level and Xerox\nimplementations has proved very successful in the production of robust\nmorphological analyzer-generators, including many large-scale commercial\nsystems. However, it has long been recognized that these implementations have\nserious limitations in handling non-concatenative phenomena. We describe a new\ntechnique for constructing finite-state transducers that involves reapplying\nthe regular-expression compiler to its own output. Implemented in an algorithm\ncalled compile-replace, this technique has proved useful for handling\nnon-concatenative phenomena; and we demonstrate it on Malay full-stem\nreduplication and Arabic stem interdigitation.\n",
        "published": "2000-06-30T13:22:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0006044v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007009v1",
        "title": "Incremental construction of minimal acyclic finite-state automata",
        "summary": "  In this paper, we describe a new method for constructing minimal,\ndeterministic, acyclic finite-state automata from a set of strings. Traditional\nmethods consist of two phases: the first to construct a trie, the second one to\nminimize it. Our approach is to construct a minimal automaton in a single phase\nby adding new strings one by one and minimizing the resulting automaton\non-the-fly. We present a general algorithm as well as a specialization that\nrelies upon the lexicographical ordering of the input strings.\n",
        "published": "2000-07-06T14:15:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007018v1",
        "title": "Bootstrapping a Tagged Corpus through Combination of Existing\n  Heterogeneous Taggers",
        "summary": "  This paper describes a new method, Combi-bootstrap, to exploit existing\ntaggers and lexical resources for the annotation of corpora with new tagsets.\nCombi-bootstrap uses existing resources as features for a second level machine\nlearning module, that is trained to make the mapping to the new tagset on a\nvery small sample of annotated corpus material. Experiments show that\nCombi-bootstrap: i) can integrate a wide variety of existing resources, and ii)\nachieves much higher accuracy (up to 44.7 % error reduction) than both the best\nsingle tagger and an ensemble tagger constructed out of the same small training\nsample.\n",
        "published": "2000-07-13T12:46:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007018v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007022v1",
        "title": "ATLAS: A flexible and extensible architecture for linguistic annotation",
        "summary": "  We describe a formal model for annotating linguistic artifacts, from which we\nderive an application programming interface (API) to a suite of tools for\nmanipulating these annotations. The abstract logical model provides for a range\nof storage formats and promotes the reuse of tools that interact through this\nAPI. We focus first on ``Annotation Graphs,'' a graph model for annotations on\nlinear signals (such as text and speech) indexed by intervals, for which\nefficient database storage and querying techniques are applicable. We note how\na wide range of existing annotated corpora can be mapped to this annotation\ngraph model. This model is then generalized to encompass a wider variety of\nlinguistic ``signals,'' including both naturally occuring phenomena (as\nrecorded in images, video, multi-modal interactions, etc.), as well as the\nderived resources that are increasingly important to the engineering of natural\nlanguage processing systems (such as word lists, dictionaries, aligned\nbilingual corpora, etc.). We conclude with a review of the current efforts\ntowards implementing key pieces of this architecture.\n",
        "published": "2000-07-13T18:26:38Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007024v1",
        "title": "Many uses, many annotations for large speech corpora: Switchboard and\n  TDT as case studies",
        "summary": "  This paper discusses the challenges that arise when large speech corpora\nreceive an ever-broadening range of diverse and distinct annotations. Two case\nstudies of this process are presented: the Switchboard Corpus of telephone\nconversations and the TDT2 corpus of broadcast news. Switchboard has undergone\ntwo independent transcriptions and various types of additional annotation, all\ncarried out as separate projects that were dispersed both geographically and\nchronologically. The TDT2 corpus has also received a variety of annotations,\nbut all directly created or managed by a core group. In both cases, issues\narise involving the propagation of repairs, consistency of references, and the\nability to integrate annotations having different formats and levels of detail.\nWe describe a general framework whereby these issues can be addressed\nsuccessfully.\n",
        "published": "2000-07-13T18:51:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007024v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007031v2",
        "title": "Parameter-free Model of Rank Polysemantic Distribution",
        "summary": "  A model of rank polysemantic distribution with a minimal number of fitting\nparameters is offered. In an ideal case a parameter-free description of the\ndependence on the basis of one or several immediate features of the\ndistribution is possible.\n",
        "published": "2000-07-21T06:07:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007031v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007035v1",
        "title": "Mapping WordNets Using Structural Information",
        "summary": "  We present a robust approach for linking already existing lexical/semantic\nhierarchies. We used a constraint satisfaction algorithm (relaxation labeling)\nto select --among a set of candidates-- the node in a target taxonomy that\nbests matches each node in a source taxonomy. In particular, we use it to map\nthe nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision\nand a very low remaining ambiguity.\n",
        "published": "2000-07-25T17:20:47Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0007036v1",
        "title": "Language identification of controlled systems: Modelling, control and\n  anomaly detection",
        "summary": "  Formal language techniques have been used in the past to study autonomous\ndynamical systems. However, for controlled systems, new features are needed to\ndistinguish between information generated by the system and input control. We\nshow how the modelling framework for controlled dynamical systems leads\nnaturally to a formulation in terms of context-dependent grammars. A learning\nalgorithm is proposed for on-line generation of the grammar productions, this\nformulation being then used for modelling, control and anomaly detection.\nPractical applications are described for electromechanical drives. Grammatical\ninterpolation techniques yield accurate results and the pattern detection\ncapabilities of the language-based formulation makes it a promising technique\nfor the early detection of anomalies or faulty behaviour.\n",
        "published": "2000-07-25T17:40:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0007036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008003v1",
        "title": "Interfacing Constraint-Based Grammars and Generation Algorithms",
        "summary": "  Constraint-based grammars can, in principle, serve as the major linguistic\nknowledge source for both parsing and generation. Surface generation starts\nfrom input semantics representations that may vary across grammars. For many\ndeclarative grammars, the concept of derivation implicitly built in is that of\nparsing. They may thus not be interpretable by a generation algorithm. We show\nthat linguistically plausible semantic analyses can cause severe problems for\nsemantic-head-driven approaches for generation (SHDG). We use SeReal, a variant\nof SHDG and the DISCO grammar of German as our source of examples. We propose a\nnew, general approach that explicitly accounts for the interface between the\ngrammar and the generation algorithm by adding a control-oriented layer to the\nlinguistic knowledge base that reorganizes the semantics in a way suitable for\ngeneration.\n",
        "published": "2000-08-07T16:06:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008004v1",
        "title": "Comparing two trainable grammatical relations finders",
        "summary": "  Grammatical relationships (GRs) form an important level of natural language\nprocessing, but different sets of GRs are useful for different purposes.\nTherefore, one may often only have time to obtain a small training corpus with\nthe desired GR annotations. On such a small training corpus, we compare two\nsystems. They use different learning techniques, but we find that this\ndifference by itself only has a minor effect. A larger factor is that in\nEnglish, a different GR length measure appears better suited for finding simple\nargument GRs than for finding modifier GRs. We also find that partitioning the\ndata may help memory-based learning.\n",
        "published": "2000-08-08T22:42:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008004v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008005v1",
        "title": "More accurate tests for the statistical significance of result\n  differences",
        "summary": "  Statistical significance testing of differences in values of metrics like\nrecall, precision and balanced F-score is a necessary part of empirical natural\nlanguage processing. Unfortunately, we find in a set of experiments that many\ncommonly used tests often underestimate the significance and so are less likely\nto detect differences that exist between different techniques. This\nunderestimation comes from an independence assumption that is often violated.\nWe point out some useful tests that do not make this assumption, including\ncomputationally-intensive randomization tests.\n",
        "published": "2000-08-08T23:52:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008007v1",
        "title": "Tagger Evaluation Given Hierarchical Tag Sets",
        "summary": "  We present methods for evaluating human and automatic taggers that extend\ncurrent practice in three ways. First, we show how to evaluate taggers that\nassign multiple tags to each test instance, even if they do not assign\nprobabilities. Second, we show how to accommodate a common property of manually\nconstructed ``gold standards'' that are typically used for objective\nevaluation, namely that there is often more than one correct answer. Third, we\nshow how to measure performance when the set of possible tags is\ntree-structured in an IS-A hierarchy. To illustrate how our methods can be used\nto measure inter-annotator agreement, we show how to compute the kappa\ncoefficient over hierarchical tag sets.\n",
        "published": "2000-08-10T03:21:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008012v1",
        "title": "Applying System Combination to Base Noun Phrase Identification",
        "summary": "  We use seven machine learning algorithms for one task: identifying base noun\nphrases. The results have been processed by different system combination\nmethods and all of these outperformed the best individual result. We have\napplied the seven learners with the best combinator, a majority vote of the top\nfive systems, to a standard data set and managed to improve the best published\nresult for this data set.\n",
        "published": "2000-08-17T13:13:42Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008013v1",
        "title": "Meta-Learning for Phonemic Annotation of Corpora",
        "summary": "  We apply rule induction, classifier combination and meta-learning (stacked\nclassifiers) to the problem of bootstrapping high accuracy automatic annotation\nof corpora with pronunciation information. The task we address in this paper\nconsists of generating phonemic representations reflecting the Flemish and\nDutch pronunciations of a word on the basis of its orthographic representation\n(which in turn is based on the actual speech recordings). We compare several\npossible approaches to achieve the text-to-pronunciation mapping task:\nmemory-based learning, transformation-based learning, rule induction, maximum\nentropy modeling, combination of classifiers in stacked learning, and stacking\nof meta-learners. We are interested both in optimal accuracy and in obtaining\ninsight into the linguistic regularities involved. As far as accuracy is\nconcerned, an already high accuracy level (93% for Celex and 86% for Fonilex at\nword level) for single classifiers is boosted significantly with additional\nerror reductions of 31% and 38% respectively using combination of classifiers,\nand a further 5% using combination of meta-learners, bringing overall word\nlevel accuracy to 96% for the Dutch variant and 92% for the Flemish variant. We\nalso show that the application of machine learning methods indeed leads to\nincreased insight into the linguistic regularities determining the variation\nbetween the two pronunciation variants studied.\n",
        "published": "2000-08-18T12:06:36Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008013v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008014v1",
        "title": "Aspects of Pattern-Matching in Data-Oriented Parsing",
        "summary": "  Data-Oriented Parsing (dop) ranks among the best parsing schemes, pairing\nstate-of-the art parsing accuracy to the psycholinguistic insight that larger\nchunks of syntactic structures are relevant grammatical and probabilistic\nunits. Parsing with the dop-model, however, seems to involve a lot of CPU\ncycles and a considerable amount of double work, brought on by the concept of\nmultiple derivations, which is necessary for probabilistic processing, but\nwhich is not convincingly related to a proper linguistic backbone. It is\nhowever possible to re-interpret the dop-model as a pattern-matching model,\nwhich tries to maximize the size of the substructures that construct the parse,\nrather than the probability of the parse. By emphasizing this memory-based\naspect of the dop-model, it is possible to do away with multiple derivations,\nopening up possibilities for efficient Viterbi-style optimizations, while still\nretaining acceptable parsing accuracy through enhanced context-sensitivity.\n",
        "published": "2000-08-18T13:56:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008015v1",
        "title": "Temiar Reduplication in One-Level Prosodic Morphology",
        "summary": "  Temiar reduplication is a difficult piece of prosodic morphology. This paper\npresents the first computational analysis of Temiar reduplication, using the\nnovel finite-state approach of One-Level Prosodic Morphology originally\ndeveloped by Walther (1999b, 2000). After reviewing both the data and the basic\ntenets of One-level Prosodic Morphology, the analysis is laid out in some\ndetail, using the notation of the FSA Utilities finite-state toolkit (van Noord\n1997). One important discovery is that in this approach one can easily define a\nregular expression operator which ambiguously scans a string in the left- or\nrightward direction for a certain prosodic property. This yields an elegant\naccount of base-length-dependent triggering of reduplication as found in\nTemiar.\n",
        "published": "2000-08-18T16:07:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008017v1",
        "title": "Efficient probabilistic top-down and left-corner parsing",
        "summary": "  This paper examines efficient predictive broad-coverage parsing without\ndynamic programming. In contrast to bottom-up methods, depth-first top-down\nparsing produces partial parses that are fully connected trees spanning the\nentire left context, from which any kind of non-local dependency or partial\nsemantic interpretation can in principle be read. We contrast two predictive\nparsing approaches, top-down and left-corner parsing, and find both to be\nviable. In addition, we find that enhancement with non-local information not\nonly improves parser accuracy, but also substantially improves the search\nefficiency.\n",
        "published": "2000-08-21T19:27:18Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008021v1",
        "title": "Compact non-left-recursive grammars using the selective left-corner\n  transform and factoring",
        "summary": "  The left-corner transform removes left-recursion from (probabilistic)\ncontext-free grammars and unification grammars, permitting simple top-down\nparsing techniques to be used. Unfortunately the grammars produced by the\nstandard left-corner transform are usually much larger than the original. The\nselective left-corner transform described in this paper produces a transformed\ngrammar which simulates left-corner recognition of a user-specified set of the\noriginal productions, and top-down recognition of the others. Combined with two\nfactorizations, it produces non-left-recursive grammars that are not much\nlarger than the original.\n",
        "published": "2000-08-22T15:16:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008023v1",
        "title": "Selectional Restrictions in HPSG",
        "summary": "  Selectional restrictions are semantic sortal constraints imposed on the\nparticipants of linguistic constructions to capture contextually-dependent\nconstraints on interpretation. Despite their limitations, selectional\nrestrictions have proven very useful in natural language applications, where\nthey have been used frequently in word sense disambiguation, syntactic\ndisambiguation, and anaphora resolution. Given their practical value, we\nexplore two methods to incorporate selectional restrictions in the HPSG theory,\nassuming that the reader is familiar with HPSG. The first method employs HPSG's\nBackground feature and a constraint-satisfaction component pipe-lined after the\nparser. The second method uses subsorts of referential indices, and blocks\nreadings that violate selectional restrictions during parsing. While\ntheoretically less satisfactory, we have found the second method particularly\nuseful in the development of practical systems.\n",
        "published": "2000-08-23T07:38:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008023v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008024v1",
        "title": "Estimation of Stochastic Attribute-Value Grammars using an Informative\n  Sample",
        "summary": "  We argue that some of the computational complexity associated with estimation\nof stochastic attribute-value grammars can be reduced by training upon an\ninformative subset of the full training set. Results using the parsed Wall\nStreet Journal corpus show that in some circumstances, it is possible to obtain\nbetter estimation results using an informative sample than when training upon\nall the available material. Further experimentation demonstrates that with\nunlexicalised models, a Gaussian Prior can reduce overfitting. However, when\nmodels are lexicalised and contain overlapping features, overfitting does not\nseem to be a problem, and a Gaussian Prior makes minimal difference to\nperformance. Our approach is applicable for situations when there are an\ninfeasibly large number of parses in the training set, or else for when\nrecovery of these parses from a packed representation is itself computationally\nexpensive.\n",
        "published": "2000-08-23T12:38:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008024v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008026v1",
        "title": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon\n  construction",
        "summary": "  Generating semantic lexicons semi-automatically could be a great time saver,\nrelative to creating them by hand. In this paper, we present an algorithm for\nextracting potential entries for a category from an on-line corpus, based upon\na small set of exemplars. Our algorithm finds more correct terms and fewer\nincorrect ones than previous work in this area. Additionally, the entries that\nare generated potentially provide broader coverage of the category than would\noccur to an individual coding them by hand. Our algorithm finds many terms not\nincluded within Wordnet (many more than previous algorithms), and could be\nviewed as an ``enhancer'' of existing broad-coverage resources.\n",
        "published": "2000-08-24T13:28:25Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008027v1",
        "title": "Measuring efficiency in high-accuracy, broad-coverage statistical\n  parsing",
        "summary": "  Very little attention has been paid to the comparison of efficiency between\nhigh accuracy statistical parsers. This paper proposes one machine-independent\nmetric that is general enough to allow comparisons across very different\nparsing architectures. This metric, which we call ``events considered'',\nmeasures the number of ``events'', however they are defined for a particular\nparser, for which a probability must be calculated, in order to find the parse.\nIt is applicable to single-pass or multi-stage parsers. We discuss the\nadvantages of the metric, and demonstrate its usefulness by using it to compare\ntwo parsers which differ in several fundamental ways.\n",
        "published": "2000-08-24T16:38:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008028v1",
        "title": "Estimators for Stochastic ``Unification-Based'' Grammars",
        "summary": "  Log-linear models provide a statistically sound framework for Stochastic\n``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds\nof grammars. We describe two computationally-tractable ways of estimating the\nparameters of such grammars from a training corpus of syntactic analyses, and\napply these to estimate a stochastic version of Lexical-Functional Grammar.\n",
        "published": "2000-08-25T17:23:07Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008029v1",
        "title": "Exploiting auxiliary distributions in stochastic unification-based\n  grammars",
        "summary": "  This paper describes a method for estimating conditional probability\ndistributions over the parses of ``unification-based'' grammars which can\nutilize auxiliary distributions that are estimated by other means. We show how\nthis can be used to incorporate information about lexical selectional\npreferences gathered from other sources into Stochastic ``Unification-based''\nGrammars (SUBGs). While we apply this estimator to a Stochastic\nLexical-Functional Grammar, the method is general, and should be applicable to\nstochastic versions of HPSGs, categorial grammars and transformational\ngrammars.\n",
        "published": "2000-08-25T17:31:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008030v1",
        "title": "Metonymy Interpretation Using X NO Y Examples",
        "summary": "  We developed on example-based method of metonymy interpretation. One\nadvantages of this method is that a hand-built database of metonymy is not\nnecessary because it instead uses examples in the form ``Noun X no Noun Y (Noun\nY of Noun X).'' Another advantage is that we will be able to interpret\nnewly-coined metonymic sentences by using a new corpus. We experimented with\nmetonymy interpretation and obtained a precision rate of 66% when using this\nmethod.\n",
        "published": "2000-08-28T08:10:14Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008031v1",
        "title": "Bunsetsu Identification Using Category-Exclusive Rules",
        "summary": "  This paper describes two new bunsetsu identification methods using supervised\nlearning. Since Japanese syntactic analysis is usually done after bunsetsu\nidentification, bunsetsu identification is important for analyzing Japanese\nsentences. In experiments comparing the four previously available\nmachine-learning methods (decision tree, maximum-entropy method, example-based\napproach and decision list) and two new methods using category-exclusive rules,\nthe new method using the category-exclusive rules with the highest similarity\nperformed best.\n",
        "published": "2000-08-28T08:17:18Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008031v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008032v1",
        "title": "Japanese Probabilistic Information Retrieval Using Location and Category\n  Information",
        "summary": "  Robertson's 2-poisson information retrieve model does not use location and\ncategory information. We constructed a framework using location and category\ninformation in a 2-poisson model. We submitted two systems based on this\nframework to the IREX contest, Japanese language information retrieval contest\nheld in Japan in 1999. For precision in the A-judgement measure they scored\n0.4926 and 0.4827, the highest values among the 15 teams and 22 systems that\nparticipated in the IREX contest. We describe our systems and the comparative\nexperiments done when various parameters were changed. These experiments\nconfirmed the effectiveness of using location and category information.\n",
        "published": "2000-08-28T08:27:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008032v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008033v1",
        "title": "Temporal Expressions in Japanese-to-English Machine Translation",
        "summary": "  This paper describes in outline a method for translating Japanese temporal\nexpressions into English. We argue that temporal expressions form a special\nsubset of language that is best handled as a special module in machine\ntranslation. The paper deals with problems of lexical idiosyncrasy as well as\nthe choice of articles and prepositions within temporal expressions. In\naddition temporal expressions are considered as parts of larger structures, and\nthe question of whether to translate them as noun phrases or adverbials is\naddressed.\n",
        "published": "2000-08-28T19:51:32Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008033v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008034v1",
        "title": "Lexicalized Stochastic Modeling of Constraint-Based Grammars using\n  Log-Linear Measures and EM Training",
        "summary": "  We present a new approach to stochastic modeling of constraint-based grammars\nthat is based on log-linear models and uses EM for estimation from unannotated\ndata. The techniques are applied to an LFG grammar for German. Evaluation on an\nexact match task yields 86% precision for an ambiguity rate of 5.4, and 90%\nprecision on a subcat frame match for an ambiguity rate of 25. Experimental\ncomparison to training from a parsebank shows a 10% gain from EM training.\nAlso, a new class-based grammar lexicalization is presented, showing a 10% gain\nover unlexicalized models.\n",
        "published": "2000-08-30T13:14:58Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008034v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008035v1",
        "title": "Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity\n  Resolution",
        "summary": "  This paper presents the use of probabilistic class-based lexica for\ndisambiguation in target-word selection. Our method employs minimal but precise\ncontextual information for disambiguation. That is, only information provided\nby the target-verb, enriched by the condensed information of a probabilistic\nclass-based lexicon, is used. Induction of classes and fine-tuning to verbal\narguments is done in an unsupervised manner by EM-based clustering techniques.\nThe method shows promising results in an evaluation on real-world translations.\n",
        "published": "2000-08-30T13:24:06Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0008036v1",
        "title": "Probabilistic Constraint Logic Programming. Formal Foundations of\n  Quantitative and Statistical Inference in Constraint-Based Natural Language\n  Processing",
        "summary": "  In this thesis, we present two approaches to a rigorous mathematical and\nalgorithmic foundation of quantitative and statistical inference in\nconstraint-based natural language processing. The first approach, called\nquantitative constraint logic programming, is conceptualized in a clear logical\nframework, and presents a sound and complete system of quantitative inference\nfor definite clauses annotated with subjective weights. This approach combines\na rigorous formal semantics for quantitative inference based on subjective\nweights with efficient weight-based pruning for constraint-based systems. The\nsecond approach, called probabilistic constraint logic programming, introduces\na log-linear probability distribution on the proof trees of a constraint logic\nprogram and an algorithm for statistical inference of the parameters and\nproperties of such probability models from incomplete, i.e., unparsed data. The\npossibility of defining arbitrary properties of proof trees as properties of\nthe log-linear probability model and efficiently estimating appropriate\nparameter values for them permits the probabilistic modeling of arbitrary\ncontext-dependencies in constraint logic programs. The usefulness of these\nideas is evaluated empirically in a small-scale experiment on finding the\ncorrect parses of a constraint-based grammar. In addition, we address the\nproblem of computational intractability of the calculation of expectations in\nthe inference task and present various techniques to approximately solve this\ntask. Moreover, we present an approximate heuristic technique for searching for\nthe most probable analysis in probabilistic constraint logic programs.\n",
        "published": "2000-08-30T13:57:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0008036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009003v1",
        "title": "Automatic Extraction of Subcategorization Frames for Czech",
        "summary": "  We present some novel machine learning techniques for the identification of\nsubcategorization information for verbs in Czech. We compare three different\nstatistical techniques applied to this problem. We show how the learning\nalgorithm can be used to discover previously unknown subcategorization frames\nfrom the Czech Prague Dependency Treebank. The algorithm can then be used to\nlabel dependents of a verb in the Czech treebank as either arguments or\nadjuncts. Using our techniques, we ar able to achieve 88% precision on unseen\nparsed text.\n",
        "published": "2000-09-08T15:48:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009008v1",
        "title": "Introduction to the CoNLL-2000 Shared Task: Chunking",
        "summary": "  We describe the CoNLL-2000 shared task: dividing text into syntactically\nrelated non-overlapping groups of words, so-called text chunking. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance.\n",
        "published": "2000-09-18T12:08:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009011v1",
        "title": "Anaphora Resolution in Japanese Sentences Using Surface Expressions and\n  Examples",
        "summary": "  Anaphora resolution is one of the major problems in natural language\nprocessing. It is also one of the important tasks in machine translation and\nman/machine dialogue. We solve the problem by using surface expressions and\nexamples. Surface expressions are the words in sentences which provide clues\nfor anaphora resolution. Examples are linguistic data which are actually used\nin conversations and texts. The method using surface expressions and examples\nis a practical method. This thesis handles almost all kinds of anaphora: i. The\nreferential property and number of a noun phrase ii. Noun phrase direct\nanaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase\nellipsis\n",
        "published": "2000-09-19T00:44:47Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009011v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009015v1",
        "title": "A Tableaux Calculus for Ambiguous Quantification",
        "summary": "  Coping with ambiguity has recently received a lot of attention in natural\nlanguage processing. Most work focuses on the semantic representation of\nambiguous expressions. In this paper we complement this work in two ways.\nFirst, we provide an entailment relation for a language with ambiguous\nexpressions. Second, we give a sound and complete tableaux calculus for\nreasoning with statements involving ambiguous quantification. The calculus\ninterleaves partial disambiguation steps with steps in a traditional deductive\nprocess, so as to minimize and postpone branching in the proof process, and\nthereby increases its efficiency.\n",
        "published": "2000-09-20T13:23:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009025v1",
        "title": "Parsing with the Shortest Derivation",
        "summary": "  Common wisdom has it that the bias of stochastic grammars in favor of shorter\nderivations of a sentence is harmful and should be redressed. We show that the\ncommon wisdom is wrong for stochastic grammars that use elementary trees\ninstead of context-free rules, such as Stochastic Tree-Substitution Grammars\nused by Data-Oriented Parsing models. For such grammars a non-probabilistic\nmetric based on the shortest derivation outperforms a probabilistic metric on\nthe ATIS and OVIS corpora, while it obtains very competitive results on the\nWall Street Journal corpus. This paper also contains the first published\nexperiments with DOP on the Wall Street Journal.\n",
        "published": "2000-09-27T13:22:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0009026v1",
        "title": "An improved parser for data-oriented lexical-functional analysis",
        "summary": "  We present an LFG-DOP parser which uses fragments from LFG-annotated\nsentences to parse new sentences. Experiments with the Verbmobil and Homecentre\ncorpora show that (1) Viterbi n best search performs about 100 times faster\nthan Monte Carlo search while both achieve the same accuracy; (2) the DOP\nhypothesis which states that parse accuracy increases with increasing fragment\nsize is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator\nperforms worse than a discounted frequency estimator; and (4) LFG-DOP\nsignificantly outperforms Tree-DOP is evaluated on tree structures only.\n",
        "published": "2000-09-27T13:38:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0009026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010012v1",
        "title": "Finding consensus in speech recognition: word error minimization and\n  other applications of confusion networks",
        "summary": "  We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources.\n",
        "published": "2000-10-07T02:24:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010020v1",
        "title": "Using existing systems to supplement small amounts of annotated\n  grammatical relations training data",
        "summary": "  Grammatical relationships (GRs) form an important level of natural language\nprocessing, but different sets of GRs are useful for different purposes.\nTherefore, one may often only have time to obtain a small training corpus with\nthe desired GR annotations. To boost the performance from using such a small\ntraining corpus on a transformation rule learner, we use existing systems that\nfind related types of annotations.\n",
        "published": "2000-10-11T22:30:09Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010024v1",
        "title": "Exploring automatic word sense disambiguation with decision lists and\n  the Web",
        "summary": "  The most effective paradigm for word sense disambiguation, supervised\nlearning, seems to be stuck because of the knowledge acquisition bottleneck. In\nthis paper we take an in-depth study of the performance of decision lists on\ntwo publicly available corpora and an additional corpus automatically acquired\nfrom the Web, using the fine-grained highly polysemous senses in WordNet.\nDecision lists are shown a versatile state-of-the-art technique. The\nexperiments reveal, among other facts, that SemCor can be an acceptable (0.7\nprecision for polysemous words) starting point for an all-words system. The\nresults on the DSO corpus show that for some highly polysemous words 0.7\nprecision seems to be the current state-of-the-art limit. On the other hand,\nindependently constructed hand-tagged corpora are not mutually useful, and a\ncorpus automatically acquired from the Web is shown to fail.\n",
        "published": "2000-10-17T08:31:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010024v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010025v1",
        "title": "Extraction of semantic relations from a Basque monolingual dictionary\n  using Constraint Grammar",
        "summary": "  This paper deals with the exploitation of dictionaries for the semi-automatic\nconstruction of lexicons and lexical knowledge bases. The final goal of our\nresearch is to enrich the Basque Lexical Database with semantic information\nsuch as senses, definitions, semantic relations, etc., extracted from a Basque\nmonolingual dictionary. The work here presented focuses on the extraction of\nthe semantic relations that best characterise the headword, that is, those of\nsynonymy, antonymy, hypernymy, and other relations marked by specific relators\nand derivation. All nominal, verbal and adjectival entries were treated. Basque\nuses morphological inflection to mark case, and therefore semantic relations\nhave to be inferred from suffixes rather than from prepositions. Our approach\ncombines a morphological analyser and surface syntax parsing (based on\nConstraint Grammar), and has proven very successful for highly inflected\nlanguages such as Basque. Both the effort to write the rules and the actual\nprocessing time of the dictionary have been very low. At present we have\nextracted 42,533 relations, leaving only 2,943 (9%) definitions without any\nextracted relation. The error rate is extremely low, as only 2.2% of the\nextracted relations are wrong.\n",
        "published": "2000-10-17T09:05:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010026v1",
        "title": "Enriching very large ontologies using the WWW",
        "summary": "  This paper explores the possibility to exploit text on the world wide web in\norder to enrich the concepts in existing ontologies. First, a method to\nretrieve documents from the WWW related to a concept is described. These\ndocument collections are used 1) to construct topic signatures (lists of\ntopically related words) for each concept in WordNet, and 2) to build\nhierarchical clusters of the concepts (the word senses) that lexicalize a given\nword. The overall goal is to overcome two shortcomings of WordNet: the lack of\ntopical links among concepts, and the proliferation of senses. Topic signatures\nare validated on a word sense disambiguation task with good results, which are\nimproved when the hierarchical clusters are used.\n",
        "published": "2000-10-17T09:56:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010027v1",
        "title": "One Sense per Collocation and Genre/Topic Variations",
        "summary": "  This paper revisits the one sense per collocation hypothesis using\nfine-grained sense distinctions and two different corpora. We show that the\nhypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported\nearlier on 2-way ambiguities). We also show that one sense per collocation does\nhold across corpora, but that collocations vary from one corpus to the other,\nfollowing genre and topic variations. This explains the low results when\nperforming word sense disambiguation across corpora. In fact, we demonstrate\nthat when two independent corpora share a related genre/topic, the word sense\ndisambiguation results would be better. Future work on word sense\ndisambiguation will have to take into account genre and topic as important\nparameters on their models.\n",
        "published": "2000-10-17T10:26:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0010030v1",
        "title": "Reduction of Intermediate Alphabets in Finite-State Transducer Cascades",
        "summary": "  This article describes an algorithm for reducing the intermediate alphabets\nin cascades of finite-state transducers (FSTs). Although the method modifies\nthe component FSTs, there is no change in the overall relation described by the\nwhole cascade. No additional information or special algorithm, that could\ndecelerate the processing of input, is required at runtime. Two examples from\nNatural Language Processing are used to illustrate the effect of the algorithm\non the sizes of the FSTs and their alphabets. With some FSTs the number of arcs\nand symbols shrank considerably.\n",
        "published": "2000-10-23T15:14:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0010030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011001v1",
        "title": "Utilizing the World Wide Web as an Encyclopedia: Extracting Term\n  Descriptions from Semi-Structured Texts",
        "summary": "  In this paper, we propose a method to extract descriptions of technical terms\nfrom Web pages in order to utilize the World Wide Web as an encyclopedia. We\nuse linguistic patterns and HTML text structures to extract text fragments\ncontaining term descriptions. We also use a language model to discard\nextraneous descriptions, and a clustering method to summarize resultant\ndescriptions. We show the effectiveness of our method by way of experiments.\n",
        "published": "2000-11-02T09:02:12Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011001v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011002v1",
        "title": "A Novelty-based Evaluation Method for Information Retrieval",
        "summary": "  In information retrieval research, precision and recall have long been used\nto evaluate IR systems. However, given that a number of retrieval systems\nresembling one another are already available to the public, it is valuable to\nretrieve novel relevant documents, i.e., documents that cannot be retrieved by\nthose existing systems. In view of this problem, we propose an evaluation\nmethod that favors systems retrieving as many novel documents as possible. We\nalso used our method to evaluate systems that participated in the IREX\nworkshop.\n",
        "published": "2000-11-02T10:00:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011003v1",
        "title": "Applying Machine Translation to Two-Stage Cross-Language Information\n  Retrieval",
        "summary": "  Cross-language information retrieval (CLIR), where queries and documents are\nin different languages, needs a translation of queries and/or documents, so as\nto standardize both of them into a common representation. For this purpose, the\nuse of machine translation is an effective approach. However, computational\ncost is prohibitive in translating large-scale document collections. To resolve\nthis problem, we propose a two-stage CLIR method. First, we translate a given\nquery into the document language, and retrieve a limited number of foreign\ndocuments. Second, we machine translate only those documents into the user\nlanguage, and re-rank them based on the translation result. We also show the\neffectiveness of our method by way of experiments using Japanese queries and\nEnglish technical documents.\n",
        "published": "2000-11-02T10:32:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011020v2",
        "title": "The Use of Instrumentation in Grammar Engineering",
        "summary": "  This paper explores the usefulness of a technique from software engineering,\ncode instrumentation, for the development of large-scale natural language\ngrammars. Information about the usage of grammar rules in test and corpus\nsentences is used to improve grammar and testsuite, as well as adapting a\ngrammar to a specific genre. Results show that less than half of a\nlarge-coverage grammar for German is actually tested by two large testsuites,\nand that 10--30% of testing time is redundant. This methodology applied can be\nseen as a re-use of grammar writing knowledge for testsuite compilation.\n",
        "published": "2000-11-16T17:40:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011020v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011034v1",
        "title": "Semantic interpretation of temporal information by abductive inference",
        "summary": "  Besides temporal information explicitly available in verbs and adjuncts, the\ntemporal interpretation of a text also depends on general world knowledge and\ndefault assumptions. We will present a theory for describing the relation\nbetween, on the one hand, verbs, their tenses and adjuncts and, on the other,\nthe eventualities and periods of time they represent and their relative\ntemporal locations.\n  The theory is formulated in logic and is a practical implementation of the\nconcepts described in Ness Schelkens et al. We will show how an abductive\nresolution procedure can be used on this representation to extract temporal\ninformation from texts.\n",
        "published": "2000-11-22T15:35:46Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011034v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011035v1",
        "title": "Abductive reasoning with temporal information",
        "summary": "  Texts in natural language contain a lot of temporal information, both\nexplicit and implicit. Verbs and temporal adjuncts carry most of the explicit\ninformation, but for a full understanding general world knowledge and default\nassumptions have to be taken into account. We will present a theory for\ndescribing the relation between, on the one hand, verbs, their tenses and\nadjuncts and, on the other, the eventualities and periods of time they\nrepresent and their relative temporal locations, while allowing interaction\nwith general world knowledge.\n  The theory is formulated in an extension of first order logic and is a\npractical implementation of the concepts described in Van Eynde 2001 and\nSchelkens et al. 2000. We will show how an abductive resolution procedure can\nbe used on this representation to extract temporal information from texts. The\ntheory presented here is an extension of that in Verdoolaege et al. 2000,\nadapted to VanEynde 2001, with a simplified and extended analysis of adjuncts\nand with more emphasis on how a model can be constructed.\n",
        "published": "2000-11-23T08:41:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0011040v1",
        "title": "Do All Fragments Count?",
        "summary": "  We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank. We isolate a number of dependency relations which\nprevious models neglect but which contribute to higher parse accuracy.\n",
        "published": "2000-11-24T23:51:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0011040v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0102020v1",
        "title": "Multi-Syllable Phonotactic Modelling",
        "summary": "  This paper describes a novel approach to constructing phonotactic models. The\nunderlying theoretical approach to phonological description is the\nmultisyllable approach in which multiple syllable classes are defined that\nreflect phonotactically idiosyncratic syllable subcategories. A new\nfinite-state formalism, OFS Modelling, is used as a tool for encoding,\nautomatically constructing and generalising phonotactic descriptions.\nLanguage-independent prototype models are constructed which are instantiated on\nthe basis of data sets of phonological strings, and generalised with a\nclustering algorithm. The resulting approach enables the automatic construction\nof phonotactic models that encode arbitrarily close approximations of a\nlanguage's set of attested phonological forms. The approach is applied to the\nconstruction of multi-syllable word-level phonotactic models for German,\nEnglish and Dutch.\n",
        "published": "2001-02-22T21:05:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0102020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0102021v1",
        "title": "Taking Primitive Optimality Theory Beyond the Finite State",
        "summary": "  Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a\ncomputational model of Optimality Theory (Prince and Smolensky, 1993), employs\na finite state machine to represent the set of active candidates at each stage\nof an Optimality Theoretic derivation, as well as weighted finite state\nmachines to represent the constraints themselves. For some purposes, however,\nit would be convenient if the set of candidates were limited by some set of\ncriteria capable of being described only in a higher-level grammar formalism,\nsuch as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple\nContext Free Grammar (Seki et al., 1991). Examples include reduplication and\nphrasal stress models. Here we introduce a mechanism for OTP-like Optimality\nTheory in which the constraints remain weighted finite state machines, but sets\nof candidates are represented by higher-level grammars. In particular, we use\nmultiple context-free grammars to model reduplication in the manner of\nCorrespondence Theory (McCarthy and Prince, 1995), and develop an extended\nversion of the Earley Algorithm (Earley, 1970) to apply the constraints to a\nreduplicating candidate set.\n",
        "published": "2001-02-22T13:09:25Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0102021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0102022v2",
        "title": "Finite-State Phonology: Proceedings of the 5th Workshop of the ACL\n  Special Interest Group in Computational Phonology (SIGPHON)",
        "summary": "  Home page of the workshop proceedings, with pointers to the individually\narchived papers. Includes front matter from the printed version of the\nproceedings.\n",
        "published": "2001-02-22T14:10:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0102022v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0102026v1",
        "title": "Mathematical Model of Word Length on the Basis of the Cebanov-Fucks\n  Distribution with Uniform Parameter Distribution",
        "summary": "  The data on 13 typologically different languages have been processed using a\ntwo-parameter word length model, based on 1-displaced uniform Poisson\ndistribution. Statistical dependencies of the 2nd parameter on the 1st one are\nrevealed for the German texts and genre of letters.\n",
        "published": "2001-02-24T06:56:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0102026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103007v1",
        "title": "Two-parameter Model of Word Length \"Language - Genre\"",
        "summary": "  A two-parameter model of word length measured by the number of syllables\ncomprising it is proposed. The first parameter is dependent on language type,\nthe second one - on text genre and reflects the degree of completion of\nsynergetic processes of language optimization.\n",
        "published": "2001-03-08T06:30:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103010v1",
        "title": "Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition\n  in Japanese and English Sentences",
        "summary": "  George A. Miller said that human beings have only seven chunks in short-term\nmemory, plus or minus two. We counted the number of bunsetsus (phrases) whose\nmodifiees are undetermined in each step of an analysis of the dependency\nstructure of Japanese sentences, and which therefore must be stored in\nshort-term memory. The number was roughly less than nine, the upper bound of\nseven plus or minus two. We also obtained similar results with English\nsentences under the assumption that human beings recognize a series of words,\nsuch as a noun phrase (NP), as a unit. This indicates that if we assume that\nthe human cognitive units in Japanese and English are bunsetsu and NP\nrespectively, analysis will support Miller's $7 \\pm 2$ theory.\n",
        "published": "2001-03-12T08:23:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103010v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103011v1",
        "title": "A Machine-Learning Approach to Estimating the Referential Properties of\n  Japanese Noun Phrases",
        "summary": "  The referential properties of noun phrases in the Japanese language, which\nhas no articles, are useful for article generation in Japanese-English machine\ntranslation and for anaphora resolution in Japanese noun phrases. They are\ngenerally classified as generic noun phrases, definite noun phrases, and\nindefinite noun phrases. In the previous work, referential properties were\nestimated by developing rules that used clue words. If two or more rules were\nin conflict with each other, the category having the maximum total score given\nby the rules was selected as the desired category. The score given by each rule\nwas established by hand, so the manpower cost was high. In this work, we\nautomatically adjusted these scores by using a machine-learning method and\nsucceeded in reducing the amount of manpower needed to adjust these scores.\n",
        "published": "2001-03-12T08:29:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103011v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103012v1",
        "title": "Meaning Sort - Three examples: dictionary construction, tagged corpus\n  construction, and information presentation system",
        "summary": "  It is often useful to sort words into an order that reflects relations among\ntheir meanings as obtained by using a thesaurus. In this paper, we introduce a\nmethod of arranging words semantically by using several types of `{\\sf is-a}'\nthesauri and a multi-dimensional thesaurus. We also describe three major\napplications where a meaning sort is useful and show the effectiveness of a\nmeaning sort. Since there is no doubt that a word list in meaning-order is\neasier to use than a word list in some random order, a meaning sort, which can\neasily produce a word list in meaning-order, must be useful and effective.\n",
        "published": "2001-03-12T08:43:11Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103013v1",
        "title": "CRL at Ntcir2",
        "summary": "  We have developed systems of two types for NTCIR2. One is an enhenced version\nof the system we developed for NTCIR1 and IREX. It submitted retrieval results\nfor JJ and CC tasks. A variety of parameters were tried with the system. It\nused such characteristics of newspapers as locational information in the CC\ntasks. The system got good results for both of the tasks. The other system is a\nportable system which avoids free parameters as much as possible. The system\nsubmitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system\nautomatically determined the number of top documents and the weight of the\noriginal query used in automatic-feedback retrieval. It also determined\nrelevant terms quite robustly. For EJ and JE tasks, it used document expansion\nto augment the initial queries. It achieved good results, except on the CC\ntasks.\n",
        "published": "2001-03-12T09:36:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103013v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0103026v1",
        "title": "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense",
        "summary": "  This paper presents a corpus-based approach to word sense disambiguation\nwhere a decision tree assigns a sense to an ambiguous word based on the bigrams\nthat occur nearby. This approach is evaluated using the sense-tagged corpora\nfrom the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate\nthan the average results reported for 30 of 36 words, and is more accurate than\nthe best results for 19 of 36 words.\n",
        "published": "2001-03-29T23:08:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0103026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0104010v1",
        "title": "Type Arithmetics: Computation based on the theory of types",
        "summary": "  The present paper shows meta-programming turn programming, which is rich\nenough to express arbitrary arithmetic computations. We demonstrate a type\nsystem that implements Peano arithmetics, slightly generalized to negative\nnumbers. Certain types in this system denote numerals. Arithmetic operations on\nsuch types-numerals - addition, subtraction, and even division - are expressed\nas type reduction rules executed by a compiler. A remarkable trait is that\ndivision by zero becomes a type error - and reported as such by a compiler.\n",
        "published": "2001-04-03T23:22:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0104010v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0104019v1",
        "title": "Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based\n  Adaptation",
        "summary": "  This paper presents a novel method of generating and applying hierarchical,\ndynamic topic-based language models. It proposes and evaluates new cluster\ngeneration, hierarchical smoothing and adaptive topic-probability estimation\ntechniques. These combined models help capture long-distance lexical\ndependencies. Experiments on the Broadcast News corpus show significant\nimprovement in perplexity (10.5% overall and 33.5% on target vocabulary).\n",
        "published": "2001-04-27T22:50:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0104019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0104022v1",
        "title": "Microplanning with Communicative Intentions: The SPUD System",
        "summary": "  The process of microplanning encompasses a range of problems in Natural\nLanguage Generation (NLG), such as referring expression generation, lexical\nchoice, and aggregation, problems in which a generator must bridge underlying\ndomain-specific representations and general linguistic representations. In this\npaper, we describe a uniform approach to microplanning based on declarative\nrepresentations of a generator's communicative intent. These representations\ndescribe the results of NLG: communicative intent associates the concrete\nlinguistic structure planned by the generator with inferences that show how the\nmeaning of that structure communicates needed information about some\napplication domain in the current discourse context. Our approach, implemented\nin the SPUD (sentence planning using description) microplanner, uses the\nlexicalized tree-adjoining grammar formalism (LTAG) to connect structure to\nmeaning and uses modal logic programming to connect meaning to context. At the\nsame time, communicative intent representations provide a resource for the\nprocess of NLG. Using representations of communicative intent, a generator can\naugment the syntax, semantics and pragmatics of an incomplete sentence\nsimultaneously, and can assess its progress on the various problems of\nmicroplanning incrementally. The declarative formulation of communicative\nintent translates into a well-defined methodology for designing grammatical and\nconceptual resources which the generator can use to achieve desired\nmicroplanning behavior in a specified domain.\n",
        "published": "2001-04-30T15:12:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0104022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105001v1",
        "title": "Correction of Errors in a Modality Corpus Used for Machine Translation\n  by Using Machine-learning Method",
        "summary": "  We performed corpus correction on a modality corpus for machine translation\nby using such machine-learning methods as the maximum-entropy method. We thus\nconstructed a high-quality modality corpus based on corpus correction. We\ncompared several kinds of methods for corpus correction in our experiments and\ndeveloped a good method for corpus correction.\n",
        "published": "2001-05-02T05:29:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105001v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105002v1",
        "title": "Man [and Woman] vs. Machine: A Case Study in Base Noun Phrase Learning",
        "summary": "  A great deal of work has been done demonstrating the ability of machine\nlearning algorithms to automatically extract linguistic knowledge from\nannotated corpora. Very little work has gone into quantifying the difference in\nability at this task between a person and a machine. This paper is a first step\nin that direction.\n",
        "published": "2001-05-02T07:49:14Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105005v1",
        "title": "A Complete WordNet1.5 to WordNet1.6 Mapping",
        "summary": "  We describe a robust approach for linking already existing lexical/semantic\nhierarchies. We use a constraint satisfaction algorithm (relaxation labelling)\nto select --among a set of candidates-- the node in a target taxonomy that\nbests matches each node in a source taxonomy. In this paper we present the\ncomplete mapping of the nominal, verbal, adjectival and adverbial parts of\nWordNet 1.5 onto WordNet 1.6.\n",
        "published": "2001-05-04T08:55:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105012v1",
        "title": "Joint and conditional estimation of tagging and parsing models",
        "summary": "  This paper compares two different ways of estimating statistical language\nmodels. Many statistical NLP tagging and parsing models are estimated by\nmaximizing the (joint) likelihood of the fully-observed training data. However,\nsince these applications only require the conditional probability\ndistributions, these distributions can in principle be learnt by maximizing the\nconditional likelihood of the training data. Perhaps somewhat surprisingly,\nmodels estimated by maximizing the joint were superior to models estimated by\nmaximizing the conditional, even though some of the latter models intuitively\nhad access to ``more information''.\n",
        "published": "2001-05-07T14:25:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105012v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105016v1",
        "title": "Probabilistic top-down parsing and language modeling",
        "summary": "  This paper describes the functioning of a broad-coverage probabilistic\ntop-down parser, and its application to the problem of language modeling for\nspeech recognition. The paper first introduces key notions in language modeling\nand probabilistic parsing, and briefly reviews some previous approaches to\nusing syntactic structure for language modeling. A lexicalized probabilistic\ntop-down parser is then presented, which performs very well, in terms of both\nthe accuracy of returned parses and the efficiency with which they are found,\nrelative to the best broad-coverage statistical parsers. A new language model\nwhich utilizes probabilistic top-down parsing is then outlined, and empirical\nresults show that it improves upon previous work in test corpus perplexity.\nInterpolation with a trigram model yields an exceptional improvement relative\nto the improvement observed by other models, demonstrating the degree to which\nthe information captured by our parsing model is orthogonal to that captured by\na trigram model. A small recognition experiment also demonstrates the utility\nof the model.\n",
        "published": "2001-05-08T15:35:07Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105019v1",
        "title": "Robust Probabilistic Predictive Syntactic Processing",
        "summary": "  This thesis presents a broad-coverage probabilistic top-down parser, and its\napplication to the problem of language modeling for speech recognition. The\nparser builds fully connected derivations incrementally, in a single pass from\nleft-to-right across the string. We argue that the parsing approach that we\nhave adopted is well-motivated from a psycholinguistic perspective, as a model\nthat captures probabilistic dependencies between lexical items, as part of the\nprocess of building connected syntactic structures. The basic parser and\nconditional probability models are presented, and empirical results are\nprovided for its parsing accuracy on both newspaper text and spontaneous\ntelephone conversations. Modifications to the probability model are presented\nthat lead to improved performance. A new language model which uses the output\nof the parser is then defined. Perplexity and word error rate reduction are\ndemonstrated over trigram models, even when the trigram is trained on\nsignificantly more data. Interpolation on a word-by-word basis with a trigram\nmodel yields additional improvements.\n",
        "published": "2001-05-09T17:01:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105023v1",
        "title": "Generating a 3D Simulation of a Car Accident from a Written Description\n  in Natural Language: the CarSim System",
        "summary": "  This paper describes a prototype system to visualize and animate 3D scenes\nfrom car accident reports, written in French. The problem of generating such a\n3D simulation can be divided into two subtasks: the linguistic analysis and the\nvirtual scene generation. As a means of communication between these two\nmodules, we first designed a template formalism to represent a written accident\nreport. The CarSim system first processes written reports, gathers relevant\ninformation, and converts it into a formal description. Then, it creates the\ncorresponding 3D scene and animates the vehicles.\n",
        "published": "2001-05-14T09:05:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105023v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105035v1",
        "title": "Historical Dynamics of Lexical System as Random Walk Process",
        "summary": "  It is offered to consider word meanings changes in diachrony as\nsemicontinuous random walk with reflecting and swallowing screens. The basic\ncharacteristics of word life cycle are defined. Verification of the model has\nbeen realized on the data of Russian words distribution on various age periods.\n",
        "published": "2001-05-30T03:55:23Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0105037v1",
        "title": "Integrating Prosodic and Lexical Cues for Automatic Topic Segmentation",
        "summary": "  We present a probabilistic model that uses both prosodic and lexical cues for\nthe automatic segmentation of speech into topically coherent units. We propose\ntwo methods for combining lexical and prosodic information using hidden Markov\nmodels and decision trees. Lexical information is obtained from a speech\nrecognizer, and prosodic features are extracted automatically from speech\nwaveforms. We evaluate our approach on the Broadcast News corpus, using the\nDARPA-TDT evaluation metrics. Results show that the prosodic model alone is\ncompetitive with word-based segmentation methods. Furthermore, we achieve a\nsignificant reduction in error by combining the prosodic and word-based\nknowledge sources.\n",
        "published": "2001-05-31T18:08:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0105037v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0106015v1",
        "title": "Organizing Encyclopedic Knowledge based on the Web and its Application\n  to Question Answering",
        "summary": "  We propose a method to generate large-scale encyclopedic knowledge, which is\nvaluable for much NLP research, based on the Web. We first search the Web for\npages containing a term in question. Then we use linguistic patterns and HTML\nstructures to extract text fragments describing the term. Finally, we organize\nextracted term descriptions based on word senses and domains. In addition, we\napply an automatically generated encyclopedia to a question answering system\ntargeting the Japanese Information-Technology Engineers Examination.\n",
        "published": "2001-06-10T06:14:09Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0106015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0106043v1",
        "title": "Using the Distribution of Performance for Studying Statistical NLP\n  Systems and Corpora",
        "summary": "  Statistical NLP systems are frequently evaluated and compared on the basis of\ntheir performances on a single split of training and test data. Results\nobtained using a single split are, however, subject to sampling noise. In this\npaper we argue in favour of reporting a distribution of performance figures,\nobtained by resampling the training data, rather than a single number. The\nadditional information from distributions can be used to make statistically\nquantified statements about differences across parameter settings, systems, and\ncorpora.\n",
        "published": "2001-06-20T14:16:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0106043v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0106047v1",
        "title": "Modeling informational novelty in a conversational system with a hybrid\n  statistical and grammar-based approach to natural language generation",
        "summary": "  We present a hybrid statistical and grammar-based system for surface natural\nlanguage generation (NLG) that uses grammar rules, conditions on using those\ngrammar rules, and corpus statistics to determine the word order. We also\ndescribe how this surface NLG module is implemented in a prototype\nconversational system, and how it attempts to model informational novelty by\nvarying the word order. Using a combination of rules and statistical\ninformation, the conversational system expresses the novel information\ndifferently than the given information, based on the run-time dialog state. We\nalso discuss our plans for evaluating the generation strategy.\n",
        "published": "2001-06-21T20:37:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0106047v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107005v1",
        "title": "The Role of Conceptual Relations in Word Sense Disambiguation",
        "summary": "  We explore many ways of using conceptual distance measures in Word Sense\nDisambiguation, starting with the Agirre-Rigau conceptual density measure. We\nuse a generalized form of this measure, introducing many (parameterized)\nrefinements and performing an exhaustive evaluation of all meaningful\ncombinations. We finally obtain a 42% improvement over the original algorithm,\nand show that measures of conceptual distance are not worse indicators for\nsense disambiguation than measures based on word-coocurrence (exemplified by\nthe Lesk algorithm). Our results, however, reinforce the idea that only a\ncombination of different sources of knowledge might eventually lead to accurate\nword sense disambiguation.\n",
        "published": "2001-07-03T10:27:44Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107006v1",
        "title": "Looking Under the Hood : Tools for Diagnosing your Question Answering\n  Engine",
        "summary": "  In this paper we analyze two question answering tasks : the TREC-8 question\nanswering task and a set of reading comprehension exams. First, we show that\nQ/A systems perform better when there are multiple answer opportunities per\nquestion. Next, we analyze common approaches to two subproblems: term overlap\nfor answer sentence identification, and answer typing for short answer\nextraction. We present general tools for analyzing the strengths and\nlimitations of techniques for these subproblems. Our results quantify the\nlimitations of both term overlap and answer typing to distinguish between\ncompeting answer candidates.\n",
        "published": "2001-07-03T18:06:05Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107016v1",
        "title": "Introduction to the CoNLL-2001 Shared Task: Clause Identification",
        "summary": "  We describe the CoNLL-2001 shared task: dividing text into clauses. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance.\n",
        "published": "2001-07-15T12:51:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107017v1",
        "title": "Learning Computational Grammars",
        "summary": "  This paper reports on the \"Learning Computational Grammars\" (LCG) project, a\npostdoc network devoted to studying the application of machine learning\ntechniques to grammars suitable for computational use. We were interested in a\nmore systematic survey to understand the relevance of many factors to the\nsuccess of learning, esp. the availability of annotated data, the kind of\ndependencies in the data, and the availability of knowledge bases (grammars).\nWe focused on syntax, esp. noun phrase (NP) syntax.\n",
        "published": "2001-07-15T13:21:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107018v1",
        "title": "Combining a self-organising map with memory-based learning",
        "summary": "  Memory-based learning (MBL) has enjoyed considerable success in corpus-based\nnatural language processing (NLP) tasks and is thus a reliable method of\ngetting a high-level of performance when building corpus-based NLP systems.\nHowever there is a bottleneck in MBL whereby any novel testing item has to be\ncompared against all the training items in memory base. For this reason there\nhas been some interest in various forms of memory editing whereby some method\nof selecting a subset of the memory base is employed to reduce the number of\ncomparisons. This paper investigates the use of a modified self-organising map\n(SOM) to select a subset of the memory items for comparison. This method\ninvolves reducing the number of comparisons to a value proportional to the\nsquare root of the number of training items. The method is tested on the\nidentification of base noun-phrases in the Wall Street Journal corpus, using\nsections 15 to 18 for training and section 20 for testing.\n",
        "published": "2001-07-15T13:32:36Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107018v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107019v2",
        "title": "Applying Natural Language Generation to Indicative Summarization",
        "summary": "  The task of creating indicative summaries that help a searcher decide whether\nto read a particular document is a difficult task. This paper examines the\nindicative summarization task from a generation perspective, by first analyzing\nits required content via published guidelines and corpus analysis. We show how\nthese summaries can be factored into a set of document features, and how an\nimplemented content planner uses the topicality document feature to create\nindicative multidocument query-based summaries.\n",
        "published": "2001-07-16T22:59:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107019v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107020v1",
        "title": "Transformation-Based Learning in the Fast Lane",
        "summary": "  Transformation-based learning has been successfully employed to solve many\nnatural language processing problems. It achieves state-of-the-art performance\non many natural language processing tasks and does not overtrain easily.\nHowever, it does have a serious drawback: the training time is often\nintorelably long, especially on the large corpora which are often used in NLP.\nIn this paper, we present a novel and realistic method for speeding up the\ntraining time of a transformation-based learner without sacrificing\nperformance. The paper compares and contrasts the training time needed and\nperformance achieved by our modified learner with two other systems: a standard\ntransformation-based learner, and the ICA system \\cite{hepple00:tbl}. The\nresults of these experiments show that our system is able to achieve a\nsignificant improvement in training time while still achieving the same\nperformance as a standard transformation-based learner. This is a valuable\ncontribution to systems and algorithms which utilize transformation-based\nlearning at any part of the execution.\n",
        "published": "2001-07-17T15:26:13Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0107021v1",
        "title": "Multidimensional Transformation-Based Learning",
        "summary": "  This paper presents a novel method that allows a machine learning algorithm\nfollowing the transformation-based learning paradigm \\cite{brill95:tagging} to\nbe applied to multiple classification tasks by training jointly and\nsimultaneously on all fields. The motivation for constructing such a system\nstems from the observation that many tasks in natural language processing are\nnaturally composed of multiple subtasks which need to be resolved\nsimultaneously; also tasks usually learned in isolation can possibly benefit\nfrom being learned in a joint framework, as the signals for the extra tasks\nusually constitute inductive bias.\n  The proposed algorithm is evaluated in two experiments: in one, the system is\nused to jointly predict the part-of-speech and text chunks/baseNP chunks of an\nEnglish corpus; and in the second it is used to learn the joint prediction of\nword segment boundaries and part-of-speech tagging for Chinese. The results\nshow that the simultaneous learning of multiple tasks does achieve an\nimprovement in each task upon training the same tasks sequentially. The\npart-of-speech tagging result of 96.63% is state-of-the-art for individual\nsystems on the particular train/test split.\n",
        "published": "2001-07-17T15:43:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0107021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0108005v1",
        "title": "A Bit of Progress in Language Modeling",
        "summary": "  In the past several years, a number of different language modeling\nimprovements over simple trigram models have been found, including caching,\nhigher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and\nclustering. We present explorations of variations on, or of the limits of, each\nof these techniques, including showing that sentence mixture models may have\nmore potential. While all of these techniques have been studied separately,\nthey have rarely been studied in combination. We find some significant\ninteractions, especially with smoothing and clustering techniques. We compare a\ncombination of all techniques together to a Katz smoothed trigram model with no\ncount cutoffs. We achieve perplexity reductions between 38% and 50% (1 bit of\nentropy), depending on training data size, as well as a word error rate\nreduction of 8.9%. Our perplexity reductions are perhaps the highest reported\ncompared to a fair baseline. This is the extended version of the paper; it\ncontains additional details and proofs, and is designed to be a good\nintroduction to the state of the art in language modeling.\n",
        "published": "2001-08-09T19:24:28Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0108005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0108006v1",
        "title": "Classes for Fast Maximum Entropy Training",
        "summary": "  Maximum entropy models are considered by many to be one of the most promising\navenues of language modeling research. Unfortunately, long training times make\nmaximum entropy research difficult. We present a novel speedup technique: we\nchange the form of the model to use classes. Our speedup works by creating two\nmaximum entropy models, the first of which predicts the class of each word, and\nthe second of which predicts the word itself. This factoring of the model leads\nto fewer non-zero indicator functions, and faster normalization, achieving\nspeedups of up to a factor of 35 over one of the best previous techniques. It\nalso results in typically slightly lower perplexities. The same trick can be\nused to speed training of other machine learning techniques, e.g. neural\nnetworks, applied to any problem with a large number of outputs, such as\nlanguage modeling.\n",
        "published": "2001-08-09T19:17:58Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0108006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0108022v1",
        "title": "Portability of Syntactic Structure for Language Modeling",
        "summary": "  The paper presents a study on the portability of statistical syntactic\nknowledge in the framework of the structured language model (SLM). We\ninvestigate the impact of porting SLM statistics from the Wall Street Journal\n(WSJ) to the Air Travel Information System (ATIS) domain. We compare this\napproach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data\nand to using a small amount of data manually parsed at UPenn for gathering the\nintial SLM statistics. Surprisingly, despite the fact that it performs modestly\nin perplexity (PPL), the model initialized on WSJ parses outperforms the other\ninitialization methods based on in-domain annotated data, achieving a\nsignificant 0.4% absolute and 7% relative reduction in word error rate (WER)\nover a baseline system whose word error rate is 5.8%; the improvement measured\nrelative to the minimum WER achievable on the N-best lists we worked with is\n12%.\n",
        "published": "2001-08-29T03:59:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0108022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109010v2",
        "title": "Anaphora and Discourse Structure",
        "summary": "  We argue in this paper that many common adverbial phrases generally taken to\nsignal a discourse relation between syntactically connected units within\ndiscourse structure, instead work anaphorically to contribute relational\nmeaning, with only indirect dependence on discourse structure. This allows a\nsimpler discourse structure to provide scaffolding for compositional semantics,\nand reveals multiple ways in which the relational meaning conveyed by adverbial\nconnectives can interact with that associated with discourse structure. We\nconclude by sketching out a lexicalised grammar for discourse that facilitates\ndiscourse interpretation as a product of compositional rules, anaphor\nresolution and inference.\n",
        "published": "2001-09-09T16:41:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109010v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109015v1",
        "title": "Boosting Trees for Anti-Spam Email Filtering",
        "summary": "  This paper describes a set of comparative experiments for the problem of\nautomatically filtering unwanted electronic mail messages. Several variants of\nthe AdaBoost algorithm with confidence-rated predictions [Schapire & Singer,\n99] have been applied, which differ in the complexity of the base learners\nconsidered. Two main conclusions can be drawn from our experiments: a) The\nboosting-based methods clearly outperform the baseline learning algorithms\n(Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very\nhigh levels of the F1 measure; b) Increasing the complexity of the base\nlearners allows to obtain better ``high-precision'' classifiers, which is a\nvery important issue when misclassification costs are considered.\n",
        "published": "2001-09-13T14:52:41Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109020v1",
        "title": "Modelling Semantic Association and Conceptual Inheritance for Semantic\n  Analysis",
        "summary": "  Allowing users to interact through language borders is an interesting\nchallenge for information technology. For the purpose of a computer assisted\nlanguage learning system, we have chosen icons for representing meaning on the\ninput interface, since icons do not depend on a particular language. However, a\nkey limitation of this type of communication is the expression of articulated\nideas instead of isolated concepts. We propose a method to interpret sequences\nof icons as complex messages by reconstructing the relations between concepts,\nso as to build conceptual graphs able to represent meaning and to be used for\nnatural language sentence generation. This method is based on an electronic\ndictionary containing semantic information.\n",
        "published": "2001-09-15T22:44:55Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109029v1",
        "title": "Learning class-to-class selectional preferences",
        "summary": "  Selectional preference learning methods have usually focused on word-to-class\nrelations, e.g., a verb selects as its subject a given nominal class. This\npapers extends previous statistical models to class-to-class preferences, and\npresents a model that learns selectional preferences for classes of verbs. The\nmotivation is twofold: different senses of a verb may have different\npreferences, and some classes of verbs can share preferences. The model is\ntested on a word sense disambiguation task which uses subject-verb and\nobject-verb relationships extracted from a small sense-disambiguated corpus.\n",
        "published": "2001-09-18T14:00:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109030v1",
        "title": "Knowledge Sources for Word Sense Disambiguation",
        "summary": "  Two kinds of systems have been defined during the long history of WSD:\nprincipled systems that define which knowledge types are useful for WSD, and\nrobust systems that use the information sources at hand, such as, dictionaries,\nlight-weight ontologies or hand-tagged corpora. This paper tries to systematize\nthe relation between desired knowledge types and actual information sources. We\nalso compare the results for a wide range of algorithms that have been\nevaluated on a common test setting in our research group. We hope that this\nanalysis will help change the shift from systems based on information sources\nto systems based on knowledge sources. This study might also shed some light on\nsemi-automatic acquisition of desired knowledge types from existing resources.\n",
        "published": "2001-09-18T14:14:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109031v2",
        "title": "Enriching WordNet concepts with topic signatures",
        "summary": "  This paper explores the possibility of enriching the content of existing\nontologies. The overall goal is to overcome the lack of topical links among\nconcepts in WordNet. Each concept is to be associated to a topic signature,\ni.e., a set of related words with associated weights. The signatures can be\nautomatically constructed from the WWW or from sense-tagged corpora. Both\napproaches are compared and evaluated on a word sense disambiguation task. The\nresults show that it is possible to construct clean signatures from the WWW\nusing some filtering techniques.\n",
        "published": "2001-09-18T14:18:58Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109031v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0109039v1",
        "title": "Testing for Mathematical Lineation in Jim Crace's \"Quarantine\" and T. S.\n  Eliot's \"Four Quartets\"",
        "summary": "  The mathematical distinction between prose and verse may be detected in\nwritings that are not apparently lineated, for example in T. S. Eliot's \"Burnt\nNorton\", and Jim Crace's \"Quarantine\". In this paper we offer comments on\nappropriate statistical methods for such work, and also on the nature of formal\ninnovation in these two texts. Additional remarks are made on the roots of\nlineation as a metrical form, and on the prose-verse continuum.\n",
        "published": "2001-09-20T06:42:11Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0109039v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0110015v1",
        "title": "Richer Syntactic Dependencies for Structured Language Modeling",
        "summary": "  The paper investigates the use of richer syntactic dependencies in the\nstructured language model (SLM). We present two simple methods of enriching the\ndependencies in the syntactic parse trees used for intializing the SLM. We\nevaluate the impact of both methods on the perplexity (PPL) and\nword-error-rate(WER, N-best rescoring) performance of the SLM. We show that the\nnew model achieves an improvement in PPL and WER over the baseline results\nreported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)\ncorpora, respectively.\n",
        "published": "2001-10-03T18:34:36Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0110015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0110027v1",
        "title": "Part-of-Speech Tagging with Two Sequential Transducers",
        "summary": "  We present a method of constructing and using a cascade consisting of a left-\nand a right-sequential finite-state transducer (FST), T1 and T2, for\npart-of-speech (POS) disambiguation. Compared to an HMM, this FST cascade has\nthe advantage of significantly higher processing speed, but at the cost of\nslightly lower accuracy. Applications such as Information Retrieval, where the\nspeed can be more important than accuracy, could benefit from this approach.\n  In the process of tagging, we first assign every word a unique ambiguity\nclass c_i that can be looked up in a lexicon encoded by a sequential FST. Every\nc_i is denoted by a single symbol, e.g. [ADJ_NOUN], although it represents a\nset of alternative tags that a given word can occur with. The sequence of the\nc_i of all words of one sentence is the input to our FST cascade. It is mapped\nby T1, from left to right, to a sequence of reduced ambiguity classes r_i.\nEvery r_i is denoted by a single symbol, although it represents a set of\nalternative tags. Intuitively, T1 eliminates the less likely tags from c_i,\nthus creating r_i. Finally, T2 maps the sequence of r_i, from right to left, to\na sequence of single POS tags t_i. Intuitively, T2 selects the most likely t_i\nfrom every r_i.\n  The probabilities of all t_i, r_i, and c_i are used only at compile time, not\nat run time. They do not (directly) occur in the FSTs, but are \"implicitly\ncontained\" in their structure.\n",
        "published": "2001-10-11T11:29:49Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0110027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0110050v1",
        "title": "What is the minimal set of fragments that achieves maximal parse\n  accuracy?",
        "summary": "  We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank (a precision of 90.8% and a recall of 90.6%). We\nisolate some dependency relations which previous models neglect but which\ncontribute to higher parse accuracy.\n",
        "published": "2001-10-24T11:01:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0110050v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0110051v1",
        "title": "Combining semantic and syntactic structure for language modeling",
        "summary": "  Structured language models for speech recognition have been shown to remedy\nthe weaknesses of n-gram models. All current structured language models are,\nhowever, limited in that they do not take into account dependencies between\nnon-headwords. We show that non-headword dependencies contribute to\nsignificantly improved word error rate, and that a data-oriented parsing model\ntrained on semantically and syntactically annotated data can exploit these\ndependencies. This paper also contains the first DOP model trained by means of\na maximum likelihood reestimation procedure, which solves some of the\ntheoretical shortcomings of previous DOP models.\n",
        "published": "2001-10-24T11:30:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0110051v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0111064v1",
        "title": "A procedure for unsupervised lexicon learning",
        "summary": "  We describe an incremental unsupervised procedure to learn words from\ntranscribed continuous speech. The algorithm is based on a conservative and\ntraditional statistical model, and results of empirical tests show that it is\ncompetitive with other algorithms that have been proposed recently for this\ntask.\n",
        "published": "2001-11-30T20:30:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0111064v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0111065v1",
        "title": "A Statistical Model for Word Discovery in Transcribed Speech",
        "summary": "  A statistical model for segmentation and word discovery in continuous speech\nis presented. An incremental unsupervised learning algorithm to infer word\nboundaries based on this model is described. Results of empirical tests showing\nthat the algorithm is competitive with other models that have been used for\nsimilar tasks are also presented.\n",
        "published": "2001-11-30T20:40:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0111065v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0112003v1",
        "title": "Using a Support-Vector Machine for Japanese-to-English Translation of\n  Tense, Aspect, and Modality",
        "summary": "  This paper describes experiments carried out using a variety of\nmachine-learning methods, including the k-nearest neighborhood method that was\nused in a previous study, for the translation of tense, aspect, and modality.\nIt was found that the support-vector machine method was the most precise of all\nthe methods tested.\n",
        "published": "2001-12-05T05:35:07Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0112003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0112004v1",
        "title": "Part of Speech Tagging in Thai Language Using Support Vector Machine",
        "summary": "  The elastic-input neuro tagger and hybrid tagger, combined with a neural\nnetwork and Brill's error-driven learning, have already been proposed for the\npurpose of constructing a practical tagger using as little training data as\npossible. When a small Thai corpus is used for training, these taggers have\ntagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words\nin terms of the part of speech), respectively. In this study, in order to\nconstruct more accurate taggers we developed new tagging methods using three\nmachine learning methods: the decision-list, maximum entropy, and support\nvector machine methods. We then performed tagging experiments by using these\nmethods. Our results showed that the support vector machine method has the best\nprecision (96.1%), and that it is capable of improving the accuracy of tagging\nin the Thai language. Finally, we theoretically examined all these methods and\ndiscussed how the improvements were achived.\n",
        "published": "2001-12-05T05:48:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0112004v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0112005v1",
        "title": "Universal Model for Paraphrasing -- Using Transformation Based on a\n  Defined Criteria --",
        "summary": "  This paper describes a universal model for paraphrasing that transforms\naccording to defined criteria. We showed that by using different criteria we\ncould construct different kinds of paraphrasing systems including one for\nanswering questions, one for compressing sentences, one for polishing up, and\none for transforming written language to spoken language.\n",
        "published": "2001-12-05T05:56:13Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0112005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204003v1",
        "title": "Blind Normalization of Speech From Different Channels and Speakers",
        "summary": "  This paper describes representations of time-dependent signals that are\ninvariant under any invertible time-independent transformation of the signal\ntime series. Such a representation is created by rescaling the signal in a\nnon-linear dynamic manner that is determined by recently encountered signal\nlevels. This technique may make it possible to normalize signals that are\nrelated by channel-dependent and speaker-dependent transformations, without\nhaving to characterize the form of the signal transformations, which remain\nunknown. The technique is illustrated by applying it to the time-dependent\nspectra of speech that has been filtered to simulate the effects of different\nchannels. The experimental results show that the rescaled speech\nrepresentations are largely normalized (i.e., channel-independent), despite the\nchannel-dependence of the raw (unrescaled) speech.\n",
        "published": "2002-04-02T21:14:40Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204007v1",
        "title": "An Integrated Framework for Treebanks and Multilayer Annotations",
        "summary": "  Treebank formats and associated software tools are proliferating rapidly,\nwith little consideration for interoperability. We survey a wide variety of\ntreebank structures and operations, and show how they can be mapped onto the\nannotation graph model, and leading to an integrated framework encompassing\ntree and non-tree annotations alike. This development opens up new\npossibilities for managing and exploiting multilayer annotations.\n",
        "published": "2002-04-03T18:55:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204022v1",
        "title": "Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure\n  for Interdisciplinary Education, Research and Development",
        "summary": "  Annotation graphs and annotation servers offer infrastructure to support the\nanalysis of human language resources in the form of time-series data such as\ntext, audio and video. This paper outlines areas of common need among empirical\nlinguists and computational linguists. After reviewing examples of data and\ntools used or under development for each of several areas, it proposes a common\nframework for future tool development, data annotation and resource sharing\nbased upon annotation graphs and servers.\n",
        "published": "2002-04-10T15:37:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204023v1",
        "title": "Computational Phonology",
        "summary": "  Phonology, as it is practiced, is deeply computational. Phonological analysis\nis data-intensive and the resulting models are nothing other than specialized\ndata structures and algorithms. In the past, phonological computation -\nmanaging data and developing analyses - was done manually with pencil and\npaper. Increasingly, with the proliferation of affordable computers, IPA fonts\nand drawing software, phonologists are seeking to move their computation work\nonline. Computational Phonology provides the theoretical and technological\nframework for this migration, building on methodologies and tools from\ncomputational linguistics. This piece consists of an apology for computational\nphonology, a history, and an overview of current research.\n",
        "published": "2002-04-10T15:49:24Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204023v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204025v1",
        "title": "Phonology",
        "summary": "  Phonology is the systematic study of the sounds used in language, their\ninternal structure, and their composition into syllables, words and phrases.\nComputational phonology is the application of formal and computational\ntechniques to the representation and processing of phonological information.\nThis chapter will present the fundamentals of descriptive phonology along with\na brief overview of computational phonology.\n",
        "published": "2002-04-11T11:22:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204027v1",
        "title": "Integrating selectional preferences in WordNet",
        "summary": "  Selectional preference learning methods have usually focused on word-to-class\nrelations, e.g., a verb selects as its subject a given nominal class. This\npaper extends previous statistical models to class-to-class preferences, and\npresents a model that learns selectional preferences for classes of verbs,\ntogether with an algorithm to integrate the learned preferences in WordNet. The\ntheoretical motivation is twofold: different senses of a verb may have\ndifferent preferences, and classes of verbs may share preferences. On the\npractical side, class-to-class selectional preferences can be learned from\nuntagged corpora (the same as word-to-class), they provide selectional\npreferences for less frequent word senses via inheritance, and more important,\nthey allow for easy integration in WordNet. The model is trained on\nsubject-verb and object-verb relationships extracted from a small corpus\ndisambiguated with WordNet senses. Examples are provided illustrating that the\ntheoretical motivations are well founded, and showing that the approach is\nfeasible. Experimental results on a word sense disambiguation task are also\nprovided.\n",
        "published": "2002-04-11T16:41:28Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204028v1",
        "title": "Decision Lists for English and Basque",
        "summary": "  In this paper we describe the systems we developed for the English (lexical\nand all-words) and Basque tasks. They were all supervised systems based on\nYarowsky's Decision Lists. We used Semcor for training in the English all-words\ntask. We defined different feature sets for each language. For Basque, in order\nto extract all the information from the text, we defined features that have not\nbeen used before in the literature, using a morphological analyzer. We also\nimplemented systems that selected automatically good features and were able to\nobtain a prefixed precision (85%) at the cost of coverage. The systems that\nused all the features were identified as BCU-ehu-dlist-all and the systems that\nselected some features as BCU-ehu-dlist-best.\n",
        "published": "2002-04-12T07:41:55Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204029v1",
        "title": "The Basque task: did systems perform in the upperbound?",
        "summary": "  In this paper we describe the Senseval 2 Basque lexical-sample task. The task\ncomprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal\nHiztegia, the main Basque dictionary. Most examples were taken from the\nEgunkaria newspaper. The method used to hand-tag the examples produced low\ninter-tagger agreement (75%) before arbitration. The four competing systems\nattained results well above the most frequent baseline and the best system\nscored 75% precision at 100% coverage. The paper includes an analysis of the\ntagging procedure used, as well as the performance of the competing systems. In\nparticular, we argue that inter-tagger agreement is not a real upperbound for\nthe Basque WSD task.\n",
        "published": "2002-04-12T07:49:32Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0204049v1",
        "title": "Memory-Based Shallow Parsing",
        "summary": "  We present memory-based learning approaches to shallow parsing and apply\nthese to five tasks: base noun phrase identification, arbitrary base phrase\nrecognition, clause detection, noun phrase parsing and full parsing. We use\nfeature selection techniques and system combination methods for improving the\nperformance of the memory-based learner. Our approach is evaluated on standard\ndata sets and the results are compared with that of other systems. This reveals\nthat our approach works well for base phrase identification while its\napplication towards recognizing embedded structures leaves some room for\nimprovement.\n",
        "published": "2002-04-24T14:48:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0204049v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205006v1",
        "title": "Unsupervised discovery of morphologically related words based on\n  orthographic and semantic similarity",
        "summary": "  We present an algorithm that takes an unannotated corpus as its input, and\nreturns a ranked list of probable morphologically related pairs as its output.\nThe algorithm tries to discover morphologically related pairs by looking for\npairs that are both orthographically and semantically similar, where\northographic similarity is measured in terms of minimum edit distance, and\nsemantic similarity is measured in terms of mutual information. The procedure\ndoes not rely on a morpheme concatenation model, nor on distributional\nproperties of word substrings (such as affix frequency). Experiments with\nGerman and English input give encouraging results, both in terms of precision\n(proportion of good pairs found at various cutoff points of the ranked list),\nand in terms of a qualitative analysis of the types of morphological patterns\ndiscovered by the algorithm.\n",
        "published": "2002-05-08T14:39:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205009v1",
        "title": "Mostly-Unsupervised Statistical Segmentation of Japanese Kanji Sequences",
        "summary": "  Given the lack of word delimiters in written Japanese, word segmentation is\ngenerally considered a crucial first step in processing Japanese texts. Typical\nJapanese segmentation algorithms rely either on a lexicon and syntactic\nanalysis or on pre-segmented data; but these are labor-intensive, and the\nlexico-syntactic techniques are vulnerable to the unknown word problem. In\ncontrast, we introduce a novel, more robust statistical method utilizing\nunsegmented training data. Despite its simplicity, the algorithm yields\nperformance on long kanji sequences comparable to and sometimes surpassing that\nof state-of-the-art morphological analyzers over a variety of error metrics.\nThe algorithm also outperforms another mostly-unsupervised statistical\nalgorithm previously proposed for Chinese.\n  Additionally, we present a two-level annotation scheme for Japanese to\nincorporate multiple segmentation granularities, and introduce two novel\nevaluation metrics, both based on the notion of a compatible bracket, that can\naccount for multiple granularities simultaneously.\n",
        "published": "2002-05-10T18:55:25Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205017v1",
        "title": "Ellogon: A New Text Engineering Platform",
        "summary": "  This paper presents Ellogon, a multi-lingual, cross-platform, general-purpose\ntext engineering environment. Ellogon was designed in order to aid both\nresearchers in natural language processing, as well as companies that produce\nlanguage engineering systems for the end-user. Ellogon provides a powerful\nTIPSTER-based infrastructure for managing, storing and exchanging textual data,\nembedding and managing text processing components as well as visualising\ntextual data and their associated linguistic information. Among its key\nfeatures are full Unicode support, an extensive multi-lingual graphical user\ninterface, its modular architecture and the reduced hardware requirements.\n",
        "published": "2002-05-13T11:18:08Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205027v1",
        "title": "A variable-free dynamic semantics",
        "summary": "  I propose a variable-free treatment of dynamic semantics. By \"dynamic\nsemantics\" I mean analyses of donkey sentences (\"Every farmer who owns a donkey\nbeats it\") and other binding and anaphora phenomena in natural language where\nmeanings of constituents are updates to information states, for instance as\nproposed by Groenendijk and Stokhof. By \"variable-free\" I mean denotational\nsemantics in which functional combinators replace variable indices and\nassignment functions, for instance as advocated by Jacobson.\n  The new theory presented here achieves a compositional treatment of dynamic\nanaphora that does not involve assignment functions, and separates the\ncombinatorics of variable-free semantics from the particular linguistic\nphenomena it treats. Integrating variable-free semantics and dynamic semantics\ngives rise to interactions that make new empirical predictions, for example\n\"donkey weak crossover\" effects.\n",
        "published": "2002-05-17T09:33:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205028v1",
        "title": "NLTK: The Natural Language Toolkit",
        "summary": "  NLTK, the Natural Language Toolkit, is a suite of open source program\nmodules, tutorials and problem sets, providing ready-to-use computational\nlinguistics courseware. NLTK covers symbolic and statistical natural language\nprocessing, and is interfaced to annotated corpora. Students augment and\nreplace existing components, learn structured programming by example, and\nmanipulate sophisticated models from the outset.\n",
        "published": "2002-05-17T12:51:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205057v1",
        "title": "Unsupervised Discovery of Morphemes",
        "summary": "  We present two methods for unsupervised segmentation of words into\nmorpheme-like units. The model utilized is especially suited for languages with\na rich morphology, such as Finnish. The first method is based on the Minimum\nDescription Length (MDL) principle and works online. In the second method,\nMaximum Likelihood (ML) optimization is used. The quality of the segmentations\nis measured using an evaluation method that compares the segmentations produced\nto an existing morphological analysis. Experiments on both Finnish and English\ncorpora show that the presented methods perform well compared to a current\nstate-of-the-art system.\n",
        "published": "2002-05-21T14:37:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205057v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205065v1",
        "title": "Bootstrapping Lexical Choice via Multiple-Sequence Alignment",
        "summary": "  An important component of any generation system is the mapping dictionary, a\nlexicon of elementary semantic expressions and corresponding natural language\nrealizations. Typically, labor-intensive knowledge-based methods are used to\nconstruct the dictionary. We instead propose to acquire it automatically via a\nnovel multiple-pass algorithm employing multiple-sequence alignment, a\ntechnique commonly used in bioinformatics. Crucially, our method leverages\nlatent information contained in multi-parallel corpora -- datasets that supply\nseveral verbalizations of the corresponding semantics rather than just one.\n  We used our techniques to generate natural language versions of\ncomputer-generated mathematical proofs, with good results on both a\nper-component and overall-output basis. For example, in evaluations involving a\ndozen human judges, our system produced output whose readability and\nfaithfulness to the semantic input rivaled that of a traditional generation\nsystem.\n",
        "published": "2002-05-25T21:32:09Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205065v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205067v1",
        "title": "Evaluating the Effectiveness of Ensembles of Decision Trees in\n  Disambiguating Senseval Lexical Samples",
        "summary": "  This paper presents an evaluation of an ensemble--based system that\nparticipated in the English and Spanish lexical sample tasks of Senseval-2. The\nsystem combines decision trees of unigrams, bigrams, and co--occurrences into a\nsingle classifier. The analysis is extended to include the Senseval-1 data.\n",
        "published": "2002-05-27T18:42:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205067v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205068v1",
        "title": "Assessing System Agreement and Instance Difficulty in the Lexical Sample\n  Tasks of Senseval-2",
        "summary": "  This paper presents a comparative evaluation among the systems that\nparticipated in the Spanish and English lexical sample tasks of Senseval-2. The\nfocus is on pairwise comparisons among systems to assess the degree to which\nthey agree, and on measuring the difficulty of the test instances included in\nthese tasks.\n",
        "published": "2002-05-27T18:49:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205068v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0205069v1",
        "title": "Machine Learning with Lexical Features: The Duluth Approach to\n  Senseval-2",
        "summary": "  This paper describes the sixteen Duluth entries in the Senseval-2 comparative\nexercise among word sense disambiguation systems. There were eight pairs of\nDuluth systems entered in the Spanish and English lexical sample tasks. These\nare all based on standard machine learning algorithms that induce classifiers\nfrom sense-tagged training text where the context in which ambiguous words\noccur are represented by simple lexical features. These are highly portable,\nrobust methods that can serve as a foundation for more tailored approaches.\n",
        "published": "2002-05-27T18:57:11Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0205069v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206014v1",
        "title": "A Method for Open-Vocabulary Speech-Driven Text Retrieval",
        "summary": "  While recent retrieval techniques do not limit the number of index terms,\nout-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at\nretrieving information with spoken queries, we fill the gap between speech\nrecognition and text retrieval in terms of the vocabulary size. Given a spoken\nquery, we generate a transcription and detect OOV words through speech\nrecognition. We then correspond detected OOV words to terms indexed in a target\ncollection to complete the transcription, and search the collection for\ndocuments relevant to the completed transcription. We show the effectiveness of\nour method by way of experiments.\n",
        "published": "2002-06-09T10:07:36Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206015v1",
        "title": "Japanese/English Cross-Language Information Retrieval: Exploration of\n  Query Translation and Transliteration",
        "summary": "  Cross-language information retrieval (CLIR), where queries and documents are\nin different languages, has of late become one of the major topics within the\ninformation retrieval community. This paper proposes a Japanese/English CLIR\nsystem, where we combine a query translation and retrieval modules. We\ncurrently target the retrieval of technical documents, and therefore the\nperformance of our system is highly dependent on the quality of the translation\nof technical terms. However, the technical term translation is still\nproblematic in that technical terms are often compound words, and thus new\nterms are progressively created by combining existing base words. In addition,\nJapanese often represents loanwords based on its special phonogram.\nConsequently, existing dictionaries find it difficult to achieve sufficient\ncoverage. To counter the first problem, we produce a Japanese/English\ndictionary for base words, and translate compound words on a word-by-word\nbasis. We also use a probabilistic method to resolve translation ambiguity. For\nthe second problem, we use a transliteration method, which corresponds words\nunlisted in the base word dictionary to their phonetic equivalents in the\ntarget language. We evaluate our system using a test collection for CLIR, and\nshow that both the compound word translation and transliteration methods\nimprove the system performance.\n",
        "published": "2002-06-09T10:53:49Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206030v1",
        "title": "A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero\n  Pronoun Detection and Resolution",
        "summary": "  This paper proposes a method to analyze Japanese anaphora, in which zero\npronouns (omitted obligatory cases) are used to refer to preceding entities\n(antecedents). Unlike the case of general coreference resolution, zero pronouns\nhave to be detected prior to resolution because they are not expressed in\ndiscourse. Our method integrates two probability parameters to perform zero\npronoun detection and resolution in a single framework. The first parameter\nquantifies the degree to which a given case is a zero pronoun. The second\nparameter quantifies the degree to which a given entity is the antecedent for a\ndetected zero pronoun. To compute these parameters efficiently, we use corpora\nwith/without annotations of anaphoric relations. We show the effectiveness of\nour method by way of experiments.\n",
        "published": "2002-06-20T07:13:59Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206034v1",
        "title": "Applying a Hybrid Query Translation Method to Japanese/English\n  Cross-Language Patent Retrieval",
        "summary": "  This paper applies an existing query translation method to cross-language\npatent retrieval. In our method, multiple dictionaries are used to derive all\npossible translations for an input query, and collocational statistics are used\nto resolve translation ambiguity. We used Japanese/English parallel patent\nabstracts to perform comparative experiments, where our method outperformed a\nsimple dictionary-based query translation method, and achieved 76% of\nmonolingual retrieval in terms of average precision.\n",
        "published": "2002-06-24T07:46:06Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206034v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206035v1",
        "title": "PRIME: A System for Multi-lingual Patent Retrieval",
        "summary": "  Given the growing number of patents filed in multiple countries, users are\ninterested in retrieving patents across languages. We propose a multi-lingual\npatent retrieval system, which translates a user query into the target\nlanguage, searches a multilingual database for patents relevant to the query,\nand improves the browsing efficiency by way of machine translation and\nclustering. Our system also extracts new translations from patent families\nconsisting of comparable patents, to enhance the translation dictionary.\n",
        "published": "2002-06-24T08:00:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206036v1",
        "title": "Language Modeling for Multi-Domain Speech-Driven Text Retrieval",
        "summary": "  We report experimental results associated with speech-driven text retrieval,\nwhich facilitates retrieving information in multiple domains with spoken\nqueries. Since users speak contents related to a target collection, we produce\nlanguage models used for speech recognition based on the target collection, so\nas to improve both the recognition and retrieval accuracy. Experiments using\nexisting test collections combined with dictated queries showed the\neffectiveness of our method.\n",
        "published": "2002-06-24T08:38:36Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0206037v1",
        "title": "Speech-Driven Text Retrieval: Using Target IR Collections for\n  Statistical Language Model Adaptation in Speech Recognition",
        "summary": "  Speech recognition has of late become a practical technology for real world\napplications. Aiming at speech-driven text retrieval, which facilitates\nretrieving information with spoken queries, we propose a method to integrate\nspeech recognition and retrieval methods. Since users speak contents related to\na target collection, we adapt statistical language models used for speech\nrecognition based on the target collection, so as to improve both the\nrecognition and retrieval accuracy. Experiments using existing test collections\ncombined with dictated queries showed the effectiveness of our method.\n",
        "published": "2002-06-24T10:28:02Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0206037v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0207002v1",
        "title": "Using eigenvectors of the bigram graph to infer morpheme identity",
        "summary": "  This paper describes the results of some experiments exploring statistical\nmethods to infer syntactic behavior of words and morphemes from a raw corpus in\nan unsupervised fashion. It shares certain points in common with Brown et al\n(1992) and work that has grown out of that: it employs statistical techniques\nto analyze syntactic behavior based on what words occur adjacent to a given\nword. However, we use an eigenvector decomposition of a nearest-neighbor graph\nto produce a two-dimensional rendering of the words of a corpus in which words\nof the same syntactic category tend to form neighborhoods. We exploit this\ntechnique for extending the value of automatic learning of morphology. In\nparticular, we look at the suffixes derived from a corpus by unsupervised\nlearning of morphology, and we ask which of these suffixes have a consistent\nsyntactic function (e.g., in English, -tion is primarily a mark of nouns, but\n-s marks both noun plurals and 3rd person present on verbs), and we determine\nthat this method works well for this task.\n",
        "published": "2002-07-02T01:15:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0207002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0207003v1",
        "title": "Analysis of Titles and Readers For Title Generation Centered on the\n  Readers",
        "summary": "  The title of a document has two roles, to give a compact summary and to lead\nthe reader to read the document. Conventional title generation focuses on\nfinding key expressions from the author's wording in the document to give a\ncompact summary and pays little attention to the reader's interest. To make the\ntitle play its second role properly, it is indispensable to clarify the content\n(``what to say'') and wording (``how to say'') of titles that are effective to\nattract the target reader's interest. In this article, we first identify\ntypical content and wording of titles aimed at general readers in a comparative\nstudy between titles of technical papers and headlines rewritten for\nnewspapers. Next, we describe the results of a questionnaire survey on the\neffects of the content and wording of titles on the reader's interest. The\nsurvey of general and knowledgeable readers shows both common and different\ntendencies in interest.\n",
        "published": "2002-07-02T15:08:44Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0207003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0207005v1",
        "title": "Efficient Deep Processing of Japanese",
        "summary": "  We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages.\n",
        "published": "2002-07-03T11:54:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0207005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0208020v1",
        "title": "Using the DIFF Command for Natural Language Processing",
        "summary": "  Diff is a software program that detects differences between two data sets and\nis useful in natural language processing. This paper shows several examples of\nthe application of diff. They include the detection of differences between two\ndifferent datasets, extraction of rewriting rules, merging of two different\ndatasets, and the optimal matching of two different data sets. Since diff comes\nwith any standard UNIX system, it is readily available and very easy to use.\nOur studies showed that diff is a practical tool for research into natural\nlanguage processing.\n",
        "published": "2002-08-13T03:39:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0208020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0208035v1",
        "title": "Evaluation of Coreference Rules on Complex Narrative Texts",
        "summary": "  This article studies the problem of assessing relevance to each of the rules\nof a reference resolution system. The reference solver described here stems\nfrom a formal model of reference and is integrated in a reference processing\nworkbench. Evaluation of the reference resolution is essential, as it enables\ndifferential evaluation of individual rules. Numerical values of these measures\nare given, and discussed, for simple selection rules and other processing\nrules; such measures are then studied for numerical parameters.\n",
        "published": "2002-08-21T14:09:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0208035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0208036v1",
        "title": "Three New Methods for Evaluating Reference Resolution",
        "summary": "  Reference resolution on extended texts (several thousand references) cannot\nbe evaluated manually. An evaluation algorithm has been proposed for the MUC\ntests, using equivalence classes for the coreference relation. However, we show\nhere that this algorithm is too indulgent, yielding good scores even for poor\nresolution strategies. We elaborate on the same formalism to propose two new\nevaluation algorithms, comparing them first with the MUC algorithm and giving\nthen results on a variety of examples. A third algorithm using only\ndistributional comparison of equivalence classes is finally described; it\nassesses the relative importance of the recall vs. precision errors.\n",
        "published": "2002-08-21T14:28:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0208036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0208037v1",
        "title": "Cooperation between Pronoun and Reference Resolution for Unrestricted\n  Texts",
        "summary": "  Anaphora resolution is envisaged in this paper as part of the reference\nresolution process. A general open architecture is proposed, which can be\nparticularized and configured in order to simulate some classic anaphora\nresolution methods. With the aim of improving pronoun resolution, the system\ntakes advantage of elementary cues about characters of the text, which are\nrepresented through a particular data structure. In its most robust\nconfiguration, the system uses only a general lexicon, a local morpho-syntactic\nparser and a dictionary of synonyms. A short comparative corpus analysis shows\nthat narrative texts are the most suitable for testing such a system.\n",
        "published": "2002-08-21T14:36:13Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0208037v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0208038v1",
        "title": "Reference Resolution Beyond Coreference: a Conceptual Frame and its\n  Application",
        "summary": "  A model for reference use in communication is proposed, from a\nrepresentationist point of view. Both the sender and the receiver of a message\nhandle representations of their common environment, including mental\nrepresentations of objects. Reference resolution by a computer is viewed as the\nconstruction of object representations using referring expressions from the\ndiscourse, whereas often only coreference links between such expressions are\nlooked for. Differences between these two approaches are discussed. The model\nhas been implemented with elementary rules, and tested on complex narrative\ntexts (hundreds to thousands of referring expressions). The results support the\nmental representations paradigm.\n",
        "published": "2002-08-21T14:43:18Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0208038v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0209002v1",
        "title": "A Chart-Parsing Algorithm for Efficient Semantic Analysis",
        "summary": "  In some contexts, well-formed natural language cannot be expected as input to\ninformation or communication systems. In these contexts, the use of\ngrammar-independent input (sequences of uninflected semantic units like e.g.\nlanguage-independent icons) can be an answer to the users' needs. A semantic\nanalysis can be performed, based on lexical semantic knowledge: it is\nequivalent to a dependency analysis with no syntactic or morphological clues.\nHowever, this requires that an intelligent system should be able to interpret\nthis input with reasonable accuracy and in reasonable time. Here we propose a\nmethod allowing a purely semantic-based analysis of sequences of semantic\nunits. It uses an algorithm inspired by the idea of ``chart parsing'' known in\nNatural Language Processing, which stores intermediate parsing results in order\nto bring the calculation time down. In comparison with using declarative logic\nprogramming - where the calculation time, left to a prolog engine, is\nhyperexponential -, this method brings the calculation time down to a\npolynomial time, where the order depends on the valency of the predicates.\n",
        "published": "2002-09-02T16:55:34Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0209002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0209003v1",
        "title": "Rerendering Semantic Ontologies: Automatic Extensions to UMLS through\n  Corpus Analytics",
        "summary": "  In this paper, we discuss the utility and deficiencies of existing ontology\nresources for a number of language processing applications. We describe a\ntechnique for increasing the semantic type coverage of a specific ontology, the\nNational Library of Medicine's UMLS, with the use of robust finite state\nmethods used in conjunction with large-scale corpus analytics of the domain\ncorpus. We call this technique \"semantic rerendering\" of the ontology. This\nresearch has been done in the context of Medstract, a joint Brandeis-Tufts\neffort aimed at developing tools for analyzing biomedical language (i.e.,\nMedline), as well as creating targeted databases of bio-entities, biological\nrelations, and pathway data for biological researchers. Motivating the current\nresearch is the need to have robust and reliable semantic typing of syntactic\nelements in the Medline corpus, in order to improve the overall performance of\nthe information extraction applications mentioned above.\n",
        "published": "2002-09-03T05:28:56Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0209003v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0209010v1",
        "title": "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named\n  Entity Recognition",
        "summary": "  We describe the CoNLL-2002 shared task: language-independent named entity\nrecognition. We give background information on the data sets and the evaluation\nmethod, present a general overview of the systems that have taken part in the\ntask and discuss their performance.\n",
        "published": "2002-09-05T19:03:06Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0209010v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0211017v1",
        "title": "Probabilistic Parsing Strategies",
        "summary": "  We present new results on the relation between purely symbolic context-free\nparsing strategies and their probabilistic counter-parts. Such parsing\nstrategies are seen as constructions of push-down devices from grammars. We\nshow that preservation of probability distribution is possible under two\nconditions, viz. the correct-prefix property and the property of strong\npredictiveness. These results generalize existing results in the literature\nthat were obtained by considering parsing strategies in isolation. From our\ngeneral results we also derive negative results on so-called generalized LR\nparsing.\n",
        "published": "2002-11-14T16:16:44Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0211017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0212015v1",
        "title": "Answering Subcognitive Turing Test Questions: A Reply to French",
        "summary": "  Robert French has argued that a disembodied computer is incapable of passing\na Turing Test that includes subcognitive questions. Subcognitive questions are\ndesigned to probe the network of cultural and perceptual associations that\nhumans naturally develop as we live, embodied and embedded in the world. In\nthis paper, I show how it is possible for a disembodied computer to answer\nsubcognitive questions appropriately, contrary to French's claim. My approach\nto answering subcognitive questions is to use statistical information extracted\nfrom a very large collection of text. In particular, I show how it is possible\nto answer a sample of subcognitive questions taken from French, by issuing\nqueries to a search engine that indexes about 350 million Web pages. This\nsimple algorithm may shed light on the nature of human (sub-) cognition, but\nthe scope of this paper is limited to demonstrating that French is mistaken: a\ndisembodied computer can answer subcognitive questions.\n",
        "published": "2002-12-09T13:09:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0212015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0302014v1",
        "title": "An Algorithm for Aligning Sentences in Bilingual Corpora Using Lexical\n  Information",
        "summary": "  In this paper we describe an algorithm for aligning sentences with their\ntranslations in a bilingual corpus using lexical information of the languages.\nExisting efficient algorithms ignore word identities and consider only the\nsentence lengths (Brown, 1991; Gale and Church, 1993). For a sentence in the\nsource language text, the proposed algorithm picks the most likely translation\nfrom the target language text using lexical information and certain heuristics.\nIt does not do statistical analysis using sentence lengths. The algorithm is\nlanguage independent. It also aids in detecting addition and deletion of text\nin translations. The algorithm gives comparable results with the existing\nalgorithms in most of the cases while it does better in cases where statistical\nalgorithms do not give good results.\n",
        "published": "2003-02-12T06:31:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0302014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0302032v1",
        "title": "Empirical Methods for Compound Splitting",
        "summary": "  Compounded words are a challenge for NLP applications such as machine\ntranslation (MT). We introduce methods to learn splitting rules from\nmonolingual and parallel corpora. We evaluate them against a gold standard and\nmeasure their impact on performance of statistical MT systems. Results show\naccuracy of 99.1% and performance gains for MT of 0.039 BLEU on a\nGerman-English noun phrase translation task.\n",
        "published": "2003-02-22T23:37:26Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0302032v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0303002v2",
        "title": "About compression of vocabulary in computer oriented languages",
        "summary": "  The author uses the entropy of the ideal Bose-Einstein gas to minimize losses\nin computer-oriented languages.\n",
        "published": "2003-03-05T08:50:42Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0303002v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0303007v1",
        "title": "Glottochronology and problems of protolanguage reconstruction",
        "summary": "  A method of languages genealogical trees construction is proposed.\n",
        "published": "2003-03-14T05:15:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0303007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304006v1",
        "title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence\n  Alignment",
        "summary": "  We address the text-to-text generation problem of sentence-level paraphrasing\n-- a phenomenon distinct from and more difficult than word- or phrase-level\nparaphrasing. Our approach applies multiple-sequence alignment to sentences\ngathered from unannotated comparable corpora: it learns a set of paraphrasing\npatterns represented by word lattice pairs and automatically determines how to\napply these patterns to rewrite new sentences. The results of our evaluation\nexperiments show that the system derives accurate paraphrases, outperforming\nbaseline systems.\n",
        "published": "2003-04-02T23:02:44Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304019v1",
        "title": "Blind Normalization of Speech From Different Channels",
        "summary": "  We show how to construct a channel-independent representation of speech that\nhas propagated through a noisy reverberant channel. This is done by blindly\nrescaling the cepstral time series by a non-linear function, with the form of\nthis scale function being determined by previously encountered cepstra from\nthat channel. The rescaled form of the time series is an invariant property of\nit in the following sense: it is unaffected if the time series is transformed\nby any time-independent invertible distortion. Because a linear channel with\nstationary noise and impulse response transforms cepstra in this way, the new\ntechnique can be used to remove the channel dependence of a cepstral time\nseries. In experiments, the method achieved greater channel-independence than\ncepstral mean normalization, and it was comparable to the combination of\ncepstral mean normalization and spectral subtraction, despite the fact that no\nmeasurements of channel noise or reverberations were required (unlike spectral\nsubtraction).\n",
        "published": "2003-04-10T22:25:01Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304024v1",
        "title": "Glottochronologic Retrognostic of Language System",
        "summary": "  A glottochronologic retrognostic of language system is proposed\n",
        "published": "2003-04-17T02:22:06Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304024v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304027v1",
        "title": "\"I'm sorry Dave, I'm afraid I can't do that\": Linguistics, Statistics,\n  and Natural Language Processing circa 2001",
        "summary": "  A brief, general-audience overview of the history of natural language\nprocessing, focusing on data-driven approaches.Topics include \"Ambiguity and\nlanguage analysis\", \"Firth things first\", \"A 'C' change\", and \"The empiricists\nstrike back\".\n",
        "published": "2003-04-21T22:10:21Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304029v1",
        "title": "An XML based Document Suite",
        "summary": "  We report about the current state of development of a document suite and its\napplications. This collection of tools for the flexible and robust processing\nof documents in German is based on the use of XML as unifying formalism for\nencoding input and output data as well as process information. It is organized\nin modules with limited responsibilities that can easily be combined into\npipelines to solve complex tasks. Strong emphasis is laid on a number of\ntechniques to deal with lexical and conceptual gaps that are typical when\nstarting a new application.\n",
        "published": "2003-04-22T13:45:37Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304029v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304035v1",
        "title": "Exploiting Sublanguage and Domain Characteristics in a Bootstrapping\n  Approach to Lexicon and Ontology Creation",
        "summary": "  It is very costly to build up lexical resources and domain ontologies.\nEspecially when confronted with a new application domain lexical gaps and a\npoor coverage of domain concepts are a problem for the successful exploitation\nof natural language document analysis systems that need and exploit such\nknowledge sources. In this paper we report about ongoing experiments with\n`bootstrapping techniques' for lexicon and ontology creation.\n",
        "published": "2003-04-23T08:02:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304035v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0304036v1",
        "title": "An Approach for Resource Sharing in Multilingual NLP",
        "summary": "  In this paper we describe an approach for the analysis of documents in German\nand English with a shared pool of resources. For the analysis of German\ndocuments we use a document suite, which supports the user in tasks like\ninformation retrieval and information extraction. The core of the document\nsuite is based on our tool XDOC. Now we want to exploit these methods for the\nanalysis of English documents as well. For this aim we need a multilingual\npresentation format of the resources. These resources must be transformed into\nan unified format, in which we can set additional information about linguistic\ncharacteristics of the language depending on the analyzed documents. In this\npaper we describe our approach for such an exchange model for multilingual\nresources based on XML.\n",
        "published": "2003-04-23T08:32:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0304036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0305041v2",
        "title": "Factorization of Language Models through Backing-Off Lattices",
        "summary": "  Factorization of statistical language models is the task that we resolve the\nmost discriminative model into factored models and determine a new model by\ncombining them so as to provide better estimate. Most of previous works mainly\nfocus on factorizing models of sequential events, each of which allows only one\nfactorization manner. To enable parallel factorization, which allows a model\nevent to be resolved in more than one ways at the same time, we propose a\ngeneral framework, where we adopt a backing-off lattice to reflect parallel\nfactorizations and to define the paths along which a model is resolved into\nfactored models, we use a mixture model to combine parallel paths in the\nlattice, and generalize Katz's backing-off method to integrate all the mixture\nmodels got by traversing the entire lattice. Based on this framework, we\nformulate two types of model factorizations that are used in natural language\nmodeling.\n",
        "published": "2003-05-23T19:38:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0305041v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0306050v1",
        "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named\n  Entity Recognition",
        "summary": "  We describe the CoNLL-2003 shared task: language-independent named entity\nrecognition. We give background information on the data sets (English and\nGerman) and the evaluation method, present a general overview of the systems\nthat have taken part in the task and discuss their performance.\n",
        "published": "2003-06-12T12:35:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0306050v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0306062v1",
        "title": "Learning to Order Facts for Discourse Planning in Natural Language\n  Generation",
        "summary": "  This paper presents a machine learning approach to discourse planning in\nnatural language generation. More specifically, we address the problem of\nlearning the most natural ordering of facts in discourse plans for a specific\ndomain. We discuss our methodology and how it was instantiated using two\ndifferent machine learning algorithms. A quantitative evaluation performed in\nthe domain of museum exhibit descriptions indicates that our approach performs\nsignificantly better than manually constructed ordering rules. Being\nretrainable, the resulting planners can be ported easily to other similar\ndomains, without requiring language technology expertise.\n",
        "published": "2003-06-13T09:05:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0306062v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0306099v1",
        "title": "An Improved k-Nearest Neighbor Algorithm for Text Categorization",
        "summary": "  k is the most important parameter in a text categorization system based on\nk-Nearest Neighbor algorithm (kNN).In the classification process, k nearest\ndocuments to the test one in the training set are determined firstly. Then, the\npredication can be made according to the category distribution among these k\nnearest neighbors. Generally speaking, the class distribution in the training\nset is uneven. Some classes may have more samples than others. Therefore, the\nsystem performance is very sensitive to the choice of the parameter k. And it\nis very likely that a fixed k value will result in a bias on large categories.\nTo deal with these problems, we propose an improved kNN algorithm, which uses\ndifferent numbers of nearest neighbors for different categories, rather than a\nfixed number across all categories. More samples (nearest neighbors) will be\nused for deciding whether a test document should be classified to a category,\nwhich has more samples in the training set. Preliminary experiments on Chinese\ntext categorization show that our method is less sensitive to the parameter k\nthan the traditional one, and it can properly classify documents belonging to\nsmaller classes with a large k. The method is promising for some cases, where\nestimating the parameter k via cross-validation is not allowed.\n",
        "published": "2003-06-16T13:54:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0306099v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0307028v1",
        "title": "Issues in Communication Game",
        "summary": "  As interaction between autonomous agents, communication can be analyzed in\ngame-theoretic terms. Meaning game is proposed to formalize the core of\nintended communication in which the sender sends a message and the receiver\nattempts to infer its meaning intended by the sender. Basic issues involved in\nthe game of natural language communication are discussed, such as salience,\ngrammaticality, common sense, and common belief, together with some\ndemonstration of the feasibility of game-theoretic account of language.\n",
        "published": "2003-07-11T14:43:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0307028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0307030v1",
        "title": "Parsing and Generation with Tabulation and Compilation",
        "summary": "  The standard tabulation techniques for logic programming presuppose fixed\norder of computation. Some data-driven control should be introduced in order to\ndeal with diverse contexts. The present paper describes a data-driven method of\nconstraint transformation with a sort of compilation which subsumes\naccessibility check and last-call optimization, which characterize standard\nnatural-language parsing techniques, semantic-head-driven generation, etc.\n",
        "published": "2003-07-11T15:42:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0307030v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0307044v1",
        "title": "The Linguistic DS: Linguisitic Description in MPEG-7",
        "summary": "  MPEG-7 (Moving Picture Experts Group Phase 7) is an XML-based international\nstandard on semantic description of multimedia content. This document discusses\nthe Linguistic DS and related tools. The linguistic DS is a tool, based on the\nGDA tag set (http://i-content.org/GDA/tagset.html), for semantic annotation of\nlinguistic data in or associated with multimedia content. The current document\ntext reflects `Study of FPDAM - MPEG-7 MDS Extensions' issued in March 2003,\nand not most part of MPEG-7 MDS, for which the readers are referred to the\nfirst version of MPEG-7 MDS document available from ISO (http://www.iso.org).\nWithout that reference, however, this document should be mostly intelligible to\nthose who are familiar with XML and linguistic theories. Comments are welcome\nand will be considered in the standardization process.\n",
        "published": "2003-07-19T12:24:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0307044v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0308016v1",
        "title": "Collaborative Creation of Digital Content in Indian Languages",
        "summary": "  The world is passing through a major revolution called the information\nrevolution, in which information and knowledge is becoming available to people\nin unprecedented amounts wherever and whenever they need it. Those societies\nwhich fail to take advantage of the new technology will be left behind, just\nlike in the industrial revolution.\n  The information revolution is based on two major technologies: computers and\ncommunication. These technologies have to be delivered in a COST EFFECTIVE\nmanner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable\ntechnology choices, and to allow people to access through shared resources.\nThis could be done throuch street corner shops (for computer usage, e-mail\netc.), schools, community centres and local library centres.\n",
        "published": "2003-08-07T09:01:56Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0308016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0308017v1",
        "title": "Information Revolution",
        "summary": "  The world is passing through a major revolution called the information\nrevolution, in which information and knowledge is becoming available to people\nin unprecedented amounts wherever and whenever they need it. Those societies\nwhich fail to take advantage of the new technology will be left behind, just\nlike in the industrial revolution.\n  The information revolution is based on two major technologies: computers and\ncommunication. These technologies have to be delivered in a COST EFFECTIVE\nmanner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable\ntechnology choices (discussed later), and to allow people to access through\nshared resources. This could be done throuch street corner shops (for computer\nusage, e-mail etc.), schools, community centers and local library centres.\n",
        "published": "2003-08-07T09:17:16Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0308017v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0308018v1",
        "title": "Anusaaraka: Overcoming the Language Barrier in India",
        "summary": "  The anusaaraka system makes text in one Indian language accessible in another\nIndian language. In the anusaaraka approach, the load is so divided between man\nand computer that the language load is taken by the machine, and the\ninterpretation of the text is left to the man. The machine presents an image of\nthe source text in a language close to the target language.In the image, some\nconstructions of the source language (which do not have equivalents) spill over\nto the output. Some special notation is also devised. The user after some\ntraining learns to read and understand the output. Because the Indian languages\nare close, the learning time of the output language is short, and is expected\nto be around 2 weeks.\n  The output can also be post-edited by a trained user to make it grammatically\ncorrect in the target language. Style can also be changed, if necessary. Thus,\nin this scenario, it can function as a human assisted translation system.\n  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali\nand Punjabi to Hindi. They can be built for all Indian languages in the near\nfuture. Everybody must pitch in to build such systems connecting all Indian\nlanguages, using the free software model.\n",
        "published": "2003-08-07T09:24:46Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0308018v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0308019v1",
        "title": "Language Access: An Information Based Approach",
        "summary": "  The anusaaraka system (a kind of machine translation system) makes text in\none Indian language accessible through another Indian language. The machine\npresents an image of the source text in a language close to the target\nlanguage. In the image, some constructions of the source language (which do not\nhave equivalents in the target language) spill over to the output. Some special\nnotation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,\nMarathi, Bengali and Punjabi to Hindi. They are available for use through Email\nservers.\n  Anusaarkas follows the principle of substitutibility and reversibility of\nstrings produced. This implies preservation of information while going from a\nsource language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject\ndomain knowledge into the system, which produce good quality grammatical\noutput. However, it should be remembered, that such modules will work only in\nnarrow areas, and will sometimes go wrong. In such a situation, anusaaraka\noutput will still remain useful.\n",
        "published": "2003-08-07T09:40:04Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0308019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0308020v1",
        "title": "LERIL : Collaborative Effort for Creating Lexical Resources",
        "summary": "  The paper reports on efforts taken to create lexical resources pertaining to\nIndian languages, using the collaborative model. The lexical resources being\ndeveloped are: (1) Transfer lexicon and grammar from English to several Indian\nlanguages. (2) Dependencey tree bank of annotated corpora for several Indian\nlanguages. The dependency trees are based on the Paninian model. (3) Bilingual\ndictionary of 'core meanings'.\n",
        "published": "2003-08-07T10:08:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0308020v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0309019v1",
        "title": "Building a Test Collection for Speech-Driven Web Retrieval",
        "summary": "  This paper describes a test collection (benchmark data) for retrieval systems\ndriven by spoken queries. This collection was produced in the subtask of the\nNTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation\nworkshop. The search topics and document collection for the Web retrieval task\nwere used to produce spoken queries and language models for speech recognition,\nrespectively. We used this collection to evaluate the performance of our\nretrieval system. Experimental results showed that (a) the use of target\ndocuments for language modeling and (b) enhancement of the vocabulary size in\nspeech recognition were effective in improving the system performance.\n",
        "published": "2003-09-12T12:43:00Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0309019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0309021v1",
        "title": "A Cross-media Retrieval System for Lecture Videos",
        "summary": "  We propose a cross-media lecture-on-demand system, in which users can\nselectively view specific segments of lecture videos by submitting text\nqueries. Users can easily formulate queries by using the textbook associated\nwith a target lecture, even if they cannot come up with effective keywords. Our\nsystem extracts the audio track from a target lecture video, generates a\ntranscription by large vocabulary continuous speech recognition, and produces a\ntext index. Experimental results showed that by adapting speech recognition to\nthe topic of the lecture, the recognition accuracy increased and the retrieval\naccuracy was comparable with that obtained by human transcription.\n",
        "published": "2003-09-13T06:54:58Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0309021v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0310014v1",
        "title": "Effective XML Representation for Spoken Language in Organisations",
        "summary": "  Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables.\n",
        "published": "2003-10-08T16:40:33Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0310014v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0310058v1",
        "title": "Application Architecture for Spoken Language Resources in Organisational\n  Settings",
        "summary": "  Special technologies need to be used to take advantage of, and overcome, the\nchallenges associated with acquiring, transforming, storing, processing, and\ndistributing spoken language resources in organisations. This paper introduces\nan application architecture consisting of tools and supporting utilities for\nindexing and transcription, and describes how these tools, together with\ndownstream processing and distribution systems, can be integrated into a\nworkflow. Two sample applications for this architecture are outlined- the\nanalysis of decision-making processes in organisations and the deployment of\nsystems development methods by designers in the field.\n",
        "published": "2003-10-29T20:13:30Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0310058v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0311033v1",
        "title": "The Rank-Frequency Analysis for the Functional Style Corpora in the\n  Ukrainian Language",
        "summary": "  We use the rank-frequency analysis for the estimation of Kernel Vocabulary\nsize within specific corpora of Ukrainian. The extrapolation of high-rank\nbehaviour is utilized for estimation of the total vocabulary size.\n",
        "published": "2003-11-21T19:48:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0311033v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0311036v1",
        "title": "Measuring the Functional Load of Phonological Contrasts",
        "summary": "  Frequency counts are a measure of how much use a language makes of a\nlinguistic unit, such as a phoneme or word. However, what is often important is\nnot the units themselves, but the contrasts between them. A measure is\ntherefore needed for how much use a language makes of a contrast, i.e. the\nfunctional load (FL) of the contrast. We generalize previous work in\nlinguistics and speech recognition and propose a family of measures for the FL\nof several phonological contrasts, including phonemic oppositions, distinctive\nfeatures, suprasegmentals, and phonological rules. We then test it for\nrobustness to changes of corpora. Finally, we provide examples in Cantonese,\nDutch, English, German and Mandarin, in the context of historical linguistics,\nlanguage acquisition and speech recognition. More information can be found at\nhttp://dinoj.info/research/fload\n",
        "published": "2003-11-24T18:05:40Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0311036v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0402055v1",
        "title": "Lexical Base as a Compressed Language Model of the World (on the\n  material of the Ukrainian language)",
        "summary": "  In the article the fact is verified that the list of words selected by formal\nstatistical methods (frequency and functional genre unrestrictedness) is not a\nconglomerate of non-related words. It creates a system of interrelated items\nand it can be named \"lexical base of language\". This selected list of words\ncovers all the spheres of human activities. To verify this statement the\ninvariant synoptical scheme common for ideographic dictionaries of different\nlanguage was determined.\n",
        "published": "2004-02-24T09:34:16Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0402055v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0404007v1",
        "title": "Polarity sensitivity and evaluation order in type-logical grammar",
        "summary": "  We present a novel, type-logical analysis of_polarity sensitivity_: how\nnegative polarity items (like \"any\" and \"ever\") or positive ones (like \"some\")\nare licensed or prohibited. It takes not just scopal relations but also linear\norder into account, using the programming-language notions of delimited\ncontinuations and evaluation order, respectively. It thus achieves greater\nempirical coverage than previous proposals.\n",
        "published": "2004-04-05T02:14:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0404007v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0404009v1",
        "title": "Tabular Parsing",
        "summary": "  This is a tutorial on tabular parsing, on the basis of tabulation of\nnondeterministic push-down automata. Discussed are Earley's algorithm, the\nCocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse\ntrees, and further issues.\n",
        "published": "2004-04-05T11:51:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0404009v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0404025v1",
        "title": "Test Collections for Patent-to-Patent Retrieval and Patent Map\n  Generation in NTCIR-4 Workshop",
        "summary": "  This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop,\nand the test collections produced in this task. We perform the invalidity\nsearch task, in which each participant group searches a patent collection for\nthe patents that can invalidate the demand in an existing claim. We also\nperform the automatic patent map generation task, in which the patents\nassociated with a specific topic are organized in a multi-dimensional matrix.\n",
        "published": "2004-04-10T08:43:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0404025v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0405037v1",
        "title": "A Probabilistic Model of Machine Translation",
        "summary": "  A probabilistic model for computer-based generation of a machine translation\nsystem on the basis of English-Russian parallel text corpora is suggested. The\nmodel is trained using parallel text corpora with pre-aligned source and target\nsentences. The training of the model results in a bilingual dictionary of words\nand \"word blocks\" with relevant translation probability.\n",
        "published": "2004-05-10T16:05:23Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0405037v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0405039v1",
        "title": "Catching the Drift: Probabilistic Content Models, with Applications to\n  Generation and Summarization",
        "summary": "  We consider the problem of modeling the content structure of texts within a\nspecific domain, in terms of the topics the texts address and the order in\nwhich these topics appear. We first present an effective knowledge-lean method\nfor learning content models from un-annotated documents, utilizing a novel\nadaptation of algorithms for Hidden Markov Models. We then apply our method to\ntwo complementary tasks: information ordering and extractive summarization. Our\nexperiments show that incorporating content models in these applications yields\nsubstantial improvement over previously-proposed methods.\n",
        "published": "2004-05-12T14:14:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0405039v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0406031v1",
        "title": "A Public Reference Implementation of the RAP Anaphora Resolution\n  Algorithm",
        "summary": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n",
        "published": "2004-06-17T12:29:29Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0406031v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0406054v1",
        "title": "Building a linguistic corpus from bee dance data",
        "summary": "  This paper discusses the problems and possibility of collecting bee dance\ndata in a linguistic \\textit{corpus} and use linguistic instruments such as\nZipf's law and entropy statistics to decide on the question whether the dance\ncarries information of any kind. We describe this against the historical\nbackground of attempts to analyse non-human communication systems.\n",
        "published": "2004-06-28T10:25:22Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0406054v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407002v1",
        "title": "Annotating Predicate-Argument Structure for a Parallel Treebank",
        "summary": "  We report on a recently initiated project which aims at building a\nmulti-layered parallel treebank of English and German. Particular attention is\ndevoted to a dedicated predicate-argument layer which is used for aligning\ntranslationally equivalent sentences of the two languages. We describe both our\nconceptual decisions and aspects of their technical realisation. We discuss\nsome selected problems and conclude with a few remarks on how this project\nrelates to similar projects in the field.\n",
        "published": "2004-07-01T16:18:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407002v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407005v3",
        "title": "Statistical Machine Translation by Generalized Parsing",
        "summary": "  Designers of statistical machine translation (SMT) systems have begun to\nemploy tree-structured translation models. Systems involving tree-structured\ntranslation models tend to be complex. This article aims to reduce the\nconceptual complexity of such systems, in order to make them easier to design,\nimplement, debug, use, study, understand, explain, modify, and improve. In\nservice of this goal, the article extends the theory of semiring parsing to\narrive at a novel abstract parsing algorithm with five functional parameters: a\nlogic, a grammar, a semiring, a search strategy, and a termination condition.\nThe article then shows that all the common algorithms that revolve around\ntree-structured translation models, including hierarchical alignment, inference\nfor parameter estimation, translation, and structured evaluation, can be\nderived by generalizing two of these parameters -- the grammar and the logic.\nThe article culminates with a recipe for using such generalized parsers to\ntrain, apply, and evaluate an SMT system that is driven by tree-structured\ntranslation models.\n",
        "published": "2004-07-01T22:02:10Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407005v3"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407026v1",
        "title": "Summarizing Encyclopedic Term Descriptions on the Web",
        "summary": "  We are developing an automatic method to compile an encyclopedic corpus from\nthe Web. In our previous work, paragraph-style descriptions for a term are\nextracted from Web pages and organized based on domains. However, these\ndescriptions are independent and do not comprise a condensed text as in\nhand-crafted encyclopedias. To resolve this problem, we propose a summarization\nmethod, which produces a single text from multiple descriptions. The resultant\nsummary concisely describes a term from different viewpoints. We also show the\neffectiveness of our method by means of experiments.\n",
        "published": "2004-07-10T11:18:42Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407027v1",
        "title": "Unsupervised Topic Adaptation for Lecture Speech Retrieval",
        "summary": "  We are developing a cross-media information retrieval system, in which users\ncan view specific segments of lecture videos by submitting text queries. To\nproduce a text index, the audio track is extracted from a lecture video and a\ntranscription is generated by automatic speech recognition. In this paper, to\nimprove the quality of our retrieval system, we extensively investigate the\neffects of adapting acoustic and language models on speech recognition. We\nperform an MLLR-based method to adapt an acoustic model. To obtain a corpus for\nlanguage model adaptation, we use the textbook for a target lecture to search a\nWeb collection for the pages associated with the lecture topic. We show the\neffectiveness of our method by means of experiments.\n",
        "published": "2004-07-10T11:45:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407028v1",
        "title": "Effects of Language Modeling on Speech-driven Question Answering",
        "summary": "  We integrate automatic speech recognition (ASR) and question answering (QA)\nto realize a speech-driven QA system, and evaluate its performance. We adapt an\nN-gram language model to natural language questions, so that the input of our\nsystem can be recognized with a high accuracy. We target WH-questions which\nconsist of the topic part and fixed phrase used to ask about something. We\nfirst produce a general N-gram model intended to recognize the topic and\nemphasize the counts of the N-grams that correspond to the fixed phrases. Given\na transcription by the ASR engine, the QA engine extracts the answer candidates\nfrom target documents. We propose a passage retrieval method robust against\nrecognition errors in the transcription. We use the QA test collection produced\nin NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness\nof our method by means of experiments.\n",
        "published": "2004-07-10T11:57:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407028v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0407046v1",
        "title": "A Bimachine Compiler for Ranked Tagging Rules",
        "summary": "  This paper describes a novel method of compiling ranked tagging rules into a\ndeterministic finite-state device called a bimachine. The rules are formulated\nin the framework of regular rewrite operations and allow unrestricted regular\nexpressions in both left and right rule contexts. The compiler is illustrated\nby an application within a speech synthesis system.\n",
        "published": "2004-07-19T11:57:42Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0407046v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0408052v1",
        "title": "Application of the Double Metaphone Algorithm to Amharic Orthography",
        "summary": "  The Metaphone algorithm applies the phonetic encoding of orthographic\nsequences to simplify words prior to comparison. While Metaphone has been\nhighly successful for the English language, for which it was designed, it may\nnot be applied directly to Ethiopian languages. The paper details how the\nprinciples of Metaphone can be applied to Ethiopic script and uses Amharic as a\ncase study. Match results improve as specific considerations are made for\nAmharic writing practices. Results are shown to improve further when common\nerrors from Amharic input methods are considered.\n",
        "published": "2004-08-22T19:32:48Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0408052v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0408059v1",
        "title": "Proofing Tools Technology at Neurosoft S.A.",
        "summary": "  The aim of this paper is to present the R&D activities carried out at\nNeurosoft S.A. regarding the development of proofing tools for Modern Greek.\nFirstly, we focus on infrastructure issues that we faced during our initial\nsteps. Subsequently, we describe the most important insights of three proofing\ntools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the\nthesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss\nsome improvement ideas and give our future directions.\n",
        "published": "2004-08-26T10:50:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0408059v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0408060v1",
        "title": "Verbal chunk extraction in French using limited resources",
        "summary": "  A way of extracting French verbal chunks, inflected and infinitive, is\nexplored and tested on effective corpus. Declarative morphological and local\ngrammar rules specifying chunks and some simple contextual structures are used,\nrelying on limited lexical information and some simple heuristic/statistic\nproperties obtained from restricted corpora. The specific goals, the\narchitecture and the formalism of the system, the linguistic information on\nwhich it relies and the obtained results on effective corpus are presented.\n",
        "published": "2004-08-26T12:44:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0408060v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0408061v1",
        "title": "An electronic dictionary as a basis for NLP tools: The Greek case",
        "summary": "  The existence of a Dictionary in electronic form for Modern Greek (MG) is\nmandatory if one is to process MG at the morphological and syntactic levels\nsince MG is a highly inflectional language with marked stress and a spelling\nsystem with many characteristics carried over from Ancient Greek. Moreover,\nsuch a tool becomes necessary if one is to create efficient and sophisticated\nNLP applications with substantial linguistic backing and coverage. The present\npaper will focus on the deployment of such an electronic dictionary for Modern\nGreek, which was built in two phases: first it was constructed to be the basis\nfor a spelling correction schema and then it was reconstructed in order to\nbecome the platform for the deployment of a wider spectrum of NLP tools.\n",
        "published": "2004-08-26T13:17:38Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0408061v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0409008v1",
        "title": "A Model for Fine-Grained Alignment of Multilingual Texts",
        "summary": "  While alignment of texts on the sentential level is often seen as being too\ncoarse, and word alignment as being too fine-grained, bi- or multilingual texts\nwhich are aligned on a level in-between are a useful resource for many\npurposes. Starting from a number of examples of non-literal translations, which\ntend to make alignment difficult, we describe an alignment model which copes\nwith these cases by explicitly coding them. The model is based on\npredicate-argument structures and thus covers the middle ground between\nsentence and word alignment. The model is currently used in a recently\ninitiated project of a parallel English-German treebank (FuSe), which can in\nprinciple be extended with additional languages.\n",
        "published": "2004-09-07T13:46:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0409008v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0409058v1",
        "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity\n  Summarization Based on Minimum Cuts",
        "summary": "  Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;\nan example application is classifying a movie review as \"thumbs up\" or \"thumbs\ndown\". To determine this sentiment polarity, we propose a novel\nmachine-learning method that applies text-categorization techniques to just the\nsubjective portions of the document. Extracting these portions can be\nimplemented using efficient techniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence contextual constraints.\n",
        "published": "2004-09-29T20:34:04Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0409058v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0412015v2",
        "title": "A Tutorial on the Expectation-Maximization Algorithm Including\n  Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free\n  Grammars",
        "summary": "  The paper gives a brief review of the expectation-maximization algorithm\n(Dempster 1977) in the comprehensible framework of discrete mathematics. In\nSection 2, two prominent estimation methods, the relative-frequency estimation\nand the maximum-likelihood estimation are presented. Section 3 is dedicated to\nthe expectation-maximization algorithm and a simpler variant, the generalized\nexpectation-maximization algorithm. In Section 4, two loaded dice are rolled. A\nmore interesting example is presented in Section 5: The estimation of\nprobabilistic context-free grammars.\n",
        "published": "2004-12-03T17:10:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0412015v2"
    },
    {
        "id": "http://arxiv.org/abs/cs/0412016v1",
        "title": "Inside-Outside Estimation Meets Dynamic EM",
        "summary": "  We briefly review the inside-outside and EM algorithm for probabilistic\ncontext-free grammars. As a result, we formally prove that inside-outside\nestimation is a dynamic-programming variant of EM. This is interesting in its\nown right, but even more when considered in a theoretical context since the\nwell-known convergence behavior of inside-outside estimation has been confirmed\nby many experiments but apparently has never been formally proved. However,\nbeing a version of EM, inside-outside estimation also inherits the good\nconvergence behavior of EM. Therefore, the as yet imperfect line of\nargumentation can be transformed into a coherent proof.\n",
        "published": "2004-12-03T18:10:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0412016v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0412114v1",
        "title": "State of the Art, Evaluation and Recommendations regarding \"Document\n  Processing and Visualization Techniques\"",
        "summary": "  Several Networks of Excellence have been set up in the framework of the\nEuropean FP5 research program. Among these Networks of Excellence, the NEMIS\nproject focuses on the field of Text Mining.\n  Within this field, document processing and visualization was identified as\none of the key topics and the WG1 working group was created in the NEMIS\nproject, to carry out a detailed survey of techniques associated with the text\nmining process and to identify the relevant research topics in related research\nareas.\n  In this document we present the results of this comprehensive survey. The\nreport includes a description of the current state-of-the-art and practice, a\nroadmap for follow-up research in the identified areas, and recommendations for\nanticipated technological development in the domain of text mining.\n",
        "published": "2004-12-29T15:19:03Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0412114v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0412117v1",
        "title": "Thematic Annotation: extracting concepts out of documents",
        "summary": "  Contrarily to standard approaches to topic annotation, the technique used in\nthis work does not centrally rely on some sort of -- possibly statistical --\nkeyword extraction. In fact, the proposed annotation algorithm uses a large\nscale semantic database -- the EDR Electronic Dictionary -- that provides a\nconcept hierarchy based on hyponym and hypernym relations. This concept\nhierarchy is used to generate a synthetic representation of the document by\naggregating the words present in topically homogeneous document segments into a\nset of concepts best preserving the document's content.\n  This new extraction technique uses an unexplored approach to topic selection.\nInstead of using semantic similarity measures based on a semantic resource, the\nlater is processed to extract the part of the conceptual hierarchy relevant to\nthe document content. Then this conceptual hierarchy is searched to extract the\nmost relevant set of concepts to represent the topics discussed in the\ndocument. Notice that this algorithm is able to extract generic concepts that\nare not directly present in the document.\n",
        "published": "2004-12-30T02:01:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0412117v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0501078v1",
        "title": "Multi-document Biography Summarization",
        "summary": "  In this paper we describe a biography summarization system using sentence\nclassification and ideas from information retrieval. Although the individual\ntechniques are not new, assembling and applying them to generate multi-document\nbiographies is new. Our system was evaluated in DUC2004. It is among the top\nperformers in task 5-short summaries focused by person questions.\n",
        "published": "2005-01-26T22:43:17Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0501078v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0504022v1",
        "title": "A Matter of Opinion: Sentiment Analysis and Business Intelligence\n  (position paper)",
        "summary": "  A general-audience introduction to the area of \"sentiment analysis\", the\ncomputational treatment of subjective, opinion-oriented language (an example\napplication is determining whether a review is \"thumbs up\" or \"thumbs down\").\nSome challenges, applications to business-intelligence tasks, and potential\nfuture directions are described.\n",
        "published": "2005-04-06T20:04:55Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0504022v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0510015v1",
        "title": "Word sense disambiguation criteria: a systematic study",
        "summary": "  This article describes the results of a systematic in-depth study of the\ncriteria used for word sense disambiguation. Our study is based on 60 target\nwords: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line\nwith some practices in the field. For example, we show that omitting\nnon-content words decreases performance and that bigrams yield better results\nthan unigrams.\n",
        "published": "2005-10-05T14:23:19Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0510015v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0511076v1",
        "title": "Using phonetic constraints in acoustic-to-articulatory inversion",
        "summary": "  The goal of this work is to recover articulatory information from the speech\nsignal by acoustic-to-articulatory inversion. One of the main difficulties with\ninversion is that the problem is underdetermined and inversion methods\ngenerally offer no guarantee on the phonetical realism of the inverse\nsolutions. A way to adress this issue is to use additional phonetic\nconstraints. Knowledge of the phonetic caracteristics of French vowels enable\nthe derivation of reasonable articulatory domains in the space of Maeda\nparameters: given the formants frequencies (F1,F2,F3) of a speech sample, and\nthus the vowel identity, an \"ideal\" articulatory domain can be derived. The\nspace of formants frequencies is partitioned into vowels, using either\nspeaker-specific data or generic information on formants. Then, to each\narticulatory vector can be associated a phonetic score varying with the\ndistance to the \"ideal domain\" associated with the corresponding vowel.\nInversion experiments were conducted on isolated vowels and vowel-to-vowel\ntransitions. Articulatory parameters were compared with those obtained without\nusing these constraints and those measured from X-ray data.\n",
        "published": "2005-11-21T14:50:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0511076v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0511079v1",
        "title": "An elitist approach for extracting automatically well-realized speech\n  sounds with high confidence",
        "summary": "  This paper presents an \"elitist approach\" for extracting automatically\nwell-realized speech sounds with high confidence. The elitist approach uses a\nspeech recognition system based on Hidden Markov Models (HMM). The HMM are\ntrained on speech sounds which are systematically well-detected in an iterative\nprocedure. The results show that, by using the HMM models defined in the\ntraining phase, the speech recognizer detects reliably specific speech sounds\nwith a small rate of errors.\n",
        "published": "2005-11-22T07:06:43Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0511079v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0512102v1",
        "title": "Statistical Parameters of the Novel \"Perekhresni stezhky\" (\"The\n  Cross-Paths\") by Ivan Franko",
        "summary": "  In the paper, a complex statistical characteristics of a Ukrainian novel is\ngiven for the first time. The distribution of word-forms with respect to their\nsize is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath\nare analyzed.\n",
        "published": "2005-12-28T13:45:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0512102v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0601005v1",
        "title": "Analyzing language development from a network approach",
        "summary": "  In this paper we propose some new measures of language development using\nnetwork analyses, which is inspired by the recent surge of interests in network\nstudies of many real-world systems. Children's and care-takers' speech data\nfrom a longitudinal study are represented as a series of networks, word forms\nbeing taken as nodes and collocation of words as links. Measures on the\nproperties of the networks, such as size, connectivity, hub and authority\nanalyses, etc., allow us to make quantitative comparison so as to reveal\ndifferent paths of development. For example, the asynchrony of development in\nnetwork size and average degree suggests that children cannot be simply\nclassified as early talkers or late talkers by one or two measures. Children\nfollow different paths in a multi-dimensional space. They may develop faster in\none dimension but slower in another dimension. The network approach requires\nlittle preprocessing of words and analyses on sentence structures, and the\ncharacteristics of words and their usage emerge from the network and are\nindependent of any grammatical presumptions. We show that the change of the two\narticles \"the\" and \"a\" in their roles as important nodes in the network\nreflects the progress of children's syntactic development: the two articles\noften start in children's networks as hubs and later shift to authorities,\nwhile they are authorities constantly in the adult's networks. The network\nanalyses provide a new approach to study language development, and at the same\ntime language development also presents a rich area for network theories to\nexplore.\n",
        "published": "2006-01-04T18:33:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0601005v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0604027v1",
        "title": "Unification of multi-lingual scientific terminological resources using\n  the ISO 16642 standard. The TermSciences initiative",
        "summary": "  This paper presents the TermSciences portal, which deals with the\nimplementation of a conceptual model that uses the recent ISO 16642 standard\n(Terminological Markup Framework). This standard turns out to be suitable for\nconcept modeling since it allowed for organizing the original resources by\nconcepts and to associate the various terms for a given concept. Additional\nstructuring is produced by sharing conceptual relationships, that is,\ncross-linking of resource results through the introduction of semantic\nrelations which may have initially be missing.\n",
        "published": "2006-04-07T13:10:30Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0604027v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0606006v1",
        "title": "Foundations of Modern Language Resource Archives",
        "summary": "  A number of serious reasons will convince an increasing amount of researchers\nto store their relevant material in centers which we will call \"language\nresource archives\". They combine the duty of taking care of long-term\npreservation as well as the task to give access to their material to different\nuser groups. Access here is meant in the sense that an active interaction with\nthe data will be made possible to support the integration of new data, new\nversions or commentaries of all sort. Modern Language Resource Archives will\nhave to adhere to a number of basic principles to fulfill all requirements and\nthey will have to be involved in federations to create joint language resource\ndomains making it even more simple for the researchers to access the data. This\npaper makes an attempt to formulate the essential pillars language resource\narchives have to adhere to.\n",
        "published": "2006-06-01T09:14:51Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0606006v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0606096v1",
        "title": "Building a resource for studying translation shifts",
        "summary": "  This paper describes an interdisciplinary approach which brings together the\nfields of corpus linguistics and translation studies. It presents ongoing work\non the creation of a corpus resource in which translation shifts are explicitly\nannotated. Translation shifts denote departures from formal correspondence\nbetween source and target text, i.e. deviations that have occurred during the\ntranslation process. A resource in which such shifts are annotated in a\nsystematic way will make it possible to study those phenomena that need to be\naddressed if machine translation output is to resemble human translation. The\nresource described in this paper contains English source texts (parliamentary\nproceedings) and their German translations. The shift annotation is based on\npredicate-argument structures and proceeds in two steps: first, predicates and\ntheir arguments are annotated monolingually in a straightforward manner. Then,\nthe corresponding English and German predicates and arguments are aligned with\neach other. Whenever a shift - mainly grammatical or semantic -has occurred,\nthe alignment is tagged accordingly.\n",
        "published": "2006-06-22T13:26:52Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0606096v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0607051v1",
        "title": "Raisonner avec des diagrammes : perspectives cognitives et\n  computationnelles",
        "summary": "  Diagrammatic, analogical or iconic representations are often contrasted with\nlinguistic or logical representations, in which the shape of the symbols is\narbitrary. The aim of this paper is to make a case for the usefulness of\ndiagrams in inferential knowledge representation systems. Although commonly\nused, diagrams have for a long time suffered from the reputation of being only\na heuristic tool or a mere support for intuition. The first part of this paper\nis an historical background paying tribute to the logicians, psychologists and\ncomputer scientists who put an end to this formal prejudice against diagrams.\nThe second part is a discussion of their characteristics as opposed to those of\nlinguistic forms. The last part is aimed at reviving the interest for\nheterogeneous representation systems including both linguistic and diagrammatic\nrepresentations.\n",
        "published": "2006-07-11T19:13:25Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0607051v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0609019v1",
        "title": "Improving Term Extraction with Terminological Resources",
        "summary": "  Studies of different term extractors on a corpus of the biomedical domain\nrevealed decreasing performances when applied to highly technical texts. The\ndifficulty or impossibility of customising them to new domains is an additional\nlimitation. In this paper, we propose to use external terminologies to\ninfluence generic linguistic data in order to augment the quality of the\nextraction. The tool we implemented exploits testified terms at different steps\nof the process: chunking, parsing and extraction of term candidates.\nExperiments reported here show that, using this method, more term candidates\ncan be acquired with a higher level of reliability. We further describe the\nextraction process involving endogenous disambiguation implemented in the term\nextractor YaTeA.\n",
        "published": "2006-09-06T11:41:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0609019v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0609043v1",
        "title": "Challenging the principle of compositionality in interpreting natural\n  language texts",
        "summary": "  The paper aims at emphasizing that, even relaxed, the hypothesis of\ncompositionality has to face many problems when used for interpreting natural\nlanguage texts. Rather than fixing these problems within the compositional\nframework, we believe that a more radical change is necessary, and propose\nanother approach.\n",
        "published": "2006-09-08T14:01:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0609043v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0609044v1",
        "title": "The role of time in considering collections",
        "summary": "  The paper concerns the understanding of plurals in the framework of\nArtificial Intelligence and emphasizes the role of time. The construction of\ncollection(s) and their evolution across time is often crucial and has to be\naccounted for. The paper contrasts a \"de dicto\" collection where the collection\ncan be considered as persisting over these situations even if its members\nchange with a \"de re\" collection whose composition does not vary through time.\nIt expresses different criteria of choice between the two interpretations (de\nre and de dicto) depending on the context of enunciation.\n",
        "published": "2006-09-08T14:11:50Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0609044v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0609058v1",
        "title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+\n  languages",
        "summary": "  We present a new, unique and freely available parallel corpus containing\nEuropean Union (EU) documents of mostly legal nature. It is available in all 20\nofficial EUanguages, with additional documents being available in the languages\nof the EU candidate countries. The corpus consists of almost 8,000 documents\nper language, with an average size of nearly 9 million words per language.\nPair-wise paragraph alignment information produced by two different aligners\n(Vanilla and HunAlign) is available for all 190+ language pair combinations.\nMost texts have been manually classified according to the EUROVOC subject\ndomains so that the collection can also be used to train and test multi-label\nclassification algorithms and keyword-assignment software. The corpus is\nencoded in XML, according to the Text Encoding Initiative Guidelines. Due to\nthe large number of parallel texts in many languages, the JRC-Acquis is\nparticularly suitable to carry out all types of cross-language research, as\nwell as to test and benchmark text analysis software across different languages\n(for instance for alignment, sentence splitting and term extraction).\n",
        "published": "2006-09-12T07:10:15Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0609058v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0610116v1",
        "title": "DepAnn - An Annotation Tool for Dependency Treebanks",
        "summary": "  DepAnn is an interactive annotation tool for dependency treebanks, providing\nboth graphical and text-based annotation interfaces. The tool is aimed for\nsemi-automatic creation of treebanks. It aids the manual inspection and\ncorrection of automatically created parses, making the annotation process\nfaster and less error-prone. A novel feature of the tool is that it enables the\nuser to view outputs from several parsers as the basis for creating the final\ntree to be saved to the treebank. DepAnn uses TIGER-XML, an XML-based general\nencoding format for both, representing the parser outputs and saving the\nannotated treebank. The tool includes an automatic consistency checker for\nsentence structures. In addition, the tool enables users to build structures\nmanually, add comments on the annotations, modify the tagsets, and mark\nsentences for further revision.\n",
        "published": "2006-10-19T17:42:57Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0610116v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0610124v1",
        "title": "Dependency Treebanks: Methods, Annotation Schemes and Tools",
        "summary": "  In this paper, current dependencybased treebanks are introduced and analyzed.\nThe methods used for building the resources, the annotation schemes applied,\nand the tools used (such as POS taggers, parsers and annotation software) are\ndiscussed.\n",
        "published": "2006-10-20T11:48:38Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0610124v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0611026v1",
        "title": "Un modle gnrique d'organisation de corpus en ligne: application\n   la FReeBank",
        "summary": "  The few available French resources for evaluating linguistic models or\nalgorithms on other linguistic levels than morpho-syntax are either\ninsufficient from quantitative as well as qualitative point of view or not\nfreely accessible. Based on this fact, the FREEBANK project intends to create\nFrench corpora constructed using manually revised output from a hybrid\nConstraint Grammar parser and annotated on several linguistic levels\n(structure, morpho-syntax, syntax, coreference), with the objective to make\nthem available on-line for research purposes. Therefore, we will focus on using\nstandard annotation schemes, integration of existing resources and maintenance\nallowing for continuous enrichment of the annotations. Prior to the actual\npresentation of the prototype that has been implemented, this paper describes a\ngeneric model for the organization and deployment of a linguistic resource\narchive, in compliance with the various works currently conducted within\ninternational standardization initiatives (TEI and ISO/TC 37/SC 4).\n",
        "published": "2006-11-06T14:37:27Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0611026v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0611069v1",
        "title": "Scaling Construction Grammar up to Production Systems: the SCIM",
        "summary": "  While a great effort has concerned the development of fully integrated\nmodular understanding systems, few researches have focused on the problem of\nunifying existing linguistic formalisms with cognitive processing models. The\nSituated Constructional Interpretation Model is one of these attempts. In this\nmodel, the notion of \"construction\" has been adapted in order to be able to\nmimic the behavior of Production Systems. The Construction Grammar approach\nestablishes a model of the relations between linguistic forms and meaning, by\nthe mean of constructions. The latter can be considered as pairings from a\ntopologically structured space to an unstructured space, in some way a special\nkind of production rules.\n",
        "published": "2006-11-15T12:35:45Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0611069v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0611113v1",
        "title": "An Anthological Review of Research Utilizing MontyLingua, a Python-Based\n  End-to-End Text Processor",
        "summary": "  MontyLingua, an integral part of ConceptNet which is currently the largest\ncommonsense knowledge base, is an English text processor developed using Python\nprogramming language in MIT Media Lab. The main feature of MontyLingua is the\ncoverage for all aspects of English text processing from raw input text to\nsemantic meanings and summary generation, yet each component in MontyLingua is\nloosely-coupled to each other at the architectural and code level, which\nenabled individual components to be used independently or substituted. However,\nthere has been no review exploring the role of MontyLingua in recent research\nwork utilizing it. This paper aims to review the use of and roles played by\nMontyLingua and its components in research work published in 19 articles\nbetween October 2004 and August 2006. We had observed a diversified use of\nMontyLingua in many different areas, both generic and domain-specific. Although\nthe use of text summarizing component had not been observe, we are optimistic\nthat it will have a crucial role in managing the current trend of information\noverload in future research.\n",
        "published": "2006-11-22T03:24:54Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0611113v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0701135v1",
        "title": "Complex networks and human language",
        "summary": "  This paper introduces how human languages can be studied in light of recent\ndevelopment of network theories. There are two directions of exploration. One\nis to study networks existing in the language system. Various lexical networks\ncan be built based on different relationships between words, being semantic or\nsyntactic. Recent studies have shown that these lexical networks exhibit\nsmall-world and scale-free features. The other direction of exploration is to\nstudy networks of language users (i.e. social networks of people in the\nlinguistic community), and their role in language evolution. Social networks\nalso show small-world and scale-free features, which cannot be captured by\nrandom or regular network models. In the past, computational models of language\nchange and language emergence often assume a population to have a random or\nregular structure, and there has been little discussion how network structures\nmay affect the dynamics. In the second part of the paper, a series of\nsimulation models of diffusion of linguistic innovation are used to illustrate\nthe importance of choosing realistic conditions of population structure for\nmodeling language change. Four types of social networks are compared, which\nexhibit two categories of diffusion dynamics. While the questions about which\ntype of networks are more appropriate for modeling still remains, we give some\npreliminary suggestions for choosing the type of social networks for modeling.\n",
        "published": "2007-01-22T00:45:31Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0701135v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0701181v1",
        "title": "A Note on Local Ultrametricity in Text",
        "summary": "  High dimensional, sparsely populated data spaces have been characterized in\nterms of ultrametric topology. This implies that there are natural, not\nnecessarily unique, tree or hierarchy structures defined by the ultrametric\ntopology. In this note we study the extent of local ultrametric topology in\ntexts, with the aim of finding unique ``fingerprints'' for a text or corpus,\ndiscriminating between texts from different domains, and opening up the\npossibility of exploiting hierarchical structures in the data. We use coherent\nand meaningful collections of over 1000 texts, comprising over 1.3 million\nwords.\n",
        "published": "2007-01-27T19:09:53Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0701181v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0701194v1",
        "title": "Menzerath-Altmann Law for Syntactic Structures in Ukrainian",
        "summary": "  In the paper, the definition of clause suitable for an automated processing\nof a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the\nsentence level and the parameters for the dependences of the clause length\ncounted in words and syllables on the sentence length counted in clauses are\ncalculated for \"Perekhresni Stezhky\" (\"The Cross-Paths\"), a novel by Ivan\nFranko.\n",
        "published": "2007-01-30T16:58:07Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0701194v1"
    },
    {
        "id": "http://arxiv.org/abs/cs/0702081v1",
        "title": "Random Sentences from a Generalized Phrase-Structure Grammar Interpreter",
        "summary": "  In numerous domains in cognitive science it is often useful to have a source\nfor randomly generated corpora. These corpora may serve as a foundation for\nartificial stimuli in a learning experiment (e.g., Ellefson & Christiansen,\n2000), or as input into computational models (e.g., Christiansen & Dale, 2001).\nThe following compact and general C program interprets a phrase-structure\ngrammar specified in a text file. It follows parameters set at a Unix or\nUnix-based command-line and generates a corpus of random sentences from that\ngrammar.\n",
        "published": "2007-02-14T06:05:20Z",
        "pdf_link": "http://arxiv.org/pdf/cs/0702081v1"
    },
    {
        "id": "http://arxiv.org/abs/0704.3708v2",
        "title": "Network statistics on early English Syntax: Structural criteria",
        "summary": "  This paper includes a reflection on the role of networks in the study of\nEnglish language acquisition, as well as a collection of practical criteria to\nannotate free-speech corpora from children utterances. At the theoretical\nlevel, the main claim of this paper is that syntactic networks should be\ninterpreted as the outcome of the use of the syntactic machinery. Thus, the\nintrinsic features of such machinery are not accessible directly from (known)\nnetwork properties. Rather, what one can see are the global patterns of its use\nand, thus, a global view of the power and organization of the underlying\ngrammar. Taking a look into more practical issues, the paper examines how to\nbuild a net from the projection of syntactic relations. Recall that, as opposed\nto adult grammars, early-child language has not a well-defined concept of\nstructure. To overcome such difficulty, we develop a set of systematic criteria\nassuming constituency hierarchy and a grammar based on lexico-thematic\nrelations. At the end, what we obtain is a well defined corpora annotation that\nenables us i) to perform statistics on the size of structures and ii) to build\na network from syntactic relations over which we can perform the standard\nmeasures of complexity. We also provide a detailed example.\n",
        "published": "2007-04-27T17:13:37Z",
        "pdf_link": "http://arxiv.org/pdf/0704.3708v2"
    },
    {
        "id": "http://arxiv.org/abs/0707.3269v1",
        "title": "International Standard for a Linguistic Annotation Framework",
        "summary": "  This paper describes the Linguistic Annotation Framework under development\nwithin ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to\nserve as a basis for harmonizing existing language resources as well as\ndeveloping new ones.\n",
        "published": "2007-07-22T15:24:48Z",
        "pdf_link": "http://arxiv.org/pdf/0707.3269v1"
    },
    {
        "id": "http://arxiv.org/abs/0707.3270v1",
        "title": "A Formal Model of Dictionary Structure and Content",
        "summary": "  We show that a general model of lexical information conforms to an abstract\nmodel that reflects the hierarchy of information found in a typical dictionary\nentry. We show that this model can be mapped into a well-formed XML document,\nand how the XSL transformation language can be used to implement a semantics\ndefined over the abstract model to enable extraction and manipulation of the\ninformation in any format.\n",
        "published": "2007-07-22T15:25:27Z",
        "pdf_link": "http://arxiv.org/pdf/0707.3270v1"
    },
    {
        "id": "http://arxiv.org/abs/0709.2401v1",
        "title": "Bootstrapping Deep Lexical Resources: Resources for Courses",
        "summary": "  We propose a range of deep lexical acquisition methods which make use of\nmorphological, syntactic and ontological language resources to model word\nsimilarity and bootstrap from a seed lexicon. The different methods are\ndeployed in learning lexical items for a precision grammar, and shown to each\nhave strengths and weaknesses over different word classes. A particular focus\nof this paper is the relative accessibility of different language resource\ntypes, and predicted ``bang for the buck'' associated with each in deep lexical\nacquisition applications.\n",
        "published": "2007-09-15T01:37:21Z",
        "pdf_link": "http://arxiv.org/pdf/0709.2401v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.0225v1",
        "title": "On the role of autocorrelations in texts",
        "summary": "  The task of finding a criterion allowing to distinguish a text from an\narbitrary set of words is rather relevant in itself, for instance, in the\naspect of development of means for internet-content indexing or separating\nsignals and noise in communication channels. The Zipf law is currently\nconsidered to be the most reliable criterion of this kind [3]. At any rate,\nconventional stochastic word sets do not meet this law. The present paper deals\nwith one of possible criteria based on the determination of the degree of data\ncompression.\n",
        "published": "2007-10-01T08:23:24Z",
        "pdf_link": "http://arxiv.org/pdf/0710.0225v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.0228v1",
        "title": "On the fractal nature of mutual relevance sequences in the Internet news\n  message flows",
        "summary": "  In the task of information retrieval the term relevance is taken to mean\nformal conformity of a document given by the retrieval system to user's\ninformation query. As a rule, the documents found by the retrieval system\nshould be submitted to the user in a certain order. Therefore, a retrieval\nperceived as a selection of documents formally solving the user's query, should\nbe supplemented with a certain procedure of processing a relevant set. It would\nbe natural to introduce a quantitative measure of document conformity to query,\ni.e. the relevance measure. Since no single rule exists for the determination\nof the relevance measure, we shall consider two of them which are the simplest\nin our opinion. The proposed approach does not suppose any restrictions and can\nbe applied to other relevance measures.\n",
        "published": "2007-10-01T08:31:56Z",
        "pdf_link": "http://arxiv.org/pdf/0710.0228v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.2852v1",
        "title": "Generating models for temporal representations",
        "summary": "  We discuss the use of model building for temporal representations. We chose\nPolish to illustrate our discussion because it has an interesting aspectual\nsystem, but the points we wish to make are not language specific. Rather, our\ngoal is to develop theoretical and computational tools for temporal model\nbuilding tasks in computational semantics. To this end, we present a\nfirst-order theory of time and events which is rich enough to capture\ninteresting semantic distinctions, and an algorithm which takes minimal models\nfor first-order theories and systematically attempts to ``perturb'' their\ntemporal component to provide non-minimal, but semantically significant,\nmodels.\n",
        "published": "2007-10-15T15:45:13Z",
        "pdf_link": "http://arxiv.org/pdf/0710.2852v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.2988v1",
        "title": "Using Description Logics for Recognising Textual Entailment",
        "summary": "  The aim of this paper is to show how we can handle the Recognising Textual\nEntailment (RTE) task by using Description Logics (DLs). To do this, we propose\na representation of natural language semantics in DLs inspired by existing\nrepresentations in first-order logic. But our most significant contribution is\nthe definition of two novel inference tasks: A-Box saturation and subgraph\ndetection which are crucial for our approach to RTE.\n",
        "published": "2007-10-16T09:16:24Z",
        "pdf_link": "http://arxiv.org/pdf/0710.2988v1"
    },
    {
        "id": "http://arxiv.org/abs/0710.5382v1",
        "title": "Some Reflections on the Task of Content Determination in the Context of\n  Multi-Document Summarization of Evolving Events",
        "summary": "  Despite its importance, the task of summarizing evolving events has received\nsmall attention by researchers in the field of multi-document summariztion. In\na previous paper (Afantenos et al. 2007) we have presented a methodology for\nthe automatic summarization of documents, emitted by multiple sources, which\ndescribe the evolution of an event. At the heart of this methodology lies the\nidentification of similarities and differences between the various documents,\nin two axes: the synchronic and the diachronic. This is achieved by the\nintroduction of the notion of Synchronic and Diachronic Relations. Those\nrelations connect the messages that are found in the documents, resulting thus\nin a graph which we call grid. Although the creation of the grid completes the\nDocument Planning phase of a typical NLG architecture, it can be the case that\nthe number of messages contained in a grid is very large, exceeding thus the\nrequired compression rate. In this paper we provide some initial thoughts on a\nprobabilistic model which can be applied at the Content Determination stage,\nand which tries to alleviate this problem.\n",
        "published": "2007-10-29T10:48:48Z",
        "pdf_link": "http://arxiv.org/pdf/0710.5382v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.0666v1",
        "title": "Discriminative Phoneme Sequences Extraction for Non-Native Speaker's\n  Origin Classification",
        "summary": "  In this paper we present an automated method for the classification of the\norigin of non-native speakers. The origin of non-native speakers could be\nidentified by a human listener based on the detection of typical pronunciations\nfor each nationality. Thus we suppose the existence of several phoneme\nsequences that might allow the classification of the origin of non-native\nspeakers. Our new method is based on the extraction of discriminative sequences\nof phonemes from a non-native English speech database. These sequences are used\nto construct a probabilistic classifier for the speakers' origin. The existence\nof discriminative phone sequences in non-native speech is a significant result\nof this work. The system that we have developed achieved a significant correct\nclassification rate of 96.3% and a significant error reduction compared to some\nother tested techniques.\n",
        "published": "2007-11-05T15:20:47Z",
        "pdf_link": "http://arxiv.org/pdf/0711.0666v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.0811v1",
        "title": "Combined Acoustic and Pronunciation Modelling for Non-Native Speech\n  Recognition",
        "summary": "  In this paper, we present several adaptation methods for non-native speech\nrecognition. We have tested pronunciation modelling, MLLR and MAP non-native\npronunciation adaptation and HMM models retraining on the HIWIRE foreign\naccented English speech database. The ``phonetic confusion'' scheme we have\ndeveloped consists in associating to each spoken phone several sequences of\nconfused phones. In our experiments, we have used different combinations of\nacoustic models representing the canonical and the foreign pronunciations:\nspoken and native models, models adapted to the non-native accent with MAP and\nMLLR. The joint use of pronunciation modelling and acoustic adaptation led to\nfurther improvements in recognition accuracy. The best combination of the above\nmentioned techniques resulted in a relative word error reduction ranging from\n46% to 71%.\n",
        "published": "2007-11-06T08:23:49Z",
        "pdf_link": "http://arxiv.org/pdf/0711.0811v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.1038v1",
        "title": "Amlioration des Performances des Systmes Automatiques de\n  Reconnaissance de la Parole pour la Parole Non Native",
        "summary": "  In this article, we present an approach for non native automatic speech\nrecognition (ASR). We propose two methods to adapt existing ASR systems to the\nnon-native accents. The first method is based on the modification of acoustic\nmodels through integration of acoustic models from the mother tong. The\nphonemes of the target language are pronounced in a similar manner to the\nnative language of speakers. We propose to combine the models of confused\nphonemes so that the ASR system could recognize both concurrent\npronounciations. The second method we propose is a refinment of the\npronounciation error detection through the introduction of graphemic\nconstraints. Indeed, non native speakers may rely on the writing of words in\ntheir uttering. Thus, the pronounctiation errors might depend on the characters\ncomposing the words. The average error rate reduction that we observed is\n(22.5%) relative for the sentence error rate, and 34.5% (relative) in word\nerror rate.\n",
        "published": "2007-11-07T08:51:09Z",
        "pdf_link": "http://arxiv.org/pdf/0711.1038v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.2444v1",
        "title": "Proof nets for display logic",
        "summary": "  This paper explores several extensions of proof nets for the Lambek calculus\nin order to handle the different connectives of display logic in a natural way.\nThe new proof net calculus handles some recent additions to the Lambek\nvocabulary such as Galois connections and Grishin interactions. It concludes\nwith an exploration of the generative capacity of the Lambek-Grishin calculus,\npresenting an embedding of lexicalized tree adjoining grammars into the\nLambek-Grishin calculus.\n",
        "published": "2007-11-15T15:39:48Z",
        "pdf_link": "http://arxiv.org/pdf/0711.2444v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3412v1",
        "title": "Morphological annotation of Korean with Directly Maintainable Resources",
        "summary": "  This article describes an exclusively resource-based method of morphological\nannotation of written Korean text. Korean is an agglutinative language. Our\nannotator is designed to process text before the operation of a syntactic\nparser. In its present state, it annotates one-stem words only. The output is a\ngraph of morphemes annotated with accurate linguistic information. The\ngranularity of the tagset is 3 to 5 times higher than usual tagsets. A\ncomparison with a reference annotated corpus showed that it achieves 89% recall\nwithout any corpus training. The language resources used by the system are\nlexicons of stems, transducers of suffixes and transducers of generation of\nallomorphs. All can be easily updated, which allows users to control the\nevolution of the performances of the system. It has been claimed that\nmorphological annotation of Korean text could only be performed by a\nmorphological analysis module accessing a lexicon of morphemes. We show that it\ncan also be performed directly with a lexicon of words and without applying\nmorphological rules at annotation time, which speeds up annotation to 1,210\nword/s. The lexicon of words is obtained from the maintainable language\nresources through a fully automated compilation process.\n",
        "published": "2007-11-21T16:47:57Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3412v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3449v1",
        "title": "Lexicon management and standard formats",
        "summary": "  International standards for lexicon formats are in preparation. To a certain\nextent, the proposed formats converge with prior results of standardization\nprojects. However, their adequacy for (i) lexicon management and (ii)\nlexicon-driven applications have been little debated in the past, nor are they\nas a part of the present standardization effort. We examine these issues. IGM\nhas developed XML formats compatible with the emerging international standards,\nand we report experimental results on large-coverage lexica.\n",
        "published": "2007-11-21T20:34:08Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3449v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3452v1",
        "title": "In memoriam Maurice Gross",
        "summary": "  Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural\nlanguage processing. This article is written in homage to his memory\n",
        "published": "2007-11-21T20:38:29Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3452v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3453v1",
        "title": "A resource-based Korean morphological annotation system",
        "summary": "  We describe a resource-based method of morphological annotation of written\nKorean text. Korean is an agglutinative language. The output of our system is a\ngraph of morphemes annotated with accurate linguistic information. The language\nresources used by the system can be easily updated, which allows us-ers to\ncontrol the evolution of the per-formances of the system. We show that\nmorphological annotation of Korean text can be performed directly with a\nlexicon of words and without morpho-logical rules.\n",
        "published": "2007-11-21T20:41:59Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3453v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3454v1",
        "title": "Graphes paramtrs et outils de lexicalisation",
        "summary": "  Shifting to a lexicalized grammar reduces the number of parsing errors and\nimproves application results. However, such an operation affects a syntactic\nparser in all its aspects. One of our research objectives is to design a\nrealistic model for grammar lexicalization. We carried out experiments for\nwhich we used a grammar with a very simple content and formalism, and a very\ninformative syntactic lexicon, the lexicon-grammar of French elaborated by the\nLADL. Lexicalization was performed by applying the parameterized-graph\napproach. Our results tend to show that most information in the lexicon-grammar\ncan be transferred into a grammar and exploited successfully for the syntactic\nparsing of sentences.\n",
        "published": "2007-11-21T20:44:04Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3454v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3457v1",
        "title": "Evaluation of a Grammar of French Determiners",
        "summary": "  Existing syntactic grammars of natural languages, even with a far from\ncomplete coverage, are complex objects. Assessments of the quality of parts of\nsuch grammars are useful for the validation of their construction. We evaluated\nthe quality of a grammar of French determiners that takes the form of a\nrecursive transition network. The result of the application of this local\ngrammar gives deeper syntactic information than chunking or information\navailable in treebanks. We performed the evaluation by comparison with a corpus\nindependently annotated with information on determiners. We obtained 86%\nprecision and 92% recall on text not tagged for parts of speech.\n",
        "published": "2007-11-21T20:49:21Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3457v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3605v1",
        "title": "Very strict selectional restrictions",
        "summary": "  We discuss the characteristics and behaviour of two parallel classes of verbs\nin two Romance languages, French and Portuguese. Examples of these verbs are\nPort. abater [gado] and Fr. abattre [b\\'etail], both meaning \"slaughter\n[cattle]\". In both languages, the definition of the class of verbs includes\nseveral features: - They have only one essential complement, which is a direct\nobject. - The nominal distribution of the complement is very limited, i.e., few\nnouns can be selected as head nouns of the complement. However, this selection\nis not restricted to a single noun, as would be the case for verbal idioms such\nas Fr. monter la garde \"mount guard\". - We excluded from the class\nconstructions which are reductions of more complex constructions, e.g. Port.\nafinar [instrumento] com \"tune [instrument] with\".\n",
        "published": "2007-11-22T15:54:31Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3605v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.3691v2",
        "title": "Outilex, plate-forme logicielle de traitement de textes crits",
        "summary": "  The Outilex software platform, which will be made available to research,\ndevelopment and industry, comprises software components implementing all the\nfundamental operations of written text processing: processing without lexicons,\nexploitation of lexicons and grammars, language resource management. All data\nare structured in XML formats, and also in more compact formats, either\nreadable or binary, whenever necessary; the required format converters are\nincluded in the platform; the grammar formats allow for combining statistical\napproaches with resource-based approaches. Manually constructed lexicons for\nFrench and English, originating from the LADL, and of substantial coverage,\nwill be distributed with the platform under LGPL-LR license.\n",
        "published": "2007-11-23T09:45:13Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3691v2"
    },
    {
        "id": "http://arxiv.org/abs/0711.3726v1",
        "title": "Let's get the student into the driver's seat",
        "summary": "  Speaking a language and achieving proficiency in another one is a highly\ncomplex process which requires the acquisition of various kinds of knowledge\nand skills, like the learning of words, rules and patterns and their connection\nto communicative goals (intentions), the usual starting point. To help the\nlearner to acquire these skills we propose an enhanced, electronic version of\nan age old method: pattern drills (henceforth PDs). While being highly regarded\nin the fifties, PDs have become unpopular since then, partially because of\ntheir lack of grounding (natural context) and rigidity. Despite these\nshortcomings we do believe in the virtues of this approach, at least with\nregard to the acquisition of basic linguistic reflexes or skills (automatisms),\nnecessary to survive in the new language. Of course, the method needs\nimprovement, and we will show here how this can be achieved. Unlike tapes or\nbooks, computers are open media, allowing for dynamic changes, taking users'\nperformances and preferences into account. Building an electronic version of\nPDs amounts to building an open resource, accomodatable to the users' ever\nchanging needs.\n",
        "published": "2007-11-23T13:44:55Z",
        "pdf_link": "http://arxiv.org/pdf/0711.3726v1"
    },
    {
        "id": "http://arxiv.org/abs/0711.4475v6",
        "title": "Valence extraction using EM selection and co-occurrence matrices",
        "summary": "  This paper discusses two new procedures for extracting verb valences from raw\ntexts, with an application to the Polish language. The first novel technique,\nthe EM selection algorithm, performs unsupervised disambiguation of valence\nframe forests, obtained by applying a non-probabilistic deep grammar parser and\nsome post-processing to the text. The second new idea concerns filtering of\nincorrect frames detected in the parsed text and is motivated by an observation\nthat verbs which take similar arguments tend to have similar frames. This\nphenomenon is described in terms of newly introduced co-occurrence matrices.\nUsing co-occurrence matrices, we split filtering into two steps. The list of\nvalid arguments is first determined for each verb, whereas the pattern\naccording to which the arguments are combined into frames is computed in the\nfollowing stage. Our best extracted dictionary reaches an $F$-score of 45%,\ncompared to an $F$-score of 39% for the standard frame-based BHT filtering.\n",
        "published": "2007-11-28T12:16:08Z",
        "pdf_link": "http://arxiv.org/pdf/0711.4475v6"
    },
    {
        "id": "http://arxiv.org/abs/0712.3705v1",
        "title": "Framework and Resources for Natural Language Parser Evaluation",
        "summary": "  Because of the wide variety of contemporary practices used in the automatic\nsyntactic parsing of natural languages, it has become necessary to analyze and\nevaluate the strengths and weaknesses of different approaches. This research is\nall the more necessary because there are currently no genre- and\ndomain-independent parsers that are able to analyze unrestricted text with 100%\npreciseness (I use this term to refer to the correctness of analyses assigned\nby a parser). All these factors create a need for methods and resources that\ncan be used to evaluate and compare parsing systems. This research describes:\n(1) A theoretical analysis of current achievements in parsing and parser\nevaluation. (2) A framework (called FEPa) that can be used to carry out\npractical parser evaluations and comparisons. (3) A set of new evaluation\nresources: FiEval is a Finnish treebank under construction, and MGTS and RobSet\nare parser evaluation resources in English. (4) The results of experiments in\nwhich the developed evaluation framework and the two resources for English were\nused for evaluating a set of selected parsers.\n",
        "published": "2007-12-21T08:55:17Z",
        "pdf_link": "http://arxiv.org/pdf/0712.3705v1"
    },
    {
        "id": "http://arxiv.org/abs/0801.3817v1",
        "title": "Robustness Evaluation of Two CCG, a PCFG and a Link Grammar Parsers",
        "summary": "  Robustness in a parser refers to an ability to deal with exceptional\nphenomena. A parser is robust if it deals with phenomena outside its normal\nrange of inputs. This paper reports on a series of robustness evaluations of\nstate-of-the-art parsers in which we concentrated on one aspect of robustness:\nits ability to parse sentences containing misspelled words. We propose two\nmeasures for robustness evaluation based on a comparison of a parser's output\nfor grammatical input sentences and their noisy counterparts. In this paper, we\nuse these measures to compare the overall robustness of the four evaluated\nparsers, and we present an analysis of the decline in parser performance with\nincreasing error levels. Our results indicate that performance typically\ndeclines tens of percentage units when parsers are presented with texts\ncontaining misspellings. When it was tested on our purpose-built test set of\n443 sentences, the best parser in the experiment (C&C parser) was able to\nreturn exactly the same parse tree for the grammatical and ungrammatical\nsentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three\nmisspelled words respectively.\n",
        "published": "2008-01-24T18:41:01Z",
        "pdf_link": "http://arxiv.org/pdf/0801.3817v1"
    },
    {
        "id": "http://arxiv.org/abs/0801.4716v1",
        "title": "Methods to integrate a language model with semantic information for a\n  word prediction component",
        "summary": "  Most current word prediction systems make use of n-gram language models (LM)\nto estimate the probability of the following word in a phrase. In the past\nyears there have been many attempts to enrich such language models with further\nsyntactic or semantic information. We want to explore the predictive powers of\nLatent Semantic Analysis (LSA), a method that has been shown to provide\nreliable information on long-distance semantic dependencies between words in a\ncontext. We present and evaluate here several methods that integrate LSA-based\ninformation with a standard language model: a semantic cache, partial\nreranking, and different forms of interpolation. We found that all methods show\nsignificant improvements, compared to the 4-gram baseline, and most of them to\na simple cache model as well.\n",
        "published": "2008-01-30T17:10:24Z",
        "pdf_link": "http://arxiv.org/pdf/0801.4716v1"
    },
    {
        "id": "http://arxiv.org/abs/0802.4198v1",
        "title": "Some properties of the Ukrainian writing system",
        "summary": "  We investigate the grapheme-phoneme relation in Ukrainian and some properties\nof the Ukrainian version of the Cyrillic alphabet.\n",
        "published": "2008-02-28T12:58:49Z",
        "pdf_link": "http://arxiv.org/pdf/0802.4198v1"
    },
    {
        "id": "http://arxiv.org/abs/0804.0143v1",
        "title": "Effects of High-Order Co-occurrences on Word Semantic Similarities",
        "summary": "  A computational model of the construction of word meaning through exposure to\ntexts is built in order to simulate the effects of co-occurrence values on word\nsemantic similarities, paragraph by paragraph. Semantic similarity is here\nviewed as association. It turns out that the similarity between two words W1\nand W2 strongly increases with a co-occurrence, decreases with the occurrence\nof W1 without W2 or W2 without W1, and slightly increases with high-order\nco-occurrences. Therefore, operationalizing similarity as a frequency of\nco-occurrence probably introduces a bias: first, there are cases in which there\nis similarity without co-occurrence and, second, the frequency of co-occurrence\noverestimates similarity.\n",
        "published": "2008-04-01T11:33:39Z",
        "pdf_link": "http://arxiv.org/pdf/0804.0143v1"
    },
    {
        "id": "http://arxiv.org/abs/0804.4584v1",
        "title": "Feature Unification in TAG Derivation Trees",
        "summary": "  The derivation trees of a tree adjoining grammar provide a first insight into\nthe sentence semantics, and are thus prime targets for generation systems. We\ndefine a formalism, feature-based regular tree grammars, and a translation from\nfeature based tree adjoining grammars into this new formalism. The translation\npreserves the derivation structures of the original grammar, and accounts for\nfeature unification.\n",
        "published": "2008-04-29T11:39:18Z",
        "pdf_link": "http://arxiv.org/pdf/0804.4584v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.2581v1",
        "title": "A chain dictionary method for Word Sense Disambiguation and applications",
        "summary": "  A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)\nis that of dictionary-based methods. Various algorithms have as the root Lesk's\nalgorithm, which exploits the sense definitions in the dictionary directly. Our\napproach uses the lexical base WordNet for a new algorithm originated in\nLesk's, namely \"chain algorithm for disambiguation of all words\", CHAD. We show\nhow translation from a language into another one and also text entailment\nverification could be accomplished by this disambiguation.\n",
        "published": "2008-06-16T13:48:55Z",
        "pdf_link": "http://arxiv.org/pdf/0806.2581v1"
    },
    {
        "id": "http://arxiv.org/abs/0806.3787v2",
        "title": "Computational Approaches to Measuring the Similarity of Short Contexts :\n  A Review of Applications and Methods",
        "summary": "  Measuring the similarity of short written contexts is a fundamental problem\nin Natural Language Processing. This article provides a unifying framework by\nwhich short context problems can be categorized both by their intended\napplication and proposed solution. The goal is to show that various problems\nand methodologies that appear quite different on the surface are in fact very\nclosely related. The axes by which these categorizations are made include the\nformat of the contexts (headed versus headless), the way in which the contexts\nare to be measured (first-order versus second-order similarity), and the\ninformation used to represent the features in the contexts (micro versus macro\nviews). The unifying thread that binds together many short context applications\nand methods is the fact that similarity decisions must be made between contexts\nthat share few (if any) words in common.\n",
        "published": "2008-06-23T23:27:20Z",
        "pdf_link": "http://arxiv.org/pdf/0806.3787v2"
    },
    {
        "id": "http://arxiv.org/abs/0807.0311v1",
        "title": "About the creation of a parallel bilingual corpora of web-publications",
        "summary": "  The algorithm of the creation texts parallel corpora was presented. The\nalgorithm is based on the use of \"key words\" in text documents, and on the\nmeans of their automated translation. Key words were singled out by means of\nusing Russian and Ukrainian morphological dictionaries, as well as dictionaries\nof the translation of nouns for the Russian and Ukrainianlanguages. Besides, to\ncalculate the weights of the terms in the documents, empiric-statistic rules\nwere used. The algorithm under consideration was realized in the form of a\nprogram complex, integrated into the content-monitoring InfoStream system. As a\nresult, a parallel bilingual corpora of web-publications containing about 30\nthousand documents, was created\n",
        "published": "2008-07-02T09:49:14Z",
        "pdf_link": "http://arxiv.org/pdf/0807.0311v1"
    },
    {
        "id": "http://arxiv.org/abs/0807.3622v1",
        "title": "TuLiPA: Towards a Multi-Formalism Parsing Environment for Grammar\n  Engineering",
        "summary": "  In this paper, we present an open-source parsing environment (Tuebingen\nLinguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar\n(RCG) as a pivot formalism, thus opening the way to the parsing of several\nmildly context-sensitive formalisms. This environment currently supports\ntree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component\nTree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not\nonly of syntactic structures, but also of the corresponding semantic\nrepresentations. It is used for the development of a tree-based grammar for\nGerman.\n",
        "published": "2008-07-23T09:05:50Z",
        "pdf_link": "http://arxiv.org/pdf/0807.3622v1"
    },
    {
        "id": "http://arxiv.org/abs/0808.2904v1",
        "title": "Investigation of the Zipf-plot of the extinct Meroitic language",
        "summary": "  The ancient and extinct language Meroitic is investigated using Zipf's Law.\nIn particular, since Meroitic is still undeciphered, the Zipf law analysis\nallows us to assess the quality of current texts and possible avenues for\nfuture investigation using statistical techniques.\n",
        "published": "2008-08-21T10:54:54Z",
        "pdf_link": "http://arxiv.org/pdf/0808.2904v1"
    },
    {
        "id": "http://arxiv.org/abs/0808.3563v1",
        "title": "What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes",
        "summary": "  Julian Jaynes's profound humanitarian convictions not only prevented him from\ngoing to war, but would have prevented him from ever kicking a dog. Yet\naccording to his theory, not only are language-less dogs unconscious, but so\ntoo were the speaking/hearing Greeks in the Bicameral Era, when they heard\ngods' voices telling them what to do rather than thinking for themselves. I\nargue that to be conscious is to be able to feel, and that all mammals (and\nprobably lower vertebrates and invertebrates too) feel, hence are conscious.\nJulian Jaynes's brilliant analysis of our concepts of consciousness\nnevertheless keeps inspiring ever more inquiry and insights into the age-old\nmind/body problem and its relation to cognition and language.\n",
        "published": "2008-08-26T18:17:44Z",
        "pdf_link": "http://arxiv.org/pdf/0808.3563v1"
    },
    {
        "id": "http://arxiv.org/abs/0808.3616v3",
        "title": "Constructing word similarities in Meroitic as an aid to decipherment",
        "summary": "  Meroitic is the still undeciphered language of the ancient civilization of\nKush. Over the years, various techniques for decipherment such as finding a\nbilingual text or cognates from modern or other ancient languages in the Sudan\nand surrounding areas has not been successful. Using techniques borrowed from\ninformation theory and natural language statistics, similar words are paired\nand attempts are made to use currently defined words to extract at least\npartial meaning from unknown words.\n",
        "published": "2008-08-27T02:02:40Z",
        "pdf_link": "http://arxiv.org/pdf/0808.3616v3"
    },
    {
        "id": "http://arxiv.org/abs/0808.3889v1",
        "title": "Open architecture for multilingual parallel texts",
        "summary": "  Multilingual parallel texts (abbreviated to parallel texts) are linguistic\nversions of the same content (\"translations\"); e.g., the Maastricht Treaty in\nEnglish and Spanish are parallel texts. This document is about creating an open\narchitecture for the whole Authoring, Translation and Publishing Chain\n(ATP-chain) for the processing of parallel texts.\n",
        "published": "2008-08-28T11:59:34Z",
        "pdf_link": "http://arxiv.org/pdf/0808.3889v1"
    },
    {
        "id": "http://arxiv.org/abs/0809.3250v1",
        "title": "Using descriptive mark-up to formalize translation quality assessment",
        "summary": "  The paper deals with using descriptive mark-up to emphasize translation\nmistakes. The author postulates the necessity to develop a standard and formal\nXML-based way of describing translation mistakes. It is considered to be\nimportant for achieving impersonal translation quality assessment. Marked-up\ntranslations can be used in corpus translation studies; moreover, automatic\ntranslation assessment based on marked-up mistakes is possible. The paper\nconcludes with setting up guidelines for further activity within the described\nfield.\n",
        "published": "2008-09-18T20:48:13Z",
        "pdf_link": "http://arxiv.org/pdf/0809.3250v1"
    },
    {
        "id": "http://arxiv.org/abs/0810.0200v1",
        "title": "Distribution of complexities in the Vai script",
        "summary": "  In the paper, we analyze the distribution of complexities in the Vai script,\nan indigenous syllabic writing system from Liberia. It is found that the\nuniformity hypothesis for complexities fails for this script. The models using\nPoisson distribution for the number of components and hyper-Poisson\ndistribution for connections provide good fits in the case of the Vai script.\n",
        "published": "2008-10-01T15:53:36Z",
        "pdf_link": "http://arxiv.org/pdf/0810.0200v1"
    },
    {
        "id": "http://arxiv.org/abs/0810.1199v1",
        "title": "Une grammaire formelle du crole martiniquais pour la gnration\n  automatique",
        "summary": "  In this article, some first elements of a computational modelling of the\ngrammar of the Martiniquese French Creole dialect are presented. The sources of\ninspiration for the modelling is the functional description given by Damoiseau\n(1984), and Pinalie's & Bernabe's (1999) grammar manual. Based on earlier works\nin text generation (Vaillant, 1997), a unification grammar formalism, namely\nTree Adjoining Grammars (TAG), and a modelling of lexical functional categories\nbased on syntactic and semantic properties, are used to implement a grammar of\nMartiniquese Creole which is used in a prototype of text generation system. One\nof the main applications of the system could be its use as a tool software\nsupporting the task of learning Creole as a second language. -- Nous\npr\\'esenterons dans cette communication les premiers travaux de mod\\'elisation\ninformatique d'une grammaire de la langue cr\\'eole martiniquaise, en nous\ninspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du\nmanuel de Pinalie & Bernab\\'e (1999). Prenant appui sur des travaux\nant\\'erieurs en g\\'en\\'eration de texte (Vaillant, 1997), nous utilisons un\nformalisme de grammaires d'unification, les grammaires d'adjonction d'arbres\n(TAG d'apr\\`es l'acronyme anglais), ainsi qu'une mod\\'elisation de cat\\'egories\nlexicales fonctionnelles \\`a base syntaxico-s\\'emantique, pour mettre en oeuvre\nune grammaire du cr\\'eole martiniquais utilisable dans une maquette de\nsyst\\`eme de g\\'en\\'eration automatique. L'un des int\\'er\\^ets principaux de ce\nsyst\\`eme pourrait \\^etre son utilisation comme logiciel outil pour l'aide \\`a\nl'apprentissage du cr\\'eole en tant que langue seconde.\n",
        "published": "2008-10-07T14:40:19Z",
        "pdf_link": "http://arxiv.org/pdf/0810.1199v1"
    },
    {
        "id": "http://arxiv.org/abs/0810.1207v1",
        "title": "A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common\n  Syntactic Kernel for Related Dialects",
        "summary": "  This article describes the design of a common syntactic description for the\ncore grammar of a group of related dialects. The common description does not\nrely on an abstract sub-linguistic structure like a metagrammar: it consists in\na single FS-LTAG where the actual specific language is included as one of the\nattributes in the set of attribute types defined for the features. When the\nlang attribute is instantiated, the selected subset of the grammar is\nequivalent to the grammar of one dialect. When it is not, we have a model of a\nhybrid multidialectal linguistic system. This principle is used for a group of\ncreole languages of the West-Atlantic area, namely the French-based Creoles of\nHaiti, Guadeloupe, Martinique and French Guiana.\n",
        "published": "2008-10-07T14:50:59Z",
        "pdf_link": "http://arxiv.org/pdf/0810.1207v1"
    },
    {
        "id": "http://arxiv.org/abs/0901.3017v1",
        "title": "Statistical analysis of the Indus script using $n$-grams",
        "summary": "  The Indus script is one of the major undeciphered scripts of the ancient\nworld. The small size of the corpus, the absence of bilingual texts, and the\nlack of definite knowledge of the underlying language has frustrated efforts at\ndecipherment since the discovery of the remains of the Indus civilisation.\nRecently, some researchers have questioned the premise that the Indus script\nencodes spoken language. Building on previous statistical approaches, we apply\nthe tools of statistical language processing, specifically $n$-gram Markov\nchains, to analyse the Indus script for syntax. Our main results are that the\nscript has well-defined signs which begin and end texts, that there is\ndirectionality and strong correlations in the sign order, and that there are\ngroups of signs which appear to have identical syntactic function. All these\nrequire no {\\it a priori} suppositions regarding the syntactic or semantic\ncontent of the signs, but follow directly from the statistical analysis. Using\ninformation theoretic measures, we find the information in the script to be\nintermediate between that of a completely random and a completely fixed\nordering of signs. Our study reveals that the Indus script is a structured sign\nsystem showing features of a formal language, but, at present, cannot\nconclusively establish that it encodes {\\it natural} language. Our $n$-gram\nMarkov model is useful for predicting signs which are missing or illegible in a\ncorpus of Indus texts. This work forms the basis for the development of a\nstochastic grammar which can be used to explore the syntax of the Indus script\nin greater detail.\n",
        "published": "2009-01-20T12:55:55Z",
        "pdf_link": "http://arxiv.org/pdf/0901.3017v1"
    },
    {
        "id": "http://arxiv.org/abs/0901.4180v2",
        "title": "Google distance between words",
        "summary": "  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the\nmeaning of words from the world-wide web. To achieve this, they rely on the\nnumber of webpages that are found through a Google search containing a given\nword and they associate the page count to the probability that the word appears\non a webpage. Thus, conditional probabilities allow them to correlate one word\nwith another word's meaning. Furthermore, they have developed a similarity\ndistance function that gauges how closely related a pair of words is. We\npresent a specific counterexample to the triangle inequality for this\nsimilarity distance function.\n",
        "published": "2009-01-27T06:29:10Z",
        "pdf_link": "http://arxiv.org/pdf/0901.4180v2"
    },
    {
        "id": "http://arxiv.org/abs/0902.1033v1",
        "title": "New Confidence Measures for Statistical Machine Translation",
        "summary": "  A confidence measure is able to estimate the reliability of an hypothesis\nprovided by a machine translation system. The problem of confidence measure can\nbe seen as a process of testing : we want to decide whether the most probable\nsequence of words provided by the machine translation system is correct or not.\nIn the following we describe several original word-level confidence measures\nfor machine translation, based on mutual information, n-gram language model and\nlexical features language model. We evaluate how well they perform individually\nor together, and show that using a combination of confidence measures based on\nmutual information yields a classification error rate as low as 25.1% with an\nF-measure of 0.708.\n",
        "published": "2009-02-06T09:28:58Z",
        "pdf_link": "http://arxiv.org/pdf/0902.1033v1"
    },
    {
        "id": "http://arxiv.org/abs/0902.2345v1",
        "title": "What's in a Message?",
        "summary": "  In this paper we present the first step in a larger series of experiments for\nthe induction of predicate/argument structures. The structures that we are\ninducing are very similar to the conceptual structures that are used in Frame\nSemantics (such as FrameNet). Those structures are called messages and they\nwere previously used in the context of a multi-document summarization system of\nevolving events. The series of experiments that we are proposing are\nessentially composed from two stages. In the first stage we are trying to\nextract a representative vocabulary of words. This vocabulary is later used in\nthe second stage, during which we apply to it various clustering approaches in\norder to identify the clusters of predicates and arguments--or frames and\nsemantic roles, to use the jargon of Frame Semantics. This paper presents in\ndetail and evaluates the first stage.\n",
        "published": "2009-02-13T17:08:10Z",
        "pdf_link": "http://arxiv.org/pdf/0902.2345v1"
    },
    {
        "id": "http://arxiv.org/abs/0902.3072v1",
        "title": "Syntactic variation of support verb constructions",
        "summary": "  We report experiments about the syntactic variations of support verb\nconstructions, a special type of multiword expressions (MWEs) containing\npredicative nouns. In these expressions, the noun can occur with or without the\nverb, with no clear-cut semantic difference. We extracted from a large French\ncorpus a set of examples of the two situations and derived statistical results\nfrom these data. The extraction involved large-coverage language resources and\nfinite-state techniques. The results show that, most frequently, predicative\nnouns occur without a support verb. This fact has consequences on methods of\nextracting or recognising MWEs.\n",
        "published": "2009-02-18T08:51:28Z",
        "pdf_link": "http://arxiv.org/pdf/0902.3072v1"
    },
    {
        "id": "http://arxiv.org/abs/0903.5168v1",
        "title": "Mathematical Model for Transformation of Sentences from Active Voice to\n  Passive Voice",
        "summary": "  Formal work in linguistics has both produced and used important mathematical\ntools. Motivated by a survey of models for context and word meaning, syntactic\ncategories, phrase structure rules and trees, an attempt is being made in the\npresent paper to present a mathematical model for structuring of sentences from\nactive voice to passive voice, which is is the form of a transitive verb whose\ngrammatical subject serves as the patient, receiving the action of the verb.\n  For this purpose we have parsed all sentences of a corpus and have generated\nBoolean groups for each of them. It has been observed that when we take\nconstituents of the sentences as subgroups, the sequences of phrases form\npermutation roups. Application of isomorphism property yields permutation\nmapping between the important subgroups. It has resulted in a model for\ntransformation of sentences from active voice to passive voice. A computer\nprogram has been written to enable the software developers to evolve grammar\nsoftware for sentence transformations.\n",
        "published": "2009-03-30T09:45:20Z",
        "pdf_link": "http://arxiv.org/pdf/0903.5168v1"
    },
    {
        "id": "http://arxiv.org/abs/0905.1609v1",
        "title": "Acquisition of morphological families and derivational series from a\n  machine readable dictionary",
        "summary": "  The paper presents a linguistic and computational model aiming at making the\nmorphological structure of the lexicon emerge from the formal and semantic\nregularities of the words it contains. The model is word-based. The proposed\nmorphological structure consists of (1) binary relations that connect each\nheadword with words that are morphologically related, and especially with the\nmembers of its morphological family and its derivational series, and of (2) the\nanalogies that hold between the words. The model has been tested on the lexicon\nof French using the TLFi machine readable dictionary.\n",
        "published": "2009-05-11T12:17:36Z",
        "pdf_link": "http://arxiv.org/pdf/0905.1609v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.0675v1",
        "title": "Encoding models for scholarly literature",
        "summary": "  We examine the issue of digital formats for document encoding, archiving and\npublishing, through the specific example of \"born-digital\" scholarly journal\narticles. We will begin by looking at the traditional workflow of journal\nediting and publication, and how these practices have made the transition into\nthe online domain. We will examine the range of different file formats in which\nelectronic articles are currently stored and published. We will argue strongly\nthat, despite the prevalence of binary and proprietary formats such as PDF and\nMS Word, XML is a far superior encoding choice for journal articles. Next, we\nlook at the range of XML document structures (DTDs, Schemas) which are in\ncommon use for encoding journal articles, and consider some of their strengths\nand weaknesses. We will suggest that, despite the existence of specialized\nschemas intended specifically for journal articles (such as NLM), and more\nbroadly-used publication-oriented schemas such as DocBook, there are strong\narguments in favour of developing a subset or customization of the Text\nEncoding Initiative (TEI) schema for the purpose of journal-article encoding;\nTEI is already in use in a number of journal publication projects, and the\nscale and precision of the TEI tagset makes it particularly appropriate for\nencoding scholarly articles. We will outline the document structure of a\nTEI-encoded journal article, and look in detail at suggested markup patterns\nfor specific features of journal articles.\n",
        "published": "2009-06-03T09:53:12Z",
        "pdf_link": "http://arxiv.org/pdf/0906.0675v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.2415v1",
        "title": "Without a 'doubt'? Unsupervised discovery of downward-entailing\n  operators",
        "summary": "  An important part of textual inference is making deductions involving\nmonotonicity, that is, determining whether a given assertion entails\nrestrictions or relaxations of that assertion. For instance, the statement 'We\nknow the epidemic spread quickly' does not entail 'We know the epidemic spread\nquickly via fleas', but 'We doubt the epidemic spread quickly' entails 'We\ndoubt the epidemic spread quickly via fleas'. Here, we present the first\nalgorithm for the challenging lexical-semantics problem of learning linguistic\nconstructions that, like 'doubt', are downward entailing (DE). Our algorithm is\nunsupervised, resource-lean, and effective, accurately recovering many DE\noperators that are missing from the hand-constructed lists that\ntextual-inference systems currently use.\n",
        "published": "2009-06-12T23:50:06Z",
        "pdf_link": "http://arxiv.org/pdf/0906.2415v1"
    },
    {
        "id": "http://arxiv.org/abs/0906.5114v1",
        "title": "Non-Parametric Bayesian Areal Linguistics",
        "summary": "  We describe a statistical model over linguistic areas and phylogeny.\n  Our model recovers known areas and identifies a plausible hierarchy of areal\nfeatures. The use of areas improves genetic reconstruction of languages both\nqualitatively and quantitatively according to a variety of metrics. We model\nlinguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman's\ncoalescent.\n",
        "published": "2009-06-28T02:32:53Z",
        "pdf_link": "http://arxiv.org/pdf/0906.5114v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.0785v1",
        "title": "A Bayesian Model for Discovering Typological Implications",
        "summary": "  A standard form of analysis for linguistic typology is the universal\nimplication. These implications state facts about the range of extant\nlanguages, such as ``if objects come after verbs, then adjectives come after\nnouns.'' Such implications are typically discovered by painstaking hand\nanalysis over a small sample of languages. We propose a computational model for\nassisting at this process. Our model is able to discover both well-known\nimplications as well as some novel implications that deserve further study.\nMoreover, through a careful application of hierarchical analysis, we are able\nto cope with the well-known sampling problem: languages are not independent.\n",
        "published": "2009-07-04T18:43:16Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0785v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.0804v1",
        "title": "Induction of Word and Phrase Alignments for Automatic Document\n  Summarization",
        "summary": "  Current research in automatic single document summarization is dominated by\ntwo effective, yet naive approaches: summarization by sentence extraction, and\nheadline generation via bag-of-words models. While successful in some tasks,\nneither of these models is able to adequately capture the large set of\nlinguistic devices utilized by humans when they produce summaries. One possible\nexplanation for the widespread use of these models is that good techniques have\nbeen developed to extract appropriate training data for them from existing\ndocument/abstract and document/headline corpora. We believe that future\nprogress in automatic summarization will be driven both by the development of\nmore sophisticated, linguistically informed models, as well as a more effective\nleveraging of document/abstract corpora. In order to open the doors to\nsimultaneously achieving both of these goals, we have developed techniques for\nautomatically producing word-to-word and phrase-to-phrase alignments between\ndocuments and their human-written abstracts. These alignments make explicit the\ncorrespondences that exist in such document/abstract pairs, and create a\npotentially rich data source from which complex summarization algorithms may\nlearn. This paper describes experiments we have carried out to analyze the\nability of humans to perform such alignments, and based on these analyses, we\ndescribe experiments for creating them automatically. Our model for the\nalignment task is based on an extension of the standard hidden Markov model,\nand learns to create alignments in a completely unsupervised fashion. We\ndescribe our model in detail and present experimental results that show that\nour model is able to learn to reliably identify word- and phrase-level\nalignments in a corpus of <document,abstract> pairs.\n",
        "published": "2009-07-04T22:57:26Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0804v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.0806v1",
        "title": "A Noisy-Channel Model for Document Compression",
        "summary": "  We present a document compression system that uses a hierarchical\nnoisy-channel model of text production. Our compression system first\nautomatically derives the syntactic structure of each sentence and the overall\ndiscourse structure of the text given as input. The system then uses a\nstatistical hierarchical model of text production in order to drop\nnon-important syntactic and discourse constituents so as to generate coherent,\ngrammatical document compressions of arbitrary length. The system outperforms\nboth a baseline and a sentence-based compression system that operates by\nsimplifying sequentially all sentences in a text. Our results support the claim\nthat discourse knowledge plays an important role in document summarization.\n",
        "published": "2009-07-04T22:26:47Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0806v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.0807v1",
        "title": "A Large-Scale Exploration of Effective Global Features for a Joint\n  Entity Detection and Tracking Model",
        "summary": "  Entity detection and tracking (EDT) is the task of identifying textual\nmentions of real-world entities in documents, extending the named entity\ndetection and coreference resolution task by considering mentions other than\nnames (pronouns, definite descriptions, etc.). Like NE tagging and coreference\nresolution, most solutions to the EDT task separate out the mention detection\naspect from the coreference aspect. By doing so, these solutions are limited to\nusing only local features for learning. In contrast, by modeling both aspects\nof the EDT task simultaneously, we are able to learn using highly complex,\nnon-local features. We develop a new joint EDT model and explore the utility of\nmany features, demonstrating their effectiveness on this task.\n",
        "published": "2009-07-04T22:28:15Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0807v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.2452v1",
        "title": "Pattern Based Term Extraction Using ACABIT System",
        "summary": "  In this paper, we propose a pattern-based term extraction approach for\nJapanese, applying ACABIT system originally developed for French. The proposed\napproach evaluates termhood using morphological patterns of basic terms and\nterm variants. After extracting term candidates, ACABIT system filters out\nnon-terms from the candidates based on log-likelihood. This approach is\nsuitable for Japanese term extraction because most of Japanese terms are\ncompound nouns or simple phrasal patterns.\n",
        "published": "2009-07-14T21:02:14Z",
        "pdf_link": "http://arxiv.org/pdf/0907.2452v1"
    },
    {
        "id": "http://arxiv.org/abs/0907.3781v1",
        "title": "Un systme modulaire d'acquisition automatique de traductions \n  partir du Web",
        "summary": "  We present a method of automatic translation (French/English) of Complex\nLexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular\nsystem is based on linguistic properties (compositionality, polysemy, etc.).\nDifferent aspects of the multilingual Web are used to validate candidate\ntranslations and collect new terms. We first build a French corpus of Web pages\nto collect CLU. Three adapted processing stages are applied for each linguistic\nproperty : compositional and non polysemous translations, compositional\npolysemous translations and non compositional translations. Our evaluation on a\nsample of CLU shows that our technique based on the Web can reach a very high\nprecision.\n",
        "published": "2009-07-22T06:25:59Z",
        "pdf_link": "http://arxiv.org/pdf/0907.3781v1"
    },
    {
        "id": "http://arxiv.org/abs/0908.4413v1",
        "title": "Multiple Retrieval Models and Regression Models for Prior Art Search",
        "summary": "  This paper presents the system called PATATRAS (PATent and Article Tracking,\nRetrieval and AnalysiS) realized for the IP track of CLEF 2009. Our approach\npresents three main characteristics: 1. The usage of multiple retrieval models\n(KL, Okapi) and term index definitions (lemma, phrase, concept) for the three\nlanguages considered in the present track (English, French, German) producing\nten different sets of ranked results. 2. The merging of the different results\nbased on multiple regression models using an additional validation set created\nfrom the patent collection. 3. The exploitation of patent metadata and of the\ncitation structures for creating restricted initial working sets of patents and\nfor producing a final re-ranking regression model. As we exploit specific\nmetadata of the patent documents and the citation relations only at the\ncreation of initial working sets and during the final post ranking step, our\narchitecture remains generic and easy to extend.\n",
        "published": "2009-08-30T18:50:19Z",
        "pdf_link": "http://arxiv.org/pdf/0908.4413v1"
    },
    {
        "id": "http://arxiv.org/abs/0908.4431v1",
        "title": "An OLAC Extension for Dravidian Languages",
        "summary": "  OLAC was founded in 2000 for creating online databases of language resources.\nThis paper intends to review the bottom-up distributed character of the project\nand proposes an extension of the architecture for Dravidian languages. An\nontological structure is considered for effective natural language processing\n(NLP) and its advantages over statistical methods are reviewed\n",
        "published": "2009-08-30T23:20:41Z",
        "pdf_link": "http://arxiv.org/pdf/0908.4431v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.1147v1",
        "title": "Empowering OLAC Extension using Anusaaraka and Effective text processing\n  using Double Byte coding",
        "summary": "  The paper reviews the hurdles while trying to implement the OLAC extension\nfor Dravidian / Indian languages. The paper further explores the possibilities\nwhich could minimise or solve these problems. In this context, the Chinese\nsystem of text processing and the anusaaraka system are scrutinised.\n",
        "published": "2009-09-07T06:29:51Z",
        "pdf_link": "http://arxiv.org/pdf/0909.1147v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.2379v1",
        "title": "Implementation of Rule Based Algorithm for Sandhi-Vicheda Of Compound\n  Hindi Words",
        "summary": "  Sandhi means to join two or more words to coin new word. Sandhi literally\nmeans `putting together' or combining (of sounds), It denotes all combinatory\nsound-changes effected (spontaneously) for ease of pronunciation.\nSandhi-vicheda describes [5] the process by which one letter (whether single or\ncojoined) is broken to form two words. Part of the broken letter remains as the\nlast letter of the first word and part of the letter forms the first letter of\nthe next letter. Sandhi- Vicheda is an easy and interesting way that can give\nentirely new dimension that add new way to traditional approach to Hindi\nTeaching. In this paper using the Rule based algorithm we have reported an\naccuracy of 60-80% depending upon the number of rules to be implemented.\n",
        "published": "2009-09-12T22:27:57Z",
        "pdf_link": "http://arxiv.org/pdf/0909.2379v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.2626v1",
        "title": "Reference Resolution within the Framework of Cognitive Grammar",
        "summary": "  Following the principles of Cognitive Grammar, we concentrate on a model for\nreference resolution that attempts to overcome the difficulties previous\napproaches, based on the fundamental assumption that all reference (independent\non the type of the referring expression) is accomplished via access to and\nrestructuring of domains of reference rather than by direct linkage to the\nentities themselves. The model accounts for entities not explicitly mentioned\nbut understood in a discourse, and enables exploitation of discursive and\nperceptual context to limit the set of potential referents for a given\nreferring expression. As the most important feature, we note that a single\nmechanism is required to handle what are typically treated as diverse\nphenomena. Our approach, then, provides a fresh perspective on the relations\nbetween Cognitive Grammar and the problem of reference.\n",
        "published": "2009-09-14T19:10:59Z",
        "pdf_link": "http://arxiv.org/pdf/0909.2626v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.2715v1",
        "title": "Marking-up multiple views of a Text: Discourse and Reference",
        "summary": "  We describe an encoding scheme for discourse structure and reference, based\non the TEI Guidelines and the recommendations of the Corpus Encoding\nSpecification (CES). A central feature of the scheme is a CES-based data\narchitecture enabling the encoding of and access to multiple views of a\nmarked-up document. We describe a tool architecture that supports the encoding\nscheme, and then show how we have used the encoding scheme and the tools to\nperform a discourse analytic task in support of a model of global discourse\ncohesion called Veins Theory (Cristea & Ide, 1998).\n",
        "published": "2009-09-15T06:02:56Z",
        "pdf_link": "http://arxiv.org/pdf/0909.2715v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.2718v1",
        "title": "A Common XML-based Framework for Syntactic Annotations",
        "summary": "  It is widely recognized that the proliferation of annotation schemes runs\ncounter to the need to re-use language resources, and that standards for\nlinguistic annotation are becoming increasingly mandatory. To answer this need,\nwe have developed a framework comprised of an abstract model for a variety of\ndifferent annotation types (e.g., morpho-syntactic tagging, syntactic\nannotation, co-reference annotation, etc.), which can be instantiated in\ndifferent ways depending on the annotator's approach and goals. In this paper\nwe provide an overview of the framework, demonstrate its applicability to\nsyntactic annotation, and show how it can contribute to comparative evaluation\nof parser output and diverse syntactic annotation schemes.\n",
        "published": "2009-09-15T06:09:47Z",
        "pdf_link": "http://arxiv.org/pdf/0909.2718v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.2719v1",
        "title": "Standards for Language Resources",
        "summary": "  This paper presents an abstract data model for linguistic annotations and its\nimplementation using XML, RDF and related standards; and to outline the work of\na newly formed committee of the International Standards Organization (ISO),\nISO/TC 37/SC 4 Language Resource Management, which will use this work as its\nstarting point. The primary motive for presenting the latter is to solicit the\nparticipation of members of the research community to contribute to the work of\nthe committee.\n",
        "published": "2009-09-15T06:10:45Z",
        "pdf_link": "http://arxiv.org/pdf/0909.2719v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.3027v1",
        "title": "Language Models for Handwritten Short Message Services",
        "summary": "  Handwriting is an alternative method for entering texts composing Short\nMessage Services. However, a whole new language features the texts which are\nproduced. They include for instance abbreviations and other consonantal writing\nwhich sprung up for time saving and fashion. We have collected and processed a\nsignificant number of such handwriting SMS, and used various strategies to\ntackle this challenging area of handwriting recognition. We proposed to study\nmore specifically three different phenomena: consonant skeleton, rebus, and\nphonetic writing. For each of them, we compare the rough results produced by a\nstandard recognition system with those obtained when using a specific language\nmodel.\n",
        "published": "2009-09-16T14:38:19Z",
        "pdf_link": "http://arxiv.org/pdf/0909.3027v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.3028v1",
        "title": "Vers la reconnaissance de mini-messages manuscrits",
        "summary": "  Handwriting is an alternative method for entering texts which composed Short\nMessage Services. However, a whole new language features the texts which are\nproduced. They include for instance abbreviations and other consonantal writing\nwhich sprung up for time saving and fashion. We have collected and processed a\nsignificant number of such handwritten SMS, and used various strategies to\ntackle this challenging area of handwriting recognition. We proposed to study\nmore specifically three different phenomena: consonant skeleton, rebus, and\nphonetic writing. For each of them, we compare the rough results produced by a\nstandard recognition system with those obtained when using a specific language\nmodel to take care of them.\n",
        "published": "2009-09-16T14:39:05Z",
        "pdf_link": "http://arxiv.org/pdf/0909.3028v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.3444v1",
        "title": "Analyse en dpendances  l'aide des grammaires d'interaction",
        "summary": "  This article proposes a method to extract dependency structures from\nphrase-structure level parsing with Interaction Grammars. Interaction Grammars\nare a formalism which expresses interactions among words using a polarity\nsystem. Syntactical composition is led by the saturation of polarities.\nInteractions take place between constituents, but as grammars are lexicalized,\nthese interactions can be translated at the level of words. Dependency\nrelations are extracted from the parsing process: every dependency is the\nconsequence of a polarity saturation. The dependency relations we obtain can be\nseen as a refinement of the usual dependency tree. Generally speaking, this\nwork sheds new light on links between phrase structure and dependency parsing.\n",
        "published": "2009-09-18T14:23:05Z",
        "pdf_link": "http://arxiv.org/pdf/0909.3444v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.3445v1",
        "title": "Grouping Synonyms by Definitions",
        "summary": "  We present a method for grouping the synonyms of a lemma according to its\ndictionary senses. The senses are defined by a large machine readable\ndictionary for French, the TLFi (Tr\\'esor de la langue fran\\c{c}aise\ninformatis\\'e) and the synonyms are given by 5 synonym dictionaries (also for\nFrench). To evaluate the proposed method, we manually constructed a gold\nstandard where for each (word, definition) pair and given the set of synonyms\ndefined for that word by the 5 synonym dictionaries, 4 lexicographers specified\nthe set of synonyms they judge adequate. While inter-annotator agreement ranges\non that task from 67% to at best 88% depending on the annotator pair and on the\nsynonym dictionary being considered, the automatic procedure we propose scores\na precision of 67% and a recall of 71%. The proposed method is compared with\nrelated work namely, word sense disambiguation, synonym lexicon acquisition and\nWordNet construction.\n",
        "published": "2009-09-18T14:25:29Z",
        "pdf_link": "http://arxiv.org/pdf/0909.3445v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.3591v1",
        "title": "Mathematics, Recursion, and Universals in Human Languages",
        "summary": "  There are many scientific problems generated by the multiple and conflicting\nalternative definitions of linguistic recursion and human recursive processing\nthat exist in the literature. The purpose of this article is to make available\nto the linguistic community the standard mathematical definition of recursion\nand to apply it to discuss linguistic recursion. As a byproduct, we obtain an\ninsight into certain \"soft universals\" of human languages, which are related to\ncognitive constructs necessary to implement mathematical reasoning, i.e.\nmathematical model theory.\n",
        "published": "2009-09-19T15:38:55Z",
        "pdf_link": "http://arxiv.org/pdf/0909.3591v1"
    },
    {
        "id": "http://arxiv.org/abs/0909.4280v1",
        "title": "Towards Multimodal Content Representation",
        "summary": "  Multimodal interfaces, combining the use of speech, graphics, gestures, and\nfacial expressions in input and output, promise to provide new possibilities to\ndeal with information in more effective and efficient ways, supporting for\ninstance: - the understanding of possibly imprecise, partial or ambiguous\nmultimodal input; - the generation of coordinated, cohesive, and coherent\nmultimodal presentations; - the management of multimodal interaction (e.g.,\ntask completion, adapting the interface, error prevention) by representing and\nexploiting models of the user, the domain, the task, the interactive context,\nand the media (e.g. text, audio, video). The present document is intended to\nsupport the discussion on multimodal content representation, its possible\nobjectives and basic constraints, and how the definition of a generic\nrepresentation framework for multimodal content representation may be\napproached. It takes into account the results of the Dagstuhl workshop, in\nparticular those of the informal working group on multimodal meaning\nrepresentation that was active during the workshop (see\nhttp://www.dfki.de/~wahlster/Dagstuhl_Multi_Modality, Working Group 4).\n",
        "published": "2009-09-23T18:55:53Z",
        "pdf_link": "http://arxiv.org/pdf/0909.4280v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.0537v1",
        "title": "A Note On Higher Order Grammar",
        "summary": "  Both syntax-phonology and syntax-semantics interfaces in Higher Order Grammar\n(HOG) are expressed as axiomatic theories in higher-order logic (HOL), i.e. a\nlanguage is defined entirely in terms of provability in the single logical\nsystem. An important implication of this elegant architecture is that the\nmeaning of a valid expression turns out to be represented not by a single, nor\neven by a few \"discrete\" terms (in case of ambiguity), but by a \"continuous\"\nset of logically equivalent terms. The note is devoted to precise formulation\nand proof of this observation.\n",
        "published": "2009-10-03T12:26:47Z",
        "pdf_link": "http://arxiv.org/pdf/0910.0537v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.1484v1",
        "title": "Ludics and its Applications to natural Language Semantics",
        "summary": "  Proofs, in Ludics, have an interpretation provided by their counter-proofs,\nthat is the objects they interact with. We follow the same idea by proposing\nthat sentence meanings are given by the counter-meanings they are opposed to in\na dialectical interaction. The conception is at the intersection of a\nproof-theoretic and a game-theoretic accounts of semantics, but it enlarges\nthem by allowing to deal with possibly infinite processes.\n",
        "published": "2009-10-08T12:21:22Z",
        "pdf_link": "http://arxiv.org/pdf/0910.1484v1"
    },
    {
        "id": "http://arxiv.org/abs/0910.1868v1",
        "title": "Evaluation of Hindi to Punjabi Machine Translation System",
        "summary": "  Machine Translation in India is relatively young. The earliest efforts date\nfrom the late 80s and early 90s. The success of every system is judged from its\nevaluation experimental results. Number of machine translation systems has been\nstarted for development but to the best of author knowledge, no high quality\nsystem has been completed which can be used in real applications. Recently,\nPunjabi University, Patiala, India has developed Punjabi to Hindi Machine\ntranslation system with high accuracy of about 92%. Both the systems i.e.\nsystem under question and developed system are between same closely related\nlanguages. Thus, this paper presents the evaluation results of Hindi to Punjabi\nmachine translation system. It makes sense to use same evaluation criteria as\nthat of Punjabi to Hindi Punjabi Machine Translation System. After evaluation,\nthe accuracy of the system is found to be about 95%.\n",
        "published": "2009-10-09T21:40:26Z",
        "pdf_link": "http://arxiv.org/pdf/0910.1868v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.0894v1",
        "title": "A New Computational Schema for Euphonic Conjunctions in Sanskrit\n  Processing",
        "summary": "  Automated language processing is central to the drive to enable facilitated\nreferencing of increasingly available Sanskrit E texts. The first step towards\nprocessing Sanskrit text involves the handling of Sanskrit compound words that\nare an integral part of Sanskrit texts. This firstly necessitates the\nprocessing of euphonic conjunctions or sandhis, which are points in words or\nbetween words, at which adjacent letters coalesce and transform. The ancient\nSanskrit grammarian Panini's codification of the Sanskrit grammar is the\naccepted authority in the subject. His famed sutras or aphorisms, numbering\napproximately four thousand, tersely, precisely and comprehensively codify the\nrules of the grammar, including all the rules pertaining to sandhis. This work\npresents a fresh new approach to processing sandhis in terms of a computational\nschema. This new computational model is based on Panini's complex codification\nof the rules of grammar. The model has simple beginnings and is yet powerful,\ncomprehensive and computationally lean.\n",
        "published": "2009-11-04T19:33:21Z",
        "pdf_link": "http://arxiv.org/pdf/0911.0894v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.0907v1",
        "title": "ANN-based Innovative Segmentation Method for Handwritten text in\n  Assamese",
        "summary": "  Artificial Neural Network (ANN) s has widely been used for recognition of\noptically scanned character, which partially emulates human thinking in the\ndomain of the Artificial Intelligence. But prior to recognition, it is\nnecessary to segment the character from the text to sentences, words etc.\nSegmentation of words into individual letters has been one of the major\nproblems in handwriting recognition. Despite several successful works all over\nthe work, development of such tools in specific languages is still an ongoing\nprocess especially in the Indian context. This work explores the application of\nANN as an aid to segmentation of handwritten characters in Assamese- an\nimportant language in the North Eastern part of India. The work explores the\nperformance difference obtained in applying an ANN-based dynamic segmentation\nalgorithm compared to projection- based static segmentation. The algorithm\ninvolves, first training of an ANN with individual handwritten characters\nrecorded from different individuals. Handwritten sentences are separated out\nfrom text using a static segmentation method. From the segmented line,\nindividual characters are separated out by first over segmenting the entire\nline. Each of the segments thus obtained, next, is fed to the trained ANN. The\npoint of segmentation at which the ANN recognizes a segment or a combination of\nseveral segments to be similar to a handwritten character, a segmentation\nboundary for the character is assumed to exist and segmentation performed. The\nsegmented character is next compared to the best available match and the\nsegmentation boundary confirmed.\n",
        "published": "2009-11-04T18:32:08Z",
        "pdf_link": "http://arxiv.org/pdf/0911.0907v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.1516v2",
        "title": "A Discourse-based Approach in Text-based Machine Translation",
        "summary": "  This paper presents a theoretical research based approach to ellipsis\nresolution in machine translation. The formula of discourse is applied in order\nto resolve ellipses. The validity of the discourse formula is analyzed by\napplying it to the real world text, i.e., newspaper fragments. The source text\nis converted into mono-sentential discourses where complex discourses require\nfurther dissection either directly into primitive discourses or first into\ncompound discourses and later into primitive ones. The procedure of dissection\nneeds further improvement, i.e., discovering as many primitive discourse forms\nas possible. An attempt has been made to investigate new primitive discourses\nor patterns from the given text.\n",
        "published": "2009-11-08T10:57:04Z",
        "pdf_link": "http://arxiv.org/pdf/0911.1516v2"
    },
    {
        "id": "http://arxiv.org/abs/0911.1517v2",
        "title": "Resolution of Unidentified Words in Machine Translation",
        "summary": "  This paper presents a mechanism of resolving unidentified lexical units in\nText-based Machine Translation (TBMT). In a Machine Translation (MT) system it\nis unlikely to have a complete lexicon and hence there is intense need of a new\nmechanism to handle the problem of unidentified words. These unknown words\ncould be abbreviations, names, acronyms and newly introduced terms. We have\nproposed an algorithm for the resolution of the unidentified words. This\nalgorithm takes discourse unit (primitive discourse) as a unit of analysis and\nprovides real time updates to the lexicon. We have manually applied the\nalgorithm to news paper fragments. Along with anaphora and cataphora\nresolution, many unknown words especially names and abbreviations were updated\nto the lexicon.\n",
        "published": "2009-11-09T13:52:05Z",
        "pdf_link": "http://arxiv.org/pdf/0911.1517v2"
    },
    {
        "id": "http://arxiv.org/abs/0911.1842v1",
        "title": "Standards for Language Resources",
        "summary": "  The goal of this paper is two-fold: to present an abstract data model for\nlinguistic annotations and its implementation using XML, RDF and related\nstandards; and to outline the work of a newly formed committee of the\nInternational Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource\nManagement, which will use this work as its starting point.\n",
        "published": "2009-11-10T07:12:03Z",
        "pdf_link": "http://arxiv.org/pdf/0911.1842v1"
    },
    {
        "id": "http://arxiv.org/abs/0911.2284v2",
        "title": "A New Look at the Classical Entropy of Written English",
        "summary": "  A simple method for finding the entropy and redundancy of a reasonable long\nsample of English text by direct computer processing and from first principles\naccording to Shannon theory is presented. As an example, results on the entropy\nof the English language have been obtained based on a total of 20.3 million\ncharacters of written English, considering symbols from one to five hundred\ncharacters in length. Besides a more realistic value of the entropy of English,\na new perspective on some classic entropy-related concepts is presented. This\nmethod can also be extended to other Latin languages. Some implications for\npractical applications such as plagiarism-detection software, and the minimum\nnumber of words that should be used in social Internet network messaging, are\ndiscussed.\n",
        "published": "2009-11-12T01:48:12Z",
        "pdf_link": "http://arxiv.org/pdf/0911.2284v2"
    },
    {
        "id": "http://arxiv.org/abs/0911.5116v1",
        "title": "Standardization of the formal representation of lexical information for\n  NLP",
        "summary": "  A survey of dictionary models and formats is presented as well as a\npresentation of corresponding recent standardisation activities.\n",
        "published": "2009-11-26T16:27:58Z",
        "pdf_link": "http://arxiv.org/pdf/0911.5116v1"
    },
    {
        "id": "http://arxiv.org/abs/1001.2267v1",
        "title": "Speech Recognition by Machine, A Review",
        "summary": "  This paper presents a brief survey on Automatic Speech Recognition and\ndiscusses the major themes and advances made in the past 60 years of research,\nso as to provide a technological perspective and an appreciation of the\nfundamental progress that has been accomplished in this important area of\nspeech communication. After years of research and development the accuracy of\nautomatic speech recognition remains one of the important research challenges\n(e.g., variations of the context, speakers, and environment).The design of\nSpeech Recognition system requires careful attentions to the following issues:\nDefinition of various types of speech classes, speech representation, feature\nextraction techniques, speech classifiers, database and performance evaluation.\nThe problems that are existing in ASR and the various techniques to solve these\nproblems constructed by various research workers have been presented in a\nchronological order. Hence authors hope that this work shall be a contribution\nin the area of speech recognition. The objective of this review paper is to\nsummarize and compare some of the well known methods used in various stages of\nspeech recognition system and identify research topic and applications which\nare at the forefront of this exciting and challenging field.\n",
        "published": "2010-01-13T19:02:18Z",
        "pdf_link": "http://arxiv.org/pdf/1001.2267v1"
    },
    {
        "id": "http://arxiv.org/abs/1001.4273v1",
        "title": "Sentence Simplification Aids Protein-Protein Interaction Extraction",
        "summary": "  Accurate systems for extracting Protein-Protein Interactions (PPIs)\nautomatically from biomedical articles can help accelerate biomedical research.\nBiomedical Informatics researchers are collaborating to provide metaservices\nand advance the state-of-art in PPI extraction. One problem often neglected by\ncurrent Natural Language Processing systems is the characteristic complexity of\nthe sentences in biomedical literature. In this paper, we report on the impact\nthat automatic simplification of sentences has on the performance of a\nstate-of-art PPI extraction system, showing a substantial improvement in recall\n(8%) when the sentence simplification method is applied, without significant\nimpact to precision.\n",
        "published": "2010-01-24T20:23:10Z",
        "pdf_link": "http://arxiv.org/pdf/1001.4273v1"
    },
    {
        "id": "http://arxiv.org/abs/1001.4277v1",
        "title": "Towards Effective Sentence Simplification for Automatic Processing of\n  Biomedical Text",
        "summary": "  The complexity of sentences characteristic to biomedical articles poses a\nchallenge to natural language parsers, which are typically trained on\nlarge-scale corpora of non-technical text. We propose a text simplification\nprocess, bioSimplify, that seeks to reduce the complexity of sentences in\nbiomedical abstracts in order to improve the performance of syntactic parsers\non the processed sentences. Syntactic parsing is typically one of the first\nsteps in a text mining pipeline. Thus, any improvement in performance would\nhave a ripple effect over all processing steps. We evaluated our method using a\ncorpus of biomedical sentences annotated with syntactic links. Our empirical\nresults show an improvement of 2.90% for the Charniak-McClosky parser and of\n4.23% for the Link Grammar parser when processing simplified sentences rather\nthan the original sentences in the corpus.\n",
        "published": "2010-01-24T20:35:29Z",
        "pdf_link": "http://arxiv.org/pdf/1001.4277v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.0478v1",
        "title": "tude et traitement automatique de l'anglais du XVIIe sicle :\n  outils morphosyntaxiques et dictionnaires",
        "summary": "  In this article, we record the main linguistic differences or singularities\nof 17th century English, analyse them morphologically and syntactically and\npropose equivalent forms in contemporary English. We show how 17th century\ntexts may be transcribed into modern English, combining the use of electronic\ndictionaries with rules of transcription implemented as transducers. Apr\\`es\navoir expos\\'e la constitution du corpus, nous recensons les principales\ndiff\\'erences ou particularit\\'es linguistiques de la langue anglaise du XVIIe\nsi\\`ecle, les analysons du point de vue morphologique et syntaxique et\nproposons des \\'equivalents en anglais contemporain (AC). Nous montrons comment\nnous pouvons effectuer une transcription automatique de textes anglais du XVIIe\nsi\\`ecle en anglais moderne, en combinant l'utilisation de dictionnaires\n\\'electroniques avec des r\\`egles de transcriptions impl\\'ement\\'ees sous forme\nde transducteurs.\n",
        "published": "2010-02-02T13:23:47Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0478v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.0479v1",
        "title": "\"Mind your p's and q's\": or the peregrinations of an apostrophe in 17th\n  Century English",
        "summary": "  If the use of the apostrophe in contemporary English often marks the Saxon\ngenitive, it may also indicate the omission of one or more let-ters. Some\nwriters (wrongly?) use it to mark the plural in symbols or abbreviations,\nvisual-ised thanks to the isolation of the morpheme \"s\". This punctuation mark\nwas imported from the Continent in the 16th century. During the 19th century\nits use was standardised. However the rules of its usage still seem problematic\nto many, including literate speakers of English. \"All too often, the apostrophe\nis misplaced\", or \"errant apostrophes are springing up every-where\" is a\ncomplaint that Internet users fre-quently come across when visiting grammar\nwebsites. Many of them detail its various uses and misuses, and attempt to\ncorrect the most common mistakes about it, especially its mis-use in the\nplural, called greengrocers' apostro-phes and humorously misspelled\n\"greengro-cers apostrophe's\". While studying English travel accounts published\nin the seventeenth century, we noticed that the different uses of this symbol\nmay accompany various models of metaplasms. We were able to highlight the\nlinguistic variations of some lexemes, and trace the origin of modern grammar\nrules gov-erning its usage.\n",
        "published": "2010-02-02T13:24:20Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0479v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.0481v2",
        "title": "Recognition and translation Arabic-French of Named Entities: case of the\n  Sport places",
        "summary": "  The recognition of Arabic Named Entities (NE) is a problem in different\ndomains of Natural Language Processing (NLP) like automatic translation.\nIndeed, NE translation allows the access to multilingual in-formation. This\ntranslation doesn't always lead to expected result especially when NE contains\na person name. For this reason and in order to ameliorate translation, we can\ntransliterate some part of NE. In this context, we propose a method that\nintegrates translation and transliteration together. We used the linguis-tic\nNooJ platform that is based on local grammars and transducers. In this paper,\nwe focus on sport domain. We will firstly suggest a refinement of the\ntypological model presented at the MUC Conferences we will describe the\nintegration of an Arabic transliteration module into translation system.\nFinally, we will detail our method and give the results of the evaluation.\n",
        "published": "2010-02-02T13:24:38Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0481v2"
    },
    {
        "id": "http://arxiv.org/abs/1002.0485v1",
        "title": "Morphological study of Albanian words, and processing with NooJ",
        "summary": "  We are developing electronic dictionaries and transducers for the automatic\nprocessing of the Albanian Language. We will analyze the words inside a linear\nsegment of text. We will also study the relationship between units of sense and\nunits of form. The composition of words takes different forms in Albanian. We\nhave found that morphemes are frequently concatenated or simply juxtaposed or\ncontracted. The inflected grammar of NooJ allows constructing the dictionaries\nof flexed forms (declensions or conjugations). The diversity of word structures\nrequires tools to identify words created by simple concatenation, or to treat\ncontractions. The morphological tools of NooJ allow us to create grammatical\ntools to represent and treat these phenomena. But certain problems exceed the\nmorphological analysis and must be represented by syntactical grammars.\n",
        "published": "2010-02-02T13:35:02Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0485v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.0773v1",
        "title": "Approximations to the MMI criterion and their effect on lattice-based\n  MMI",
        "summary": "  Maximum mutual information (MMI) is a model selection criterion used for\nhidden Markov model (HMM) parameter estimation that was developed more than\ntwenty years ago as a discriminative alternative to the maximum likelihood\ncriterion for HMM-based speech recognition. It has been shown in the speech\nrecognition literature that parameter estimation using the current MMI\nparadigm, lattice-based MMI, consistently outperforms maximum likelihood\nestimation, but this is at the expense of undesirable convergence properties.\nIn particular, recognition performance is sensitive to the number of times that\nthe iterative MMI estimation algorithm, extended Baum-Welch, is performed. In\nfact, too many iterations of extended Baum-Welch will lead to degraded\nperformance, despite the fact that the MMI criterion improves at each\niteration. This phenomenon is at variance with the analogous behavior of\nmaximum likelihood estimation -- at least for the HMMs used in speech\nrecognition -- and it has previously been attributed to `over fitting'. In this\npaper, we present an analysis of lattice-based MMI that demonstrates, first of\nall, that the asymptotic behavior of lattice-based MMI is much worse than was\npreviously understood, i.e. it does not appear to converge at all, and, second\nof all, that this is not due to `over fitting'. Instead, we demonstrate that\nthe `over fitting' phenomenon is the result of standard methodology that\nexacerbates the poor behavior of two key approximations in the lattice-based\nMMI machinery. We also demonstrate that if we modify the standard methodology\nto improve the validity of these approximations, then the convergence\nproperties of lattice-based MMI become benign without sacrificing improvements\nto recognition accuracy.\n",
        "published": "2010-02-03T16:06:24Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0773v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.0904v1",
        "title": "On Event Structure in the Torn Dress",
        "summary": "  Using Pustejovsky's \"The Syntax of Event Structure\" and Fong's \"On Mending a\nTorn Dress\" we give a glimpse of a Pustejovsky-like analysis to some example\nsentences in Fong. We attempt to give a framework for semantics to the noun\nphrases and adverbs as appropriate as well as the lexical entries for all words\nin the examples and critique both papers in light of our findings and\ndifficulties.\n",
        "published": "2010-02-04T06:30:14Z",
        "pdf_link": "http://arxiv.org/pdf/1002.0904v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.1095v1",
        "title": "Towards a Heuristic Categorization of Prepositional Phrases in English\n  with WordNet",
        "summary": "  This document discusses an approach and its rudimentary realization towards\nautomatic classification of PPs; the topic, that has not received as much\nattention in NLP as NPs and VPs. The approach is a rule-based heuristics\noutlined in several levels of our research. There are 7 semantic categories of\nPPs considered in this document that we are able to classify from an annotated\ncorpus.\n",
        "published": "2010-02-04T22:48:31Z",
        "pdf_link": "http://arxiv.org/pdf/1002.1095v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.1919v2",
        "title": "Thai Rhetorical Structure Analysis",
        "summary": "  Rhetorical structure analysis (RSA) explores discourse relations among\nelementary discourse units (EDUs) in a text. It is very useful in many text\nprocessing tasks employing relationships among EDUs such as text understanding,\nsummarization, and question-answering. Thai language with its distinctive\nlinguistic characteristics requires a unique technique. This article proposes\nan approach for Thai rhetorical structure analysis. First, EDUs are segmented\nby two hidden Markov models derived from syntactic rules. A rhetorical\nstructure tree is constructed from a clustering technique with its similarity\nmeasure derived from Thai semantic rules. Then, a decision tree whose features\nderived from the semantic rules is used to determine discourse relations.\n",
        "published": "2010-02-09T20:01:06Z",
        "pdf_link": "http://arxiv.org/pdf/1002.1919v2"
    },
    {
        "id": "http://arxiv.org/abs/1002.3320v1",
        "title": "Co-channel Interference Cancellation for Space-Time Coded OFDM Systems\n  Using Adaptive Beamforming and Null Deepening",
        "summary": "  Combined with space-time coding, the orthogonal frequency division\nmultiplexing (OFDM) system explores space diversity. It is a potential scheme\nto offer spectral efficiency and robust high data rate transmissions over\nfrequency-selective fading channel. However, space-time coding impairs the\nsystem ability to suppress interferences as the signals transmitted from two\ntransmit antennas are superposed and interfered at the receiver antennas. In\nthis paper, we developed an adaptive beamforming based on least mean squared\nerror algorithm and null deepening to combat co-channel interference (CCI) for\nthe space-time coded OFDM (STC-OFDM) system. To illustrate the performance of\nthe presented approach, it is compared to the null steering beamformer which\nrequires a prior knowledge of directions of arrival (DOAs). The structure of\nspace-time decoders are preserved although there is the use of beamformers\nbefore decoding. By incorporating the proposed beamformer as a CCI canceller in\nthe STC-OFDM systems, the performance improvement is achieved as shown in the\nsimulation results.\n",
        "published": "2010-02-17T17:30:22Z",
        "pdf_link": "http://arxiv.org/pdf/1002.3320v1"
    },
    {
        "id": "http://arxiv.org/abs/1002.4820v1",
        "title": "SLAM : Solutions lexicales automatique pour mtaphores",
        "summary": "  This article presents SLAM, an Automatic Solver for Lexical Metaphors like\n?d\\'eshabiller* une pomme? (to undress* an apple). SLAM calculates a\nconventional solution for these productions. To carry on it, SLAM has to\nintersect the paradigmatic axis of the metaphorical verb ?d\\'eshabiller*?,\nwhere ?peler? (?to peel?) comes closer, with a syntagmatic axis that comes from\na corpus where ?peler une pomme? (to peel an apple) is semantically and\nsyntactically regular. We test this model on DicoSyn, which is a ?small world?\nnetwork of synonyms, to compute the paradigmatic axis and on Frantext.20, a\nFrench corpus, to compute the syntagmatic axis. Further, we evaluate the model\nwith a sample of an experimental corpus of the database of Flexsem\n",
        "published": "2010-02-25T16:27:36Z",
        "pdf_link": "http://arxiv.org/pdf/1002.4820v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.0206v1",
        "title": "Why has (reasonably accurate) Automatic Speech Recognition been so hard\n  to achieve?",
        "summary": "  Hidden Markov models (HMMs) have been successfully applied to automatic\nspeech recognition for more than 35 years in spite of the fact that a key HMM\nassumption -- the statistical independence of frames -- is obviously violated\nby speech data. In fact, this data/model mismatch has inspired many attempts to\nmodify or replace HMMs with alternative models that are better able to take\ninto account the statistical dependence of frames. However it is fair to say\nthat in 2010 the HMM is the consensus model of choice for speech recognition\nand that HMMs are at the heart of both commercially available products and\ncontemporary research systems. In this paper we present a preliminary\nexploration aimed at understanding how speech data depart from HMMs and what\neffect this departure has on the accuracy of HMM-based speech recognition. Our\nanalysis uses standard diagnostic tools from the field of statistics --\nhypothesis testing, simulation and resampling -- which are rarely used in the\nfield of speech recognition. Our main result, obtained by novel manipulations\nof real and resampled data, demonstrates that real data have statistical\ndependency and that this dependency is responsible for significant numbers of\nrecognition errors. We also demonstrate, using simulation and resampling, that\nif we `remove' the statistical dependency from data, then the resulting\nrecognition error rates become negligible. Taken together, these results\nsuggest that a better understanding of the structure of the statistical\ndependency in speech data is a crucial first step towards improving HMM-based\nspeech recognition.\n",
        "published": "2010-02-28T19:00:12Z",
        "pdf_link": "http://arxiv.org/pdf/1003.0206v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.0337v1",
        "title": "Change of word types to word tokens ratio in the course of translation\n  (based on Russian translations of K. Vonnegut novels)",
        "summary": "  The article provides lexical statistical analysis of K. Vonnegut's two novels\nand their Russian translations. It is found out that there happen some changes\nbetween the speed of word types and word tokens ratio change in the source and\ntarget texts. The author hypothesizes that these changes are typical for\nEnglish-Russian translations, and moreover, they represent an example of\nBaker's translation feature of levelling out.\n",
        "published": "2010-03-01T18:04:39Z",
        "pdf_link": "http://arxiv.org/pdf/1003.0337v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.0628v1",
        "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction",
        "summary": "  Text documents are complex high dimensional objects. To effectively visualize\nsuch data it is important to reduce its dimensionality and visualize the low\ndimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore\ndimensionality reduction methods that draw upon domain knowledge in order to\nachieve a better low dimensional embedding and visualization of documents. We\nconsider the use of geometries specified manually by an expert, geometries\nderived automatically from corpus statistics, and geometries computed from\nlinguistic resources.\n",
        "published": "2010-03-02T16:52:32Z",
        "pdf_link": "http://arxiv.org/pdf/1003.0628v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.1399v1",
        "title": "Automatic derivation of domain terms and concept location based on the\n  analysis of the identifiers",
        "summary": "  Developers express the meaning of the domain ideas in specifically selected\nidentifiers and comments that form the target implemented code. Software\nmaintenance requires knowledge and understanding of the encoded ideas. This\npaper presents a way how to create automatically domain vocabulary. Knowledge\nof domain vocabulary supports the comprehension of a specific domain for later\ncode maintenance or evolution. We present experiments conducted in two selected\ndomains: application servers and web frameworks. Knowledge of domain terms\nenables easy localization of chunks of code that belong to a certain term. We\nconsider these chunks of code as \"concepts\" and their placement in the code as\n\"concept location\". Application developers may also benefit from the obtained\ndomain terms. These terms are parts of speech that characterize a certain\nconcept. Concepts are encoded in \"classes\" (OO paradigm) and the obtained\nvocabulary of terms supports the selection and the comprehension of the class'\nappropriate identifiers. We measured the following software products with our\ntool: JBoss, JOnAS, GlassFish, Tapestry, Google Web Toolkit and Echo2.\n",
        "published": "2010-03-06T16:34:40Z",
        "pdf_link": "http://arxiv.org/pdf/1003.1399v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.1455v1",
        "title": "A Computational Algorithm based on Empirical Analysis, that Composes\n  Sanskrit Poetry",
        "summary": "  Poetry-writing in Sanskrit is riddled with problems for even those who know\nthe language well. This is so because the rules that govern Sanskrit prosody\nare numerous and stringent. We propose a computational algorithm that converts\nprose given as E-text into poetry in accordance with the metrical rules of\nSanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic\nconjunction, which is compulsory in verse, is handled. The algorithm is\nconsiderably speeded up by a novel method of reducing the target search\ndatabase. The algorithm further gives suggestions to the poet in case what\nhe/she has given as the input prose is impossible to fit into any allowed\nmetrical format. There is also an interactive component of the algorithm by\nwhich the algorithm interacts with the poet to resolve ambiguities. In\naddition, this unique work, which provides a solution to a problem that has\nnever been addressed before, provides a simple yet effective speech recognition\ninterface that would help the visually impaired dictate words in E-text, which\nis in turn versified by our Poetry Composer Engine.\n",
        "published": "2010-03-07T11:28:08Z",
        "pdf_link": "http://arxiv.org/pdf/1003.1455v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.4149v1",
        "title": "Les Entits Nommes : usage et degrs de prcision et de\n  dsambigusation",
        "summary": "  The recognition and classification of Named Entities (NER) are regarded as an\nimportant component for many Natural Language Processing (NLP) applications.\nThe classification is usually made by taking into account the immediate context\nin which the NE appears. In some cases, this immediate context does not allow\ngetting the right classification. We show in this paper that the use of an\nextended syntactic context and large-scale resources could be very useful in\nthe NER task.\n",
        "published": "2010-03-22T13:09:57Z",
        "pdf_link": "http://arxiv.org/pdf/1003.4149v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.4894v1",
        "title": "La reprsentation formelle des concepts spatiaux dans la langue",
        "summary": "  In this chapter, we assume that systematically studying spatial markers\nsemantics in language provides a means to reveal fundamental properties and\nconcepts characterizing conceptual representations of space. We propose a\nformal system accounting for the properties highlighted by the linguistic\nanalysis, and we use these tools for representing the semantic content of\nseveral spatial relations of French. The first part presents a semantic\nanalysis of the expression of space in French aiming at describing the\nconstraints that formal representations have to take into account. In the\nsecond part, after presenting the structure of our formal system, we set out\nits components. A commonsense geometry is sketched out and several functional\nand pragmatic spatial concepts are formalized. We take a special attention in\nshowing that these concepts are well suited to representing the semantic\ncontent of several prepositions of French ('sur' (on), 'dans' (in), 'devant'\n(in front of), 'au-dessus' (above)), and in illustrating the inferential\nadequacy of these representations.\n",
        "published": "2010-03-25T14:03:51Z",
        "pdf_link": "http://arxiv.org/pdf/1003.4894v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.4898v1",
        "title": "Les entits spatiales dans la langue : tude descriptive, formelle\n  et exprimentale de la catgorisation",
        "summary": "  While previous linguistic and psycholinguistic research on space has mainly\nanalyzed spatial relations, the studies reported in this paper focus on how\nlanguage distinguishes among spatial entities. Descriptive and experimental\nstudies first propose a classification of entities, which accounts for both\nstatic and dynamic space, has some cross-linguistic validity, and underlies\nadults' cognitive processing. Formal and computational analyses then introduce\ntheoretical elements aiming at modelling these categories, while fulfilling\nvarious properties of formal ontologies (generality, parsimony, coherence...).\nThis formal framework accounts, in particular, for functional dependences among\nentities underlying some part-whole descriptions. Finally, developmental\nresearch shows that language-specific properties have a clear impact on how\nchildren talk about space. The results suggest some cross-linguistic\nvariability in children's spatial representations from an early age onwards,\nbringing into question models in which general cognitive capacities are the\nonly determinants of spatial cognition during the course of development.\n",
        "published": "2010-03-25T14:08:44Z",
        "pdf_link": "http://arxiv.org/pdf/1003.4898v1"
    },
    {
        "id": "http://arxiv.org/abs/1003.5372v1",
        "title": "Learning Recursive Segments for Discourse Parsing",
        "summary": "  Automatically detecting discourse segments is an important preliminary step\ntowards full discourse parsing. Previous research on discourse segmentation\nhave relied on the assumption that elementary discourse units (EDUs) in a\ndocument always form a linear sequence (i.e., they can never be nested).\nUnfortunately, this assumption turns out to be too strong, for some theories of\ndiscourse like SDRT allows for nested discourse units. In this paper, we\npresent a simple approach to discourse segmentation that is able to produce\nnested EDUs. Our approach builds on standard multi-class classification\ntechniques combined with a simple repairing heuristic that enforces global\ncoherence. Our system was developed and evaluated on the first round of\nannotations provided by the French Annodis project (an ongoing effort to create\na discourse bank for French). Cross-validated on only 47 documents (1,445\nEDUs), our system achieves encouraging performance results with an F-score of\n73% for finding EDUs.\n",
        "published": "2010-03-28T15:17:22Z",
        "pdf_link": "http://arxiv.org/pdf/1003.5372v1"
    },
    {
        "id": "http://arxiv.org/abs/1005.3902v1",
        "title": "Morphonette: a morphological network of French",
        "summary": "  This paper describes in details the first version of Morphonette, a new\nFrench morphological resource and a new radically lexeme-based method of\nmorphological analysis. This research is grounded in a paradigmatic conception\nof derivational morphology where the morphological structure is a structure of\nthe entire lexicon and not one of the individual words it contains. The\ndiscovery of this structure relies on a measure of morphological similarity\nbetween words, on formal analogy and on the properties of two morphological\nparadigms:\n",
        "published": "2010-05-21T08:12:12Z",
        "pdf_link": "http://arxiv.org/pdf/1005.3902v1"
    },
    {
        "id": "http://arxiv.org/abs/1005.4697v3",
        "title": "The Lambek-Grishin calculus is NP-complete",
        "summary": "  The Lambek-Grishin calculus LG is the symmetric extension of the\nnon-associative Lambek calculus NL. In this paper we prove that the\nderivability problem for LG is NP-complete.\n",
        "published": "2010-05-25T20:36:09Z",
        "pdf_link": "http://arxiv.org/pdf/1005.4697v3"
    },
    {
        "id": "http://arxiv.org/abs/1005.5466v1",
        "title": "Quantitative parametrization of texts written by Ivan Franko: An attempt\n  of the project",
        "summary": "  In the article, the project of quantitative parametrization of all texts by\nIvan Franko is manifested. It can be made only by using modern computer\ntechniques after the frequency dictionaries for all Franko's works are\ncompiled. The paper describes the application spheres, methodology, stages,\nprinciples and peculiarities in the compilation of the frequency dictionary of\nthe second half of the 19th century - the beginning of the 20th century. The\nrelation between the Ivan Franko frequency dictionary, explanatory dictionary\nof writer's language and text corpus is discussed.\n",
        "published": "2010-05-29T16:37:02Z",
        "pdf_link": "http://arxiv.org/pdf/1005.5466v1"
    },
    {
        "id": "http://arxiv.org/abs/1005.5596v1",
        "title": "A generic tool to generate a lexicon for NLP from Lexicon-Grammar tables",
        "summary": "  Lexicon-Grammar tables constitute a large-coverage syntactic lexicon but they\ncannot be directly used in Natural Language Processing (NLP) applications\nbecause they sometimes rely on implicit information. In this paper, we\nintroduce LGExtract, a generic tool for generating a syntactic lexicon for NLP\nfrom the Lexicon-Grammar tables. It is based on a global table that contains\nundefined information and on a unique extraction script including all\noperations to be performed for all tables. We also present an experiment that\nhas been conducted to generate a new lexicon of French verbs and predicative\nnouns.\n",
        "published": "2010-05-31T06:37:40Z",
        "pdf_link": "http://arxiv.org/pdf/1005.5596v1"
    },
    {
        "id": "http://arxiv.org/abs/1006.0153v1",
        "title": "Ivan Franko's novel Dlja domashnjoho ohnyshcha (For the Hearth) in the\n  light of the frequency dictionary",
        "summary": "  In the article, the methodology and the principles of the compilation of the\nFrequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For\nthe Hearth) are described. The following statistical parameters of the novel\nvocabulary are obtained: variety, exclusiveness, concentration indexes,\ncorrelation between word rank and text coverage, etc. The main quantitative\ncharacteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and\nDlja domashnjoho ohnyshcha are compared on the basis of their frequency\ndictionaries.\n",
        "published": "2010-06-01T15:20:59Z",
        "pdf_link": "http://arxiv.org/pdf/1006.0153v1"
    },
    {
        "id": "http://arxiv.org/abs/1006.2809v1",
        "title": "Offline Arabic Handwriting Recognition Using Artificial Neural Network",
        "summary": "  The ambition of a character recognition system is to transform a text\ndocument typed on paper into a digital format that can be manipulated by word\nprocessor software Unlike other languages, Arabic has unique features, while\nother language doesn't have, from this language these are seven or eight\nlanguage such as ordo, jewie and Persian writing, Arabic has twenty eight\nletters, each of which can be linked in three different ways or separated\ndepending on the case. The difficulty of the Arabic handwriting recognition is\nthat, the accuracy of the character recognition which affects on the accuracy\nof the word recognition, in additional there is also two or three from for each\ncharacter, the suggested solution by using artificial neural network can solve\nthe problem and overcome the difficulty of Arabic handwriting recognition.\n",
        "published": "2010-06-14T19:20:31Z",
        "pdf_link": "http://arxiv.org/pdf/1006.2809v1"
    },
    {
        "id": "http://arxiv.org/abs/1006.2835v1",
        "title": "Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit\n  Grammar",
        "summary": "  Indian languages have long history in World Natural languages. Panini was the\nfirst to define Grammar for Sanskrit language with about 4000 rules in fifth\ncentury. These rules contain uncertainty information. It is not possible to\nComputer processing of Sanskrit language with uncertain information. In this\npaper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate\nuncertain information for reasoning with Sanskrit grammar. The Sanskrit\nlanguage processing is also discussed in this paper.\n",
        "published": "2010-06-14T20:07:32Z",
        "pdf_link": "http://arxiv.org/pdf/1006.2835v1"
    },
    {
        "id": "http://arxiv.org/abs/1006.3787v7",
        "title": "Complete Complementary Results Report of the MARF's NLP Approach to the\n  DEFT 2010 Competition",
        "summary": "  This companion paper complements the main DEFT'10 article describing the MARF\napproach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described at\nhttp://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimed\nto present the complete result sets of all the conducted experiments and their\nsettings in the resulting tables highlighting the approach and the best\nresults, but also showing the worse and the worst and their subsequent\nanalysis. This particular work focuses on application of the MARF's classical\nand NLP pipelines to identification tasks within various francophone corpora to\nidentify decades when certain articles were published for the first track\n(Piste 1) and place of origin of a publication (Piste 2), such as the journal\nand location (France vs. Quebec). This is the sixth iteration of the release of\nthe results.\n",
        "published": "2010-06-18T19:54:29Z",
        "pdf_link": "http://arxiv.org/pdf/1006.3787v7"
    },
    {
        "id": "http://arxiv.org/abs/1006.5880v1",
        "title": "Testing SDRT's Right Frontier",
        "summary": "  The Right Frontier Constraint (RFC), as a constraint on the attachment of new\nconstituents to an existing discourse structure, has important implications for\nthe interpretation of anaphoric elements in discourse and for Machine Learning\n(ML) approaches to learning discourse structures. In this paper we provide\nstrong empirical support for SDRT's version of RFC. The analysis of about 100\ndoubly annotated documents by five different naive annotators shows that SDRT's\nRFC is respected about 95% of the time. The qualitative analysis of presumed\nviolations that we have performed shows that they are either click-errors or\nstructural misconceptions.\n",
        "published": "2010-06-30T15:10:33Z",
        "pdf_link": "http://arxiv.org/pdf/1006.5880v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.0170v1",
        "title": "Symmetric categorial grammar: residuation and Galois connections",
        "summary": "  The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:\nin addition to the residuated family of product, left and right division\noperations of Lambek's original calculus, one also considers a family of\ncoproduct, right and left difference operations, related to the former by an\narrow-reversing duality. Communication between the two families is implemented\nin terms of linear distributivity principles. The aim of this paper is to\ncomplement the symmetry between (dual) residuated type-forming operations with\nan orthogonal opposition that contrasts residuated and Galois connected\noperations. Whereas the (dual) residuated operations are monotone, the Galois\nconnected operations (and their duals) are antitone. We discuss the algebraic\nproperties of the (dual) Galois connected operations, and generalize the\n(co)product distributivity principles to include the negative operations. We\ngive a continuation-passing-style translation for the new type-forming\noperations, and discuss some linguistic applications.\n",
        "published": "2010-08-01T12:45:58Z",
        "pdf_link": "http://arxiv.org/pdf/1008.0170v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.1986v1",
        "title": "For the sake of simplicity: Unsupervised extraction of lexical\n  simplifications from Wikipedia",
        "summary": "  We report on work in progress on extracting lexical simplifications (e.g.,\n\"collaborate\" -> \"work together\"), focusing on utilizing edit histories in\nSimple English Wikipedia for this task. We consider two main approaches: (1)\nderiving simplification probabilities via an edit model that accounts for a\nmixture of different operations, and (2) using metadata to focus on edits that\nare more likely to be simplification operations. We find our methods to\noutperform a reasonable baseline and yield many high-quality lexical\nsimplifications not included in an independently-created manually prepared\nlist.\n",
        "published": "2010-08-11T20:01:59Z",
        "pdf_link": "http://arxiv.org/pdf/1008.1986v1"
    },
    {
        "id": "http://arxiv.org/abs/1008.3169v2",
        "title": "Don't 'have a clue'? Unsupervised co-learning of downward-entailing\n  operators",
        "summary": "  Researchers in textual entailment have begun to consider inferences involving\n'downward-entailing operators', an interesting and important class of lexical\nitems that change the way inferences are made. Recent work proposed a method\nfor learning English downward-entailing operators that requires access to a\nhigh-quality collection of 'negative polarity items' (NPIs). However, English\nis one of the very few languages for which such a list exists. We propose the\nfirst approach that can be applied to the many languages for which there is no\npre-existing high-precision database of NPIs. As a case study, we apply our\nmethod to Romanian and show that our method yields good results. Also, we\nperform a cross-linguistic analysis that suggests interesting connections to\nsome findings in linguistic typology.\n",
        "published": "2010-08-18T20:08:22Z",
        "pdf_link": "http://arxiv.org/pdf/1008.3169v2"
    },
    {
        "id": "http://arxiv.org/abs/1009.1117v2",
        "title": "Constructions dfinitoires des tables du Lexique-Grammaire",
        "summary": "  Lexicon-Grammar tables are a very rich syntactic lexicon for the French\nlanguage. This linguistic database is nevertheless not directly suitable for\nuse by computer programs, as it is incomplete and lacks consistency. Tables are\ndefined on the basis of features which are not explicitly recorded in the\nlexicon. These features are only described in literature. Our aim is to define\nfor each tables these essential properties to make them usable in various\nNatural Language Processing (NLP) applications, such as parsing.\n",
        "published": "2010-09-06T18:35:08Z",
        "pdf_link": "http://arxiv.org/pdf/1009.1117v2"
    },
    {
        "id": "http://arxiv.org/abs/1009.3238v1",
        "title": "Tableaux for the Lambek-Grishin calculus",
        "summary": "  Categorial type logics, pioneered by Lambek, seek a proof-theoretic\nunderstanding of natural language syntax by identifying categories with\nformulas and derivations with proofs. We typically observe an intuitionistic\nbias: a structural configuration of hypotheses (a constituent) derives a single\nconclusion (the category assigned to it). Acting upon suggestions of Grishin to\ndualize the logical vocabulary, Moortgat proposed the Lambek-Grishin calculus\n(LG) with the aim of restoring symmetry between hypotheses and conclusions. We\ndevelop a theory of labeled modal tableaux for LG, inspired by the\ninterpretation of its connectives as binary modal operators in the relational\nsemantics of Kurtonina and Moortgat. As a linguistic application of our method,\nwe show that grammars based on LG are context-free through use of an\ninterpolation lemma. This result complements that of Melissen, who proved that\nLG augmented by mixed associativity and -commutativity was exceeds LTAG in\nexpressive power.\n",
        "published": "2010-09-16T18:19:57Z",
        "pdf_link": "http://arxiv.org/pdf/1009.3238v1"
    },
    {
        "id": "http://arxiv.org/abs/1010.1826v1",
        "title": "A probabilistic top-down parser for minimalist grammars",
        "summary": "  This paper describes a probabilistic top-down parser for minimalist grammars.\nTop-down parsers have the great advantage of having a certain predictive power\nduring the parsing, which takes place in a left-to-right reading of the\nsentence. Such parsers have already been well-implemented and studied in the\ncase of Context-Free Grammars, which are already top-down, but these are\ndifficult to adapt to Minimalist Grammars, which generate sentences bottom-up.\nI propose here a way of rewriting Minimalist Grammars as Linear Context-Free\nRewriting Systems, allowing to easily create a top-down parser. This rewriting\nallows also to put a probabilistic field on these grammars, which can be used\nto accelerate the parser. Finally, I propose a method of refining the\nprobabilistic field by using algorithms used in data compression.\n",
        "published": "2010-10-09T10:09:18Z",
        "pdf_link": "http://arxiv.org/pdf/1010.1826v1"
    },
    {
        "id": "http://arxiv.org/abs/1010.2384v1",
        "title": "Learning Taxonomy for Text Segmentation by Formal Concept Analysis",
        "summary": "  In this paper the problems of deriving a taxonomy from a text and\nconcept-oriented text segmentation are approached. Formal Concept Analysis\n(FCA) method is applied to solve both of these linguistic problems. The\nproposed segmentation method offers a conceptual view for text segmentation,\nusing a context-driven clustering of sentences. The Concept-oriented Clustering\nSegmentation algorithm (COCS) is based on k-means linear clustering of the\nsentences. Experimental results obtained using COCS algorithm are presented.\n",
        "published": "2010-10-12T13:20:30Z",
        "pdf_link": "http://arxiv.org/pdf/1010.2384v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.0519v1",
        "title": "Stabilizing knowledge through standards - A perspective for the\n  humanities",
        "summary": "  It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences.\n",
        "published": "2010-11-02T06:37:31Z",
        "pdf_link": "http://arxiv.org/pdf/1011.0519v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.0835v1",
        "title": "A PDTB-Styled End-to-End Discourse Parser",
        "summary": "  We have developed a full discourse parser in the Penn Discourse Treebank\n(PDTB) style. Our trained parser first identifies all discourse and\nnon-discourse relations, locates and labels their arguments, and then\nclassifies their relation types. When appropriate, the attribution spans to\nthese relations are also determined. We present a comprehensive evaluation from\nboth component-wise and error-cascading perspectives.\n",
        "published": "2010-11-03T10:05:15Z",
        "pdf_link": "http://arxiv.org/pdf/1011.0835v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.2922v1",
        "title": "Emoticonsciousness",
        "summary": "  A temporal analysis of emoticon use in Swedish, Italian, German and English\nasynchronous electronic communication is reported. Emoticons are classified as\npositive, negative and neutral. Postings to newsgroups over a 66 week period\nare considered. The aggregate analysis of emoticon use in newsgroups for\nscience and politics tend on the whole to be consistent over the entire time\nperiod. Where possible, events that coincide with divergences from trends in\nlanguage-subject pairs are noted. Political discourse in Italian over the\nperiod shows marked use of negative emoticons, and in Swedish, positive\nemoticons.\n",
        "published": "2010-11-12T14:43:04Z",
        "pdf_link": "http://arxiv.org/pdf/1011.2922v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.4155v1",
        "title": "Motifs de graphe pour le calcul de dpendances syntaxiques compltes",
        "summary": "  This article describes a method to build syntactical dependencies starting\nfrom the phrase structure parsing process. The goal is to obtain all the\ninformation needed for a detailled semantical analysis. Interaction Grammars\nare used for parsing; the saturation of polarities which is the core of this\nformalism can be mapped to dependency relation. Formally, graph patterns are\nused to express the set of constraints which control dependency creations.\n",
        "published": "2010-11-18T08:59:55Z",
        "pdf_link": "http://arxiv.org/pdf/1011.4155v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.4623v1",
        "title": "Opinion Polarity Identification through Adjectives",
        "summary": "  \"What other people think\" has always been an important piece of information\nduring various decision-making processes. Today people frequently make their\nopinions available via the Internet, and as a result, the Web has become an\nexcellent source for gathering consumer opinions. There are now numerous Web\nresources containing such opinions, e.g., product reviews forums, discussion\ngroups, and Blogs. But, due to the large amount of information and the wide\nrange of sources, it is essentially impossible for a customer to read all of\nthe reviews and make an informed decision on whether to purchase the product.\nIt is also difficult for the manufacturer or seller of a product to accurately\nmonitor customer opinions. For this reason, mining customer reviews, or opinion\nmining, has become an important issue for research in Web information\nextraction. One of the important topics in this research area is the\nidentification of opinion polarity. The opinion polarity of a review is usually\nexpressed with values 'positive', 'negative' or 'neutral'. We propose a\ntechnique for identifying polarity of reviews by identifying the polarity of\nthe adjectives that appear in them. Our evaluation shows the technique can\nprovide accuracy in the area of 73%, which is well above the 58%-64% provided\nby naive Bayesian classifiers.\n",
        "published": "2010-11-21T00:15:27Z",
        "pdf_link": "http://arxiv.org/pdf/1011.4623v1"
    },
    {
        "id": "http://arxiv.org/abs/1011.5188v2",
        "title": "La rduction de termes complexes dans les langues de spcialit",
        "summary": "  Our study applies statistical methods to French and Italian corpora to\nexamine the phenomenon of multi-word term reduction in specialty languages.\nThere are two kinds of reduction: anaphoric and lexical. We show that anaphoric\nreduction depends on the discourse type (vulgarization, pedagogical,\nspecialized) but is independent of both domain and language; that lexical\nreduction depends on domain and is more frequent in technical, rapidly evolving\ndomains; and that anaphoric reductions tend to follow full terms rather than\nprecede them. We define the notion of the anaphoric tree of the term and study\nits properties. Concerning lexical reduction, we attempt to prove statistically\nthat there is a notion of term lifecycle, where the full form is progressively\nreplaced by a lexical reduction. ----- Nous \\'etudions par des m\\'ethodes\nstatistiques sur des corpus fran\\c{c}ais et italiens, le ph\\'enom\\`ene de\nr\\'eduction des termes complexes dans les langues de sp\\'ecialit\\'e. Il existe\ndeux types de r\\'eductions : anaphorique et lexicale. Nous montrons que la\nr\\'eduction anaphorique d\\'epend du type de discours (de vulgarisation,\np\\'edagogique, sp\\'ecialis\\'e) mais ne d\\'epend ni du domaine, ni de la langue,\nalors que la r\\'eduction lexicale d\\'epend du domaine et est plus fr\\'equente\ndans les domaines techniques \\`a \\'evolution rapide. D'autre part, nous\nmontrons que la r\\'eduction anaphorique a tendance \\`a suivre la forme pleine\ndu terme, nous d\\'efinissons une notion d'arbre anaphorique de terme et nous\n\\'etudions ses propri\\'et\\'es. Concernant la r\\'eduction lexicale, nous tentons\nde d\\'emontrer statistiquement qu'il existe une notion de cycle de vie de\nterme, o\\`u la forme pleine est progressivement remplac\\'ee par une r\\'eduction\nlexicale.\n",
        "published": "2010-11-23T18:20:40Z",
        "pdf_link": "http://arxiv.org/pdf/1011.5188v2"
    },
    {
        "id": "http://arxiv.org/abs/1101.5076v6",
        "title": "Geometric representations for minimalist grammars",
        "summary": "  We reformulate minimalist grammars as partial functions on term algebras for\nstrings and trees. Using filler/role bindings and tensor product\nrepresentations, we construct homomorphisms for these data structures into\ngeometric vector spaces. We prove that the structure-building functions as well\nas simple processors for minimalist languages can be realized by piecewise\nlinear operators in representation space. We also propose harmony, i.e. the\ndistance of an intermediate processing step from the final well-formed state in\nrepresentation space, as a measure of processing complexity. Finally, we\nillustrate our findings by means of two particular arithmetic and fractal\nrepresentations.\n",
        "published": "2011-01-26T14:56:20Z",
        "pdf_link": "http://arxiv.org/pdf/1101.5076v6"
    },
    {
        "id": "http://arxiv.org/abs/1101.5494v1",
        "title": "Developing a New Approach for Arabic Morphological Analysis and\n  Generation",
        "summary": "  Arabic morphological analysis is one of the essential stages in Arabic\nNatural Language Processing. In this paper we present an approach for Arabic\nmorphological analysis. This approach is based on Arabic morphological\nautomaton (AMAUT). The proposed technique uses a morphological database\nrealized using XMODEL language. Arabic morphology represents a special type of\nmorphological systems because it is based on the concept of scheme to represent\nArabic words. We use this concept to develop the Arabic morphological automata.\nThe proposed approach has development standardization aspect. It can be\nexploited by NLP applications such as syntactic and semantic analysis,\ninformation retrieval, machine translation and orthographical correction. The\nproposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic\nAnalyzer.\n",
        "published": "2011-01-28T09:58:39Z",
        "pdf_link": "http://arxiv.org/pdf/1101.5494v1"
    },
    {
        "id": "http://arxiv.org/abs/1101.5757v1",
        "title": "Polarized Montagovian Semantics for the Lambek-Grishin calculus",
        "summary": "  Grishin proposed enriching the Lambek calculus with multiplicative\ndisjunction (par) and coresiduals. Applications to linguistics were discussed\nby Moortgat, who spoke of the Lambek-Grishin calculus (LG). In this paper, we\nadapt Girard's polarity-sensitive double negation embedding for classical logic\nto extract a compositional Montagovian semantics from a display calculus for\nfocused proof search in LG. We seize the opportunity to illustrate our approach\nalongside an analysis of extraction, providing linguistic motivation for linear\ndistributivity of tensor over par, thus answering a question of\nKurtonina&Moortgat. We conclude by comparing our proposal to the continuation\nsemantics of Bernardi&Moortgat, corresponding to call-by- name and\ncall-by-value evaluation strategies.\n",
        "published": "2011-01-30T10:41:46Z",
        "pdf_link": "http://arxiv.org/pdf/1101.5757v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.1898v1",
        "title": "Recognizing Uncertainty in Speech",
        "summary": "  We address the problem of inferring a speaker's level of certainty based on\nprosodic information in the speech signal, which has application in\nspeech-based dialogue systems. We show that using phrase-level prosodic\nfeatures centered around the phrases causing uncertainty, in addition to\nutterance-level prosodic features, improves our model's level of certainty\nclassification. In addition, our models can be used to predict which phrase a\nperson is uncertain about. These results rely on a novel method for eliciting\nutterances of varying levels of certainty that allows us to compare the utility\nof contextually-based feature sets. We elicit level of certainty ratings from\nboth the speakers themselves and a panel of listeners, finding that there is\noften a mismatch between speakers' internal states and their perceived states,\nand highlighting the importance of this distinction.\n",
        "published": "2011-03-09T21:43:46Z",
        "pdf_link": "http://arxiv.org/pdf/1103.1898v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.2950v1",
        "title": "Fitting Ranked English and Spanish Letter Frequency Distribution in U.S.\n  and Mexican Presidential Speeches",
        "summary": "  The limited range in its abscissa of ranked letter frequency distributions\ncauses multiple functions to fit the observed distribution reasonably well. In\norder to critically compare various functions, we apply the statistical model\nselections on ten functions, using the texts of U.S. and Mexican presidential\nspeeches in the last 1-2 centuries. Dispite minor switching of ranking order of\ncertain letters during the temporal evolution for both datasets, the letter\nusage is generally stable. The best fitting function, judged by either\nleast-square-error or by AIC/BIC model selection, is the Cocho/Beta function.\nWe also use a novel method to discover clusters of letters by their\nobserved-over-expected frequency ratios.\n",
        "published": "2011-03-15T16:21:24Z",
        "pdf_link": "http://arxiv.org/pdf/1103.2950v1"
    },
    {
        "id": "http://arxiv.org/abs/1103.5676v1",
        "title": "Codeco: A Grammar Notation for Controlled Natural Language in Predictive\n  Editors",
        "summary": "  Existing grammar frameworks do not work out particularly well for controlled\nnatural languages (CNL), especially if they are to be used in predictive\neditors. I introduce in this paper a new grammar notation, called Codeco, which\nis designed specifically for CNLs and predictive editors. Two different parsers\nhave been implemented and a large subset of Attempto Controlled English (ACE)\nhas been represented in Codeco. The results show that Codeco is practical,\nadequate and efficient.\n",
        "published": "2011-03-29T15:05:11Z",
        "pdf_link": "http://arxiv.org/pdf/1103.5676v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.2034v1",
        "title": "Materials to the Russian-Bulgarian Comparative Dictionary \"EAD\"",
        "summary": "  This article presents a fragment of a new comparative dictionary \"A\ncomparative dictionary of names of expansive action in Russian and Bulgarian\nlanguages\". Main features of the new web-based comparative dictionary are\nplaced, the principles of its formation are shown, primary links between the\nword-matches are classified. The principal difference between translation\ndictionaries and the model of double comparison is also shown. The\nclassification scheme of the pages is proposed. New concepts and keywords have\nbeen introduced. The real prototype of the dictionary with a few key pages is\npublished. The broad debate about the possibility of this prototype to become a\nversion of Russian-Bulgarian comparative dictionary of a new generation is\navailable.\n",
        "published": "2011-04-11T19:50:50Z",
        "pdf_link": "http://arxiv.org/pdf/1104.2034v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.2086v1",
        "title": "A Universal Part-of-Speech Tagset",
        "summary": "  To facilitate future research in unsupervised induction of syntactic\nstructure and to standardize best-practices, we propose a tagset that consists\nof twelve universal part-of-speech categories. In addition to the tagset, we\ndevelop a mapping from 25 different treebank tagsets to this universal set. As\na result, when combined with the original treebank data, this universal tagset\nand mapping produce a dataset consisting of common parts-of-speech for 22\ndifferent languages. We highlight the use of this resource via two experiments,\nincluding one that reports competitive accuracies for unsupervised grammar\ninduction without gold standard part-of-speech tags.\n",
        "published": "2011-04-11T23:06:54Z",
        "pdf_link": "http://arxiv.org/pdf/1104.2086v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.4321v1",
        "title": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and\n  Compounds",
        "summary": "  Chinese characters can be compared to a molecular structure: a character is\nanalogous to a molecule, radicals are like atoms, calligraphic strokes\ncorrespond to elementary particles, and when characters form compounds, they\nare like molecular structures. In chemistry the conjunction of all of these\nstructural levels produces what we perceive as matter. In language, the\nconjunction of strokes, radicals, characters, and compounds produces meaning.\nBut when does meaning arise? We all know that radicals are, in some sense, the\nbasic semantic components of Chinese script, but what about strokes?\nConsidering the fact that many characters are made by adding individual strokes\nto (combinations of) radicals, we can legitimately ask the question whether\nstrokes carry meaning, or not. In this talk I will present my project of\nextending traditional NLP techniques to radicals and strokes, aiming to obtain\na deeper understanding of the way ideographic languages model the world.\n",
        "published": "2011-04-21T17:56:50Z",
        "pdf_link": "http://arxiv.org/pdf/1104.4321v1"
    },
    {
        "id": "http://arxiv.org/abs/1104.4681v1",
        "title": "Performance Evaluation of Statistical Approaches for Text Independent\n  Speaker Recognition Using Source Feature",
        "summary": "  This paper introduces the performance evaluation of statistical approaches\nfor TextIndependent speaker recognition system using source feature. Linear\nprediction LP residual is used as a representation of excitation information in\nspeech. The speaker-specific information in the excitation of voiced speech is\ncaptured using statistical approaches such as Gaussian Mixture Models GMMs and\nHidden Markov Models HMMs. The decrease in the error during training and\nrecognizing speakers during testing phase close to 100 percent accuracy\ndemonstrates that the excitation component of speech contains speaker-specific\ninformation and is indeed being effectively captured by continuous Ergodic HMM\nthan GMM. The performance of the speaker recognition system is evaluated on GMM\nand 2 state ergodic HMM with different mixture components and test speech\nduration. We demonstrate the speaker recognition studies on TIMIT database for\nboth GMM and Ergodic HMM.\n",
        "published": "2011-04-25T05:00:25Z",
        "pdf_link": "http://arxiv.org/pdf/1104.4681v1"
    },
    {
        "id": "http://arxiv.org/abs/1105.1072v1",
        "title": "English-Lithuanian-English Machine Translation lexicon and engine:\n  current state and future work",
        "summary": "  This article overviews the current state of the English-Lithuanian-English\nmachine translation system. The first part of the article describes the\nproblems that system poses today and what actions will be taken to solve them\nin the future. The second part of the article tackles the main issue of the\ntranslation process. Article briefly overviews the word sense disambiguation\nfor MT technique using Google.\n",
        "published": "2011-05-05T13:51:46Z",
        "pdf_link": "http://arxiv.org/pdf/1105.1072v1"
    },
    {
        "id": "http://arxiv.org/abs/1105.1226v1",
        "title": "Multilingual lexicon design tool and database management system for MT",
        "summary": "  The paper presents the design and development of English-Lithuanian-English\ndictionarylexicon tool and lexicon database management system for MT. The\nsystem is oriented to support two main requirements: to be open to the user and\nto describe much more attributes of speech parts as a regular dictionary that\nare required for the MT. Programming language Java and database management\nsystem MySql is used to implement the designing tool and lexicon database\nrespectively. This solution allows easily deploying this system in the\nInternet. The system is able to run on various OS such as: Windows, Linux, Mac\nand other OS where Java Virtual Machine is supported. Since the modern lexicon\ndatabase managing system is used, it is not a problem accessing the same\ndatabase for several users.\n",
        "published": "2011-05-06T05:26:43Z",
        "pdf_link": "http://arxiv.org/pdf/1105.1226v1"
    },
    {
        "id": "http://arxiv.org/abs/1105.6162v2",
        "title": "A statistical learning algorithm for word segmentation",
        "summary": "  In natural speech, the speaker does not pause between words, yet a human\nlistener somehow perceives this continuous stream of phonemes as a series of\ndistinct words. The detection of boundaries between spoken words is an instance\nof a general capability of the human neocortex to remember and to recognize\nrecurring sequences. This paper describes a computer algorithm that is designed\nto solve the problem of locating word boundaries in blocks of English text from\nwhich the spaces have been removed. This problem avoids the complexities of\nspeech processing but requires similar capabilities for detecting recurring\nsequences. The algorithm relies entirely on statistical relationships between\nletters in the input stream to infer the locations of word boundaries. A\nViterbi trellis is used to simultaneously evaluate a set of hypothetical\nsegmentations of a block of adjacent words. This technique improves accuracy\nbut incurs a small latency between the arrival of letters in the input stream\nand the sending of words to the output stream. The source code for a C++\nversion of this algorithm is presented in an appendix.\n",
        "published": "2011-05-31T05:03:06Z",
        "pdf_link": "http://arxiv.org/pdf/1105.6162v2"
    },
    {
        "id": "http://arxiv.org/abs/1106.0673v1",
        "title": "Computational Approach to Anaphora Resolution in Spanish Dialogues",
        "summary": "  This paper presents an algorithm for identifying noun-phrase antecedents of\npronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora\nresolution requires numerous sources of information in order to find the\ncorrect antecedent of the anaphor. These sources can be of different kinds,\ne.g., linguistic information, discourse/dialogue structure information, or\ntopic information. For this reason, our algorithm uses various different kinds\nof information (hybrid information). The algorithm is based on linguistic\nconstraints and preferences and uses an anaphoric accessibility space within\nwhich the algorithm finds the noun phrase. We present some experiments related\nto this algorithm and this space using a corpus of 204 dialogues. The algorithm\nis implemented in Prolog. According to this study, 95.9% of antecedents were\nlocated in the proposed space, a precision of 81.3% was obtained for pronominal\nanaphora resolution, and 81.5% for adjectival anaphora.\n",
        "published": "2011-06-03T14:54:46Z",
        "pdf_link": "http://arxiv.org/pdf/1106.0673v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.5264v1",
        "title": "Acquiring Correct Knowledge for Natural Language Generation",
        "summary": "  Natural language generation (NLG) systems are computer software systems that\nproduce texts in English and other human languages, often from non-linguistic\ninput data. NLG systems, like most AI systems, need substantial amounts of\nknowledge. However, our experience in two NLG projects suggests that it is\ndifficult to acquire correct knowledge for NLG systems; indeed, every knowledge\nacquisition (KA) technique we tried had significant problems. In general terms,\nthese problems were due to the complexity, novelty, and poorly understood\nnature of the tasks our systems attempted, and were worsened by the fact that\npeople write so differently. This meant in particular that corpus-based KA\napproaches suffered because it was impossible to assemble a sizable corpus of\nhigh-quality consistent manually written texts in our domains; and structured\nexpert-oriented KA techniques suffered because experts disagreed and because we\ncould not get enough information about special and unusual cases to build\nrobust systems. We believe that such problems are likely to affect many other\nNLG systems as well. In the long term, we hope that new KA techniques may\nemerge to help NLG system builders. In the shorter term, we believe that\nunderstanding how individual KA techniques can fail, and using a mixture of\ndifferent KA techniques with different strengths and weaknesses, can help\ndevelopers acquire NLG knowledge that is mostly correct.\n",
        "published": "2011-06-26T21:05:14Z",
        "pdf_link": "http://arxiv.org/pdf/1106.5264v1"
    },
    {
        "id": "http://arxiv.org/abs/1106.5973v1",
        "title": "Entropy of Telugu",
        "summary": "  This paper presents an investigation of the entropy of the Telugu script.\nSince this script is syllabic, and not alphabetic, the computation of entropy\nis somewhat complicated.\n",
        "published": "2011-06-27T20:13:07Z",
        "pdf_link": "http://arxiv.org/pdf/1106.5973v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.0193v3",
        "title": "On the origin of ambiguity in efficient communication",
        "summary": "  This article studies the emergence of ambiguity in communication through the\nconcept of logical irreversibility and within the framework of Shannon's\ninformation theory. This leads us to a precise and general expression of the\nintuition behind Zipf's vocabulary balance in terms of a symmetry equation\nbetween the complexities of the coding and the decoding processes that imposes\nan unavoidable amount of logical uncertainty in natural communication.\nAccordingly, the emergence of irreversible computations is required if the\ncomplexities of the coding and the decoding processes are balanced in a\nsymmetric scenario, which means that the emergence of ambiguous codes is a\nnecessary condition for natural communication to succeed.\n",
        "published": "2011-07-01T11:01:52Z",
        "pdf_link": "http://arxiv.org/pdf/1107.0193v3"
    },
    {
        "id": "http://arxiv.org/abs/1107.1753v1",
        "title": "Notes on Electronic Lexicography",
        "summary": "  These notes are a continuation of topics covered by V. Selegej in his article\n\"Electronic Dictionaries and Computational lexicography\". How can an electronic\ndictionary have as its object the description of closely related languages?\nObviously, such a question allows multiple answers.\n",
        "published": "2011-07-09T00:40:06Z",
        "pdf_link": "http://arxiv.org/pdf/1107.1753v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.4687v2",
        "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven\n  Language Specification",
        "summary": "  Model-based language specification has applications in the implementation of\nlanguage processors, the design of domain-specific languages, model-driven\nsoftware development, data integration, text mining, natural language\nprocessing, and corpus-based induction of models. Model-based language\nspecification decouples language design from language processing and, unlike\ntraditional grammar-driven approaches, which constrain language designers to\nspecific kinds of grammars, it needs general parser generators able to deal\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\nparsing algorithm with lexical and syntactic ambiguity support that enables the\nuse of model-based language specification in practice.\n",
        "published": "2011-07-23T12:56:02Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4687v2"
    },
    {
        "id": "http://arxiv.org/abs/1107.4723v2",
        "title": "A Semantic Relatedness Measure Based on Combined Encyclopedic,\n  Ontological and Collocational Knowledge",
        "summary": "  We describe a new semantic relatedness measure combining the Wikipedia-based\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\ncollocation index. Our measure achieves the currently highest results on the\nWS-353 test: a Spearman rho coefficient of 0.79 (vs. 0.75 in (Gabrilovich and\nMarkovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs.\n0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM\nclassifier trained on our measure.\n  In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as\nwell as various unsuccessful attempts to enhance ESA by filtering at word,\nsentence, and section level.\n",
        "published": "2011-07-24T06:27:56Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4723v2"
    },
    {
        "id": "http://arxiv.org/abs/1107.4734v1",
        "title": "Design of Arabic Diacritical Marks",
        "summary": "  Diacritical marks play a crucial role in meeting the criteria of usability of\ntypographic text, such as: homogeneity, clarity and legibility. To change the\ndiacritic of a letter in a word could completely change its semantic. The\nsituation is very complicated with multilingual text. Indeed, the problem of\ndesign becomes more difficult by the presence of diacritics that come from\nvarious scripts; they are used for different purposes, and are controlled by\nvarious typographic rules. It is quite challenging to adapt rules from one\nscript to another. This paper aims to study the placement and sizing of\ndiacritical marks in Arabic script, with a comparison with the Latin's case.\nThe Arabic script is cursive and runs from right-to-left; its criteria and\nrules are quite distinct from those of the Latin script. In the beginning, we\ncompare the difficulty of processing diacritics in both scripts. After, we will\nstudy the limits of Latin resolution strategies when applied to Arabic. At the\nend, we propose an approach to resolve the problem for positioning and resizing\ndiacritics. This strategy includes creating an Arabic font, designed in\nOpenType format, along with suitable justification in TEX.\n",
        "published": "2011-07-24T08:35:12Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4734v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.4796v1",
        "title": "Use Pronunciation by Analogy for text to speech system in Persian\n  language",
        "summary": "  The interest in text to speech synthesis increased in the world .text to\nspeech have been developed formany popular languages such as English, Spanish\nand French and many researches and developmentshave been applied to those\nlanguages. Persian on the other hand, has been given little attentioncompared\nto other languages of similar importance and the research in Persian is still\nin its infancy.Persian language possess many difficulty and exceptions that\nincrease complexity of text to speechsystems. For example: short vowels is\nabsent in written text or existence of homograph words. in thispaper we propose\na new method for persian text to phonetic that base on pronunciations by\nanalogy inwords, semantic relations and grammatical rules for finding proper\nphonetic. Keywords:PbA, text to speech, Persian language, FPbA\n",
        "published": "2011-07-24T20:37:57Z",
        "pdf_link": "http://arxiv.org/pdf/1107.4796v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.5743v1",
        "title": "NEMO: Extraction and normalization of organization names from PubMed\n  affiliation strings",
        "summary": "  We propose NEMO, a system for extracting organization names in the\naffiliation and normalizing them to a canonical organization name. Our parsing\nprocess involves multi-layered rule matching with multiple dictionaries. The\nsystem achieves more than 98% f-score in extracting organization names. Our\nprocess of normalization that involves clustering based on local sequence\nalignment metrics and local learning based on finding connected components. A\nhigh precision was also observed in normalization. NEMO is the missing link in\nassociating each biomedical paper and its authors to an organization name in\nits canonical form and the Geopolitical location of the organization. This\nresearch could potentially help in analyzing large social networks of\norganizations for landscaping a particular topic, improving performance of\nauthor disambiguation, adding weak links in the co-author network of authors,\naugmenting NLM's MARS system for correcting errors in OCR output of affiliation\nfield, and automatically indexing the PubMed citations with the normalized\norganization name and country. Our system is available as a graphical user\ninterface available for download along with this paper.\n",
        "published": "2011-07-28T15:37:56Z",
        "pdf_link": "http://arxiv.org/pdf/1107.5743v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.5744v1",
        "title": "BioSimplify: an open source sentence simplification engine to improve\n  recall in automatic biomedical information extraction",
        "summary": "  BioSimplify is an open source tool written in Java that introduces and\nfacilitates the use of a novel model for sentence simplification tuned for\nautomatic discourse analysis and information extraction (as opposed to sentence\nsimplification for improving human readability). The model is based on a\n\"shot-gun\" approach that produces many different (simpler) versions of the\noriginal sentence by combining variants of its constituent elements. This tool\nis optimized for processing biomedical scientific literature such as the\nabstracts indexed in PubMed. We tested our tool on its impact to the task of\nPPI extraction and it improved the f-score of the PPI tool by around 7%, with\nan improvement in recall of around 20%. The BioSimplify tool and test corpus\ncan be downloaded from https://biosimplify.sourceforge.net.\n",
        "published": "2011-07-28T15:40:31Z",
        "pdf_link": "http://arxiv.org/pdf/1107.5744v1"
    },
    {
        "id": "http://arxiv.org/abs/1107.5752v2",
        "title": "An Effective Approach to Biomedical Information Extraction with Limited\n  Training Data",
        "summary": "  Overall, the two main contributions of this work include the application of\nsentence simplification to association extraction as described above, and the\nuse of distributional semantics for concept extraction. The proposed work on\nconcept extraction amalgamates for the first time two diverse research areas\n-distributional semantics and information extraction. This approach renders all\nthe advantages offered in other semi-supervised machine learning systems, and,\nunlike other proposed semi-supervised approaches, it can be used on top of\ndifferent basic frameworks and algorithms.\nhttp://gradworks.umi.com/34/49/3449837.html\n",
        "published": "2011-07-28T15:59:21Z",
        "pdf_link": "http://arxiv.org/pdf/1107.5752v2"
    },
    {
        "id": "http://arxiv.org/abs/1108.0353v2",
        "title": "Cross-moments computation for stochastic context-free grammars",
        "summary": "  In this paper we consider the problem of efficient computation of\ncross-moments of a vector random variable represented by a stochastic\ncontext-free grammar. Two types of cross-moments are discussed. The sample\nspace for the first one is the set of all derivations of the context-free\ngrammar, and the sample space for the second one is the set of all derivations\nwhich generate a string belonging to the language of the grammar. In the past,\nthis problem was widely studied, but mainly for the cross-moments of scalar\nvariables and up to the second order. This paper presents new algorithms for\ncomputing the cross-moments of an arbitrary order, and the previously developed\nones are derived as special cases.\n",
        "published": "2011-08-01T16:20:50Z",
        "pdf_link": "http://arxiv.org/pdf/1108.0353v2"
    },
    {
        "id": "http://arxiv.org/abs/1108.0631v3",
        "title": "Serialising the ISO SynAF Syntactic Object Model",
        "summary": "  This paper introduces, an XML format developed to serialise the object model\ndefined by the ISO Syntactic Annotation Framework SynAF. Based on widespread\nbest practices we adapt a popular XML format for syntactic annotation,\nTigerXML, with additional features to support a variety of syntactic phenomena\nincluding constituent and dependency structures, binding, and different node\ntypes such as compounds or empty elements. We also define interfaces to other\nformats and standards including the Morpho-syntactic Annotation Framework MAF\nand the ISOCat Data Category Registry. Finally a case study of the German\nTreebank TueBa-D/Z is presented, showcasing the handling of constituent\nstructures, topological fields and coreference annotation in tandem.\n",
        "published": "2011-08-02T17:33:36Z",
        "pdf_link": "http://arxiv.org/pdf/1108.0631v3"
    },
    {
        "id": "http://arxiv.org/abs/1108.1966v1",
        "title": "A Concise Query Language with Search and Transform Operations for\n  Corpora with Multiple Levels of Annotation",
        "summary": "  The usefulness of annotated corpora is greatly increased if there is an\nassociated tool that can allow various kinds of operations to be performed in a\nsimple way. Different kinds of annotation frameworks and many query languages\nfor them have been proposed, including some to deal with multiple layers of\nannotation. We present here an easy to learn query language for a particular\nkind of annotation framework based on 'threaded trees', which are somewhere\nbetween the complete order of a tree and the anarchy of a graph. Through\n'typed' threads, they can allow multiple levels of annotation in the same\ndocument. Our language has a simple, intuitive and concise syntax and high\nexpressive power. It allows not only to search for complicated patterns with\nshort queries but also allows data manipulation and specification of arbitrary\nreturn values. Many of the commonly used tasks that otherwise require writing\nprograms, can be performed with one or more queries. We compare the language\nwith some others and try to evaluate it.\n",
        "published": "2011-08-09T16:03:24Z",
        "pdf_link": "http://arxiv.org/pdf/1108.1966v1"
    },
    {
        "id": "http://arxiv.org/abs/1108.3843v1",
        "title": "Using Inverse lambda and Generalization to Translate English to Formal\n  Languages",
        "summary": "  We present a system to translate natural language sentences to formulas in a\nformal or a knowledge representation language. Our system uses two inverse\nlambda-calculus operators and using them can take as input the semantic\nrepresentation of some words, phrases and sentences and from that derive the\nsemantic representation of other words and phrases. Our inverse lambda operator\nworks on many formal languages including first order logic, database query\nlanguages and answer set programming. Our system uses a syntactic combinatorial\ncategorial parser to parse natural language sentences and also to construct the\nsemantic meaning of the sentences as directed by their parsing. The same parser\nis used for both. In addition to the inverse lambda-calculus operators, our\nsystem uses a notion of generalization to learn semantic representation of\nwords from the semantic representation of other words that are of the same\ncategory. Together with this, we use an existing statistical learning approach\nto assign weights to deal with multiple meanings of words. Our system produces\nimproved results on standard corpora on natural language interfaces for robot\ncommand and control and database queries.\n",
        "published": "2011-08-18T20:04:28Z",
        "pdf_link": "http://arxiv.org/pdf/1108.3843v1"
    },
    {
        "id": "http://arxiv.org/abs/1108.3848v1",
        "title": "Language understanding as a step towards human level intelligence -\n  automatizing the construction of the initial dictionary from example\n  sentences",
        "summary": "  For a system to understand natural language, it needs to be able to take\nnatural language text and answer questions given in natural language with\nrespect to that text; it also needs to be able to follow instructions given in\nnatural language. To achieve this, a system must be able to process natural\nlanguage and be able to capture the knowledge within that text. Thus it needs\nto be able to translate natural language text into a formal language. We\ndiscuss our approach to do this, where the translation is achieved by composing\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\nmethod that we developed (and other methods) to learn meaning of words from\nmeaning of sentences and an initial lexicon. We then present an improved method\nwhere the initial lexicon is also learned by analyzing the training sentence\nand meaning pairs. We evaluate our methods and compare them with other existing\nmethods on a corpora of database querying and robot command and control.\n",
        "published": "2011-08-18T20:12:50Z",
        "pdf_link": "http://arxiv.org/pdf/1108.3848v1"
    },
    {
        "id": "http://arxiv.org/abs/1108.4052v1",
        "title": "Query Expansion: Term Selection using the EWC Semantic Relatedness\n  Measure",
        "summary": "  This paper investigates the efficiency of the EWC semantic relatedness\nmeasure in an ad-hoc retrieval task. This measure combines the Wikipedia-based\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\ncollocation index. In the experiments, the open source search engine Terrier\nwas utilised as a tool to index and retrieve data. The proposed technique was\ntested on the NTCIR data collection. The experiments demonstrated promising\nresults.\n",
        "published": "2011-08-19T21:41:29Z",
        "pdf_link": "http://arxiv.org/pdf/1108.4052v1"
    },
    {
        "id": "http://arxiv.org/abs/1108.5096v1",
        "title": "Minimalist Grammars and Minimalist Categorial Grammars, definitions\n  toward inclusion of generated languages",
        "summary": "  Stabler proposes an implementation of the Chomskyan Minimalist Program,\nChomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a\nlong linguistic tradition. But the semantic calculus is more easily added if\none uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,\nbased on an extension of the Lambek calculus, the mixed logic, were introduced\nto provide a theoretically-motivated syntax-semantics interface, Amblard 07. In\nthis article, we give full definitions of MG with algebraic tree descriptions\nand of MCG, and take the first steps towards giving a proof of inclusion of\ntheir generated languages.\n",
        "published": "2011-08-25T14:15:46Z",
        "pdf_link": "http://arxiv.org/pdf/1108.5096v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.0069v2",
        "title": "Inter-rater Agreement on Sentence Formality",
        "summary": "  Formality is one of the most important dimensions of writing style variation.\nIn this study we conducted an inter-rater reliability experiment for assessing\nsentence formality on a five-point Likert scale, and obtained good agreement\nresults as well as different rating distributions for different sentence\ncategories. We also performed a difficulty analysis to identify the bottlenecks\nof our rating procedure. Our main objective is to design an automatic scoring\nmechanism for sentence-level formality, and this study is important for that\npurpose.\n",
        "published": "2011-09-01T02:20:12Z",
        "pdf_link": "http://arxiv.org/pdf/1109.0069v2"
    },
    {
        "id": "http://arxiv.org/abs/1109.0624v1",
        "title": "Building Ontologies to Understand Spoken Tunisian Dialect",
        "summary": "  This paper presents a method to understand spoken Tunisian dialect based on\nlexical semantic. This method takes into account the specificity of the\nTunisian dialect which has no linguistic processing tools. This method is\nontology-based which allows exploiting the ontological concepts for semantic\nannotation and ontological relations for speech interpretation. This\ncombination increases the rate of comprehension and limits the dependence on\nlinguistic resources. This paper also details the process of building the\nontology used for annotation and interpretation of Tunisian dialect in the\ncontext of speech understanding in dialogue systems for restricted domain.\n",
        "published": "2011-09-03T14:30:44Z",
        "pdf_link": "http://arxiv.org/pdf/1109.0624v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.2128v2",
        "title": "LexRank: Graph-based Lexical Centrality as Salience in Text\n  Summarization",
        "summary": "  We introduce a stochastic graph-based method for computing relative\nimportance of textual units for Natural Language Processing. We test the\ntechnique on the problem of Text Summarization (TS). Extractive TS relies on\nthe concept of sentence salience to identify the most important sentences in a\ndocument or set of documents. Salience is typically defined in terms of the\npresence of particular important words or in terms of similarity to a centroid\npseudo-sentence. We consider a new approach, LexRank, for computing sentence\nimportance based on the concept of eigenvector centrality in a graph\nrepresentation of sentences. In this model, a connectivity matrix based on\nintra-sentence cosine similarity is used as the adjacency matrix of the graph\nrepresentation of sentences. Our system, based on LexRank ranked in first place\nin more than one task in the recent DUC 2004 evaluation. In this paper we\npresent a detailed analysis of our approach and apply it to a larger data set\nincluding data from earlier DUC evaluations. We discuss several methods to\ncompute centrality using the similarity graph. The results show that\ndegree-based methods (including LexRank) outperform both centroid-based methods\nand other systems participating in DUC in most of the cases. Furthermore, the\nLexRank with threshold method outperforms the other degree-based techniques\nincluding continuous LexRank. We also show that our approach is quite\ninsensitive to the noise in the data that may result from an imperfect topical\nclustering of documents.\n",
        "published": "2011-09-09T20:20:38Z",
        "pdf_link": "http://arxiv.org/pdf/1109.2128v2"
    },
    {
        "id": "http://arxiv.org/abs/1109.2130v1",
        "title": "Combining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods",
        "summary": "  In this paper we concentrate on the resolution of the lexical ambiguity that\narises when a given word has several different meanings. This specific task is\ncommonly referred to as word sense disambiguation (WSD). The task of WSD\nconsists of assigning the correct sense to words using an electronic dictionary\nas the source of word definitions. We present two WSD methods based on two main\nmethodological approaches in this research area: a knowledge-based method and a\ncorpus-based method. Our hypothesis is that word-sense disambiguation requires\nseveral knowledge sources in order to solve the semantic ambiguity of the\nwords. These sources can be of different kinds--- for example, syntagmatic,\nparadigmatic or statistical information. Our approach combines various sources\nof knowledge, through combinations of the two WSD methods mentioned above.\nMainly, the paper concentrates on how to combine these methods and sources of\ninformation in order to achieve good results in the disambiguation. Finally,\nthis paper presents a comprehensive study and experimental work on evaluation\nof the methods and their combinations.\n",
        "published": "2011-09-09T20:22:04Z",
        "pdf_link": "http://arxiv.org/pdf/1109.2130v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.2136v1",
        "title": "Learning Content Selection Rules for Generating Object Descriptions in\n  Dialogue",
        "summary": "  A fundamental requirement of any task-oriented dialogue system is the ability\nto generate object descriptions that refer to objects in the task domain. The\nsubproblem of content selection for object descriptions in task-oriented\ndialogue has been the focus of much previous work and a large number of models\nhave been proposed. In this paper, we use the annotated COCONUT corpus of\ntask-oriented design dialogues to develop feature sets based on Dale and\nReiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact\nmodel, and Jordans (2000b) intentional influences model, and use these feature\nsets in a machine learning experiment to automatically learn a model of content\nselection for object descriptions. Since Dale and Reiters model requires a\nrepresentation of discourse structure, the corpus annotations are used to\nderive a representation based on Grosz and Sidners (1986) theory of the\nintentional structure of discourse, as well as two very simple representations\nof discourse structure based purely on recency. We then apply the\nrule-induction program RIPPER to train and test the content selection component\nof an object description generator on a set of 393 object descriptions from the\ncorpus. To our knowledge, this is the first reported experiment of a trainable\ncontent selection component for object description generation in dialogue.\nThree separate content selection models that are based on the three theoretical\nmodels, all independently achieve accuracies significantly above the majority\nclass baseline (17%) on unseen test data, with the intentional influences model\n(42.4%) performing significantly better than either the incremental model\n(30.4%) or the conceptual pact model (28.9%). But the best performing models\ncombine all the feature sets, achieving accuracies near 60%. Surprisingly, a\nsimple recency-based representation of discourse structure does as well as one\nbased on intentional structure. To our knowledge, this is also the first\nempirical comparison of a representation of Grosz and Sidners model of\ndiscourse structure with a simpler model for any generation task.\n",
        "published": "2011-09-09T20:24:57Z",
        "pdf_link": "http://arxiv.org/pdf/1109.2136v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.4531v1",
        "title": "A Probabilistic Approach to Pronunciation by Analogy",
        "summary": "  The relationship between written and spoken words is convoluted in languages\nwith a deep orthography such as English and therefore it is difficult to devise\nexplicit rules for generating the pronunciations for unseen words.\nPronunciation by analogy (PbA) is a data-driven method of constructing\npronunciations for novel words from concatenated segments of known words and\ntheir pronunciations. PbA performs relatively well with English and outperforms\nseveral other proposed methods. However, the best published word accuracy of\n65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for\nimprovement in it.\n  Previous PbA algorithms have used several different scoring strategies such\nas the product of the frequencies of the component pronunciations of the\nsegments, or the number of different segmentations that yield the same\npronunciation, and different combinations of these methods, to evaluate the\ncandidate pronunciations. In this article, we instead propose to use a\nprobabilistically justified scoring rule. We show that this principled approach\nalone yields better accuracy (66.21% for the NETtalk corpus) than any\npreviously published PbA algorithm. Furthermore, combined with certain ad hoc\nmodifications motivated by earlier algorithms, the performance climbs up to\n66.6%, and further improvements are possible by combining this method with\nother methods.\n",
        "published": "2011-09-21T13:57:49Z",
        "pdf_link": "http://arxiv.org/pdf/1109.4531v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.4906v1",
        "title": "Automatic transcription of 17th century English text in Contemporary\n  English with NooJ: Method and Evaluation",
        "summary": "  Since 2006 we have undertaken to describe the differences between 17th\ncentury English and contemporary English thanks to NLP software. Studying a\ncorpus spanning the whole century (tales of English travellers in the Ottoman\nEmpire in the 17th century, Mary Astell's essay A Serious Proposal to the\nLadies and other literary texts) has enabled us to highlight various lexical,\nmorphological or grammatical singularities. Thanks to the NooJ linguistic\nplatform, we created dictionaries indexing the lexical variants and their\ntranscription in CE. The latter is often the result of the validation of forms\nrecognized dynamically by morphological graphs. We also built syntactical\ngraphs aimed at transcribing certain archaic forms in contemporary English. Our\nprevious research implied a succession of elementary steps alternating textual\nanalysis and result validation. We managed to provide examples of\ntranscriptions, but we have not created a global tool for automatic\ntranscription. Therefore we need to focus on the results we have obtained so\nfar, study the conditions for creating such a tool, and analyze possible\ndifficulties. In this paper, we will be discussing the technical and linguistic\naspects we have not yet covered in our previous work. We are using the results\nof previous research and proposing a transcription method for words or\nsequences identified as archaic.\n",
        "published": "2011-09-22T18:37:17Z",
        "pdf_link": "http://arxiv.org/pdf/1109.4906v1"
    },
    {
        "id": "http://arxiv.org/abs/1109.5798v1",
        "title": "Object-oriented semantics of English in natural language understanding\n  system",
        "summary": "  A new approach to the problem of natural language understanding is proposed.\nThe knowledge domain under consideration is the social behavior of people.\nEnglish sentences are translated into set of predicates of a semantic database,\nwhich describe persons, occupations, organizations, projects, actions, events,\nmessages, machines, things, animals, location and time of actions, relations\nbetween objects, thoughts, cause-and-effect relations, abstract objects. There\nis a knowledge base containing the description of semantics of objects\n(functions and structure), actions (motives and causes), and operations.\n",
        "published": "2011-09-27T08:00:46Z",
        "pdf_link": "http://arxiv.org/pdf/1109.5798v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.1470v3",
        "title": "A Constraint-Satisfaction Parser for Context-Free Grammars",
        "summary": "  Traditional language processing tools constrain language designers to\nspecific kinds of grammars. In contrast, model-based language specification\ndecouples language design from language processing. As a consequence,\nmodel-based language specification tools need general parsers able to parse\nunrestricted context-free grammars. As languages specified following this\napproach may be ambiguous, parsers must deal with ambiguities. Model-based\nlanguage specification also allows the definition of associativity, precedence,\nand custom constraints. Therefore parsers generated by model-driven language\nspecification tools need to enforce constraints. In this paper, we propose\nFence, an efficient bottom-up chart parser with lexical and syntactic ambiguity\nsupport that allows the specification of constraints and, therefore, enables\nthe use of model-based language specification in practice.\n",
        "published": "2011-10-07T09:53:41Z",
        "pdf_link": "http://arxiv.org/pdf/1110.1470v3"
    },
    {
        "id": "http://arxiv.org/abs/1110.1758v2",
        "title": "Data formats for phonological corpora",
        "summary": "  The goal of the present chapter is to explore the possibility of providing\nthe research (but also the industrial) community that commonly uses spoken\ncorpora with a stable portfolio of well-documented standardised formats that\nallow a high re-use rate of annotated spoken resources and, as a consequence,\nbetter interoperability across tools used to produce or exploit such resources.\n",
        "published": "2011-10-08T19:15:12Z",
        "pdf_link": "http://arxiv.org/pdf/1110.1758v2"
    },
    {
        "id": "http://arxiv.org/abs/1110.2215v1",
        "title": "NP Animacy Identification for Anaphora Resolution",
        "summary": "  In anaphora resolution for English, animacy identification can play an\nintegral role in the application of agreement restrictions between pronouns and\ncandidates, and as a result, can improve the accuracy of anaphora resolution\nsystems. In this paper, two methods for animacy identification are proposed and\nevaluated using intrinsic and extrinsic measures. The first method is a\nrule-based one which uses information about the unique beginners in WordNet to\nclassify NPs on the basis of their animacy. The second method relies on a\nmachine learning algorithm which exploits a WordNet enriched with animacy\ninformation for each sense. The effect of word sense disambiguation on the two\nmethods is also assessed. The intrinsic evaluation reveals that the machine\nlearning method reaches human levels of performance. The extrinsic evaluation\ndemonstrates that animacy identification can be beneficial in anaphora\nresolution, especially in the cases where animate entities are identified with\nhigh precision.\n",
        "published": "2011-10-10T22:13:24Z",
        "pdf_link": "http://arxiv.org/pdf/1110.2215v1"
    },
    {
        "id": "http://arxiv.org/abs/1110.4248v1",
        "title": "Ideogram Based Chinese Sentiment Word Orientation Computation",
        "summary": "  This paper presents a novel algorithm to compute sentiment orientation of\nChinese sentiment word. The algorithm uses ideograms which are a distinguishing\nfeature of Chinese language. The proposed algorithm can be applied to any\nsentiment classification scheme. To compute a word's sentiment orientation\nusing the proposed algorithm, only the word itself and a precomputed character\nontology is required, rather than a corpus. The influence of three parameters\nover the algorithm performance is analyzed and verified by experiment.\nExperiment also shows that proposed algorithm achieves an F Measure of 85.02%\noutperforming existing ideogram based algorithm.\n",
        "published": "2011-10-19T11:45:16Z",
        "pdf_link": "http://arxiv.org/pdf/1110.4248v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.0048v1",
        "title": "Individual and Domain Adaptation in Sentence Planning for Dialogue",
        "summary": "  One of the biggest challenges in the development and deployment of spoken\ndialogue systems is the design of the spoken language generation module. This\nchallenge arises from the need for the generator to adapt to many features of\nthe dialogue domain, user population, and dialogue context. A promising\napproach is trainable generation, which uses general-purpose linguistic\nknowledge that is automatically adapted to the features of interest, such as\nthe application domain, individual user, or user group. In this paper we\npresent and evaluate a trainable sentence planner for providing restaurant\ninformation in the MATCH dialogue system. We show that trainable sentence\nplanning can produce complex information presentations whose quality is\ncomparable to the output of a template-based generator tuned to this domain. We\nalso show that our method easily supports adapting the sentence planner to\nindividuals, and that the individualized sentence planners generally perform\nbetter than models trained and tested on a population of individuals. Previous\nwork has documented and utilized individual preferences for content selection,\nbut to our knowledge, these results provide the first demonstration of\nindividual preferences for sentence planning operations, affecting the content\norder, discourse structure and sentence structure of system responses. Finally,\nwe evaluate the contribution of different feature sets, and show that, in our\napplication, n-gram features often do as well as features based on higher-level\nlinguistic representations.\n",
        "published": "2011-10-31T21:53:32Z",
        "pdf_link": "http://arxiv.org/pdf/1111.0048v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3122v1",
        "title": "ESLO: from transcription to speakers' personal information annotation",
        "summary": "  This paper presents the preliminary works to put online a French oral corpus\nand its transcription. This corpus is the Socio-Linguistic Survey in Orleans,\nrealized in 1968. First, we numerized the corpus, then we handwritten\ntranscribed it with the Transcriber software adding different tags about\nspeakers, time, noise, etc. Each document (audio file and XML file of the\ntranscription) was described by a set of metadata stored in an XML format to\nallow an easy consultation. Second, we added different levels of annotations,\nrecognition of named entities and annotation of personal information about\nspeakers. This two annotation tasks used the CasSys system of transducer\ncascades. We used and modified a first cascade to recognize named entities.\nThen we built a second cascade to annote the designating entities, i.e.\ninformation about the speaker. These second cascade parsed the named entity\nannotated corpus. The objective is to locate information about the speaker and,\nalso, what kind of information can designate him/her. These two cascades was\nevaluated with precision and recall measures.\n",
        "published": "2011-11-14T07:41:42Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3122v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3152v1",
        "title": "valuation de lexiques syntaxiques par leur intgartion dans\n  l'analyseur syntaxiques FRMG",
        "summary": "  In this paper, we evaluate various French lexica with the parser FRMG: the\nLefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,\nthe lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,\nobtained by merging with DICOVALENCE and partial manual validation. For this,\nall these lexica have been converted to the format of the Lefff, Alexina\nformat. The evaluation was made on the part of the EASy corpus used in the\nfirst evaluation campaign Passage.\n",
        "published": "2011-11-14T09:34:34Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3152v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3153v1",
        "title": "Construction du lexique LGLex  partir des tables du Lexique-Grammaire\n  des verbes du grec moderne",
        "summary": "  In this paper, we summerize the work done on the resources of Modern Greek on\nthe Lexicon-Grammar of verbs. We detail the definitional features of each\ntable, and all changes made to the names of features to make them consistent.\nThrough the development of the table of classes, including all the features, we\nhave considered the conversion of tables in a syntactic lexicon: LGLex. The\nlexicon, in plain text format or XML, is generated by the LGExtract tool\n(Constant & Tolone, 2010). This format is directly usable in applications of\nNatural Language Processing (NLP).\n",
        "published": "2011-11-14T09:34:59Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3153v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.3462v1",
        "title": "Extending the adverbial coverage of a NLP oriented resource for French",
        "summary": "  This paper presents a work on extending the adverbial entries of LGLex: a NLP\noriented syntactic resource for French. Adverbs were extracted from the\nLexicon-Grammar tables of both simple adverbs ending in -ment '-ly' (Molinier\nand Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies\non the exploitation of fine-grained linguistic information provided in existing\nresources. Various features are encoded in both LG tables and they haven't been\nexploited yet. They describe the relations of deleting, permuting, intensifying\nand paraphrasing that associate, on the one hand, the simple and compound\nadverbs and, on the other hand, different types of compound adverbs. The\nresulting syntactic resource is manually evaluated and freely available under\nthe LGPL-LR license.\n",
        "published": "2011-11-15T09:24:36Z",
        "pdf_link": "http://arxiv.org/pdf/1111.3462v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.4343v1",
        "title": "Question Answering in a Natural Language Understanding System Based on\n  Object-Oriented Semantics",
        "summary": "  Algorithms of question answering in a computer system oriented on input and\nlogical processing of text information are presented. A knowledge domain under\nconsideration is social behavior of a person. A database of the system includes\nan internal representation of natural language sentences and supplemental\ninformation. The answer {\\it Yes} or {\\it No} is formed for a general question.\nA special question containing an interrogative word or group of interrogative\nwords permits to find a subject, object, place, time, cause, purpose and way of\naction or event. Answer generation is based on identification algorithms of\npersons, organizations, machines, things, places, and times. Proposed\nalgorithms of question answering can be realized in information systems closely\nconnected with text processing (criminology, operation of business, medicine,\ndocument systems).\n",
        "published": "2011-11-18T12:31:49Z",
        "pdf_link": "http://arxiv.org/pdf/1111.4343v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.5293v1",
        "title": "Rule based Part of speech Tagger for Homoeopathy Clinical realm",
        "summary": "  A tagger is a mandatory segment of most text scrutiny systems, as it\nconsigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every\nword in a sentence. In this paper, we present a simple part of speech tagger\nfor homoeopathy clinical language. This paper reports about the anticipated\npart of speech tagger for homoeopathy clinical language. It exploit standard\npattern for evaluating sentences, untagged clinical corpus of 20085 words is\nused, from which we had selected 125 sentences (2322 tokens). The problem of\ntagging in natural language processing is to find a way to tag every word in a\ntext as a meticulous part of speech. The basic idea is to apply a set of rules\non clinical sentences and on each word, Accuracy is the leading factor in\nevaluating any POS tagger so the accuracy of proposed tagger is also conversed.\n",
        "published": "2011-11-13T18:19:15Z",
        "pdf_link": "http://arxiv.org/pdf/1111.5293v1"
    },
    {
        "id": "http://arxiv.org/abs/1111.6553v1",
        "title": "Exploring Twitter Hashtags",
        "summary": "  Twitter messages often contain so-called hashtags to denote keywords related\nto them. Using a dataset of 29 million messages, I explore relations among\nthese hashtags with respect to co-occurrences. Furthermore, I present an\nattempt to classify hashtags into five intuitive classes, using a\nmachine-learning approach. The overall outcome is an interactive Web\napplication to explore Twitter hashtags.\n",
        "published": "2011-11-28T19:17:57Z",
        "pdf_link": "http://arxiv.org/pdf/1111.6553v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.0168v1",
        "title": "Statistical Sign Language Machine Translation: from English written text\n  to American Sign Language Gloss",
        "summary": "  This works aims to design a statistical machine translation from English text\nto American Sign Language (ASL). The system is based on Moses tool with some\nmodifications and the results are synthesized through a 3D avatar for\ninterpretation. First, we translate the input text to gloss, a written form of\nASL. Second, we pass the output to the WebSign Plug-in to play the sign.\nContributions of this work are the use of a new couple of language English/ASL\nand an improvement of statistical machine translation based on string matching\nthanks to Jaro-distance.\n",
        "published": "2011-12-01T12:52:22Z",
        "pdf_link": "http://arxiv.org/pdf/1112.0168v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.0396v1",
        "title": "Grammatical Relations of Myanmar Sentences Augmented by\n  Transformation-Based Learning of Function Tagging",
        "summary": "  In this paper we describe function tagging using Transformation Based\nLearning (TBL) for Myanmar that is a method of extensions to the previous\nstatistics-based function tagger. Contextual and lexical rules (developed using\nTBL) were critical in achieving good results. First, we describe a method for\nexpressing lexical relations in function tagging that statistical function\ntagging are currently unable to express. Function tagging is the preprocessing\nstep to show grammatical relations of the sentences. Then we use the context\nfree grammar technique to clarify the grammatical relations in Myanmar\nsentences or to output the parse trees. The grammatical relations are the\nfunctional structure of a language. They rely very much on the function tag of\nthe tokens. We augment the grammatical relations of Myanmar sentences with\ntransformation-based learning of function tagging.\n",
        "published": "2011-12-02T07:15:49Z",
        "pdf_link": "http://arxiv.org/pdf/1112.0396v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.2468v1",
        "title": "Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus",
        "summary": "  Short Message Service (SMS) messages are largely sent directly from one\nperson to another from their mobile phones. They represent a means of personal\ncommunication that is an important communicative artifact in our current\ndigital era. As most existing studies have used private access to SMS corpora,\ncomparative studies using the same raw SMS data has not been possible up to\nnow. We describe our efforts to collect a public SMS corpus to address this\nproblem. We use a battery of methodologies to collect the corpus, paying\nparticular attention to privacy issues to address contributors' concerns. Our\nlive project collects new SMS message submissions, checks their quality and\nadds the valid messages, releasing the resultant corpus as XML and as SQL\ndumps, along with corpus statistics, every month. We opportunistically collect\nas much metadata about the messages and their sender as possible, so as to\nenable different types of analyses. To date, we have collected about 60,000\nmessages, focusing on English and Mandarin Chinese.\n",
        "published": "2011-12-12T08:07:49Z",
        "pdf_link": "http://arxiv.org/pdf/1112.2468v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.6286v1",
        "title": "Visualization and Analysis of Frames in Collections of Messages: Content\n  Analysis and the Measurement of Meaning",
        "summary": "  A step-to-step introduction is provided on how to generate a semantic map\nfrom a collection of messages (full texts, paragraphs or statements) using\nfreely available software and/or SPSS for the relevant statistics and the\nvisualization. The techniques are discussed in the various theoretical contexts\nof (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and\nsocial systems theory (e.g., the communication of meaning), and (iii)\ncommunication studies (e.g., framing and agenda-setting). We distinguish\nbetween the communication of information in the network space (social network\nanalysis) and the communication of meaning in the vector space. The vector\nspace can be considered a generated as an architecture by the network of\nrelations in the network space; words are then not only related, but also\npositioned. These positions are expected rather than observed and therefore one\ncan communicate meaning. Knowledge can be generated when these meanings can\nrecursively be communicated and therefore also further codified.\n",
        "published": "2011-12-29T11:47:05Z",
        "pdf_link": "http://arxiv.org/pdf/1112.6286v1"
    },
    {
        "id": "http://arxiv.org/abs/1112.6384v1",
        "title": "Proof nets for the Lambek-Grishin calculus",
        "summary": "  Grishin's generalization of Lambek's Syntactic Calculus combines a\nnon-commutative multiplicative conjunction and its residuals (product, left and\nright division) with a dual family: multiplicative disjunction, right and left\ndifference. Interaction between these two families takes the form of linear\ndistributivity principles. We study proof nets for the Lambek-Grishin calculus\nand the correspondence between these nets and unfocused and focused versions of\nits sequent calculus.\n",
        "published": "2011-12-29T19:16:20Z",
        "pdf_link": "http://arxiv.org/pdf/1112.6384v1"
    },
    {
        "id": "http://arxiv.org/abs/1201.1192v1",
        "title": "Formalization of semantic network of image constructions in electronic\n  content",
        "summary": "  A formal theory based on a binary operator of directional associative\nrelation is constructed in the article and an understanding of an associative\nnormal form of image constructions is introduced. A model of a commutative\nsemigroup, which provides a presentation of a sentence as three components of\nan interrogative linguistic image construction, is considered.\n",
        "published": "2012-01-05T15:22:05Z",
        "pdf_link": "http://arxiv.org/pdf/1201.1192v1"
    },
    {
        "id": "http://arxiv.org/abs/1201.2010v1",
        "title": "Recognizing Bangla Grammar using Predictive Parser",
        "summary": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
        "published": "2012-01-10T10:33:18Z",
        "pdf_link": "http://arxiv.org/pdf/1201.2010v1"
    },
    {
        "id": "http://arxiv.org/abs/1201.6224v2",
        "title": "Wikipedia Arborification and Stratified Explicit Semantic Analysis",
        "summary": "  [This is the translation of paper \"Arborification de Wikip\\'edia et analyse\ns\\'emantique explicite stratifi\\'ee\" submitted to TALN 2012.]\n  We present an extension of the Explicit Semantic Analysis method by\nGabrilovich and Markovitch. Using their semantic relatedness measure, we weight\nthe Wikipedia categories graph. Then, we extract a minimal spanning tree, using\nChu-Liu & Edmonds' algorithm. We define a notion of stratified tfidf where the\nstratas, for a given Wikipedia page and a given term, are the classical tfidf\nand categorical tfidfs of the term in the ancestor categories of the page\n(ancestors in the sense of the minimal spanning tree). Our method is based on\nthis stratified tfidf, which adds extra weight to terms that \"survive\" when\nclimbing up the category tree. We evaluate our method by a text classification\non the WikiNews corpus: it increases precision by 18%. Finally, we provide\nhints for future research\n",
        "published": "2012-01-30T14:26:31Z",
        "pdf_link": "http://arxiv.org/pdf/1201.6224v2"
    },
    {
        "id": "http://arxiv.org/abs/1202.0116v1",
        "title": "Inference and Plausible Reasoning in a Natural Language Understanding\n  System Based on Object-Oriented Semantics",
        "summary": "  Algorithms of inference in a computer system oriented to input and semantic\nprocessing of text information are presented. Such inference is necessary for\nlogical questions when the direct comparison of objects from a question and\ndatabase can not give a result. The following classes of problems are\nconsidered: a check of hypotheses for persons and non-typical actions, the\ndetermination of persons and circumstances for non-typical actions, planning\nactions, the determination of event cause and state of persons. To form an\nanswer both deduction and plausible reasoning are used. As a knowledge domain\nunder consideration is social behavior of persons, plausible reasoning is based\non laws of social psychology. Proposed algorithms of inference and plausible\nreasoning can be realized in computer systems closely connected with text\nprocessing (criminology, operation of business, medicine, document systems).\n",
        "published": "2012-02-01T08:36:50Z",
        "pdf_link": "http://arxiv.org/pdf/1202.0116v1"
    },
    {
        "id": "http://arxiv.org/abs/1202.1054v1",
        "title": "Considering a resource-light approach to learning verb valencies",
        "summary": "  Here we describe work on learning the subcategories of verbs in a\nmorphologically rich language using only minimal linguistic resources. Our goal\nis to learn verb subcategorizations for Quechua, an under-resourced\nmorphologically rich language, from an unannotated corpus. We compare results\nfrom applying this approach to an unannotated Arabic corpus with those achieved\nby processing the same text in treebank form. The original plan was to use only\na morphological analyzer and an unannotated corpus, but experiments suggest\nthat this approach by itself will not be effective for learning the\ncombinatorial potential of Arabic verbs in general. The lower bound on\nresources for acquiring this information is somewhat higher, apparently\nrequiring a a part-of-speech tagger and chunker for most languages, and a\nmorphological disambiguater for Arabic.\n",
        "published": "2012-02-06T06:33:03Z",
        "pdf_link": "http://arxiv.org/pdf/1202.1054v1"
    },
    {
        "id": "http://arxiv.org/abs/1202.1568v2",
        "title": "Beyond Sentiment: The Manifold of Human Emotions",
        "summary": "  Sentiment analysis predicts the presence of positive or negative emotions in\na text document. In this paper we consider higher dimensional extensions of the\nsentiment concept, which represent a richer set of human emotions. Our approach\ngoes beyond previous work in that our model contains a continuous manifold\nrather than a finite set of human emotions. We investigate the resulting model,\ncompare it to psychological observations, and explore its predictive\ncapabilities. Besides obtaining significant improvements over a baseline\nwithout manifold, we are also able to visualize different notions of positive\nsentiment in different domains.\n",
        "published": "2012-02-08T00:49:36Z",
        "pdf_link": "http://arxiv.org/pdf/1202.1568v2"
    },
    {
        "id": "http://arxiv.org/abs/1202.6266v1",
        "title": "Realisation d'un systeme de reconnaissance automatique de la parole\n  arabe base sur CMU Sphinx",
        "summary": "  This paper presents the continuation of the work completed by Satori and all.\n[SCH07] by the realization of an automatic speech recognition system (ASR) for\nArabic language based SPHINX 4 system. The previous work was limited to the\nrecognition of the first ten digits, whereas the present work is a remarkable\nprojection consisting in continuous Arabic speech recognition with a rate of\nrecognition of surroundings 96%.\n",
        "published": "2012-02-28T16:04:36Z",
        "pdf_link": "http://arxiv.org/pdf/1202.6266v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.0145v2",
        "title": "The Horse Raced Past: Gardenpath Processing in Dynamical Systems",
        "summary": "  I pinpoint an interesting similarity between a recent account to rational\nparsing and the treatment of sequential decisions problems in a dynamical\nsystems approach. I argue that expectation-driven search heuristics aiming at\nfast computation resembles a high-risk decision strategy in favor of large\ntransition velocities. Hale's rational parser, combining generalized\nleft-corner parsing with informed $\\mathrm{A}^*$ search to resolve processing\nconflicts, explains gardenpath effects in natural sentence processing by\nmisleading estimates of future processing costs that are to be minimized. On\nthe other hand, minimizing the duration of cognitive computations in\ntime-continuous dynamical systems can be described by combining vector space\nrepresentations of cognitive states by means of filler/role decompositions and\nsubsequent tensor product representations with the paradigm of stable\nheteroclinic sequences. Maximizing transition velocities according to a\nhigh-risk decision strategy could account for a fast race even between states\nthat are apparently remote in representation space.\n",
        "published": "2012-03-01T11:06:32Z",
        "pdf_link": "http://arxiv.org/pdf/1203.0145v2"
    },
    {
        "id": "http://arxiv.org/abs/1203.1685v1",
        "title": "Statistical Function Tagging and Grammatical Relations of Myanmar\n  Sentences",
        "summary": "  This paper describes a context free grammar (CFG) based grammatical relations\nfor Myanmar sentences which combine corpus-based function tagging system. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step to show grammatical relations\nof Myanmar sentences. In the task of function tagging, which tags the function\nof Myanmar sentences with correct segmentation, POS (part-of-speech) tagging\nand chunking information, we use Naive Bayesian theory to disambiguate the\npossible function tags of a word. We apply context free grammar (CFG) to find\nout the grammatical relations of the function tags. We also create a functional\nannotated tagged corpus for Myanmar and propose the grammar rules for Myanmar\nsentences. Experiments show that our analysis achieves a good result with\nsimple sentences and complex sentences.\n",
        "published": "2012-03-08T03:06:29Z",
        "pdf_link": "http://arxiv.org/pdf/1203.1685v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.1858v1",
        "title": "Distributional Measures of Semantic Distance: A Survey",
        "summary": "  The ability to mimic human notions of semantic distance has widespread\napplications. Some measures rely only on raw text (distributional measures) and\nsome rely on knowledge sources such as WordNet. Although extensive studies have\nbeen performed to compare WordNet-based measures with human judgment, the use\nof distributional measures as proxies to estimate semantic distance has\nreceived little attention. Even though they have traditionally performed poorly\nwhen compared to WordNet-based measures, they lay claim to certain uniquely\nattractive features, such as their applicability in resource-poor languages and\ntheir ability to mimic both semantic similarity and semantic relatedness.\nTherefore, this paper presents a detailed study of distributional measures.\nParticular attention is paid to flesh out the strengths and limitations of both\nWordNet-based and distributional measures, and how distributional measures of\ndistance can be brought more in line with human notions of semantic distance.\nWe conclude with a brief discussion of recent work on hybrid measures.\n",
        "published": "2012-03-08T17:29:33Z",
        "pdf_link": "http://arxiv.org/pdf/1203.1858v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.1889v1",
        "title": "Distributional Measures as Proxies for Semantic Relatedness",
        "summary": "  The automatic ranking of word pairs as per their semantic relatedness and\nability to mimic human notions of semantic relatedness has widespread\napplications. Measures that rely on raw data (distributional measures) and\nthose that use knowledge-rich ontologies both exist. Although extensive studies\nhave been performed to compare ontological measures with human judgment, the\ndistributional measures have primarily been evaluated by indirect means. This\npaper is a detailed study of some of the major distributional measures; it\nlists their respective merits and limitations. New measures that overcome these\ndrawbacks, that are more in line with the human notions of semantic\nrelatedness, are suggested. The paper concludes with an exhaustive comparison\nof the distributional and ontology-based measures. Along the way, significant\nresearch problems are identified. Work on these problems may lead to a better\nunderstanding of how semantic relatedness is to be measured.\n",
        "published": "2012-03-08T19:03:37Z",
        "pdf_link": "http://arxiv.org/pdf/1203.1889v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.2498v2",
        "title": "Fault detection system for Arabic language",
        "summary": "  The study of natural language, especially Arabic, and mechanisms for the\nimplementation of automatic processing is a fascinating field of study, with\nvarious potential applications. The importance of tools for natural language\nprocessing is materialized by the need to have applications that can\neffectively treat the vast mass of information available nowadays on electronic\nforms. Among these tools, mainly driven by the necessity of a fast writing in\nalignment to the actual daily life speed, our interest is on the writing\nauditors. The morphological and syntactic properties of Arabic make it a\ndifficult language to master, and explain the lack in the processing tools for\nthat language. Among these properties, we can mention: the complex structure of\nthe Arabic word, the agglutinative nature, lack of vocalization, the\nsegmentation of the text, the linguistic richness, etc.\n",
        "published": "2012-03-08T13:28:03Z",
        "pdf_link": "http://arxiv.org/pdf/1203.2498v2"
    },
    {
        "id": "http://arxiv.org/abs/1203.3023v1",
        "title": "Toward an example-based machine translation from written text to ASL\n  using virtual agent animation",
        "summary": "  Modern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth information\nmeaning. Our goals are: to translate written text from any language to ASL\nanimation; to model maximum raw information using machine learning and\ncomputational techniques; and to produce a more adapted and expressive form to\nnatural looking and understandable ASL animations. Our methods include\nlinguistic annotation of initial text and semantic orientation to generate the\nfacial expression. We use the genetic algorithms coupled to learning/recognized\nsystems to produce the most natural form. To detect emotion we are based on\nfuzzy logic to produce the degree of interpolation between facial expressions.\nRoughly, we present a new expressive language Text Adapted Sign Modeling\nLanguage TASML that describes all maximum aspects related to a natural sign\nlanguage interpretation. This paper is organized as follow: the next section is\ndevoted to present the comprehension effect of using Space/Time/SVO form in ASL\nanimation based on experimentation. In section 3, we describe our technical\nconsiderations. We present the general approach we adopted to develop our tool\nin section 4. Finally, we give some perspectives and future works.\n",
        "published": "2012-03-14T08:51:14Z",
        "pdf_link": "http://arxiv.org/pdf/1203.3023v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.3584v1",
        "title": "An Accurate Arabic Root-Based Lemmatizer for Information Retrieval\n  Purposes",
        "summary": "  In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma\nlevel analysis and generation does not yet focused in Arabic NLP literatures.\nIn the current research, we propose the first non-statistical accurate Arabic\nlemmatizer algorithm that is suitable for information retrieval (IR) systems.\nThe proposed lemmatizer makes use of different Arabic language knowledge\nresources to generate accurate lemma form and its relevant features that\nsupport IR purposes. As a POS tagger, the experimental results show that, the\nproposed algorithm achieves a maximum accuracy of 94.8%. For first seen\ndocuments, an accuracy of 89.15% is achieved, compared to 76.7% of up to date\nStanford accurate Arabic model, for the same, dataset.\n",
        "published": "2012-03-15T22:49:20Z",
        "pdf_link": "http://arxiv.org/pdf/1203.3584v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.4605v1",
        "title": "Arabic Keyphrase Extraction using Linguistic knowledge and Machine\n  Learning Techniques",
        "summary": "  In this paper, a supervised learning technique for extracting keyphrases of\nArabic documents is presented. The extractor is supplied with linguistic\nknowledge to enhance its efficiency instead of relying only on statistical\ninformation such as term frequency and distance. During analysis, an annotated\nArabic corpus is used to extract the required lexical features of the document\nwords. The knowledge also includes syntactic rules based on part of speech tags\nand allowed word sequences to extract the candidate keyphrases. In this work,\nthe abstract form of Arabic words is used instead of its stem form to represent\nthe candidate terms. The Abstract form hides most of the inflections found in\nArabic words. The paper introduces new features of keyphrases based on\nlinguistic knowledge, to capture titles and subtitles of a document. A simple\nANOVA test is used to evaluate the validity of selected features. Then, the\nlearning model is built using the LDA - Linear Discriminant Analysis - and\ntraining documents. Although, the presented system is trained using documents\nin the IT domain, experiments carried out show that it has a significantly\nbetter performance than the existing Arabic extractor systems, where precision\nand recall values reach double their corresponding values in the other systems\nespecially for lengthy and non-scientific articles.\n",
        "published": "2012-03-20T21:52:35Z",
        "pdf_link": "http://arxiv.org/pdf/1203.4605v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.4933v1",
        "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS\n  Tagger",
        "summary": "  This paper gives a detail overview about the modified features selection in\nCRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.\nSelection of features is so important in CRF that the better are the features\nthen the better are the outputs. This work is an attempt or an experiment to\nmake the previous work more efficient. Multiple new features are tried to run\nthe CRF and again tried with the Reduplicated Multiword Expression (RMWE) as\nanother feature. The CRF run with RMWE because Manipuri is rich of RMWE and\nidentification of RMWE becomes one of the necessities to bring up the result of\nPOS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%\nand F-measure of 75.60%. With the identification of RMWE and considering it as\na feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and\nF-measure of 77.14%.\n",
        "published": "2012-03-22T09:50:51Z",
        "pdf_link": "http://arxiv.org/pdf/1203.4933v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5051v1",
        "title": "Analysing Temporally Annotated Corpora with CAVaT",
        "summary": "  We present CAVaT, a tool that performs Corpus Analysis and Validation for\nTimeML. CAVaT is an open source, modular checking utility for statistical\nanalysis of features specific to temporally-annotated natural language corpora.\nIt provides reporting, highlights salient links between a variety of general\nand time-specific linguistic features, and also validates a temporal annotation\nto ensure that it is logically consistent and sufficiently annotated. Uniquely,\nCAVaT provides analysis specific to TimeML-annotated temporal information.\nTimeML is a standard for annotating temporal information in natural language\ntext. In this paper, we present the reporting part of CAVaT, and then its\nerror-checking ability, including the workings of several novel TimeML document\nverification methods. This is followed by the execution of some example tasks\nusing the tool to show relations between times, events, signals and links. We\nalso demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been\ndetected with CAVaT.\n",
        "published": "2012-03-22T17:45:39Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5051v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5055v1",
        "title": "Using Signals to Improve Automatic Classification of Temporal Relations",
        "summary": "  Temporal information conveyed by language describes how the world around us\nchanges through time. Events, durations and times are all temporal elements\nthat can be viewed as intervals. These intervals are sometimes temporally\nrelated in text. Automatically determining the nature of such relations is a\ncomplex and unsolved problem. Some words can act as \"signals\" which suggest a\ntemporal ordering between intervals. In this paper, we use these signal words\nto improve the accuracy of a recent approach to classification of temporal\nlinks.\n",
        "published": "2012-03-22T17:50:08Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5055v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5060v1",
        "title": "USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2",
        "summary": "  We describe the University of Sheffield system used in the TempEval-2\nchallenge, USFD2. The challenge requires the automatic identification of\ntemporal entities and relations in text. USFD2 identifies and anchors temporal\nexpressions, and also attempts two of the four temporal relation assignment\ntasks. A rule-based system picks out and anchors temporal expressions, and a\nmaximum entropy classifier assigns temporal link labels, based on features that\ninclude descriptions of associated temporal signal words. USFD2 identified\ntemporal expressions successfully, and correctly classified their type in 90%\nof cases. Determining the relation between an event and time expression in the\nsame sentence was performed at 63% accuracy, the second highest score in this\npart of the challenge.\n",
        "published": "2012-03-22T17:59:22Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5060v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5062v1",
        "title": "An Annotation Scheme for Reichenbach's Verbal Tense Structure",
        "summary": "  In this paper we present RTMML, a markup language for the tenses of verbs and\ntemporal relations between verbs. There is a richness to tense in language that\nis not fully captured by existing temporal annotation schemata. Following\nReichenbach we present an analysis of tense in terms of abstract time points,\nwith the aim of supporting automated processing of tense and temporal relations\nin language. This allows for precise reasoning about tense in documents, and\nthe deduction of temporal relations between the times and verbal events in a\ndiscourse. We define the syntax of RTMML, and demonstrate the markup in a range\nof situations.\n",
        "published": "2012-03-22T18:05:26Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5062v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5066v1",
        "title": "A Corpus-based Study of Temporal Signals",
        "summary": "  Automatic temporal ordering of events described in discourse has been of\ngreat interest in recent years. Event orderings are conveyed in text via va\nrious linguistic mechanisms including the use of expressions such as \"before\",\n\"after\" or \"during\" that explicitly assert a temporal relation -- temporal\nsignals. In this paper, we investigate the role of temporal signals in temporal\nrelation extraction and provide a quantitative analysis of these expres sions\nin the TimeBank annotated corpus.\n",
        "published": "2012-03-22T18:08:47Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5066v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5073v1",
        "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding",
        "summary": "  This paper describes the University of Sheffield's entry in the 2011 TAC KBP\nentity linking and slot filling tasks. We chose to participate in the\nmonolingual entity linking task, the monolingual slot filling task and the\ntemporal slot filling tasks. We set out to build a framework for\nexperimentation with knowledge base population. This framework was created, and\napplied to multiple KBP tasks. We demonstrated that our proposed framework is\neffective and suitable for collaborative development efforts, as well as useful\nin a teaching environment. Finally we present results that, while very modest,\nprovide improvements an order of magnitude greater than our 2010 attempt.\n",
        "published": "2012-03-22T18:34:19Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5073v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5076v1",
        "title": "Massively Increasing TIMEX3 Resources: A Transduction Approach",
        "summary": "  Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. Gold standard\ntemporally-annotated resources are limited in size, which makes research using\nthem difficult. Standards have also evolved over the past decade, so not all\ntemporally annotated data is in the same format. We vastly increase available\nhuman-annotated temporal expression resources by converting older format\nresources to TimeML/TIMEX3. This task is difficult due to differing annotation\nmethods. We present a robust conversion tool and a new, large temporal\nexpression resource. Using this, we evaluate our conversion process by using it\nas training data for an existing TimeML annotation tool, achieving a 0.87 F1\nmeasure -- better than any system in the TempEval-2 timex recognition exercise.\n",
        "published": "2012-03-22T18:45:07Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5076v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5255v1",
        "title": "Post-Editing Error Correction Algorithm for Speech Recognition using\n  Bing Spelling Suggestion",
        "summary": "  ASR short for Automatic Speech Recognition is the process of converting a\nspoken speech into text that can be manipulated by a computer. Although ASR has\nseveral applications, it is still erroneous and imprecise especially if used in\na harsh surrounding wherein the input speech is of low quality. This paper\nproposes a post-editing ASR error correction method and algorithm based on\nBing's online spelling suggestion. In this approach, the ASR recognized output\ntext is spell-checked using Bing's spelling suggestion technology to detect and\ncorrect misrecognized words. More specifically, the proposed algorithm breaks\ndown the ASR output text into several word-tokens that are submitted as search\nqueries to Bing search engine. A returned spelling suggestion implies that a\nquery is misspelled; and thus it is replaced by the suggested correction;\notherwise, no correction is performed and the algorithm continues with the next\ntoken until all tokens get validated. Experiments carried out on various\nspeeches in different languages indicated a successful decrease in the number\nof ASR errors and an improvement in the overall error correction rate. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor computers.\n",
        "published": "2012-03-23T14:32:50Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5255v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.5262v1",
        "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset",
        "summary": "  At the present time, computers are employed to solve complex tasks and\nproblems ranging from simple calculations to intensive digital image processing\nand intricate algorithmic optimization problems to computationally-demanding\nweather forecasting problems. ASR short for Automatic Speech Recognition is yet\nanother type of computational problem whose purpose is to recognize human\nspoken speech and convert it into text that can be processed by a computer.\nDespite that ASR has many versatile and pervasive real-world applications,it is\nstill relatively erroneous and not perfectly solved as it is prone to produce\nspelling errors in the recognized text, especially if the ASR system is\noperating in a noisy environment, its vocabulary size is limited, and its input\nspeech is of bad or low quality. This paper proposes a post-editing ASR error\ncorrection method based on MicrosoftN-Gram dataset for detecting and correcting\nspelling errors generated by ASR systems. The proposed method comprises an\nerror detection algorithm for detecting word errors; a candidate corrections\ngeneration algorithm for generating correction suggestions for the detected\nword errors; and a context-sensitive error correction algorithm for selecting\nthe best candidate for correction. The virtue of using the Microsoft N-Gram\ndataset is that it contains real-world data and word sequences extracted from\nthe web which canmimica comprehensive dictionary of words having a large and\nall-inclusive vocabulary. Experiments conducted on numerous speeches, performed\nby different speakers, showed a remarkable reduction in ASR errors. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor and distributed systems.\n",
        "published": "2012-03-23T14:51:05Z",
        "pdf_link": "http://arxiv.org/pdf/1203.5262v1"
    },
    {
        "id": "http://arxiv.org/abs/1203.6136v1",
        "title": "Tree Transducers, Machine Translation, and Cross-Language Divergences",
        "summary": "  Tree transducers are formal automata that transform trees into other trees.\nMany varieties of tree transducers have been explored in the automata theory\nliterature, and more recently, in the machine translation literature. In this\npaper I review T and xT transducers, situate them among related formalisms, and\nshow how they can be used to implement rules for machine translation systems\nthat cover all of the cross-language structural divergences described in Bonnie\nDorr's influential article on the topic. I also present an implementation of xT\ntransduction, suitable and convenient for experimenting with translation rules.\n",
        "published": "2012-03-28T02:13:39Z",
        "pdf_link": "http://arxiv.org/pdf/1203.6136v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0140v1",
        "title": "Roget's Thesaurus as a Lexical Resource for Natural Language Processing",
        "summary": "  WordNet proved that it is possible to construct a large-scale electronic\nlexical database on the principles of lexical semantics. It has been accepted\nand used extensively by computational linguists ever since it was released.\nInspired by WordNet's success, we propose as an alternative a similar resource,\nbased on the 1987 Penguin edition of Roget's Thesaurus of English Words and\nPhrases.\n  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless\nwriters, orators and students of the English language have used it.\nComputational linguists have employed Roget's for almost 50 years in Natural\nLanguage Processing, however hesitated in accepting Roget's Thesaurus because a\nproper machine tractable version was not available.\n  This dissertation presents an implementation of a machine-tractable version\nof the 1987 Penguin edition of Roget's Thesaurus - the first implementation of\nits kind to use an entire current edition. It explains the steps necessary for\ntaking a machine-readable file and transforming it into a tractable system.\nThis involves converting the lexical material into a format that can be more\neasily exploited, identifying data structures and designing classes to\ncomputerize the Thesaurus. Roget's organization is studied in detail and\ncontrasted with WordNet's.\n  We show two applications of the computerized Thesaurus: computing semantic\nsimilarity between words and phrases, and building lexical chains in a text.\nThe experiments are performed using well-known benchmarks and the results are\ncompared to those of other systems that use Roget's, WordNet and statistical\ntechniques. Roget's has turned out to be an excellent resource for measuring\nsemantic similarity; lexical chains are easily built but more difficult to\nevaluate. We also explain ways in which Roget's Thesaurus and WordNet can be\ncombined.\n",
        "published": "2012-03-31T21:53:56Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0140v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0184v1",
        "title": "Parallel Spell-Checking Algorithm Based on Yahoo! N-Grams Dataset",
        "summary": "  Spell-checking is the process of detecting and sometimes providing\nsuggestions for incorrectly spelled words in a text. Basically, the larger the\ndictionary of a spell-checker is, the higher is the error detection rate;\notherwise, misspellings would pass undetected. Unfortunately, traditional\ndictionaries suffer from out-of-vocabulary and data sparseness problems as they\ndo not encompass large vocabulary of words indispensable to cover proper names,\ndomain-specific terms, technical jargons, special acronyms, and terminologies.\nAs a result, spell-checkers will incur low error detection and correction rate\nand will fail to flag all errors in the text. This paper proposes a new\nparallel shared-memory spell-checking algorithm that uses rich real-world word\nstatistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors\nin computer text. Essentially, the proposed algorithm can be divided into three\nsub-algorithms that run in a parallel fashion: The error detection algorithm\nthat detects misspellings, the candidates generation algorithm that generates\ncorrection suggestions, and the error correction algorithm that performs\ncontextual error correction. Experiments conducted on a set of text articles\ncontaining misspellings, showed a remarkable spelling error correction rate\nthat resulted in a radical reduction of both non-word and real-word errors in\nelectronic text. In a further study, the proposed algorithm is to be optimized\nfor message-passing systems so as to become more flexible and less costly to\nscale over distributed machines.\n",
        "published": "2012-04-01T09:28:20Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0184v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0191v1",
        "title": "OCR Post-Processing Error Correction Algorithm using Google Online\n  Spelling Suggestion",
        "summary": "  With the advent of digital optical scanners, a lot of paper-based books,\ntextbooks, magazines, articles, and documents are being transformed into an\nelectronic version that can be manipulated by a computer. For this purpose,\nOCR, short for Optical Character Recognition was developed to translate scanned\ngraphical text into editable computer text. Unfortunately, OCR is still\nimperfect as it occasionally mis-recognizes letters and falsely identifies\nscanned text, leading to misspellings and linguistics errors in the OCR output\ntext. This paper proposes a post-processing context-based error correction\nalgorithm for detecting and correcting OCR non-word and real-word errors. The\nproposed algorithm is based on Google's online spelling suggestion which\nharnesses an internal database containing a huge collection of terms and word\nsequences gathered from all over the web, convenient to suggest possible\nreplacements for words that have been misspelled during the OCR process.\nExperiments carried out revealed a significant improvement in OCR error\ncorrection rate. Future research can improve upon the proposed algorithm so\nmuch so that it can be parallelized and executed over multiprocessing\nplatforms.\n",
        "published": "2012-04-01T10:34:38Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0191v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0245v1",
        "title": "Roget's Thesaurus and Semantic Similarity",
        "summary": "  We have implemented a system that measures semantic similarity using a\ncomputerized 1987 Roget's Thesaurus, and evaluated it by performing a few\ntypical tests. We compare the results of these tests with those produced by\nWordNet-based similarity measures. One of the benchmarks is Miller and Charles'\nlist of 30 noun pairs to which human judges had assigned similarity measures.\nWe correlate these measures with those computed by several NLP systems. The 30\npairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have\nalso studied. Our Roget's-based system gets correlations of .878 for the\nsmaller and .818 for the larger list of noun pairs; this is quite close to the\n.885 that Resnik obtained when he employed humans to replicate the Miller and\nCharles experiment. We further evaluate our measure by using Roget's and\nWordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the\ncorrect synonym must be selected amongst a group of four words. Our system gets\n78.75%, 82.00% and 74.33% of the questions respectively.\n",
        "published": "2012-04-01T17:04:13Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0245v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0257v1",
        "title": "Not As Easy As It Seems: Automating the Construction of Lexical Chains\n  Using Roget's Thesaurus",
        "summary": "  Morris and Hirst present a method of linking significant words that are about\nthe same topic. The resulting lexical chains are a means of identifying\ncohesive regions in a text, with applications in many natural language\nprocessing tasks, including text summarization. The first lexical chains were\nconstructed manually using Roget's International Thesaurus. Morris and Hirst\nwrote that automation would be straightforward given an electronic thesaurus.\nAll applications so far have used WordNet to produce lexical chains, perhaps\nbecause adequate electronic versions of Roget's were not available until\nrecently. We discuss the building of lexical chains using an electronic version\nof Roget's Thesaurus. We implement a variant of the original algorithm, and\nexplain the necessary design decisions. We include a comparison with other\nimplementations.\n",
        "published": "2012-04-01T19:19:36Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0257v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.0258v1",
        "title": "Roget's Thesaurus: a Lexical Resource to Treasure",
        "summary": "  This paper presents the steps involved in creating an electronic lexical\nknowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic\nrelations are labelled with the help of WordNet. The two resources are compared\nin a qualitative and quantitative manner. Differences in the organization of\nthe lexical material are discussed, as well as the possibility of merging both\nresources.\n",
        "published": "2012-04-01T19:25:29Z",
        "pdf_link": "http://arxiv.org/pdf/1204.0258v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.2847v2",
        "title": "Segmentation Similarity and Agreement",
        "summary": "  We propose a new segmentation evaluation metric, called segmentation\nsimilarity (S), that quantifies the similarity between two segmentations as the\nproportion of boundaries that are not transformed when comparing them using\nedit distance, essentially using edit distance as a penalty function and\nscaling penalties by segmentation size. We propose several adapted\ninter-annotator agreement coefficients which use S that are suitable for\nsegmentation. We show that S is configurable enough to suit a wide variety of\nsegmentation evaluations, and is an improvement upon the state of the art. We\nalso propose using inter-annotator agreement coefficients to evaluate automatic\nsegmenters in terms of human performance.\n",
        "published": "2012-04-12T22:01:27Z",
        "pdf_link": "http://arxiv.org/pdf/1204.2847v2"
    },
    {
        "id": "http://arxiv.org/abs/1204.3800v1",
        "title": "Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha)",
        "summary": "  Jules Bloch's work on formation of the Marathi language has to be expanded\nfurther to provide for a study of evolution and formation of Indian languages\nin the Indian language union (sprachbund). The paper analyses the stages in the\nevolution of early writing systems which began with the evolution of counting\nin the ancient Near East. A stage anterior to the stage of syllabic\nrepresentation of sounds of a language, is identified. Unique geometric shapes\nrequired for tokens to categorize objects became too large to handle to\nabstract hundreds of categories of goods and metallurgical processes during the\nproduction of bronze-age goods. About 3500 BCE, Indus script as a writing\nsystem was developed to use hieroglyphs to represent the 'spoken words'\nidentifying each of the goods and processes. A rebus method of representing\nsimilar sounding words of the lingua franca of the artisans was used in Indus\nscript. This method is recognized and consistently applied for the lingua\nfranca of the Indian sprachbund. That the ancient languages of India,\nconstituted a sprachbund (or language union) is now recognized by many\nlinguists. The sprachbund area is proximate to the area where most of the Indus\nscript inscriptions were discovered, as documented in the corpora. That\nhundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced\nby their use on early punch-marked coins. This explains the combined use of\nsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on\nRampurva copper bolt, and Sohgaura copper plate from about 6th century\nBCE.Indian hieroglyphs constitute a writing system for meluhha language and are\nrebus representations of archaeo-metallurgy lexemes. The rebus principle was\nemployed by the early scripts and can legitimately be used to decipher the\nIndus script, after secure pictorial identification.\n",
        "published": "2012-04-17T14:14:26Z",
        "pdf_link": "http://arxiv.org/pdf/1204.3800v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.5852v1",
        "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram\n  Information",
        "summary": "  In computing, spell checking is the process of detecting and sometimes\nproviding spelling suggestions for incorrectly spelled words in a text.\nBasically, a spell checker is a computer program that uses a dictionary of\nwords to perform spell checking. The bigger the dictionary is, the higher is\nthe error detection rate. The fact that spell checkers are based on regular\ndictionaries, they suffer from data sparseness problem as they cannot capture\nlarge vocabulary of words including proper names, domain-specific terms,\ntechnical jargons, special acronyms, and terminologies. As a result, they\nexhibit low error detection rate and often fail to catch major errors in the\ntext. This paper proposes a new context-sensitive spelling correction method\nfor detecting and correcting non-word and real-word errors in digital text\ndocuments. The approach hinges around data statistics from Google Web 1T 5-gram\ndata set which consists of a big volume of n-gram word sequences, extracted\nfrom the World Wide Web. Fundamentally, the proposed method comprises an error\ndetector that detects misspellings, a candidate spellings generator based on a\ncharacter 2-gram model that generates correction suggestions, and an error\ncorrector that performs contextual error correction. Experiments conducted on a\nset of text documents from different domains and containing misspellings,\nshowed an outstanding spelling error correction rate and a drastic reduction of\nboth non-word and real-word errors. In a further study, the proposed algorithm\nis to be parallelized so as to lower the computational cost of the error\ndetection and correction processes.\n",
        "published": "2012-04-26T07:44:18Z",
        "pdf_link": "http://arxiv.org/pdf/1204.5852v1"
    },
    {
        "id": "http://arxiv.org/abs/1204.6364v1",
        "title": "A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping\n  Prototype",
        "summary": "  The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)\nPrototype. The prototype is domain-specific, the purpose of which is to map\ninstructional text onto a knowledge domain. The context of the knowledge domain\nis DC electrical circuit. During development, the prototype has been tested\nwith a limited data set from the domain. The prototype reached a stage where it\nneeds to be evaluated with a representative linguistic data set called corpus.\nA corpus is a collection of text drawn from typical sources which can be used\nas a test data set to evaluate NLP systems. As there is no available corpus for\nthe domain, we developed and annotated a representative corpus. The evaluation\nof the prototype considers two of its major components- lexical components and\nknowledge model. Evaluation on lexical components enriches the lexical\nresources of the prototype like vocabulary and grammar structures. This leads\nthe prototype to parse a reasonable amount of sentences in the corpus. While\ndealing with the lexicon was straight forward, the identification and\nextraction of appropriate semantic relations was much more involved. It was\nnecessary, therefore, to manually develop a conceptual structure for the domain\nto formulate a domain-specific framework of semantic relations. The framework\nof semantic relationsthat has resulted from this study consisted of 55\nrelations, out of which 42 have inverse relations. We also conducted rhetorical\nanalysis on the corpus to prove its representativeness in conveying semantic.\nFinally, we conducted a topical and discourse analysis on the corpus to analyze\nthe coverage of discourse by the prototype.\n",
        "published": "2012-04-28T03:52:21Z",
        "pdf_link": "http://arxiv.org/pdf/1204.6364v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.0627v1",
        "title": "Rule-weighted and terminal-weighted context-free grammars have identical\n  expressivity",
        "summary": "  Two formalisms, both based on context-free grammars, have recently been\nproposed as a basis for a non-uniform random generation of combinatorial\nobjects. The former, introduced by Denise et al, associates weights with\nletters, while the latter, recently explored by Weinberg et al in the context\nof random generation, associates weights to transitions. In this short note, we\nuse a simple modification of the Greibach Normal Form transformation algorithm,\ndue to Blum and Koch, to show the equivalent expressivities, in term of their\ninduced distributions, of these two formalisms.\n",
        "published": "2012-05-03T06:49:59Z",
        "pdf_link": "http://arxiv.org/pdf/1205.0627v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.1603v1",
        "title": "Parsing of Myanmar sentences with function tagging",
        "summary": "  This paper describes the use of Naive Bayes to address the task of assigning\nfunction tags and context free grammar (CFG) to parse Myanmar sentences. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step for parsing. In the task of\nfunction tagging, we use the functional annotated corpus and tag Myanmar\nsentences with correct segmentation, POS (part-of-speech) tagging and chunking\ninformation. We propose Myanmar grammar rules and apply context free grammar\n(CFG) to find out the parse tree of function tagged Myanmar sentences.\nExperiments show that our analysis achieves a good result with parsing of\nsimple sentences and three types of complex sentences.\n",
        "published": "2012-05-08T07:01:40Z",
        "pdf_link": "http://arxiv.org/pdf/1205.1603v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.3183v1",
        "title": "A Model-Driven Probabilistic Parser Generator",
        "summary": "  Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser.\n",
        "published": "2012-05-14T20:12:06Z",
        "pdf_link": "http://arxiv.org/pdf/1205.3183v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.3316v1",
        "title": "Arabic Language Learning Assisted by Computer, based on Automatic Speech\n  Recognition",
        "summary": "  This work consists of creating a system of the Computer Assisted Language\nLearning (CALL) based on a system of Automatic Speech Recognition (ASR) for the\nArabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.\nTo this work, we have constructed a corpus of six hours of speech recordings\nwith a number of nine speakers. we find in the robustness to noise a grounds\nfor the choice of the HMM approach [2]. the results achieved are encouraging\nsince our corpus is made by only nine speakers, but they are always reasons\nthat open the door for other improvement works.\n",
        "published": "2012-05-15T10:34:05Z",
        "pdf_link": "http://arxiv.org/pdf/1205.3316v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.4298v1",
        "title": "Task-specific Word-Clustering for Part-of-Speech Tagging",
        "summary": "  While the use of cluster features became ubiquitous in core NLP tasks, most\ncluster features in NLP are based on distributional similarity. We propose a\nnew type of clustering criteria, specific to the task of part-of-speech\ntagging. Instead of distributional similarity, these clusters are based on the\nbeha vior of a baseline tagger when applied to a large corpus. These cluster\nfeatures provide similar gains in accuracy to those achieved by\ndistributional-similarity derived clusters. Using both types of cluster\nfeatures together further improve tagging accuracies. We show that the method\nis effective for both the in-domain and out-of-domain scenarios for English,\nand for French, German and Italian. The effect is larger for out-of-domain\ntext.\n",
        "published": "2012-05-19T05:04:31Z",
        "pdf_link": "http://arxiv.org/pdf/1205.4298v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.4387v1",
        "title": "Precision-biased Parsing and High-Quality Parse Selection",
        "summary": "  We introduce precision-biased parsing: a parsing task which favors precision\nover recall by allowing the parser to abstain from decisions deemed uncertain.\nWe focus on dependency-parsing and present an ensemble method which is capable\nof assigning parents to 84% of the text tokens while being over 96% accurate on\nthese tokens. We use the precision-biased parsing task to solve the related\nhigh-quality parse-selection task: finding a subset of high-quality (accurate)\ntrees in a large collection of parsed text. We present a method for choosing\nover a third of the input trees while keeping unlabeled dependency parsing\naccuracy of 97% on these trees. We also present a method which is not based on\nan ensemble but rather on directly predicting the risk associated with\nindividual parser decisions. In addition to its efficiency, this method\ndemonstrates that a parsing system can provide reasonable estimates of\nconfidence in its predictions without relying on ensembles or aggregate corpus\ncounts.\n",
        "published": "2012-05-20T06:36:19Z",
        "pdf_link": "http://arxiv.org/pdf/1205.4387v1"
    },
    {
        "id": "http://arxiv.org/abs/1205.5407v2",
        "title": "FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely\n  Lexical Substitutes Based on an N-gram Language Model",
        "summary": "  Lexical substitutes have found use in areas such as paraphrasing, text\nsimplification, machine translation, word sense disambiguation, and part of\nspeech induction. However the computational complexity of accurately\nidentifying the most likely substitutes for a word has made large scale\nexperiments difficult. In this paper I introduce a new search algorithm,\nFASTSUBS, that is guaranteed to find the K most likely lexical substitutes for\na given word in a sentence based on an n-gram language model. The computation\nis sub-linear in both K and the vocabulary size V. An implementation of the\nalgorithm and a dataset with the top 100 substitutes of each token in the WSJ\nsection of the Penn Treebank are available at http://goo.gl/jzKH0.\n",
        "published": "2012-05-24T11:53:41Z",
        "pdf_link": "http://arxiv.org/pdf/1205.5407v2"
    },
    {
        "id": "http://arxiv.org/abs/1205.6832v1",
        "title": "Systme d'aide  l'accs lexical : trouver le mot qu'on a sur le\n  bout de la langue",
        "summary": "  The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues\nand insights concerning the organisation of the mental lexicon (meaning, number\nof syllables, relation with other words, etc.). This paper describes a tool\nbased on psycho-linguistic observations concerning the TOT phenomenon. We've\nbuilt it to enable a speaker/writer to find the word he is looking for, word he\nmay know, but which he is unable to access in time. We try to simulate the TOT\nphenomenon by creating a situation where the system knows the target word, yet\nis unable to access it. In order to find the target word we make use of the\nparadigmatic and syntagmatic associations stored in the linguistic databases.\nOur experiment allows the following conclusion: a tool like SVETLAN, capable to\nstructure (automatically) a dictionary by domains can be used sucessfully to\nhelp the speaker/writer to find the word he is looking for, if it is combined\nwith a database rich in terms of paradigmatic links like EuroWordNet.\n",
        "published": "2012-01-20T17:33:47Z",
        "pdf_link": "http://arxiv.org/pdf/1205.6832v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.0245v2",
        "title": "Adversarial Evaluation for Models of Natural Language",
        "summary": "  We now have a rich and growing set of modeling tools and algorithms for\ninducing linguistic structure from text that is less than fully annotated. In\nthis paper, we discuss some of the weaknesses of our current methodology. We\npresent a new abstract framework for evaluating natural language processing\n(NLP) models in general and unsupervised NLP models in particular. The central\nidea is to make explicit certain adversarial roles among researchers, so that\nthe different roles in an evaluation are more clearly defined and performers of\nall roles are offered ways to make measurable contributions to the larger goal.\nAdopting this approach may help to characterize model successes and failures by\nencouraging earlier consideration of error analysis. The framework can be\ninstantiated in a variety of ways, simulating some familiar intrinsic and\nextrinsic evaluations as well as some new evaluations.\n",
        "published": "2012-07-01T21:13:05Z",
        "pdf_link": "http://arxiv.org/pdf/1207.0245v2"
    },
    {
        "id": "http://arxiv.org/abs/1207.1420v1",
        "title": "Learning to Map Sentences to Logical Form: Structured Classification\n  with Probabilistic Categorial Grammars",
        "summary": "  This paper addresses the problem of mapping natural language sentences to\nlambda-calculus encodings of their meaning. We describe a learning algorithm\nthat takes as input a training set of sentences labeled with expressions in the\nlambda calculus. The algorithm induces a grammar for the problem, along with a\nlog-linear model that represents a distribution over syntactic and semantic\nanalyses conditioned on the input sentence. We apply the method to the task of\nlearning natural language interfaces to databases and show that the learned\nparsers outperform previous methods in two benchmark database domains.\n",
        "published": "2012-07-04T16:27:56Z",
        "pdf_link": "http://arxiv.org/pdf/1207.1420v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.2714v1",
        "title": "Clustering based approach extracting collocations",
        "summary": "  The following study presents a collocation extraction approach based on\nclustering technique. This study uses a combination of several classical\nmeasures which cover all aspects of a given corpus then it suggests separating\nbigrams found in the corpus in several disjoint groups according to the\nprobability of presence of collocations. This will allow excluding groups where\nthe presence of collocations is very unlikely and thus reducing in a meaningful\nway the search space.\n",
        "published": "2012-07-11T17:31:11Z",
        "pdf_link": "http://arxiv.org/pdf/1207.2714v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.3932v1",
        "title": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units",
        "summary": "  The work of automatic segmentation of a Manipuri language (or Meiteilon) word\ninto syllabic units is demonstrated in this paper. This language is a scheduled\nIndian language of Tibeto-Burman origin, which is also a very highly\nagglutinative language. This language usages two script: a Bengali script and\nMeitei Mayek (Script). The present work is based on the second script. An\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\nkind for this language.\n",
        "published": "2012-07-17T10:14:24Z",
        "pdf_link": "http://arxiv.org/pdf/1207.3932v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.4625v1",
        "title": "Appropriate Nouns with Obligatory Modifiers",
        "summary": "  The notion of appropriate sequence as introduced by Z. Harris provides a\npowerful syntactic way of analysing the detailed meaning of various sentences,\nincluding ambiguous ones. In an adjectival sentence like 'The leather was\nyellow', the introduction of an appropriate noun, here 'colour', specifies\nwhich quality the adjective describes. In some other adjectival sentences with\nan appropriate noun, that noun plays the same part as 'colour' and seems to be\nrelevant to the description of the adjective. These appropriate nouns can\nusually be used in elementary sentences like 'The leather had some colour', but\nin many cases they have a more or less obligatory modifier. For example, you\ncan hardly mention that an object has a colour without qualifying that colour\nat all. About 300 French nouns are appropriate in at least one adjectival\nsentence and have an obligatory modifier. They enter in a number of sentence\nstructures related by several syntactic transformations. The appropriateness of\nthe noun and the fact that the modifier is obligatory are reflected in these\ntransformations. The description of these syntactic phenomena provides a basis\nfor a classification of these nouns. It also concerns the lexical properties of\nthousands of predicative adjectives, and in particular the relations between\nthe sentence without the noun : 'The leather was yellow' and the adjectival\nsentence with the noun : 'The colour of the leather was yellow'.\n",
        "published": "2012-07-19T12:04:26Z",
        "pdf_link": "http://arxiv.org/pdf/1207.4625v1"
    },
    {
        "id": "http://arxiv.org/abs/1207.5328v3",
        "title": "A prototype for projecting HPSG syntactic lexica towards LMF",
        "summary": "  The comparative evaluation of Arabic HPSG grammar lexica requires a deep\nstudy of their linguistic coverage. The complexity of this task results mainly\nfrom the heterogeneity of the descriptive components within those lexica\n(underlying linguistic resources and different data categories, for example).\nIt is therefore essential to define more homogeneous representations, which in\nturn will enable us to compare them and eventually merge them. In this context,\nwe present a method for comparing HPSG lexica based on a rule system. This\nmethod is implemented within a prototype for the projection from Arabic HPSG to\na normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup\nFramework) and serialised using a TEI (Text Encoding Initiative) based\nrepresentation. The design of this system is based on an initial study of the\nHPSG formalism looking at its adequacy for the representation of Arabic, and\nfrom this, we identify the appropriate feature structures corresponding to each\nArabic lexical category and their possible LMF counterparts.\n",
        "published": "2012-07-23T09:02:18Z",
        "pdf_link": "http://arxiv.org/pdf/1207.5328v3"
    },
    {
        "id": "http://arxiv.org/abs/1208.0200v1",
        "title": "Adaptation of pedagogical resources description standard (LOM) with the\n  specificity of Arabic language",
        "summary": "  In this article we focus firstly on the principle of pedagogical indexing and\ncharacteristics of Arabic language and secondly on the possibility of adapting\nthe standard for describing learning resources used (the LOM and its\nApplication Profiles) with learning conditions such as the educational levels\nof students and their levels of understanding,... the educational context with\ntaking into account the representative elements of text, text length, ... in\nparticular, we put in relief the specificity of the Arabic language which is a\ncomplex language, characterized by its flexion, its voyellation and\nagglutination.\n",
        "published": "2012-08-01T13:06:54Z",
        "pdf_link": "http://arxiv.org/pdf/1208.0200v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.2777v1",
        "title": "A Method for Selecting Noun Sense using Co-occurrence Relation in\n  English-Korean Translation",
        "summary": "  The sense analysis is still critical problem in machine translation system,\nespecially such as English-Korean translation which the syntactical different\nbetween source and target languages is very great. We suggest a method for\nselecting the noun sense using contextual feature in English-Korean\nTranslation.\n",
        "published": "2012-08-14T03:25:33Z",
        "pdf_link": "http://arxiv.org/pdf/1208.2777v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.3001v1",
        "title": "More than Word Frequencies: Authorship Attribution via Natural Frequency\n  Zoned Word Distribution Analysis",
        "summary": "  With such increasing popularity and availability of digital text data,\nauthorships of digital texts can not be taken for granted due to the ease of\ncopying and parsing. This paper presents a new text style analysis called\nnatural frequency zoned word distribution analysis (NFZ-WDA), and then a basic\nauthorship attribution scheme and an open authorship attribution scheme for\ndigital texts based on the analysis. NFZ-WDA is based on the observation that\nall authors leave distinct intrinsic word usage traces on texts written by them\nand these intrinsic styles can be identified and employed to analyze the\nauthorship. The intrinsic word usage styles can be estimated through the\nanalysis of word distribution within a text, which is more than normal word\nfrequency analysis and can be expressed as: which groups of words are used in\nthe text; how frequently does each group of words occur; how are the\noccurrences of each group of words distributed in the text. Next, the basic\nauthorship attribution scheme and the open authorship attribution scheme\nprovide solutions for both closed and open authorship attribution problems.\nThrough analysis and extensive experimental studies, this paper demonstrates\nthe efficiency of the proposed method for authorship attribution.\n",
        "published": "2012-08-15T00:53:39Z",
        "pdf_link": "http://arxiv.org/pdf/1208.3001v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.4079v1",
        "title": "Recent Technological Advances in Natural Language Processing and\n  Artificial Intelligence",
        "summary": "  A recent advance in computer technology has permitted scientists to implement\nand test algorithms that were known from quite some time (or not) but which\nwere computationally expensive. Two such projects are IBM's Jeopardy as a part\nof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods\nimplement natural language processing (another goal of AI scientists) and try\nto answer questions as asked by the user. Though the goal of the two projects\nis similar, both of them have a different procedure at it's core. In the\nfollowing sections, the mechanism and history of IBM's Jeopardy and Wolfram\nalpha has been explained followed by the implications of these projects in\nrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe\nof taking the above projects to a new level is also explained.\n",
        "published": "2012-08-20T18:34:27Z",
        "pdf_link": "http://arxiv.org/pdf/1208.4079v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.4503v1",
        "title": "Introduction of the weight edition errors in the Levenshtein distance",
        "summary": "  In this paper, we present a new approach dedicated to correcting the spelling\nerrors of the Arabic language. This approach corrects typographical errors like\ninserting, deleting, and permutation. Our method is inspired from the\nLevenshtein algorithm, and allows a finer and better scheduling than\nLevenshtein. The results obtained are very satisfactory and encouraging, which\nshows the interest of our new approach.\n",
        "published": "2012-08-22T14:11:07Z",
        "pdf_link": "http://arxiv.org/pdf/1208.4503v1"
    },
    {
        "id": "http://arxiv.org/abs/1208.6109v1",
        "title": "Average word length dynamics as indicator of cultural changes in society",
        "summary": "  Dynamics of average length of words in Russian and English is analysed in the\narticle. Words belonging to the diachronic text corpus Google Books Ngram and\ndated back to the last two centuries are studied. It was found out that average\nword length slightly increased in the 19th century, and then it was growing\nrapidly most of the 20th century and started decreasing over the period from\nthe end of the 20th - to the beginning of the 21th century. Words which\ncontributed mostly to increase or decrease of word average length were\nidentified. At that, content words and functional words are analysed\nseparately. Long content words contribute mostly to word average length of\nword. As it was shown, these words reflect the main tendencies of social\ndevelopment and thus, are used frequently. Change of frequency of personal\npronouns also contributes significantly to change of average word length. The\nother parameters connected with average length of word were also analysed.\n",
        "published": "2012-08-30T08:30:32Z",
        "pdf_link": "http://arxiv.org/pdf/1208.6109v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.1300v1",
        "title": "Input Scheme for Hindi Using Phonetic Mapping",
        "summary": "  Written Communication on Computers requires knowledge of writing text for the\ndesired language using Computer. Mostly people do not use any other language\nbesides English. This creates a barrier. To resolve this issue we have\ndeveloped a scheme to input text in Hindi using phonetic mapping scheme. Using\nthis scheme we generate intermediate code strings and match them with\npronunciations of input text. Our system show significant success over other\ninput systems available.\n",
        "published": "2012-08-19T02:38:12Z",
        "pdf_link": "http://arxiv.org/pdf/1209.1300v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.1301v1",
        "title": "Evaluation of Computational Grammar Formalisms for Indian Languages",
        "summary": "  Natural Language Parsing has been the most prominent research area since the\ngenesis of Natural Language Processing. Probabilistic Parsers are being\ndeveloped to make the process of parser development much easier, accurate and\nfast. In Indian context, identification of which Computational Grammar\nFormalism is to be used is still a question which needs to be answered. In this\npaper we focus on this problem and try to analyze different formalisms for\nIndian languages.\n",
        "published": "2012-08-19T02:31:29Z",
        "pdf_link": "http://arxiv.org/pdf/1209.1301v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.2400v1",
        "title": "Identification of Fertile Translations in Medical Comparable Corpora: a\n  Morpho-Compositional Approach",
        "summary": "  This paper defines a method for lexicon in the biomedical domain from\ncomparable corpora. The method is based on compositional translation and\nexploits morpheme-level translation equivalences. It can generate translations\nfor a large variety of morphologically constructed words and can also generate\n'fertile' translations. We show that fertile translations increase the overall\nquality of the extracted lexicon for English to French translation.\n",
        "published": "2012-09-11T19:18:26Z",
        "pdf_link": "http://arxiv.org/pdf/1209.2400v1"
    },
    {
        "id": "http://arxiv.org/abs/1209.6238v1",
        "title": "Natural Language Processing - A Survey",
        "summary": "  The utility and power of Natural Language Processing (NLP) seems destined to\nchange our technological society in profound and fundamental ways. However\nthere are, to date, few accessible descriptions of the science of NLP that have\nbeen written for a popular audience, or even for an audience of intelligent,\nbut uninitiated scientists. This paper aims to provide just such an overview.\nIn short, the objective of this article is to describe the purpose, procedures\nand practical applications of NLP in a clear, balanced, and readable way. We\nwill examine the most recent literature describing the methods and processes of\nNLP, analyze some of the challenges that researchers are faced with, and\nbriefly survey some of the current and future applications of this science to\nIT research in general.\n",
        "published": "2012-09-25T21:05:08Z",
        "pdf_link": "http://arxiv.org/pdf/1209.6238v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.4567v2",
        "title": "Gender identity and lexical variation in social media",
        "summary": "  We present a study of the relationship between gender, linguistic style, and\nsocial networks, using a novel corpus of 14,000 Twitter users. Prior\nquantitative work on gender often treats this social variable as a female/male\nbinary; we argue for a more nuanced approach. By clustering Twitter users, we\nfind a natural decomposition of the dataset into various styles and topical\ninterests. Many clusters have strong gender orientations, but their use of\nlinguistic resources sometimes directly conflicts with the population-level\nlanguage statistics. We view these clusters as a more accurate reflection of\nthe multifaceted nature of gendered language styles. Previous corpus-based work\nhas also had little to say about individuals whose linguistic styles defy\npopulation-level gender patterns. To identify such individuals, we train a\nstatistical classifier, and measure the classifier confidence for each\nindividual in the dataset. Examining individuals whose language does not match\nthe classifier's model for their gender, we find that they have social networks\nthat include significantly fewer same-gender social connections and that, in\ngeneral, social network homophily is correlated with the use of same-gender\nlanguage markers. Pairing computational methods and social theory thus offers a\nnew perspective on how gender emerges as individuals position themselves\nrelative to audiences, topics, and mainstream gender norms.\n",
        "published": "2012-10-16T20:22:56Z",
        "pdf_link": "http://arxiv.org/pdf/1210.4567v2"
    },
    {
        "id": "http://arxiv.org/abs/1210.5486v2",
        "title": "A Lightweight Stemmer for Gujarati",
        "summary": "  Gujarati is a resource poor language with almost no language processing tools\nbeing available. In this paper we have shown an implementation of a rule based\nstemmer of Gujarati. We have shown the creation of rules for stemming and the\nrichness in morphology that Gujarati possesses. We have also evaluated our\nresults by verifying it with a human expert.\n",
        "published": "2012-10-19T17:49:06Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5486v2"
    },
    {
        "id": "http://arxiv.org/abs/1210.5517v1",
        "title": "Design of English-Hindi Translation Memory for Efficient Translation",
        "summary": "  Developing parallel corpora is an important and a difficult activity for\nMachine Translation. This requires manual annotation by Human Translators.\nTranslating same text again is a useless activity. There are tools available to\nimplement this for European Languages, but no such tool is available for Indian\nLanguages. In this paper we present a tool for Indian Languages which not only\nprovides automatic translations of the previously available translation but\nalso provides multiple translations, in cases where a sentence has multiple\ntranslations, in ranked list of suggestive translations for a sentence.\nMoreover this tool also lets translators have global and local saving options\nof their work, so that they may share it with others, which further lightens\nthe task.\n",
        "published": "2012-10-19T17:59:56Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5517v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.5751v1",
        "title": "Extraction of domain-specific bilingual lexicon from comparable corpora:\n  compositional translation and ranking",
        "summary": "  This paper proposes a method for extracting translations of morphologically\nconstructed terms from comparable corpora. The method is based on compositional\ntranslation and exploits translation equivalences at the morpheme-level, which\nallows for the generation of \"fertile\" translations (translation pairs in which\nthe target term has more words than the source term). Ranking methods relying\non corpus-based and translation-based features are used to select the best\ncandidate translation. We obtain an average precision of 91% on the Top1\ncandidate translation. The method was tested on two language pairs\n(English-French and English-German) and with a small specialized comparable\ncorpora (400k words per language).\n",
        "published": "2012-10-21T19:06:11Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5751v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.5965v1",
        "title": "Classification Analysis Of Authorship Fiction Texts in The Space Of\n  Semantic Fields",
        "summary": "  The use of naive Bayesian classifier (NB) and the classifier by the k nearest\nneighbors (kNN) in classification semantic analysis of authors' texts of\nEnglish fiction has been analysed. The authors' works are considered in the\nvector space the basis of which is formed by the frequency characteristics of\nsemantic fields of nouns and verbs. Highly precise classification of authors'\ntexts in the vector space of semantic fields indicates about the presence of\nparticular spheres of author's idiolect in this space which characterizes the\nindividual author's style.\n",
        "published": "2012-10-22T16:40:35Z",
        "pdf_link": "http://arxiv.org/pdf/1210.5965v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.7282v1",
        "title": "The Hangulphabet: A Descriptive Alphabet",
        "summary": "  This paper describes the Hangulphabet, a new writing system that should prove\nuseful in a number of contexts. Using the Hangulphabet, a user can instantly\nsee voicing, manner and place of articulation of any phoneme found in human\nlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axis\nrepresenting the place of articulation and the y-axis representing manner of\narticulation. Each individual grapheme contains radicals from both axes where\nthe points intersect. The top radical represents manner of articulation where\nthe bottom represents place of articulation. A horizontal line running through\nthe middle of the bottom radical represents voicing. For vowels, place of\narticulation is located on a grid that represents the position of the tongue in\nthe mouth. This grid is similar to that of the IPA vowel chart (International\nPhonetic Association, 1999). The difference with the Hangulphabet being the\ntrapezoid representing the vocal apparatus is on a slight tilt. Place of\narticulation for a vowel is represented by a breakout figure from the grid.\nThis system can be used as an alternative to the International Phonetic\nAlphabet (IPA) or as a complement to it. Beginning students of linguistics may\nfind it particularly useful. A Hangulphabet font has been created to facilitate\nswitching between the Hangulphabet and the IPA.\n",
        "published": "2012-10-27T02:34:35Z",
        "pdf_link": "http://arxiv.org/pdf/1210.7282v1"
    },
    {
        "id": "http://arxiv.org/abs/1210.8440v1",
        "title": "Large Scale Language Modeling in Automatic Speech Recognition",
        "summary": "  Large language models have been proven quite beneficial for a variety of\nautomatic speech recognition tasks in Google. We summarize results on Voice\nSearch and a few YouTube speech transcription tasks to highlight the impact\nthat one can expect from increasing both the amount of training data, and the\nsize of the language model estimated from such data. Depending on the task,\navailability and amount of training data used, language model size and amount\nof work and care put into integrating them in the lattice rescoring step we\nobserve reductions in word error rate between 6% and 10% relative, for systems\non a wide range of operating points between 17% and 52% word error rate.\n",
        "published": "2012-10-31T18:57:14Z",
        "pdf_link": "http://arxiv.org/pdf/1210.8440v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.0074v1",
        "title": "Transition-Based Dependency Parsing With Pluggable Classifiers",
        "summary": "  In principle, the design of transition-based dependency parsers makes it\npossible to experiment with any general-purpose classifier without other\nchanges to the parsing algorithm. In practice, however, it often takes\nsubstantial software engineering to bridge between the different\nrepresentations used by two software packages. Here we present extensions to\nMaltParser that allow the drop-in use of any classifier conforming to the\ninterface of the Weka machine learning package, a wrapper for the TiMBL\nmemory-based learner to this interface, and experiments on multilingual\ndependency parsing with a variety of classifiers. While earlier work had\nsuggested that memory-based learners might be a good choice for low-resource\nparsing scenarios, we cannot support that hypothesis in this work. We observed\nthat support-vector machines give better parsing performance than the\nmemory-based learner, regardless of the size of the training set.\n",
        "published": "2012-11-01T02:10:06Z",
        "pdf_link": "http://arxiv.org/pdf/1211.0074v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.0498v1",
        "title": "Detecting English Writing Styles For Non-native Speakers",
        "summary": "  Analyzing writing styles of non-native speakers is a challenging task. In\nthis paper, we analyze the comments written in the discussion pages of the\nEnglish Wikipedia. Using learning algorithms, we are able to detect native\nspeakers' writing style with an accuracy of 74%. Given the diversity of the\nEnglish Wikipedia users and the large number of languages they speak, we\nmeasure the similarities among their native languages by comparing the\ninfluence they have on their English writing style. Our results show that\nlanguages known to have the same origin and development path have similar\nfootprint on their speakers' English writing style. To enable further studies,\nthe dataset we extracted from Wikipedia will be made available publicly.\n",
        "published": "2012-11-02T17:37:06Z",
        "pdf_link": "http://arxiv.org/pdf/1211.0498v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.3643v1",
        "title": "A Principled Approach to Grammars for Controlled Natural Languages and\n  Predictive Editors",
        "summary": "  Controlled natural languages (CNL) with a direct mapping to formal logic have\nbeen proposed to improve the usability of knowledge representation systems,\nquery interfaces, and formal specifications. Predictive editors are a popular\napproach to solve the problem that CNLs are easy to read but hard to write.\nSuch predictive editors need to be able to \"look ahead\" in order to show all\npossible continuations of a given unfinished sentence. Such lookahead features,\nhowever, are difficult to implement in a satisfying way with existing grammar\nframeworks, especially if the CNL supports complex nonlocal structures such as\nanaphoric references. Here, methods and algorithms are presented for a new\ngrammar notation called Codeco, which is specifically designed for controlled\nnatural languages and predictive editors. A parsing approach for Codeco based\non an extended chart parsing algorithm is presented. A large subset of Attempto\nControlled English (ACE) has been represented in Codeco. Evaluation of this\ngrammar and the parser implementation shows that the approach is practical,\nadequate and efficient.\n",
        "published": "2012-11-15T16:24:25Z",
        "pdf_link": "http://arxiv.org/pdf/1211.3643v1"
    },
    {
        "id": "http://arxiv.org/abs/1211.4161v1",
        "title": "Semantic Polarity of Adjectival Predicates in Online Reviews",
        "summary": "  Web users produce more and more documents expressing opinions. Because these\nhave become important resources for customers and manufacturers, many have\nfocused on them. Opinions are often expressed through adjectives with positive\nor negative semantic values. In extracting information from users' opinion in\nonline reviews, exact recognition of the semantic polarity of adjectives is one\nof the most important requirements. Since adjectives have different semantic\norientations according to contexts, it is not satisfying to extract opinion\ninformation without considering the semantic and lexical relations between the\nadjectives and the feature nouns appropriate to a given domain. In this paper,\nwe present a classification of adjectives by polarity, and we analyze\nadjectives that are undetermined in the absence of contexts. Our research\nshould be useful for accurately predicting semantic orientations of opinion\nsentences, and should be taken into account before relying on an automatic\nmethods.\n",
        "published": "2012-11-17T20:27:06Z",
        "pdf_link": "http://arxiv.org/pdf/1211.4161v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.1192v2",
        "title": "Using external sources of bilingual information for on-the-fly word\n  alignment",
        "summary": "  In this paper we present a new and simple language-independent method for\nword-alignment based on the use of external sources of bilingual information\nsuch as machine translation systems. We show that the few parameters of the\naligner can be trained on a very small corpus, which leads to results\ncomparable to those obtained by the state-of-the-art tool GIZA++ in terms of\nprecision. Regarding other metrics, such as alignment error rate or F-measure,\nthe parametric aligner, when trained on a very small gold-standard (450 pairs\nof sentences), provides results comparable to those produced by GIZA++ when\ntrained on an in-domain corpus of around 10,000 pairs of sentences.\nFurthermore, the results obtained indicate that the training is\ndomain-independent, which enables the use of the trained aligner 'on the fly'\non any new pair of sentences.\n",
        "published": "2012-12-05T22:10:04Z",
        "pdf_link": "http://arxiv.org/pdf/1212.1192v2"
    },
    {
        "id": "http://arxiv.org/abs/1212.3138v1",
        "title": "Identifying Metaphor Hierarchies in a Corpus Analysis of Finance\n  Articles",
        "summary": "  Using a corpus of over 17,000 financial news reports (involving over 10M\nwords), we perform an analysis of the argument-distributions of the UP- and\nDOWN-verbs used to describe movements of indices, stocks, and shares. Using\nmeasures of the overlap in the argument distributions of these verbs and\nk-means clustering of their distributions, we advance evidence for the proposal\nthat the metaphors referred to by these verbs are organised into hierarchical\nstructures of superordinate and subordinate groups.\n",
        "published": "2012-12-13T11:47:09Z",
        "pdf_link": "http://arxiv.org/pdf/1212.3138v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.3139v2",
        "title": "Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles",
        "summary": "  Using a corpus of 17,000+ financial news reports (involving over 10M words),\nwe perform an analysis of the argument-distributions of the UP and DOWN verbs\nused to describe movements of indices, stocks and shares. In Study 1\nparticipants identified antonyms of these verbs in a free-response task and a\nmatching task from which the most commonly identified antonyms were compiled.\nIn Study 2, we determined whether the argument-distributions for the verbs in\nthese antonym-pairs were sufficiently similar to predict the most\nfrequently-identified antonym. Cosine similarity correlates moderately with the\nproportions of antonym-pairs identified by people (r = 0.31). More\nimpressively, 87% of the time the most frequently-identified antonym is either\nthe first- or second-most similar pair in the set of alternatives. The\nimplications of these results for distributional approaches to determining\nmetaphoric knowledge are discussed.\n",
        "published": "2012-12-13T11:53:25Z",
        "pdf_link": "http://arxiv.org/pdf/1212.3139v2"
    },
    {
        "id": "http://arxiv.org/abs/1212.3162v1",
        "title": "Diachronic Variation in Grammatical Relations",
        "summary": "  We present a method of finding and analyzing shifts in grammatical relations\nfound in diachronic corpora. Inspired by the econometric technique of measuring\nreturn and volatility instead of relative frequencies, we propose them as a way\nto better characterize changes in grammatical patterns like nominalization,\nmodification and comparison. To exemplify the use of these techniques, we\nexamine a corpus of NIPS papers and report trends which manifest at the token,\npart-of-speech and grammatical levels. Building up from frequency observations\nto a second-order analysis, we show that shifts in frequencies overlook deeper\ntrends in language, even when part-of-speech information is included. Examining\ntoken, POS and grammatical levels of variation enables a summary view of\ndiachronic text as a whole. We conclude with a discussion about how these\nmethods can inform intuitions about specialist domains as well as changes in\nlanguage use as a whole.\n",
        "published": "2012-12-13T13:00:55Z",
        "pdf_link": "http://arxiv.org/pdf/1212.3162v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.4315v1",
        "title": "Assessing Sentiment Strength in Words Prior Polarities",
        "summary": "  Many approaches to sentiment analysis rely on lexica where words are tagged\nwith their prior polarity - i.e. if a word out of context evokes something\npositive or something negative. In particular, broad-coverage resources like\nSentiWordNet provide polarities for (almost) every word. Since words can have\nmultiple senses, we address the problem of how to compute the prior polarity of\na word starting from the polarity of each sense and returning its polarity\nstrength as an index between -1 and 1. We compare 14 such formulae that appear\nin the literature, and assess which one best approximates the human judgement\nof prior polarities, with both regression and classification models.\n",
        "published": "2012-12-18T11:33:50Z",
        "pdf_link": "http://arxiv.org/pdf/1212.4315v1"
    },
    {
        "id": "http://arxiv.org/abs/1212.4674v1",
        "title": "Natural Language Understanding Based on Semantic Relations between\n  Sentences",
        "summary": "  In this paper, we define event expression over sentences of natural language\nand semantic relations between events. Based on this definition, we formally\nconsider text understanding process having events as basic unit.\n",
        "published": "2012-12-19T14:40:38Z",
        "pdf_link": "http://arxiv.org/pdf/1212.4674v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.2444v3",
        "title": "TEI and LMF crosswalks",
        "summary": "  The present paper explores various arguments in favour of making the Text\nEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO\nstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the\nissues that would have to be resolved in order to reach an appropriate\nimplementation of these ideas, in particular in terms of infor-mational\ncoverage. We show how the customisation facilities offered by the TEI\nguidelines can provide an adequate background, not only to cover missing\ncomponents within the current Dictionary chapter of the TEI guidelines, but\nalso to allow specific lexical projects to deal with local constraints. We\nexpect this proposal to be a basis for a future ISO project in the context of\nthe on going revision of LMF.\n",
        "published": "2013-01-11T10:38:09Z",
        "pdf_link": "http://arxiv.org/pdf/1301.2444v3"
    },
    {
        "id": "http://arxiv.org/abs/1301.2857v1",
        "title": "SpeedRead: A Fast Named Entity Recognition Pipeline",
        "summary": "  Online content analysis employs algorithmic methods to identify entities in\nunstructured text. Both machine learning and knowledge-base approaches lie at\nthe foundation of contemporary named entities extraction systems. However, the\nprogress in deploying these approaches on web-scale has been been hampered by\nthe computational cost of NLP over massive text corpora. We present SpeedRead\n(SR), a named entity recognition pipeline that runs at least 10 times faster\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\ntagger and knowledge-based named entity recognizer.\n",
        "published": "2013-01-14T04:01:25Z",
        "pdf_link": "http://arxiv.org/pdf/1301.2857v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.3214v1",
        "title": "The Manifold of Human Emotions",
        "summary": "  Sentiment analysis predicts the presence of positive or negative emotions in\na text document. In this paper, we consider higher dimensional extensions of\nthe sentiment concept, which represent a richer set of human emotions. Our\napproach goes beyond previous work in that our model contains a continuous\nmanifold rather than a finite set of human emotions. We investigate the\nresulting model, compare it to psychological observations, and explore its\npredictive capabilities.\n",
        "published": "2013-01-15T03:45:27Z",
        "pdf_link": "http://arxiv.org/pdf/1301.3214v1"
    },
    {
        "id": "http://arxiv.org/abs/1301.3614v3",
        "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\n  Translation",
        "summary": "  A neural probabilistic language model (NPLM) provides an idea to achieve the\nbetter perplexity than n-gram language model and their smoothed language\nmodels. This paper investigates application area in bilingual NLP, specifically\nStatistical Machine Translation (SMT). We focus on the perspectives that NPLM\nhas potential to open the possibility to complement potentially `huge'\nmonolingual resources into the `resource-constraint' bilingual resources. We\nintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesian\nconstruction. In order to facilitate the application to various tasks, we\npropose the joint space model of ngram-HMM language model. We show an\nexperiment of system combination in the area of SMT. One discovery was that our\ntreatment of noise improved the results 0.20 BLEU points if NPLM is trained in\nrelatively small corpus, in our case 500,000 sentence pairs, which is often the\ncase due to the long training time of NPLM.\n",
        "published": "2013-01-16T07:56:20Z",
        "pdf_link": "http://arxiv.org/pdf/1301.3614v3"
    },
    {
        "id": "http://arxiv.org/abs/1301.3781v3",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "summary": "  We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.\n",
        "published": "2013-01-16T18:24:43Z",
        "pdf_link": "http://arxiv.org/pdf/1301.3781v3"
    },
    {
        "id": "http://arxiv.org/abs/1301.4432v1",
        "title": "Language learning from positive evidence, reconsidered: A\n  simplicity-based approach",
        "summary": "  Children learn their native language by exposure to their linguistic and\ncommunicative environment, but apparently without requiring that their mistakes\nare corrected. Such learning from positive evidence has been viewed as raising\nlogical problems for language acquisition. In particular, without correction,\nhow is the child to recover from conjecturing an over-general grammar, which\nwill be consistent with any sentence that the child hears? There have been many\nproposals concerning how this logical problem can be dissolved. Here, we review\nrecent formal results showing that the learner has sufficient data to learn\nsuccessfully from positive evidence, if it favours the simplest encoding of the\nlinguistic input. Results include the ability to learn a linguistic prediction,\ngrammaticality judgements, language production, and form-meaning mappings. The\nsimplicity approach can also be scaled-down to analyse the ability to learn a\nspecific linguistic constructions, and is amenable to empirical test as a\nframework for describing human language acquisition.\n",
        "published": "2013-01-18T16:53:13Z",
        "pdf_link": "http://arxiv.org/pdf/1301.4432v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.1123v1",
        "title": "Large Scale Distributed Acoustic Modeling With Back-off N-grams",
        "summary": "  The paper revives an older approach to acoustic modeling that borrows from\nn-gram language modeling in an attempt to scale up both the amount of training\ndata and model size (as measured by the number of parameters in the model), to\napproximately 100 times larger than current sizes used in automatic speech\nrecognition. In such a data-rich setting, we can expand the phonetic context\nsignificantly beyond triphones, as well as increase the number of Gaussian\nmixture components for the context-dependent states that allow it. We have\nexperimented with contexts that span seven or more context-independent phones,\nand up to 620 mixture components per state. Dealing with unseen phonetic\ncontexts is accomplished using the familiar back-off technique used in language\nmodeling due to implementation simplicity. The back-off acoustic model is\nestimated, stored and served using MapReduce distributed computing\ninfrastructure.\n  Speech recognition experiments are carried out in an N-best list rescoring\nframework for Google Voice Search. Training big models on large amounts of data\nproves to be an effective way to increase the accuracy of a state-of-the-art\nautomatic speech recognition system. We use 87,000 hours of training data\n(speech along with transcription) obtained by filtering utterances in Voice\nSearch logs on automatic speech recognition confidence. Models ranging in size\nbetween 20--40 million Gaussians are estimated using maximum likelihood\ntraining. They achieve relative reductions in word-error-rate of 11% and 6%\nwhen combined with first-pass models trained using maximum likelihood, and\nboosted maximum mutual information, respectively. Increasing the context size\nbeyond five phones (quinphones) does not help.\n",
        "published": "2013-02-05T17:09:49Z",
        "pdf_link": "http://arxiv.org/pdf/1302.1123v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.1380v1",
        "title": "Towards the Rapid Development of a Natural Language Understanding Module",
        "summary": "  When developing a conversational agent, there is often an urgent need to have\na prototype available in order to test the application with real users. A\nWizard of Oz is a possibility, but sometimes the agent should be simply\ndeployed in the environment where it will be used. Here, the agent should be\nable to capture as many interactions as possible and to understand how people\nreact to failure. In this paper, we focus on the rapid development of a natural\nlanguage understanding module by non experts. Our approach follows the learning\nparadigm and sees the process of understanding natural language as a\nclassification problem. We test our module with a conversational agent that\nanswers questions in the art domain. Moreover, we show how our approach can be\nused by a natural language interface to a cinema database.\n",
        "published": "2013-02-06T14:17:55Z",
        "pdf_link": "http://arxiv.org/pdf/1302.1380v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.1422v2",
        "title": "Smantique des dterminants dans un cadre richement typ",
        "summary": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n",
        "published": "2013-02-06T16:26:49Z",
        "pdf_link": "http://arxiv.org/pdf/1302.1422v2"
    },
    {
        "id": "http://arxiv.org/abs/1302.1572v1",
        "title": "Lexical Access for Speech Understanding using Minimum Message Length\n  Encoding",
        "summary": "  The Lexical Access Problem consists of determining the intended sequence of\nwords corresponding to an input sequence of phonemes (basic speech sounds) that\ncome from a low-level phoneme recognizer. In this paper we present an\ninformation-theoretic approach based on the Minimum Message Length Criterion\nfor solving the Lexical Access Problem. We model sentences using phoneme\nrealizations seen in training, and word and part-of-speech information obtained\nfrom text corpora. We show results on multiple-speaker, continuous, read speech\nand discuss a heuristic using equivalence classes of similar sounding words\nwhich speeds up the recognition process without significant deterioration in\nrecognition accuracy.\n",
        "published": "2013-02-06T15:59:24Z",
        "pdf_link": "http://arxiv.org/pdf/1302.1572v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.3057v1",
        "title": "Building a reordering system using tree-to-string hierarchical model",
        "summary": "  This paper describes our submission to the First Workshop on Reordering for\nStatistical Machine Translation. We have decided to build a reordering system\nbased on tree-to-string model, using only publicly available tools to\naccomplish this task. With the provided training data we have built a\ntranslation model using Moses toolkit, and then we applied a chart decoder,\nimplemented in Moses, to reorder the sentences. Even though our submission only\ncovered English-Farsi language pair, we believe that the approach itself should\nwork regardless of the choice of the languages, so we have also carried out the\nexperiments for English-Italian and English-Urdu. For these language pairs we\nhave noticed a significant improvement over the baseline in BLEU, Kendall-Tau\nand Hamming metrics. A detailed description is given, so that everyone can\nreproduce our results. Also, some possible directions for further improvements\nare discussed.\n",
        "published": "2013-02-13T11:54:56Z",
        "pdf_link": "http://arxiv.org/pdf/1302.3057v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.4489v1",
        "title": "Termhood-based Comparability Metrics of Comparable Corpus in Special\n  Domain",
        "summary": "  Cross-Language Information Retrieval (CLIR) and machine translation (MT)\nresources, such as dictionaries and parallel corpora, are scarce and hard to\ncome by for special domains. Besides, these resources are just limited to a few\nlanguages, such as English, French, and Spanish and so on. So, obtaining\ncomparable corpora automatically for such domains could be an answer to this\nproblem effectively. Comparable corpora, that the subcorpora are not\ntranslations of each other, can be easily obtained from web. Therefore,\nbuilding and using comparable corpora is often a more feasible option in\nmultilingual information processing. Comparability metrics is one of key issues\nin the field of building and using comparable corpus. Currently, there is no\nwidely accepted definition or metrics method of corpus comparability. In fact,\nDifferent definitions or metrics methods of comparability might be given to\nsuit various tasks about natural language processing. A new comparability,\nnamely, termhood-based metrics, oriented to the task of bilingual terminology\nextraction, is proposed in this paper. In this method, words are ranked by\ntermhood not frequency, and then the cosine similarities, calculated based on\nthe ranking lists of word termhood, is used as comparability. Experiments\nresults show that termhood-based metrics performs better than traditional\nfrequency-based metrics.\n",
        "published": "2013-02-19T00:30:57Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4489v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.4492v1",
        "title": "Bilingual Terminology Extraction Using Multi-level Termhood",
        "summary": "  Purpose: Terminology is the set of technical words or expressions used in\nspecific contexts, which denotes the core concept in a formal discipline and is\nusually applied in the fields of machine translation, information retrieval,\ninformation extraction and text categorization, etc. Bilingual terminology\nextraction plays an important role in the application of bilingual dictionary\ncompilation, bilingual Ontology construction, machine translation and\ncross-language information retrieval etc. This paper addresses the issues of\nmonolingual terminology extraction and bilingual term alignment based on\nmulti-level termhood.\n  Design/methodology/approach: A method based on multi-level termhood is\nproposed. The new method computes the termhood of the terminology candidate as\nwell as the sentence that includes the terminology by the comparison of the\ncorpus. Since terminologies and general words usually have differently\ndistribution in the corpus, termhood can also be used to constrain and enhance\nthe performance of term alignment when aligning bilingual terms on the parallel\ncorpus. In this paper, bilingual term alignment based on termhood constraints\nis presented.\n  Findings: Experiment results show multi-level termhood can get better\nperformance than existing method for terminology extraction. If termhood is\nused as constrain factor, the performance of bilingual term alignment can be\nimproved.\n",
        "published": "2013-02-19T00:37:21Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4492v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.4811v1",
        "title": "Towards a Semantic-based Approach for Modeling Regulatory Documents in\n  Building Industry",
        "summary": "  Regulations in the Building Industry are becoming increasingly complex and\ninvolve more than one technical area. They cover products, components and\nproject implementation. They also play an important role to ensure the quality\nof a building, and to minimize its environmental impact. In this paper, we are\nparticularly interested in the modeling of the regulatory constraints derived\nfrom the Technical Guides issued by CSTB and used to validate Technical\nAssessments. We first describe our approach for modeling regulatory constraints\nin the SBVR language, and formalizing them in the SPARQL language. Second, we\ndescribe how we model the processes of compliance checking described in the\nCSTB Technical Guides. Third, we show how we implement these processes to\nassist industrials in drafting Technical Documents in order to acquire a\nTechnical Assessment; a compliance report is automatically generated to explain\nthe compliance or noncompliance of this Technical Documents.\n",
        "published": "2013-02-20T05:46:53Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4811v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.4813v1",
        "title": "Probabilistic Frame Induction",
        "summary": "  In natural-language discourse, related events tend to appear near each other\nto describe a larger scenario. Such structures can be formalized by the notion\nof a frame (a.k.a. template), which comprises a set of related events and\nprototypical participants and event transitions. Identifying frames is a\nprerequisite for information extraction and natural language generation, and is\nusually done manually. Methods for inducing frames have been proposed recently,\nbut they typically use ad hoc procedures and are difficult to diagnose or\nextend. In this paper, we propose the first probabilistic approach to frame\ninduction, which incorporates frames, events, participants as latent topics and\nlearns those frame and event transitions that best explain the text. The number\nof frames is inferred by a novel application of a split-merge method from\nsyntactic parsing. In end-to-end evaluations from text to induced frames and\nextracted facts, our method produced state-of-the-art results while\nsubstantially reducing engineering effort.\n",
        "published": "2013-02-20T05:47:32Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4813v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.4814v1",
        "title": "NLP and CALL: integration is working",
        "summary": "  In the first part of this article, we explore the background of\ncomputer-assisted learning from its beginnings in the early XIXth century and\nthe first teaching machines, founded on theories of learning, at the start of\nthe XXth century. With the arrival of the computer, it became possible to offer\nlanguage learners different types of language activities such as comprehension\ntasks, simulations, etc. However, these have limits that cannot be overcome\nwithout some contribution from the field of natural language processing (NLP).\nIn what follows, we examine the challenges faced and the issues raised by\nintegrating NLP into CALL. We hope to demonstrate that the key to success in\nintegrating NLP into CALL is to be found in multidisciplinary work between\ncomputer experts, linguists, language teachers, didacticians and NLP\nspecialists.\n",
        "published": "2013-02-20T05:47:44Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4814v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.5645v1",
        "title": "Role of temporal inference in the recognition of textual inference",
        "summary": "  This project is a part of nature language processing and its aims to develop\na system of recognition inference text-appointed TIMINF. This type of system\ncan detect, given two portions of text, if a text is semantically deducted from\nthe other. We focused on making the inference time in this type of system. For\nthat we have built and analyzed a body built from questions collected through\nthe web. This study has enabled us to classify different types of times\ninferences and for designing the architecture of TIMINF which seeks to\nintegrate a module inference time in a detection system inference text. We also\nassess the performance of sorties TIMINF system on a test corpus with the same\nstrategy adopted in the challenge RTE.\n",
        "published": "2013-02-18T15:28:51Z",
        "pdf_link": "http://arxiv.org/pdf/1302.5645v1"
    },
    {
        "id": "http://arxiv.org/abs/1302.6777v1",
        "title": "Ending-based Strategies for Part-of-speech Tagging",
        "summary": "  Probabilistic approaches to part-of-speech tagging rely primarily on\nwhole-word statistics about word/tag combinations as well as contextual\ninformation. But experience shows about 4 per cent of tokens encountered in\ntest sets are unknown even when the training set is as large as a million\nwords. Unseen words are tagged using secondary strategies that exploit word\nfeatures such as endings, capitalizations and punctuation marks. In this work,\nword-ending statistics are primary and whole-word statistics are secondary.\nFirst, a tagger was trained and tested on word endings only. Subsequent\nexperiments added back whole-word statistics for the words occurring most\nfrequently in the training set. As grew larger, performance was expected to\nimprove, in the limit performing the same as word-based taggers. Surprisingly,\nthe ending-based tagger initially performed nearly as well as the word-based\ntagger; in the best case, its performance significantly exceeded that of the\nword-based tagger. Lastly, and unexpectedly, an effect of negative returns was\nobserved - as grew larger, performance generally improved and then declined. By\nvarying factors such as ending length and tag-list strategy, we achieved a\nsuccess rate of 97.5 percent.\n",
        "published": "2013-02-27T14:13:10Z",
        "pdf_link": "http://arxiv.org/pdf/1302.6777v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.0446v1",
        "title": "Statistical sentiment analysis performance in Opinum",
        "summary": "  The classification of opinion texts in positive and negative is becoming a\nsubject of great interest in sentiment analysis. The existence of many labeled\nopinions motivates the use of statistical and machine-learning methods.\nFirst-order statistics have proven to be very limited in this field. The Opinum\napproach is based on the order of the words without using any syntactic and\nsemantic information. It consists of building one probabilistic model for the\npositive and another one for the negative opinions. Then the test opinions are\ncompared to both models and a decision and confidence measure are calculated.\nIn order to reduce the complexity of the training corpus we first lemmatize the\ntexts and we replace most named-entities with wildcards. Opinum presents an\naccuracy above 81% for Spanish opinions in the financial products domain. In\nthis work we discuss which are the most important factors that have impact on\nthe classification performance.\n",
        "published": "2013-03-03T01:38:03Z",
        "pdf_link": "http://arxiv.org/pdf/1303.0446v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.1929v1",
        "title": "Towards the Fully Automatic Merging of Lexical Resources: A Step Forward",
        "summary": "  This article reports on the results of the research done towards the fully\nautomatically merging of lexical resources. Our main goal is to show the\ngenerality of the proposed approach, which have been previously applied to\nmerge Spanish Subcategorization Frames lexica. In this work we extend and apply\nthe same technique to perform the merging of morphosyntactic lexica encoded in\nLMF. The experiments showed that the technique is general enough to obtain good\nresults in these two different tasks which is an important step towards\nperforming the merging of lexical resources fully automatically.\n",
        "published": "2013-03-08T10:13:56Z",
        "pdf_link": "http://arxiv.org/pdf/1303.1929v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.1930v1",
        "title": "Automatic lexical semantic classification of nouns",
        "summary": "  The work we present here addresses cue-based noun classification in English\nand Spanish. Its main objective is to automatically acquire lexical semantic\ninformation by classifying nouns into previously known noun lexical classes.\nThis is achieved by using particular aspects of linguistic contexts as cues\nthat identify a specific lexical class. Here we concentrate on the task of\nidentifying such cues and the theoretical background that allows for an\nassessment of the complexity of the task. The results show that, despite of the\na-priori complexity of the task, cue-based classification is a useful tool in\nthe automatic acquisition of lexical semantic classes.\n",
        "published": "2013-03-08T10:14:04Z",
        "pdf_link": "http://arxiv.org/pdf/1303.1930v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.1931v1",
        "title": "A Classification of Adjectives for Polarity Lexicons Enhancement",
        "summary": "  Subjective language detection is one of the most important challenges in\nSentiment Analysis. Because of the weight and frequency in opinionated texts,\nadjectives are considered a key piece in the opinion extraction process. These\nsubjective units are more and more frequently collected in polarity lexicons in\nwhich they appear annotated with their prior polarity. However, at the moment,\nany polarity lexicon takes into account prior polarity variations across\ndomains. This paper proves that a majority of adjectives change their prior\npolarity value depending on the domain. We propose a distinction between domain\ndependent and domain independent adjectives. Moreover, our analysis led us to\npropose a further classification related to subjectivity degree: constant,\nmixed and highly subjective adjectives. Following this classification, polarity\nvalues will be a better support for Sentiment Analysis.\n",
        "published": "2013-03-08T10:14:31Z",
        "pdf_link": "http://arxiv.org/pdf/1303.1931v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.1932v1",
        "title": "Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform",
        "summary": "  The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform\nthat automates the stages involved in the acquisition, production, updating and\nmaintenance of the large language resources required by, among others, MT\nsystems. The development of a Corpus Acquisition Component (CAC) for extracting\nmonolingual and bilingual data from the web is one of the most innovative\nbuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEA\npipeline for building Language Resources, adopts an efficient and distributed\nmethodology to crawl for web documents with rich textual content in specific\nlanguages and predefined domains. The CAC includes modules that can acquire\nparallel data from sites with in-domain content available in more than one\nlanguage. In order to extrinsically evaluate the CAC methodology, we have\nconducted several experiments that used crawled parallel corpora for the\nidentification and extraction of parallel sentences using sentence alignment.\nThe corpora were then successfully used for domain adaptation of Machine\nTranslation Systems.\n",
        "published": "2013-03-08T10:15:57Z",
        "pdf_link": "http://arxiv.org/pdf/1303.1932v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.2448v1",
        "title": "Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon\n  Production",
        "summary": "  In this work we present the results of our experimental work on the\ndevelop-ment of lexical class-based lexica by automatic means. The objective is\nto as-sess the use of linguistic lexical-class based information as a feature\nselection methodology for the use of classifiers in quick lexical development.\nThe results show that the approach can help in re-ducing the human effort\nrequired in the development of language resources sig-nificantly.\n",
        "published": "2013-03-11T08:21:17Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2448v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.2449v1",
        "title": "Using qualia information to identify lexical semantic classes in an\n  unsupervised clustering task",
        "summary": "  Acquiring lexical information is a complex problem, typically approached by\nrelying on a number of contexts to contribute information for classification.\nOne of the first issues to address in this domain is the determination of such\ncontexts. The work presented here proposes the use of automatically obtained\nFORMAL role descriptors as features used to draw nouns from the same lexical\nsemantic class together in an unsupervised clustering task. We have dealt with\nthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The\nresults obtained show that it is possible to discriminate between elements from\ndifferent lexical semantic classes using only FORMAL role information, hence\nvalidating our initial hypothesis. Also, iterating our method accurately\naccounts for fine-grained distinctions within lexical classes, namely\ndistinctions involving ambiguous expressions. Moreover, a filtering and\nbootstrapping strategy employed in extracting FORMAL role descriptors proved to\nminimize effects of sparse data and noise in our task.\n",
        "published": "2013-03-11T08:21:48Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2449v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.2826v1",
        "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA",
        "summary": "  This article presents a probabilistic generative model for text based on\nsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).\nPOSLDA simultaneously uncovers short-range syntactic patterns (syntax) and\nlong-range semantic patterns (topics) that exist in document collections. This\nresults in word distributions that are specific to both topics (sports,\neducation, ...) and parts-of-speech (nouns, verbs, ...). For example,\nmultinomial distributions over words are uncovered that can be understood as\n\"nouns about weather\" or \"verbs about law\". We describe the model and an\napproximate inference algorithm and then demonstrate the quality of the learned\ntopics both qualitatively and quantitatively. Then, we discuss an NLP\napplication where the output of POSLDA can lead to strong improvements in\nquality: unsupervised part-of-speech tagging. We describe algorithms for this\ntask that make use of POSLDA-learned distributions that result in improved\nperformance beyond the state of the art.\n",
        "published": "2013-03-12T10:20:50Z",
        "pdf_link": "http://arxiv.org/pdf/1303.2826v1"
    },
    {
        "id": "http://arxiv.org/abs/1303.5960v3",
        "title": "SYNTAGMA. A Linguistic Approach to Parsing",
        "summary": "  SYNTAGMA is a rule-based parsing system, structured on two levels: a general\nparsing engine and a language specific grammar. The parsing engine is a\nlanguage independent program, while grammar and language specific rules and\nresources are given as text files, consisting in a list of constituent\nstructuresand a lexical database with word sense related features and\nconstraints. Since its theoretical background is principally Tesniere's\nElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument\nstructure (valency) in constraint satisfaction, and allows also horizontal\nbounds, for instance treating coordination. Notions such as Pro, traces, empty\ncategories are derived from Generative Grammar and some solutions are close to\nGovernment&Binding Theory, although they are the result of an autonomous\nresearch. These properties allow SYNTAGMA to manage complex syntactic\nconfigurations and well known weak points in parsing engineering. An important\nresource is the semantic network, which is used in disambiguation tasks.\nParsing process follows a bottom-up, rule driven strategy. Its behavior can be\ncontrolled and fine-tuned.\n",
        "published": "2013-03-24T15:27:51Z",
        "pdf_link": "http://arxiv.org/pdf/1303.5960v3"
    },
    {
        "id": "http://arxiv.org/abs/1304.3265v1",
        "title": "Extension of hidden markov model for recognizing large vocabulary of\n  sign language",
        "summary": "  Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.\n",
        "published": "2013-04-11T11:56:39Z",
        "pdf_link": "http://arxiv.org/pdf/1304.3265v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.4520v1",
        "title": "Sentiment Analysis : A Literature Survey",
        "summary": "  Our day-to-day life has always been influenced by what people think. Ideas\nand opinions of others have always affected our own opinions. The explosion of\nWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,\nContributing to RSS, Social Bookmarking, and Social Networking. As a result\nthere has been an eruption of interest in people to mine these vast resources\nof data for opinions. Sentiment Analysis or Opinion Mining is the computational\ntreatment of opinions, sentiments and subjectivity of text. In this report, we\ntake a look at the various challenges and applications of Sentiment Analysis.\nWe will discuss in details various approaches to perform a computational\ntreatment of sentiments and opinions. Various supervised or data-driven\ntechniques to SA like Na\\\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons\nwill be discussed and their strengths and drawbacks will be touched upon. We\nwill also see a new dimension of analyzing sentiments by Cognitive Psychology\nmainly through the work of Janyce Wiebe, where we will see ways to detect\nsubjectivity, perspective in narrative and understanding the discourse\nstructure. We will also study some specific topics in Sentiment Analysis and\nthe contemporary works in those areas.\n",
        "published": "2013-04-16T17:06:24Z",
        "pdf_link": "http://arxiv.org/pdf/1304.4520v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.5880v1",
        "title": "Dealing with natural language interfaces in a geolocation context",
        "summary": "  In the geolocation field where high-level programs and low-level devices\ncoexist, it is often difficult to find a friendly user inter- face to configure\nall the parameters. The challenge addressed in this paper is to propose\nintuitive and simple, thus natural lan- guage interfaces to interact with\nlow-level devices. Such inter- faces contain natural language processing and\nfuzzy represen- tations of words that facilitate the elicitation of\nbusiness-level objectives in our context.\n",
        "published": "2013-04-22T09:06:36Z",
        "pdf_link": "http://arxiv.org/pdf/1304.5880v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.7282v1",
        "title": "An Improved Approach for Word Ambiguity Removal",
        "summary": "  Word ambiguity removal is a task of removing ambiguity from a word, i.e.\ncorrect sense of word is identified from ambiguous sentences. This paper\ndescribes a model that uses Part of Speech tagger and three categories for word\nsense disambiguation (WSD). Human Computer Interaction is very needful to\nimprove interactions between users and computers. For this, the Supervised and\nUnsupervised methods are combined. The WSD algorithm is used to find the\nefficient and accurate sense of a word based on domain information. The\naccuracy of this work is evaluated with the aim of finding best suitable domain\nof word.\n",
        "published": "2013-04-25T10:25:41Z",
        "pdf_link": "http://arxiv.org/pdf/1304.7282v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.7289v1",
        "title": "TimeML-strict: clarifying temporal annotation",
        "summary": "  TimeML is an XML-based schema for annotating temporal information over\ndiscourse. The standard has been used to annotate a variety of resources and is\nfollowed by a number of tools, the creation of which constitute hundreds of\nthousands of man-hours of research work. However, the current state of\nresources is such that many are not valid, or do not produce valid output, or\ncontain ambiguous or custom additions and removals. Difficulties arising from\nthese variances were highlighted in the TempEval-3 exercise, which included its\nown extra stipulations over conventional TimeML as a response.\n  To unify the state of current resources, and to make progress toward easy\nadoption of its current incarnation ISO-TimeML, this paper introduces\nTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We\nalso introduce three resources -- a schema for TimeML-strict; a validator tool\nfor TimeML-strict, so that one may ensure documents are in the correct form;\nand a repair tool that corrects common invalidating errors and adds\ndisambiguating markup in order to convert documents from the laxer TimeML\nstandard to TimeML-strict.\n",
        "published": "2013-04-26T21:31:08Z",
        "pdf_link": "http://arxiv.org/pdf/1304.7289v1"
    },
    {
        "id": "http://arxiv.org/abs/1304.7942v1",
        "title": "ManTIME: Temporal expression identification and normalization in the\n  TempEval-3 challenge",
        "summary": "  This paper describes a temporal expression identification and normalization\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\nphase combines the use of conditional random fields along with a\npost-processing identification pipeline, whereas the normalization phase is\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\ninvestigate the performance variation with respect to different feature types.\nSpecifically, we show that the use of WordNet-based features in the\nidentification task negatively affects the overall performance, and that there\nis no statistically significant difference in using gazetteers, shallow parsing\nand propositional noun phrases labels on top of the morphological features. On\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\naddition to the gold annotated ones) does not improve the performance.\n",
        "published": "2013-04-30T10:12:54Z",
        "pdf_link": "http://arxiv.org/pdf/1304.7942v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.1319v1",
        "title": "New Alignment Methods for Discriminative Book Summarization",
        "summary": "  We consider the unsupervised alignment of the full text of a book with a\nhuman-written summary. This presents challenges not seen in other text\nalignment problems, including a disparity in length and, consequent to this, a\nviolation of the expectation that individual words and phrases should align,\nsince large passages and chapters can be distilled into a single summary\nphrase. We present two new methods, based on hidden Markov models, specifically\ntargeted to this problem, and demonstrate gains on an extractive book\nsummarization task. While there is still much room for improvement,\nunsupervised alignment holds intrinsic value in offering insight into what\nfeatures of a book are deemed worthy of summarization.\n",
        "published": "2013-05-06T20:27:55Z",
        "pdf_link": "http://arxiv.org/pdf/1305.1319v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.3882v2",
        "title": "Rule-Based Semantic Tagging. An Application Undergoing Dictionary\n  Glosses",
        "summary": "  The project presented in this article aims to formalize criteria and\nprocedures in order to extract semantic information from parsed dictionary\nglosses. The actual purpose of the project is the generation of a semantic\nnetwork (nearly an ontology) issued from a monolingual Italian dictionary,\nthrough unsupervised procedures. Since the project involves rule-based Parsing,\nSemantic Tagging and Word Sense Disambiguation techniques, its outcomes may\nfind an interest also beyond this immediate intent. The cooperation of both\nsyntactic and semantic features in meaning construction are investigated, and\nprocedures which allows a translation of syntactic dependencies in semantic\nrelations are discussed. The procedures that rise from this project can be\napplied also to other text types than dictionary glosses, as they convert the\noutput of a parsing process into a semantic representation. In addition some\nmechanism are sketched that may lead to a kind of procedural semantics, through\nwhich multiple paraphrases of an given expression can be generated. Which means\nthat these techniques may find an application also in 'query expansion'\nstrategies, interesting Information Retrieval, Search Engines and Question\nAnswering Systems.\n",
        "published": "2013-05-16T18:09:21Z",
        "pdf_link": "http://arxiv.org/pdf/1305.3882v2"
    },
    {
        "id": "http://arxiv.org/abs/1305.3981v1",
        "title": "Binary Tree based Chinese Word Segmentation",
        "summary": "  Chinese word segmentation is a fundamental task for Chinese language\nprocessing. The granularity mismatch problem is the main cause of the errors.\nThis paper showed that the binary tree representation can store outputs with\ndifferent granularity. A binary tree based framework is also designed to\novercome the granularity mismatch problem. There are two steps in this\nframework, namely tree building and tree pruning. The tree pruning step is\nspecially designed to focus on the granularity problem. Previous work for\nChinese word segmentation such as the sequence tagging can be easily employed\nin this framework. This framework can also provide quantitative error analysis\nmethods. The experiments showed that after using a more sophisticated tree\npruning function for a state-of-the-art conditional random field based\nbaseline, the error reduction can be up to 20%.\n",
        "published": "2013-05-17T05:14:43Z",
        "pdf_link": "http://arxiv.org/pdf/1305.3981v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.5753v3",
        "title": "A probabilistic framework for analysing the compositionality of\n  conceptual combinations",
        "summary": "  Conceptual combination performs a fundamental role in creating the broad\nrange of compound phrases utilized in everyday language. This article provides\na novel probabilistic framework for assessing whether the semantics of\nconceptual combinations are compositional, and so can be considered as a\nfunction of the semantics of the constituent concepts, or not. While the\nsystematicity and productivity of language provide a strong argument in favor\nof assuming compositionality, this very assumption is still regularly\nquestioned in both cognitive science and philosophy. Additionally, the\nprinciple of semantic compositionality is underspecified, which means that\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\nRather than adjudicating between different grades of compositionality, the\nframework presented here contributes formal methods for determining a clear\ndividing line between compositional and non-compositional semantics. In\naddition, we suggest that the distinction between these is contextually\nsensitive. Utilizing formal frameworks developed for analyzing composite\nsystems in quantum theory, we present two methods that allow the semantics of\nconceptual combinations to be classified as \"compositional\" or\n\"non-compositional\". Compositionality is first formalised by factorising the\njoint probability distribution modeling the combination, where the terms in the\nfactorisation correspond to individual concepts. This leads to the necessary\nand sufficient condition for the joint probability distribution to exist. A\nfailure to meet this condition implies that the underlying concepts cannot be\nmodeled in a single probability space when considering their combination, and\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\nare demonstrated by applying them to an empirical study of twenty-four\nnon-lexicalised conceptual combinations.\n",
        "published": "2013-05-23T03:14:50Z",
        "pdf_link": "http://arxiv.org/pdf/1305.5753v3"
    },
    {
        "id": "http://arxiv.org/abs/1305.5785v1",
        "title": "An Inventory of Preposition Relations",
        "summary": "  We describe an inventory of semantic relations that are expressed by\nprepositions. We define these relations by building on the word sense\ndisambiguation task for prepositions and propose a mapping from preposition\nsenses to the relation labels by collapsing semantically related senses across\nprepositions.\n",
        "published": "2013-05-24T16:34:22Z",
        "pdf_link": "http://arxiv.org/pdf/1305.5785v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.5918v1",
        "title": "Reduce Meaningless Words for Joint Chinese Word Segmentation and\n  Part-of-speech Tagging",
        "summary": "  Conventional statistics-based methods for joint Chinese word segmentation and\npart-of-speech tagging (S&T) have generalization ability to recognize new words\nthat do not appear in the training data. An undesirable side effect is that a\nnumber of meaningless words will be incorrectly created. We propose an\neffective and efficient framework for S&T that introduces features to\nsignificantly reduce meaningless words generation. A general lexicon, Wikepedia\nand a large-scale raw corpus of 200 billion characters are used to generate\nword-based features for the wordhood. The word-lattice based framework consists\nof a character-based model and a word-based model in order to employ our\nword-based features. Experiments on Penn Chinese treebank 5 show that this\nmethod has a 62.9% reduction of meaningless word generation in comparison with\nthe baseline. As a result, the F1 measure for segmentation is increased to\n0.984.\n",
        "published": "2013-05-25T13:20:31Z",
        "pdf_link": "http://arxiv.org/pdf/1305.5918v1"
    },
    {
        "id": "http://arxiv.org/abs/1305.6211v2",
        "title": "Development of a Hindi Lemmatizer",
        "summary": "  We live in a translingual society, in order to communicate with people from\ndifferent parts of the world we need to have an expertise in their respective\nlanguages. Learning all these languages is not at all possible; therefore we\nneed a mechanism which can do this task for us. Machine translators have\nemerged as a tool which can perform this task. In order to develop a machine\ntranslator we need to develop several different rules. The very first module\nthat comes in machine translation pipeline is morphological analysis. Stemming\nand lemmatization comes under morphological analysis. In this paper we have\ncreated a lemmatizer which generates rules for removing the affixes along with\nthe addition of rules for creating a proper root word.\n",
        "published": "2013-05-24T18:01:34Z",
        "pdf_link": "http://arxiv.org/pdf/1305.6211v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.2091v2",
        "title": "A framework for (under)specifying dependency syntax without overloading\n  annotators",
        "summary": "  We introduce a framework for lightweight dependency syntax annotation. Our\nformalism builds upon the typical representation for unlabeled dependencies,\npermitting a simple notation and annotation workflow. Moreover, the formalism\nencourages annotators to underspecify parts of the syntax if doing so would\nstreamline the annotation process. We demonstrate the efficacy of this\nannotation on three languages and develop algorithms to evaluate and compare\nunderspecified annotations.\n",
        "published": "2013-06-10T02:54:10Z",
        "pdf_link": "http://arxiv.org/pdf/1306.2091v2"
    },
    {
        "id": "http://arxiv.org/abs/1306.2158v1",
        "title": "\"Not not bad\" is not \"bad\": A distributional account of negation",
        "summary": "  With the increasing empirical success of distributional models of\ncompositional semantics, it is timely to consider the types of textual logic\nthat such models are capable of capturing. In this paper, we address\nshortcomings in the ability of current models to capture logical operations\nsuch as negation. As a solution we propose a tripartite formulation for a\ncontinuous vector space representation of semantics and subsequently use this\nrepresentation to develop a formal compositional notion of negation within such\nmodels.\n",
        "published": "2013-06-10T10:29:09Z",
        "pdf_link": "http://arxiv.org/pdf/1306.2158v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.3584v1",
        "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality",
        "summary": "  The compositionality of meaning extends beyond the single sentence. Just as\nwords combine to form the meaning of sentences, so do sentences combine to form\nthe meaning of paragraphs, dialogues and general discourse. We introduce both a\nsentence model and a discourse model corresponding to the two levels of\ncompositionality. The sentence model adopts convolution as the central\noperation for composing semantic vectors and is based on a novel hierarchical\nconvolutional neural network. The discourse model extends the sentence model\nand is based on a recurrent neural network that is conditioned in a novel way\nboth on the current sentence and on the current speaker. The discourse model is\nable to capture both the sequentiality of sentences and the interaction between\ndifferent speakers. Without feature engineering or pretraining and with simple\ngreedy decoding, the discourse model coupled to the sentence model obtains\nstate of the art performance on a dialogue act classification experiment.\n",
        "published": "2013-06-15T14:52:17Z",
        "pdf_link": "http://arxiv.org/pdf/1306.3584v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.4134v1",
        "title": "Dialogue System: A Brief Review",
        "summary": "  A Dialogue System is a system which interacts with human in natural language.\nAt present many universities are developing the dialogue system in their\nregional language. This paper will discuss about dialogue system, its\ncomponents, challenges and its evaluation. This paper helps the researchers for\ngetting info regarding dialogues system.\n",
        "published": "2013-06-18T10:32:43Z",
        "pdf_link": "http://arxiv.org/pdf/1306.4134v1"
    },
    {
        "id": "http://arxiv.org/abs/1306.6130v1",
        "title": "Competency Tracking for English as a Second or Foreign Language Learners",
        "summary": "  My system utilizes the outcomes feature found in Moodle and other learning\ncontent management systems (LCMSs) to keep track of where students are in terms\nof what language competencies they have mastered and the competencies they need\nto get where they want to go. These competencies are based on the Common\nEuropean Framework for (English) Language Learning. This data can be available\nfor everyone involved with a given student's progress (e.g. educators, parents,\nsupervisors and the students themselves). A given student's record of past\naccomplishments can also be meshed with those of his classmates. Not only are a\nstudent's competencies easily seen and tracked, educators can view competencies\nof a group of students that were achieved prior to enrollment in the class.\nThis should make curriculum decision making easier and more efficient for\neducators.\n",
        "published": "2013-06-26T05:02:26Z",
        "pdf_link": "http://arxiv.org/pdf/1306.6130v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.1004v3",
        "title": "Boundary identification of events in clinical named entity recognition",
        "summary": "  The problem of named entity recognition in the medical/clinical domain has\ngained increasing attention do to its vital role in a wide range of clinical\ndecision support applications. The identification of complete and correct term\nspan is vital for further knowledge synthesis (e.g., coding/mapping concepts\nthesauruses and classification standards). This paper investigates boundary\nadjustment by sequence labeling representations models and post-processing\ntechniques in the problem of clinical named entity recognition (recognition of\nclinical events). Using current state-of-the-art sequence labeling algorithm\n(conditional random fields), we show experimentally that sequence labeling\nrepresentation and post-processing can be significantly helpful in strict\nboundary identification of clinical events.\n",
        "published": "2013-08-05T15:14:14Z",
        "pdf_link": "http://arxiv.org/pdf/1308.1004v3"
    },
    {
        "id": "http://arxiv.org/abs/1308.1507v1",
        "title": "Logical analysis of natural language semantics to solve the problem of\n  computer understanding",
        "summary": "  An object--oriented approach to create a natural language understanding\nsystem is considered. The understanding program is a formal system built on the\nbase of predicative calculus. Horn's clauses are used as well--formed formulas.\nAn inference is based on the principle of resolution. Sentences of natural\nlanguage are represented in the view of typical predicate set. These predicates\ndescribe physical objects and processes, abstract objects, categories and\nsemantic relations between objects. Predicates for concrete assertions are\nsaved in a database. To describe the semantics of classes for physical objects,\nabstract concepts and processes, a knowledge base is applied. The proposed\nrepresentation of natural language sentences is a semantic net. Nodes of such\nnet are typical predicates. This approach is perspective as, firstly, such\ntypification of nodes facilitates essentially forming of processing algorithms\nand object descriptions, secondly, the effectiveness of algorithms is increased\n(particularly for the great number of nodes), thirdly, to describe the\nsemantics of words, encyclopedic knowledge is used, and this permits\nessentially to extend the class of solved problems.\n",
        "published": "2013-08-07T09:09:47Z",
        "pdf_link": "http://arxiv.org/pdf/1308.1507v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.2428v2",
        "title": "Hidden Structure and Function in the Lexicon",
        "summary": "  How many words are needed to define all the words in a dictionary?\nGraph-theoretic analysis reveals that about 10% of a dictionary is a unique\nKernel of words that define one another and all the rest, but this is not the\nsmallest such subset. The Kernel consists of one huge strongly connected\ncomponent (SCC), about half its size, the Core, surrounded by many small SCCs,\nthe Satellites. Core words can define one another but not the rest of the\ndictionary. The Kernel also contains many overlapping Minimal Grounding Sets\n(MGSs), each about the same size as the Core, each part-Core, part-Satellite.\nMGS words can define all the rest of the dictionary. They are learned earlier,\nmore concrete and more frequent than the rest of the dictionary. Satellite\nwords, not correlated with age or frequency, are less concrete (more abstract)\nwords that are also needed for full lexical power.\n",
        "published": "2013-08-11T20:50:27Z",
        "pdf_link": "http://arxiv.org/pdf/1308.2428v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.2696v1",
        "title": "B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language",
        "summary": "  Discourse analysis may seek to characterize not only the overall composition\nof a given text but also the dynamic patterns within the data. This technical\nreport introduces a data format intended to facilitate multi-level\ninvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by\nthe long-form data format required for mixed-effects modeling, B(eo)W(u)LF\nstructures linguistic data into an expanded matrix encoding any number of\nresearchers-specified markers, making it ideal for recurrence-based analyses.\nWhile we do not necessarily claim to be the first to use methods along these\nlines, we have created a series of tools utilizing Python and MATLAB to enable\nsuch discourse analyses and demonstrate them using 319 lines of the Old English\nepic poem, Beowulf, translated into modern English.\n",
        "published": "2013-08-12T20:57:02Z",
        "pdf_link": "http://arxiv.org/pdf/1308.2696v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.3839v2",
        "title": "Consensus Sequence Segmentation",
        "summary": "  In this paper we introduce a method to detect words or phrases in a given\nsequence of alphabets without knowing the lexicon. Our linear time unsupervised\nalgorithm relies entirely on statistical relationships among alphabets in the\ninput sequence to detect location of word boundaries. We compare our algorithm\nto previous approaches from unsupervised sequence segmentation literature and\nprovide superior segmentation over number of benchmarks.\n",
        "published": "2013-08-18T07:09:03Z",
        "pdf_link": "http://arxiv.org/pdf/1308.3839v2"
    },
    {
        "id": "http://arxiv.org/abs/1308.4479v1",
        "title": "An Investigation of the Sampling-Based Alignment Method and Its\n  Contributions",
        "summary": "  By investigating the distribution of phrase pairs in phrase translation\ntables, the work in this paper describes an approach to increase the number of\nn-gram alignments in phrase translation tables output by a sampling-based\nalignment method. This approach consists in enforcing the alignment of n-grams\nin distinct translation subtables so as to increase the number of n-grams.\nStandard normal distribution is used to allot alignment time among translation\nsubtables, which results in adjustment of the distribution of n- grams. This\nleads to better evaluation results on statistical machine translation tasks\nthan the original sampling-based alignment approach. Furthermore, the\ntranslation quality obtained by merging phrase translation tables computed from\nthe sampling-based alignment method and from MGIZA++ is examined.\n",
        "published": "2013-08-21T03:44:04Z",
        "pdf_link": "http://arxiv.org/pdf/1308.4479v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.5423v1",
        "title": "A Literature Review: Stemming Algorithms for Indian Languages",
        "summary": "  Stemming is the process of extracting root word from the given inflection\nword. It also plays significant role in numerous application of Natural\nLanguage Processing (NLP). The stemming problem has addressed in many contexts\nand by researchers in many disciplines. This expository paper presents survey\nof some of the latest developments on stemming algorithms in data mining and\nalso presents with some of the solutions for various Indian language stemming\nalgorithms along with the results.\n",
        "published": "2013-08-25T16:41:06Z",
        "pdf_link": "http://arxiv.org/pdf/1308.5423v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.5499v1",
        "title": "Linear models and linear mixed effects models in R with linguistic\n  applications",
        "summary": "  This text is a conceptual introduction to mixed effects modeling with\nlinguistic applications, using the R programming environment. The reader is\nintroduced to linear modeling and assumptions, as well as to mixed\neffects/multilevel modeling, including a discussion of random intercepts,\nrandom slopes and likelihood ratio tests. The example used throughout the text\nfocuses on the phonetic analysis of voice pitch data.\n",
        "published": "2013-08-26T07:18:17Z",
        "pdf_link": "http://arxiv.org/pdf/1308.5499v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.6242v1",
        "title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of\n  Tweets",
        "summary": "  In this paper, we describe how we created two state-of-the-art SVM\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\n(message-level task) and one to detect the sentiment of a term within a\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\nin the message-level task and 88.93 in the term-level task. We implemented a\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\nhashtags, and one from tweets with emoticons. In the message-level task, the\nlexicon-based features provided a gain of 5 F-score points over all others.\nBoth of our systems can be replicated us available resources.\n",
        "published": "2013-08-28T18:23:03Z",
        "pdf_link": "http://arxiv.org/pdf/1308.6242v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.6297v1",
        "title": "Crowdsourcing a Word-Emotion Association Lexicon",
        "summary": "  Even though considerable attention has been given to the polarity of words\n(positive and negative) and the creation of large polarity lexicons, research\nin emotion analysis has had to rely on limited and small emotion lexicons. In\nthis paper we show how the combined strength and wisdom of the crowds can be\nused to generate a large, high-quality, word-emotion and word-polarity\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\nemotion annotation in a crowdsourcing scenario and propose solutions to address\nthem. Most notably, in addition to questions about emotions associated with\nterms, we show how the inclusion of a word choice question can discourage\nmalicious data entry, help identify instances where the annotator may not be\nfamiliar with the target term (allowing us to reject such annotations), and\nhelp obtain annotations at sense level (rather than at word level). We\nconducted experiments on how to formulate the emotion-annotation questions, and\nshow that asking if a term is associated with an emotion leads to markedly\nhigher inter-annotator agreement than that obtained by asking if a term evokes\nan emotion.\n",
        "published": "2013-08-28T20:13:32Z",
        "pdf_link": "http://arxiv.org/pdf/1308.6297v1"
    },
    {
        "id": "http://arxiv.org/abs/1308.6300v1",
        "title": "Computing Lexical Contrast",
        "summary": "  Knowing the degree of semantic contrast between words has widespread\napplication in natural language processing, including machine translation,\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\nopposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such\nas antipodals, complementaries, and gradable. However, existing lexicons often\ndo not classify opposites into the different kinds. They also do not explicitly\nlist word pairs that are not opposites but yet have some degree of contrast in\nmeaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm\nfreezing}. We propose an automatic method to identify contrasting word pairs\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\nthere exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm\ntropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm\ncold}.) We will call this the contrast hypothesis. We begin with a large\ncrowdsourcing experiment to determine the amount of human agreement on the\nconcept of oppositeness and its different kinds. In the process, we flesh out\nkey features of different kinds of opposites. We then present an automatic and\nempirical measure of lexical contrast that relies on the contrast hypothesis,\ncorpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show\nthat the proposed measure of lexical contrast obtains high precision and large\ncoverage, outperforming existing methods.\n",
        "published": "2013-08-28T20:24:27Z",
        "pdf_link": "http://arxiv.org/pdf/1308.6300v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1014v1",
        "title": "Advances in the Logical Representation of Lexical Semantics",
        "summary": "  The integration of lexical semantics and pragmatics in the analysis of the\nmeaning of natural lan- guage has prompted changes to the global framework\nderived from Montague. In those works, the original lexicon, in which words\nwere assigned an atomic type of a single-sorted logic, has been re- placed by a\nset of many-facetted lexical items that can compose their meaning with salient\ncontextual properties using a rich typing system as a guide. Having related our\nproposal for such an expanded framework \\LambdaTYn, we present some recent\nadvances in the logical formalisms associated, including constraints on lexical\ntransformations and polymorphic quantifiers, and ongoing discussions and\nresearch on the granularity of the type system and the limits of transitivity.\n",
        "published": "2013-09-04T12:56:37Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1014v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1125v1",
        "title": "Learning to answer questions",
        "summary": "  We present an open-domain Question-Answering system that learns to answer\nquestions based on successful past interactions. We follow a pattern-based\napproach to Answer-Extraction, where (lexico-syntactic) patterns that relate a\nquestion to its answer are automatically learned and used to answer future\nquestions. Results show that our approach contributes to the system's best\nperformance when it is conjugated with typical Answer-Extraction strategies.\nMoreover, it allows the system to learn with the answered questions and to\nrectify wrong or unsolved past questions.\n",
        "published": "2013-09-04T18:10:22Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1125v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1129v1",
        "title": "Analysing Quality of English-Hindi Machine Translation Engine Outputs\n  Using Bayesian Classification",
        "summary": "  This paper considers the problem for estimating the quality of machine\ntranslation outputs which are independent of human intervention and are\ngenerally addressed using machine learning techniques.There are various\nmeasures through which a machine learns translations quality. Automatic\nEvaluation metrics produce good co-relation at corpus level but cannot produce\nthe same results at the same segment or sentence level. In this paper 16\nfeatures are extracted from the input sentences and their translations and a\nquality score is obtained based on Bayesian inference produced from training\ndata.\n",
        "published": "2013-09-04T18:23:30Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1129v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.1649v2",
        "title": "Preparing Korean Data for the Shared Task on Parsing Morphologically\n  Rich Languages",
        "summary": "  This document gives a brief description of Korean data prepared for the SPMRL\n2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for\nthe shared task. All constituent trees are collected from the KAIST Treebank\nand transformed to the Penn Treebank style. All dependency trees are converted\nfrom the transformed constituent trees using heuristics and labeling rules de-\nsigned specifically for the KAIST Treebank. In addition to the gold-standard\nmorphological analysis provided by the KAIST Treebank, two sets of automatic\nmorphological analysis are provided for the shared task, one is generated by\nthe HanNanum morphological analyzer, and the other is generated by the Sejong\nmorphological analyzer.\n",
        "published": "2013-09-06T14:28:02Z",
        "pdf_link": "http://arxiv.org/pdf/1309.1649v2"
    },
    {
        "id": "http://arxiv.org/abs/1309.2471v1",
        "title": "Implementation of nlization framework for verbs, pronouns and\n  determiners with eugene",
        "summary": "  UNL system is designed and implemented by a nonprofit organization, UNDL\nFoundation at Geneva in 1999. UNL applications are application softwares that\nallow end users to accomplish natural language tasks, such as translating,\nsummarizing, retrieving or extracting information, etc. Two major web based\napplication softwares are Interactive ANalyzer (IAN), which is a natural\nlanguage analysis system. It represents natural language sentences as semantic\nnetworks in the UNL format. Other application software is dEep-to-sUrface\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\nnatural language sentences out of semantic networks represented in the UNL\nformat. In this paper, NLization framework with EUGENE is focused, while using\nUNL system for accomplishing the task of machine translation. In whole\nNLization process, EUGENE takes a UNL input and delivers an output in natural\nlanguage without any human intervention. It is language-independent and has to\nbe parametrized to the natural language input through a dictionary and a\ngrammar, provided as separate interpretable files. In this paper, it is\nexplained that how UNL input is syntactically and semantically analyzed with\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\nand determiners for Punjabi natural language.\n",
        "published": "2013-09-10T12:03:32Z",
        "pdf_link": "http://arxiv.org/pdf/1309.2471v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.2853v1",
        "title": "General Purpose Textual Sentiment Analysis and Emotion Detection Tools",
        "summary": "  Textual sentiment analysis and emotion detection consists in retrieving the\nsentiment or emotion carried by a text or document. This task can be useful in\nmany domains: opinion mining, prediction, feedbacks, etc. However, building a\ngeneral purpose tool for doing sentiment analysis and emotion detection raises\na number of issues, theoretical issues like the dependence to the domain or to\nthe language but also pratical issues like the emotion representation for\ninteroperability. In this paper we present our sentiment/emotion analysis\ntools, the way we propose to circumvent the di culties and the applications\nthey are used for.\n",
        "published": "2013-09-11T15:16:26Z",
        "pdf_link": "http://arxiv.org/pdf/1309.2853v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.4168v1",
        "title": "Exploiting Similarities among Languages for Machine Translation",
        "summary": "  Dictionaries and phrase tables are the basis of modern statistical machine\ntranslation systems. This paper develops a method that can automate the process\nof generating and extending dictionaries and phrase tables. Our method can\ntranslate missing word and phrase entries by learning language structures based\non large monolingual data and mapping between languages from small bilingual\ndata. It uses distributed representation of words and learns a linear mapping\nbetween vector spaces of languages. Despite its simplicity, our method is\nsurprisingly effective: we can achieve almost 90% precision@5 for translation\nof words between English and Spanish. This method makes little assumption about\nthe languages, so it can be used to extend and refine dictionaries and\ntranslation tables for any language pairs.\n",
        "published": "2013-09-17T03:23:13Z",
        "pdf_link": "http://arxiv.org/pdf/1309.4168v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.4628v1",
        "title": "Text segmentation with character-level text embeddings",
        "summary": "  Learning word representations has recently seen much success in computational\nlinguistics. However, assuming sequences of word tokens as input to linguistic\nanalysis is often unjustified. For many languages word segmentation is a\nnon-trivial task and naturally occurring text is sometimes a mixture of natural\nlanguage strings and other character data. We propose to learn text\nrepresentations directly from raw character sequences by training a Simple\nrecurrent Network to predict the next character in text. The network uses its\nhidden layer to evolve abstract representations of the character sequences it\nsees. To demonstrate the usefulness of the learned text embeddings, we use them\nas features in a supervised character level text segmentation and labeling\ntask: recognizing spans of text containing programming language code. By using\nthe embeddings as features we are able to substantially improve over a baseline\nwhich uses only surface character n-grams.\n",
        "published": "2013-09-18T12:38:34Z",
        "pdf_link": "http://arxiv.org/pdf/1309.4628v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5223v1",
        "title": "JRC EuroVoc Indexer JEX - A freely available multi-label categorisation\n  tool",
        "summary": "  EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700\nhierarchically organised subject domains used by European Institutions and many\nauthorities in Member States of the European Union (EU) for the classification\nand retrieval of official documents. JEX is JRC-developed multi-label\nclassification software that learns from manually labelled data to\nautomatically assign EuroVoc descriptors to new documents in a profile-based\ncategory-ranking task. The JEX release consists of trained classifiers for 22\nofficial EU languages, of parallel training data in the same languages, of an\ninterface that allows viewing and amending the assignment results, and of a\nmodule that allows users to re-train the tool on their own document\ncollections. JEX allows advanced users to change the document representation so\nas to possibly improve the categorisation result through linguistic\npre-processing. JEX can be used as a tool for interactive EuroVoc descriptor\nassignment to increase speed and consistency of the human categorisation\nprocess, or it can be used fully automatically. The output of JEX is a\nlanguage-independent EuroVoc feature vector lending itself also as input to\nvarious other Language Technology tasks, including cross-lingual clustering and\nclassification, cross-lingual plagiarism detection, sentence selection and\nranking, and more.\n",
        "published": "2013-09-20T09:51:59Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5223v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5226v1",
        "title": "DGT-TM: A freely Available Translation Memory in 22 Languages",
        "summary": "  The European Commission's (EC) Directorate General for Translation, together\nwith the EC's Joint Research Centre, is making available a large translation\nmemory (TM; i.e. sentences and their professionally produced translations)\ncovering twenty-two official European Union (EU) languages and their 231\nlanguage pairs. Such a resource is typically used by translation professionals\nin combination with TM software to improve speed and consistency of their\ntranslations. However, this resource has also many uses for translation studies\nand for language technology applications, including Statistical Machine\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\nmultilingual classification and clustering, and many more. In this reference\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\nits size, and explain how it was produced and how to use it.\n",
        "published": "2013-09-20T10:02:58Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5226v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5290v1",
        "title": "An introduction to the Europe Media Monitor family of applications",
        "summary": "  Most large organizations have dedicated departments that monitor the media to\nkeep up-to-date with relevant developments and to keep an eye on how they are\nrepresented in the news. Part of this media monitoring work can be automated.\nIn the European Union with its 23 official languages, it is particularly\nimportant to cover media reports in many languages in order to capture the\ncomplementary news content published in the different countries. It is also\nimportant to be able to access the news content across languages and to merge\nthe extracted information. We present here the four publicly accessible systems\nof the Europe Media Monitor (EMM) family of applications, which cover between\n19 and 50 languages (see http://press.jrc.it/overview.html). We give an\noverview of their functionality and discuss some of the implications of the\nfact that they cover quite so many languages. We discuss design issues\nnecessary to be able to achieve this high multilinguality, as well as the\nbenefits of this multilinguality.\n",
        "published": "2013-09-20T15:03:58Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5290v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5391v1",
        "title": "Even the Abstract have Colour: Consensus in Word-Colour Associations",
        "summary": "  Colour is a key component in the successful dissemination of information.\nSince many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complemented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. A word-choice question was used to obtain sense-level\nannotations and to ensure data quality. We focus especially on abstract\nconcepts and emotions to show that even they tend to have strong colour\nassociations. Thus, using the right colours can not only improve semantic\ncoherence, but also inspire the desired emotional response.\n",
        "published": "2013-09-20T21:06:46Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5391v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5652v1",
        "title": "LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual",
        "summary": "  The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\nfor natural language processing (NLP) research. Among these are a number of\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\nsingle collection of annotated documents. NLP research, however, usually\nrequires multiple data sets for the purposes of training models, developing\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\ncorpora used into the required data sets (divisions). This document details a\nset of rules that have been defined to enable consistent divisions for old and\nnew Arabic treebanks (ATB) and related corpora.\n",
        "published": "2013-09-22T21:09:07Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5652v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5657v1",
        "title": "A Hybrid Algorithm for Matching Arabic Names",
        "summary": "  In this paper, a new hybrid algorithm which combines both of token-based and\ncharacter-based approaches is presented. The basic Levenshtein approach has\nbeen extended to token-based distance metric. The distance metric is enhanced\nto set the proper granularity level behavior of the algorithm. It smoothly maps\na threshold of misspellings differences at the character level, and the\nimportance of token level errors in terms of token's position and frequency.\nUsing a large Arabic dataset, the experimental results show that the proposed\nalgorithm overcomes successfully many types of errors such as: typographical\nerrors, omission or insertion of middle name components, omission of\nnon-significant popular name components, and different writing styles character\nvariations. When compared the results with other classical algorithms, using\nthe same dataset, the proposed algorithm was found to increase the minimum\nsuccess level of best tested algorithms, while achieving higher upper limits .\n",
        "published": "2013-09-22T22:06:26Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5657v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5843v1",
        "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet",
        "summary": "  Assigning a positive or negative score to a word out of context (i.e. a\nword's prior polarity) is a challenging task for sentiment analysis. In the\nliterature, various approaches based on SentiWordNet have been proposed. In\nthis paper, we compare the most often used techniques together with newly\nproposed ones and incorporate all of them in a learning framework to see\nwhether blending them can further improve the estimation of prior polarity\nscores. Using two different versions of SentiWordNet and testing regression and\nclassification models across tasks and datasets, our learning approach\nconsistently outperforms the single metrics, providing a new state-of-the-art\napproach in computing words' prior polarity for sentiment analysis. We conclude\nour investigation showing interesting biases in calculated prior polarity\nscores when word Part of Speech and annotator gender are considered.\n",
        "published": "2013-09-23T15:26:09Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5843v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5909v1",
        "title": "From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels\n  and Fairy Tales",
        "summary": "  Today we have access to unprecedented amounts of literary texts. However,\nsearch still relies heavily on key words. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in both individual books and across very large collections. We\nintroduce the concept of emotion word density, and using the Brothers Grimm\nfairy tales as example, we show how collections of text can be organized for\nbetter search. Using the Google Books Corpus we show how to determine an\nentity's emotion associations from co-occurring words. Finally, we compare\nemotion words in fairy tales and novels, to show that fairy tales have a much\nwider range of emotion word densities than novels.\n",
        "published": "2013-09-23T18:43:56Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5909v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.5942v1",
        "title": "Colourful Language: Measuring Word-Colour Associations",
        "summary": "  Since many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complimented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\nthat even though they cannot be physically visualized, they too tend to have\nstrong colour associations. Finally, we show how word-colour associations\nmanifest themselves in language, and quantify usefulness of co-occurrence and\npolarity cues in automatically detecting colour associations.\n",
        "published": "2013-09-20T21:10:56Z",
        "pdf_link": "http://arxiv.org/pdf/1309.5942v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6162v1",
        "title": "JRC-Names: A freely available, highly multilingual named entity resource",
        "summary": "  This paper describes a new, freely available, highly multilingual named\nentity resource for person and organisation names that has been compiled over\nseven years of large-scale multilingual news analysis combined with Wikipedia\nmining, resulting in 205,000 per-son and organisation names plus about the same\nnumber of spelling variants written in over 20 different scripts and in many\nmore languages. This resource, produced as part of the Europe Media Monitor\nactivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number\nof purposes. These include improving name search in databases or on the\ninternet, seeding machine learning systems to learn named entity recognition\nrules, improve machine translation results, and more. We describe here how this\nresource was created; we give statistics on its current size; we address the\nissue of morphological inflection; and we give details regarding its\nfunctionality. Updates to this resource will be made available daily.\n",
        "published": "2013-09-24T14:09:53Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6162v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6185v1",
        "title": "Acronym recognition and processing in 22 languages",
        "summary": "  We are presenting work on recognising acronyms of the form Long-Form\n(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\narticles in twenty-two languages, as part of our more general effort to\nrecognise entities and their variants in news text and to use them for the\nautomatic analysis of the news, including the linking of related news across\nlanguages. We show how the acronym recognition patterns, initially developed\nfor medical terms, needed to be adapted to the more general news domain and we\npresent evaluation results. We describe our effort to automatically merge the\nnumerous long-form variants referring to the same short-form, while keeping\nnon-related long-forms separate. Finally, we provide extensive statistics on\nthe frequency and the distribution of short-form/long-form pairs across\nlanguages.\n",
        "published": "2013-09-24T14:41:33Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6185v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6202v1",
        "title": "Sentiment Analysis in the News",
        "summary": "  Recent years have brought a significant growth in the volume of research in\nsentiment analysis, mostly on highly subjective text types (movie or product\nreviews). The main difference these texts have with news articles is that their\ntarget is clearly defined and unique across the text. Following different\nannotation efforts and the analysis of the issues encountered, we realised that\nnews opinion mining is different from that of other text types. We identified\nthree subtasks that need to be addressed: definition of the target; separation\nof the good and bad news content from the good and bad sentiment expressed on\nthe target; and analysis of clearly marked opinion that is expressed\nexplicitly, not needing interpretation or the use of world knowledge.\nFurthermore, we distinguish three different possible views on newspaper\narticles - author, reader and text, which have to be addressed differently at\nthe time of analysing sentiment. Given these definitions, we present work on\nmining opinions about entities in English language news, in which (a) we test\nthe relative suitability of various sentiment dictionaries and (b) we attempt\nto separate positive or negative opinion from good or bad news. In the\nexperiments described here, we tested whether or not subject domain-defining\nvocabulary should be ignored. Results showed that this idea is more appropriate\nin the context of news opinion mining and that the approaches taking this into\nconsideration produce a better performance.\n",
        "published": "2013-09-24T15:11:43Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6202v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6347v1",
        "title": "Tracking Sentiment in Mail: How Genders Differ on Emotional Axes",
        "summary": "  With the widespread use of email, we now have access to unprecedented amounts\nof text that we ourselves have written. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in many types of mail. We create a large word--emotion\nassociation lexicon by crowdsourcing, and use it to compare emotions in love\nletters, hate mail, and suicide notes. We show that there are marked\ndifferences across genders in how they use emotion words in work-place email.\nFor example, women use many words from the joy--sadness axis, whereas men\nprefer terms from the fear--trust axis. Finally, we show visualizations that\ncan help people track emotions in their emails.\n",
        "published": "2013-09-24T21:14:45Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6347v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6352v1",
        "title": "Using Nuances of Emotion to Identify Personality",
        "summary": "  Past work on personality detection has shown that frequency of lexical\ncategories such as first person pronouns, past tense verbs, and sentiment words\nhave significant correlations with personality traits. In this paper, for the\nfirst time, we show that fine affect (emotion) categories such as that of\nexcitement, guilt, yearning, and admiration are significant indicators of\npersonality. Additionally, we perform experiments to show that the gains\nprovided by the fine affect categories are not obtained by using coarse affect\ncategories alone or with specificity features alone. We employ these features\nin five SVM classifiers for detecting five personality traits through essays.\nWe find that the use of fine emotion features leads to statistically\nsignificant improvement over a competitive baseline, whereas the use of coarse\naffect and specificity features does not.\n",
        "published": "2013-09-24T21:21:02Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6352v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.6722v1",
        "title": "Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern\n  Generation",
        "summary": "  This paper focuses on the automatic extraction of domain-specific sentiment\nword (DSSW), which is a fundamental subtask of sentiment analysis. Most\nprevious work utilizes manual patterns for this task. However, the performance\nof those methods highly relies on the labelled patterns or selected seeds. In\norder to overcome the above problem, this paper presents an automatic framework\nto detect large-scale domain-specific patterns for DSSW extraction. To this\nend, sentiment seeds are extracted from massive dataset of user comments.\nSubsequently, these sentiment seeds are expanded by synonyms using a\nbootstrapping mechanism. Simultaneously, a synonymy graph is built and the\ngraph propagation algorithm is applied on the built synonymy graph. Afterwards,\nsyntactic and sequential relations between target words and high-ranked\nsentiment words are extracted automatically to construct large-scale patterns,\nwhich are further used to extracte DSSWs. The experimental results in three\ndomains reveal the effectiveness of our method.\n",
        "published": "2013-09-26T05:18:12Z",
        "pdf_link": "http://arxiv.org/pdf/1309.6722v1"
    },
    {
        "id": "http://arxiv.org/abs/1309.7312v1",
        "title": "Development and Transcription of Assamese Speech Corpus",
        "summary": "  A balanced speech corpus is the basic need for any speech processing task. In\nthis report we describe our effort on development of Assamese speech corpus. We\nmainly focused on some issues and challenges faced during development of the\ncorpus. Being a less computationally aware language, this is the first effort\nto develop speech corpus for Assamese. As corpus development is an ongoing\nprocess, in this paper we report only the initial task.\n",
        "published": "2013-09-27T17:54:14Z",
        "pdf_link": "http://arxiv.org/pdf/1309.7312v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.0573v1",
        "title": "Improving the Quality of MT Output using Novel Name Entity Translation\n  Scheme",
        "summary": "  This paper presents a novel approach to machine translation by combining the\nstate of art name entity translation scheme. Improper translation of name\nentities lapse the quality of machine translated output. In this work, name\nentities are transliterated by using statistical rule based approach. This\npaper describes the translation and transliteration of name entities from\nEnglish to Punjabi. We have experimented on four types of name entities which\nare: Proper names, Location names, Organization names and miscellaneous.\nVarious rules for the purpose of syllabification have been constructed.\nTransliteration of name entities is accomplished with the help of Probability\ncalculation. N-Gram probabilities for the extracted syllables have been\ncalculated using statistical machine translation toolkit MOSES.\n",
        "published": "2013-10-02T05:58:52Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0573v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.0575v2",
        "title": "Development of Marathi Part of Speech Tagger Using Statistical Approach",
        "summary": "  Part-of-speech (POS) tagging is a process of assigning the words in a text\ncorresponding to a particular part of speech. A fundamental version of POS\ntagging is the identification of words as nouns, verbs, adjectives etc. For\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\none of the simplest as well as most constant and statistical model for many NLP\napplications. POS Tagging is an initial stage of linguistics, text analysis\nlike information retrieval, machine translator, text to speech synthesis,\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\neach word in a sentence and literature. Various approaches have been proposed\nto implement POS taggers. In this paper we present a Marathi part of speech\ntagger. It is morphologically rich language. Marathi is spoken by the native\npeople of Maharashtra. The general approach used for development of tagger is\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\nidea about all the algorithms with suitable examples. It also introduces a tag\nset for Marathi which can be used for tagging Marathi text. In this paper we\nhave shown the development of the tagger as well as compared to check the\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\nrespectively.\n",
        "published": "2013-10-02T06:04:53Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0575v2"
    },
    {
        "id": "http://arxiv.org/abs/1310.0578v1",
        "title": "Subjective and Objective Evaluation of English to Urdu Machine\n  Translation",
        "summary": "  Machine translation is research based area where evaluation is very important\nphenomenon for checking the quality of MT output. The work is based on the\nevaluation of English to Urdu Machine translation. In this research work we\nhave evaluated the translation quality of Urdu language which has been\ntranslated by using different Machine Translation systems like Google, Babylon\nand Ijunoon. The evaluation process is done by using two approaches - Human\nevaluation and Automatic evaluation. We have worked for both the approaches\nwhere in human evaluation emphasis is given to scales and parameters while in\nautomatic evaluation emphasis is given to some automatic metric such as BLEU,\nGTM, METEOR and ATEC.\n",
        "published": "2013-10-02T06:10:49Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0578v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.0581v1",
        "title": "Rule Based Stemmer in Urdu",
        "summary": "  Urdu is a combination of several languages like Arabic, Hindi, English,\nTurkish, Sanskrit etc. It has a complex and rich morphology. This is the reason\nwhy not much work has been done in Urdu language processing. Stemming is used\nto convert a word into its respective root form. In stemming, we separate the\nsuffix and prefix from the word. It is useful in search engines, natural\nlanguage processing and word processing, spell checkers, word parsing, word\nfrequency and count studies. This paper presents a rule based stemmer for Urdu.\nThe stemmer that we have discussed here is used in information retrieval. We\nhave also evaluated our results by verifying it with a human expert.\n",
        "published": "2013-10-02T06:15:03Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0581v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.0754v1",
        "title": "Stemmers for Tamil Language: Performance Analysis",
        "summary": "  Stemming is the process of extracting root word from the given inflection\nword and also plays significant role in numerous application of Natural\nLanguage Processing (NLP). Tamil Language raises several challenges to NLP,\nsince it has rich morphological patterns than other languages. The rule based\napproach light-stemmer is proposed in this paper, to find stem word for given\ninflection Tamil word. The performance of proposed approach is compared to a\nrule based suffix removal stemmer based on correctly and incorrectly predicted.\nThe experimental result clearly show that the proposed approach light stemmer\nfor Tamil language perform better than suffix removal stemmer and also more\neffective in Information Retrieval System (IRS).\n",
        "published": "2013-10-02T16:23:00Z",
        "pdf_link": "http://arxiv.org/pdf/1310.0754v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.1285v3",
        "title": "Semantic Measures for the Comparison of Units of Language, Concepts or\n  Instances from Text and Knowledge Base Analysis",
        "summary": "  Semantic measures are widely used today to estimate the strength of the\nsemantic relationship between elements of various types: units of language\n(e.g., words, sentences, documents), concepts or even instances semantically\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\nmeasures play an important role to compare such elements according to semantic\nproxies: texts and knowledge representations, which support their meaning or\ndescribe their nature. Semantic measures are therefore essential for designing\nintelligent agents which will for example take advantage of semantic analysis\nto mimic human ability to compare abstract or concrete objects. This paper\nproposes a comprehensive survey of the broad notion of semantic measure for the\ncomparison of units of language, concepts or instances based on semantic proxy\nanalyses. Semantic measures generalize the well-known notions of semantic\nsimilarity, semantic relatedness and semantic distance, which have been\nextensively studied by various communities over the last decades (e.g.,\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).\n",
        "published": "2013-10-04T14:21:42Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1285v3"
    },
    {
        "id": "http://arxiv.org/abs/1310.1425v1",
        "title": "A State of the Art of Word Sense Induction: A Way Towards Word Sense\n  Disambiguation for Under-Resourced Languages",
        "summary": "  Word Sense Disambiguation (WSD), the process of automatically identifying the\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\npromising developments in the field of NLP and its applications. Indeed,\nimprovement over current performance levels could allow us to take a first step\ntowards natural language understanding. Due to the lack of lexical resources it\nis sometimes difficult to perform WSD for under-resourced languages. This paper\nis an investigation on how to initiate research in WSD for under-resourced\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\ntopics to focus on.\n",
        "published": "2013-10-05T00:33:46Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1425v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.1426v1",
        "title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?",
        "summary": "  This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.\n",
        "published": "2013-10-05T00:39:02Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1426v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.1590v1",
        "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study",
        "summary": "  Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind.\n",
        "published": "2013-10-06T14:37:05Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1590v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.1964v1",
        "title": "Named entity recognition using conditional random fields with non-local\n  relational constraints",
        "summary": "  We begin by introducing the Computer Science branch of Natural Language\nProcessing, then narrowing the attention on its subbranch of Information\nExtraction and particularly on Named Entity Recognition, discussing briefly its\nmain methodological approaches. It follows an introduction to state-of-the-art\nConditional Random Fields under the form of linear chains. Subsequently, the\nidea of constrained inference as a way to model long-distance relationships in\na text is presented, based on an Integer Linear Programming representation of\nthe problem. Adding such relationships to the problem as automatically inferred\nlogical formulas, translatable into linear conditions, we propose to solve the\nresulting more complex problem with the aid of Lagrangian relaxation, of which\nsome technical details are explained. Lastly, we give some experimental\nresults.\n",
        "published": "2013-10-07T22:08:18Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1964v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.1975v1",
        "title": "ARKref: a rule-based coreference resolution system",
        "summary": "  ARKref is a tool for noun phrase coreference. It is a deterministic,\nrule-based system that uses syntactic information from a constituent parser,\nand semantic information from an entity recognition component. Its architecture\nis based on the work of Haghighi and Klein (2009). ARKref was originally\nwritten in 2009. At the time of writing, the last released version was in March\n2011. This document describes that version, which is open-source and publicly\navailable at: http://www.ark.cs.cmu.edu/ARKref\n",
        "published": "2013-10-08T00:30:51Z",
        "pdf_link": "http://arxiv.org/pdf/1310.1975v1"
    },
    {
        "id": "http://arxiv.org/abs/1310.8059v1",
        "title": "Description and Evaluation of Semantic Similarity Measures Approaches",
        "summary": "  In recent years, semantic similarity measure has a great interest in Semantic\nWeb and Natural Language Processing (NLP). Several similarity measures have\nbeen developed, being given the existence of a structured knowledge\nrepresentation offered by ontologies and corpus which enable semantic\ninterpretation of terms. Semantic similarity measures compute the similarity\nbetween concepts/terms included in knowledge sources in order to perform\nestimations. This paper discusses the existing semantic similarity methods\nbased on structure, information content and feature approaches. Additionally,\nwe present a critical evaluation of several categories of semantic similarity\napproaches based on two standard benchmarks. The aim of this paper is to give\nan efficient evaluation of all these measures which help researcher and\npractitioners to select the measure that best fit for their requirements.\n",
        "published": "2013-10-30T08:08:43Z",
        "pdf_link": "http://arxiv.org/pdf/1310.8059v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.0833v1",
        "title": "A Comparative Study on Linguistic Feature Selection in Sentiment\n  Polarity Classification",
        "summary": "  Sentiment polarity classification is perhaps the most widely studied topic.\nIt classifies an opinionated document as expressing a positive or negative\nopinion. In this paper, using movie review dataset, we perform a comparative\nstudy with different single kind linguistic features and the combinations of\nthese features. We find that the classic topic-based classifier(Naive Bayes and\nSupport Vector Machine) do not perform as well on sentiment polarity\nclassification. And we find that with some combination of different linguistic\nfeatures, the classification accuracy can be boosted a lot. We give some\nreasonable explanations about these boosting outcomes.\n",
        "published": "2013-11-04T20:11:35Z",
        "pdf_link": "http://arxiv.org/pdf/1311.0833v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.1169v1",
        "title": "Using Robust PCA to estimate regional characteristics of language use\n  from geo-tagged Twitter messages",
        "summary": "  Principal component analysis (PCA) and related techniques have been\nsuccessfully employed in natural language processing. Text mining applications\nin the age of the online social media (OSM) face new challenges due to\nproperties specific to these use cases (e.g. spelling issues specific to texts\nposted by users, the presence of spammers and bots, service announcements,\netc.). In this paper, we employ a Robust PCA technique to separate typical\noutliers and highly localized topics from the low-dimensional structure present\nin language use in online social networks. Our focus is on identifying\ngeospatial features among the messages posted by the users of the Twitter\nmicroblogging service. Using a dataset which consists of over 200 million\ngeolocated tweets collected over the course of a year, we investigate whether\nthe information present in word usage frequencies can be used to identify\nregional features of language use and topics of interest. Using the PCA pursuit\nmethod, we are able to identify important low-dimensional features, which\nconstitute smoothly varying functions of the geographic location.\n",
        "published": "2013-11-05T19:31:33Z",
        "pdf_link": "http://arxiv.org/pdf/1311.1169v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.1194v1",
        "title": "Identifying Purpose Behind Electoral Tweets",
        "summary": "  Tweets pertaining to a single event, such as a national election, can number\nin the hundreds of millions. Automatically analyzing them is beneficial in many\ndownstream natural language applications such as question answering and\nsummarization. In this paper, we propose a new task: identifying the purpose\nbehind electoral tweets--why do people post election-oriented tweets? We show\nthat identifying purpose is correlated with the related phenomenon of sentiment\nand emotion detection, but yet significantly different. Detecting purpose has a\nnumber of applications including detecting the mood of the electorate,\nestimating the popularity of policies, identifying key issues of contention,\nand predicting the course of events. We create a large dataset of electoral\ntweets and annotate a few thousand tweets for purpose. We develop a system that\nautomatically classifies electoral tweets as per their purpose, obtaining an\naccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class\ntask (both accuracies well above the most-frequent-class baseline). Finally, we\nshow that resources developed for emotion detection are also helpful for\ndetecting purpose.\n",
        "published": "2013-11-05T20:55:23Z",
        "pdf_link": "http://arxiv.org/pdf/1311.1194v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.2978v1",
        "title": "Authorship Attribution Using Word Network Features",
        "summary": "  In this paper, we explore a set of novel features for authorship attribution\nof documents. These features are derived from a word network representation of\nnatural language text. As has been noted in previous studies, natural language\ntends to show complex network structure at word level, with low degrees of\nseparation and scale-free (power law) degree distribution. There has also been\nwork on authorship attribution that incorporates ideas from complex networks.\nThe goal of our paper is to explore properties of these complex networks that\nare suitable as features for machine-learning-based authorship attribution of\ndocuments. We performed experiments on three different datasets, and obtained\npromising results.\n",
        "published": "2013-11-12T23:11:40Z",
        "pdf_link": "http://arxiv.org/pdf/1311.2978v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.3011v2",
        "title": "Cornell SPF: Cornell Semantic Parsing Framework",
        "summary": "  The Cornell Semantic Parsing Framework (SPF) is a learning and inference\nframework for mapping natural language to formal representation of its meaning.\n",
        "published": "2013-11-13T03:58:38Z",
        "pdf_link": "http://arxiv.org/pdf/1311.3011v2"
    },
    {
        "id": "http://arxiv.org/abs/1311.3961v1",
        "title": "HEVAL: Yet Another Human Evaluation Metric",
        "summary": "  Machine translation evaluation is a very important activity in machine\ntranslation development. Automatic evaluation metrics proposed in literature\nare inadequate as they require one or more human reference translations to\ncompare them with output produced by machine translation. This does not always\ngive accurate results as a text can have several different translations. Human\nevaluation metrics, on the other hand, lacks inter-annotator agreement and\nrepeatability. In this paper we have proposed a new human evaluation metric\nwhich addresses these issues. Moreover this metric also provides solid grounds\nfor making sound assumptions on the quality of the text produced by a machine\ntranslation.\n",
        "published": "2013-11-15T19:45:25Z",
        "pdf_link": "http://arxiv.org/pdf/1311.3961v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.5836v1",
        "title": "Automatic Ranking of MT Outputs using Approximations",
        "summary": "  Since long, research on machine translation has been ongoing. Still, we do\nnot get good translations from MT engines so developed. Manual ranking of these\noutputs tends to be very time consuming and expensive. Identifying which one is\nbetter or worse than the others is a very taxing task. In this paper, we show\nan approach which can provide automatic ranks to MT outputs (translations)\ntaken from different MT Engines and which is based on N-gram approximations. We\nprovide a solution where no human intervention is required for ranking systems.\nFurther we also show the evaluations of our results which show equivalent\nresults as that of human ranking.\n",
        "published": "2013-11-22T18:13:06Z",
        "pdf_link": "http://arxiv.org/pdf/1311.5836v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.6045v1",
        "title": "Build Electronic Arabic Lexicon",
        "summary": "  There are many known Arabic lexicons organized on different ways, each of\nthem has a different number of Arabic words according to its organization way.\nThis paper has used mathematical relations to count a number of Arabic words,\nwhich proofs the number of Arabic words presented by Al Farahidy. The paper\nalso presents new way to build an electronic Arabic lexicon by using a hash\nfunction that converts each word (as input) to correspond a unique integer\nnumber (as output), these integer numbers will be used as an index to a lexicon\nentry.\n",
        "published": "2013-11-23T20:10:24Z",
        "pdf_link": "http://arxiv.org/pdf/1311.6045v1"
    },
    {
        "id": "http://arxiv.org/abs/1311.6063v5",
        "title": "NILE: Fast Natural Language Processing for Electronic Health Records",
        "summary": "  Objective: Narrative text in Electronic health records (EHR) contain rich\ninformation for medical and data science studies. This paper introduces the\ndesign and performance of Narrative Information Linear Extraction (NILE), a\nnatural language processing (NLP) package for EHR analysis that we share with\nthe medical informatics community. Methods: NILE uses a modified prefix-tree\nsearch algorithm for named entity recognition, which can detect prefix and\nsuffix sharing. The semantic analyses are implemented as rule-based finite\nstate machines. Analyses include negation, location, modification, family\nhistory, and ignoring. Result: The processing speed of NILE is hundreds to\nthousands times faster than existing NLP software for medical text. The\naccuracy of presence analysis of NILE is on par with the best performing models\non the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and\nbeing able to operate via API make NILE a valuable addition to the NLP software\nfor medical informatics and data science.\n",
        "published": "2013-11-23T22:39:52Z",
        "pdf_link": "http://arxiv.org/pdf/1311.6063v5"
    },
    {
        "id": "http://arxiv.org/abs/1401.0569v2",
        "title": "Natural Language Processing in Biomedicine: A Unified System\n  Architecture Overview",
        "summary": "  In modern electronic medical records (EMR) much of the clinically important\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\nprovided in structured data fields, but rather are encoded in clinician\ngenerated narrative text. Natural language processing (NLP) provides a means of\n\"unlocking\" this important data source for applications in clinical decision\nsupport, quality assurance, and public health. This chapter provides an\noverview of representative NLP systems in biomedicine based on a unified\narchitectural view. A general architecture in an NLP system consists of two\nmain components: background knowledge that includes biomedical knowledge\nresources and a framework that integrates NLP tools to process text. Systems\ndiffer in both components, which we will review briefly. Additionally,\nchallenges facing current research efforts in biomedical NLP include the\npaucity of large, publicly available annotated corpora, although initiatives\nthat facilitate data sharing, system evaluation, and collaborative work between\nresearchers in clinical NLP are starting to emerge.\n",
        "published": "2014-01-03T00:57:13Z",
        "pdf_link": "http://arxiv.org/pdf/1401.0569v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.0640v1",
        "title": "Multi-Topic Multi-Document Summarizer",
        "summary": "  Current multi-document summarization systems can successfully extract summary\nsentences, however with many limitations including: low coverage, inaccurate\nextraction to important sentences, redundancy and poor coherence among the\nselected sentences. The present study introduces a new concept of centroid\napproach and reports new techniques for extracting summary sentences for\nmulti-document. In both techniques keyphrases are used to weigh sentences and\ndocuments. The first summarization technique (Sen-Rich) prefers maximum\nrichness sentences. While the second (Doc-Rich), prefers sentences from\ncentroid document. To demonstrate the new summarization system application to\nextract summaries of Arabic documents we performed two experiments. First, we\napplied Rouge measure to compare the new techniques among systems presented at\nTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.\nSecond, the system was applied to summarize multi-topic documents. Using human\nevaluators, the results show that Doc-Rich is the superior, where summary\nsentences characterized by extra coverage and more cohesion.\n",
        "published": "2014-01-03T13:07:29Z",
        "pdf_link": "http://arxiv.org/pdf/1401.0640v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.0660v1",
        "title": "Plurals: individuals and sets in a richly typed semantics",
        "summary": "  We developed a type-theoretical framework for natural lan- guage semantics\nthat, in addition to the usual Montagovian treatment of compositional\nsemantics, includes a treatment of some phenomena of lex- ical semantic:\ncoercions, meaning, transfers, (in)felicitous co-predication. In this setting\nwe see how the various readings of plurals (collective, dis- tributive,\ncoverings,...) can be modelled.\n",
        "published": "2014-01-03T15:37:19Z",
        "pdf_link": "http://arxiv.org/pdf/1401.0660v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.1158v1",
        "title": "Effective Slot Filling Based on Shallow Distant Supervision Methods",
        "summary": "  Spoken Language Systems at Saarland University (LSV) participated this year\nwith 5 runs at the TAC KBP English slot filling track. Effective algorithms for\nall parts of the pipeline, from document retrieval to relation prediction and\nresponse post-processing, are bundled in a modular end-to-end relation\nextraction system called RelationFactory. The main run solely focuses on\nshallow techniques and achieved significant improvements over LSV's last year's\nsystem, while using the same training data and patterns. Improvements mainly\nhave been obtained by a feature representation focusing on surface skip n-grams\nand improved scoring for extracted distant supervision patterns. Important\nfactors for effective extraction are the training and tuning scheme for distant\nsupervision classifiers, and the query expansion by a translation model based\non Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the\nsubmitted main run of the LSV RelationFactory system achieved the top-ranked\nF1-score of 37.3%.\n",
        "published": "2014-01-06T18:03:11Z",
        "pdf_link": "http://arxiv.org/pdf/1401.1158v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2517v1",
        "title": "The semantic similarity ensemble",
        "summary": "  Computational measures of semantic similarity between geographic terms\nprovide valuable support across geographic information retrieval, data mining,\nand information integration. To date, a wide variety of approaches to\ngeo-semantic similarity have been devised. A judgment of similarity is not\nintrinsically right or wrong, but obtains a certain degree of cognitive\nplausibility, depending on how closely it mimics human behavior. Thus selecting\nthe most appropriate measure for a specific task is a significant challenge. To\naddress this issue, we make an analogy between computational similarity\nmeasures and soliciting domain expert opinions, which incorporate a subjective\nset of beliefs, perceptions, hypotheses, and epistemic biases. Following this\nanalogy, we define the semantic similarity ensemble (SSE) as a composition of\ndifferent similarity measures, acting as a panel of experts having to reach a\ndecision on the semantic similarity of a set of geographic terms. The approach\nis evaluated in comparison to human judgments, and results indicate that an SSE\nperforms better than the average of its parts. Although the best member tends\nto outperform the ensemble, all ensembles outperform the average performance of\neach ensemble's member. Hence, in contexts where the best measure is unknown,\nthe ensemble provides a more cognitively plausible approach.\n",
        "published": "2014-01-11T10:35:37Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2517v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2641v1",
        "title": "Towards a Generic Framework for the Development of Unicode Based Digital\n  Sindhi Dictionaries",
        "summary": "  Dictionaries are essence of any language providing vital linguistic recourse\nfor the language learners, researchers and scholars. This paper focuses on the\nmethodology and techniques used in developing software architecture for a\nUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The\nproposed system provides an accurate solution for construction and\nrepresentation of Unicode based Sindhi characters in a dictionary implementing\nHash Structure algorithm and a custom java Object as its internal data\nstructure saved in a file. The System provides facilities for Insertion,\nDeletion and Editing of new records of Sindhi. Through this framework any type\nof Sindhi to English and English to Sindhi Dictionary (belonging to different\ndomains of knowledge, e.g. engineering, medicine, computer, biology etc.) could\nbe developed easily with accurate representation of Unicode Characters in font\nindependent manner.\n",
        "published": "2014-01-12T16:49:53Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2641v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2663v1",
        "title": "Dictionary-Based Concept Mining: An Application for Turkish",
        "summary": "  In this study, a dictionary-based method is used to extract expressive\nconcepts from documents. So far, there have been many studies concerning\nconcept mining in English, but this area of study for Turkish, an agglutinative\nlanguage, is still immature. We used dictionary instead of WordNet, a lexical\ndatabase grouping words into synsets that is widely used for concept\nextraction. The dictionaries are rarely used in the domain of concept mining,\nbut taking into account that dictionary entries have synonyms, hypernyms,\nhyponyms and other relationships in their meaning texts, the success rate has\nbeen high for determining concepts. This concept extraction method is\nimplemented on documents, that are collected from different corpora.\n",
        "published": "2014-01-12T19:52:49Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2663v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2937v1",
        "title": "A survey of methods to ease the development of highly multilingual text\n  mining applications",
        "summary": "  Multilingual text processing is useful because the information content found\nin different languages is complementary, both regarding facts and opinions.\nWhile Information Extraction and other text mining software can, in principle,\nbe developed for many languages, most text analysis tools have only been\napplied to small sets of languages because the development effort per language\nis large. Self-training tools obviously alleviate the problem, but even the\neffort of providing training data and of manually tuning the results is usually\nconsiderable. In this paper, we gather insights by various multilingual system\ndevelopers on how to minimise the effort of developing natural language\nprocessing applications for many languages. We also explain the main guidelines\nunderlying our own effort to develop complex text mining software for tens of\nlanguages. While these guidelines - most of all: extreme simplicity - can be\nvery restrictive and limiting, we believe to have shown the feasibility of the\napproach through the development of the Europe Media Monitor (EMM) family of\napplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex\nmedia monitoring tools that process and analyse up to 100,000 online news\narticles per day in between twenty and fifty languages. We will also touch upon\nthe kind of language resources that would make it easier for all to develop\nhighly multilingual text mining applications. We will argue that - to achieve\nthis - the most needed resources would be freely available, simple, parallel\nand uniform multilingual dictionaries, corpora and software tools.\n",
        "published": "2014-01-13T18:05:28Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2937v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.2943v1",
        "title": "ONTS: \"Optima\" News Translation System",
        "summary": "  We propose a real-time machine translation system that allows users to select\na news category and to translate the related live news articles from Arabic,\nCzech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and\nTurkish into English. The Moses-based system was optimised for the news domain\nand differs from other available systems in four ways: (1) News items are\nautomatically categorised on the source side, before translation; (2) Named\nentity translation is optimised by recognising and extracting them on the\nsource side and by re-inserting their translation in the target language,\nmaking use of a separate entity repository; (3) News titles are translated with\na separate translation system which is optimised for the specific style of news\ntitles; (4) The system was optimised for speed in order to cope with the large\nvolume of daily news articles.\n",
        "published": "2014-01-13T18:25:10Z",
        "pdf_link": "http://arxiv.org/pdf/1401.2943v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.3669v1",
        "title": "Hrebs and Cohesion Chains as similar tools for semantic text properties\n  research",
        "summary": "  In this study it is proven that the Hrebs used in Denotation analysis of\ntexts and Cohesion Chains (defined as a fusion between Lexical Chains and\nCoreference Chains) represent similar linguistic tools. This result gives us\nthe possibility to extend to Cohesion Chains (CCs) some important indicators\nas, for example the Kernel of CCs, the topicality of a CC, text concentration,\nCC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in\nthe Lexical Chains or Coreference Chains literature these kinds of indicators\nare introduced and used since now. Similarly, some applications of CCs in the\nstudy of a text (as for example segmentation or summarization of a text) could\nbe realized starting from hrebs. As an illustration of the similarity between\nHrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is\ngiven.\n",
        "published": "2014-01-15T17:01:36Z",
        "pdf_link": "http://arxiv.org/pdf/1401.3669v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5674v1",
        "title": "Generalized Biwords for Bitext Compression and Translation Spotting",
        "summary": "  Large bilingual parallel texts (also known as bitexts) are usually stored in\na compressed form, and previous work has shown that they can be more\nefficiently compressed if the fact that the two texts are mutual translations\nis exploited. For example, a bitext can be seen as a sequence of biwords\n---pairs of parallel words with a high probability of co-occurrence--- that can\nbe used as an intermediate representation in the compression process. However,\nthe simple biword approach described in the literature can only exploit\none-to-one word alignments and cannot tackle the reordering of words. We\ntherefore introduce a generalization of biwords which can describe multi-word\nexpressions and reorderings. We also describe some methods for the binary\ncompression of generalized biword sequences, and compare their performance when\ndifferent schemes are applied to the extraction of the biword sequence. In\naddition, we show that this generalization of biwords allows for the\nimplementation of an efficient algorithm to look on the compressed bitext for\nwords or text segments in one of the texts and retrieve their counterpart\ntranslations in the other text ---an application usually referred to as\ntranslation spotting--- with only some minor modifications in the compression\nalgorithm.\n",
        "published": "2014-01-18T21:11:30Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5674v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5693v1",
        "title": "Sentence Compression as Tree Transduction",
        "summary": "  This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model.\n",
        "published": "2014-01-15T05:19:15Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5693v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5694v1",
        "title": "Cross-lingual Annotation Projection for Semantic Roles",
        "summary": "  This article considers the task of automatically inducing role-semantic\nannotations in the FrameNet paradigm for new languages. We propose a general\nframework that is based on annotation projection, phrased as a graph\noptimization problem. It is relatively inexpensive and has the potential to\nreduce the human effort involved in creating role-semantic resources. Within\nthis framework, we present projection models that exploit lexical and syntactic\ninformation. We provide an experimental evaluation on an English-German\nparallel corpus which demonstrates the feasibility of inducing high-precision\nGerman semantic role annotation both for manually and automatically annotated\nEnglish data.\n",
        "published": "2014-01-15T05:40:37Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5694v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5695v1",
        "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches",
        "summary": "  We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases.\n",
        "published": "2014-01-15T05:39:01Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5695v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5696v1",
        "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the\n  Web",
        "summary": "  The task of identifying synonymous relations and objects, or synonym\nresolution, is critical for high-quality information extraction. This paper\ninvestigates synonym resolution in the context of unsupervised information\nextraction, where neither hand-tagged training examples nor domain knowledge is\navailable. The paper presents a scalable, fully-implemented system that runs in\nO(KN log N) time in the number of extractions, N, and the maximum number of\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\nrelational model for predicting whether two strings are co-referential based on\nthe similarity of the assertions containing them. On a set of two million\nassertions extracted from the Web, Resolver resolves objects with 78% precision\nand 68% recall, and resolves relations with 90% precision and 35% recall.\nSeveral variations of resolvers probabilistic model are explored, and\nexperiments demonstrate that under appropriate conditions these variations can\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\npolysemous names with 97% precision and 95% recall on a data set from the TREC\ncorpus.\n",
        "published": "2014-01-15T05:33:07Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5696v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5697v1",
        "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing",
        "summary": "  Adequate representation of natural language semantics requires access to vast\namounts of common sense and domain-specific world knowledge. Prior work in the\nfield was based on purely statistical techniques that did not make use of\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\nor on huge manual efforts such as the CYC project. Here we propose a novel\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\ninterpretation of unrestricted natural language texts. Our method represents\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\nlargest encyclopedia in existence. We explicitly represent the meaning of any\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\nmethod on text categorization and on computing the degree of semantic\nrelatedness between fragments of natural language text. Using ESA results in\nsignificant improvements over the previous state of the art in both tasks.\nImportantly, due to the use of natural concepts, the ESA model is easy to\nexplain to human users.\n",
        "published": "2014-01-15T05:21:01Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5697v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5698v1",
        "title": "Identification of Pleonastic It Using the Web",
        "summary": "  In a significant minority of cases, certain pronouns, especially the pronoun\nit, can be used without referring to any specific entity. This phenomenon of\npleonastic pronoun usage poses serious problems for systems aiming at even a\nshallow understanding of natural language texts. In this paper, a novel\napproach is proposed to identify such uses of it: the extrapositional cases are\nidentified using a series of queries against the web, and the cleft cases are\nidentified using a simple set of syntactic rules. The system is evaluated with\nfour sets of news articles containing 679 extrapositional cases as well as 78\ncleft constructs. The identification results are comparable to those obtained\nby human efforts.\n",
        "published": "2014-01-15T05:11:43Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5698v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5699v1",
        "title": "Text Relatedness Based on a Word Thesaurus",
        "summary": "  The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches.\n",
        "published": "2014-01-15T05:41:08Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5699v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.5700v1",
        "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel\n  Corpora",
        "summary": "  This paper describes a method for the automatic inference of structural\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\nfrom small parallel corpora. The structural transfer rules are based on\nalignment templates, like those used in statistical MT. Alignment templates are\nextracted from sentence-aligned parallel corpora and extended with a set of\nrestrictions which are derived from the bilingual dictionary of the MT system\nand control their application as transfer rules. The experiments conducted\nusing three different language pairs in the free/open-source MT platform\nApertium show that translation quality is improved as compared to word-for-word\ntranslation (when no transfer rules are used), and that the resulting\ntranslation quality is close to that obtained using hand-coded transfer rules.\nThe method we present is entirely unsupervised and benefits from information in\nthe rest of modules of the MT system in which the inferred rules are applied.\n",
        "published": "2014-01-15T05:28:26Z",
        "pdf_link": "http://arxiv.org/pdf/1401.5700v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6050v1",
        "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale\n  Feature Selection",
        "summary": "  Semantic parsing, i.e., the automatic derivation of meaning representation\nsuch as an instantiated predicate-argument structure for a sentence, plays a\ncritical role in deep processing of natural language. Unlike all other top\nsystems of semantic dependency parsing that have to rely on a pipeline\nframework to chain up a series of submodels each specialized for a specific\nsubtask, the one presented in this article integrates everything into one\nmodel, in hopes of achieving desirable integrity and practicality for real\napplications while maintaining a competitive performance. This integrative\napproach tackles semantic parsing as a word pair classification problem using a\nmaximum entropy classifier. We leverage adaptive pruning of argument candidates\nand large-scale feature selection engineering to allow the largest feature\nspace ever in use so far in this field, it achieves a state-of-the-art\nperformance on the evaluation data set for CoNLL-2008 shared task, on top of\nall but one top pipeline system, confirming its feasibility and effectiveness.\n",
        "published": "2014-01-23T16:45:39Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6050v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6122v1",
        "title": "Identifying Bengali Multiword Expressions using Semantic Clustering",
        "summary": "  One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus.\n",
        "published": "2014-01-23T19:03:18Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6122v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6330v2",
        "title": "A Statistical Parsing Framework for Sentiment Classification",
        "summary": "  We present a statistical parsing framework for sentence-level sentiment\nclassification in this article. Unlike previous works that employ syntactic\nparsing results for sentiment analysis, we develop a statistical parser to\ndirectly analyze the sentiment structure of a sentence. We show that\ncomplicated phenomena in sentiment analysis (e.g., negation, intensification,\nand contrast) can be handled the same as simple and straightforward sentiment\nexpressions in a unified and probabilistic way. We formulate the sentiment\ngrammar upon Context-Free Grammars (CFGs), and provide a formal description of\nthe sentiment parsing framework. We develop the parsing model to obtain\npossible sentiment parse trees for a sentence, from which the polarity model is\nproposed to derive the sentiment strength and polarity, and the ranking model\nis dedicated to selecting the best sentiment tree. We train the parser directly\nfrom examples of sentences annotated only with sentiment polarity labels but\nwithout any syntactic annotations or polarity annotations of constituents\nwithin sentences. Therefore we can obtain training data easily. In particular,\nwe train a sentiment parser, s.parser, from a large amount of review sentences\nwith users' ratings as rough sentiment polarity labels. Extensive experiments\non existing benchmark datasets show significant improvements over baseline\nsentiment classification approaches.\n",
        "published": "2014-01-24T12:56:36Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6330v2"
    },
    {
        "id": "http://arxiv.org/abs/1401.6422v1",
        "title": "Automatic Aggregation by Joint Modeling of Aspects and Values",
        "summary": "  We present a model for aggregation of product review snippets by joint aspect\nidentification and sentiment analysis. Our model simultaneously identifies an\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\nsushi and miso for a Japanese restaurant) and determines the corresponding\nsentiment of each aspect. This approach directly enables discovery of\nhighly-rated or inconsistent aspects of a product. Our generative model admits\nan efficient variational mean-field inference algorithm. It is also easily\nextensible, and we describe several modifications and their effects on model\nstructure and inference. We test our model on two tasks, joint aspect\nidentification and sentiment analysis on a set of Yelp reviews and aspect\nidentification alone on a set of medical summaries. We evaluate the performance\nof the model on aspect identification, sentiment analysis, and per-word\nlabeling accuracy. We demonstrate that our model outperforms applicable\nbaselines by a considerable margin, yielding up to 32% relative error reduction\non aspect identification and up to 20% relative error reduction on sentiment\nanalysis.\n",
        "published": "2014-01-23T02:48:07Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6422v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6875v1",
        "title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World",
        "summary": "  To tackle the vocabulary problem in conversational systems, previous work has\napplied unsupervised learning approaches on co-occurring speech and eye gaze\nduring interaction to automatically acquire new words. Although these\napproaches have shown promise, several issues related to human language\nbehavior and human-machine conversation have not been addressed. First,\npsycholinguistic studies have shown certain temporal regularities between human\neye movement and language production. While these regularities can potentially\nguide the acquisition process, they have not been incorporated in the previous\nunsupervised approaches. Second, conversational systems generally have an\nexisting knowledge base about the domain and vocabulary. While the existing\nknowledge can potentially help bootstrap and constrain the acquired new words,\nit has not been incorporated in the previous models. Third, eye gaze could\nserve different functions in human-machine conversation. Some gaze streams may\nnot be closely coupled with speech stream, and thus are potentially detrimental\nto word acquisition. Automated recognition of closely-coupled speech-gaze\nstreams based on conversation context is important. To address these issues, we\ndeveloped new approaches that incorporate user language behavior, domain\nknowledge, and conversation context in word acquisition. We evaluated these\napproaches in the context of situated dialogue in a virtual world. Our\nexperimental results have shown that incorporating the above three types of\ncontextual information significantly improves word acquisition performance.\n",
        "published": "2014-01-16T04:48:43Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6875v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.6876v1",
        "title": "Improving Statistical Machine Translation for a Resource-Poor Language\n  Using Related Resource-Rich Languages",
        "summary": "  We propose a novel language-independent approach for improving machine\ntranslation for resource-poor languages by exploiting their similarity to\nresource-rich ones. More precisely, we improve the translation from a\nresource-poor source language X_1 into a resource-rich language Y given a\nbi-text containing a limited number of parallel sentences for X_1-Y and a\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\nrelated to X_1. This is achieved by taking advantage of the opportunities that\nvocabulary overlap and similarities between the languages X_1 and X_2 in\nspelling, word order, and syntax offer: (1) we improve the word alignments for\nthe resource-poor language, (2) we further augment it with additional\ntranslation options, and (3) we take care of potential spelling differences\nthrough appropriate transliteration. The evaluation for Indonesian- >English\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\nrespectively, which is an improvement over the best rivaling approaches, while\nusing much less additional data. Overall, our method cuts the amount of\nnecessary \"real training data by a factor of 2--5.\n",
        "published": "2014-01-23T02:42:12Z",
        "pdf_link": "http://arxiv.org/pdf/1401.6876v1"
    },
    {
        "id": "http://arxiv.org/abs/1401.7077v4",
        "title": "Quantifying literature quality using complexity criteria",
        "summary": "  We measured entropy and symbolic diversity for English and Spanish texts\nincluding literature Nobel laureates and other famous authors. Entropy, symbol\ndiversity and symbol frequency profiles were compared for these four groups. We\nalso built a scale sensitive to the quality of writing and evaluated its\nrelationship with the Flesch's readability index for English and the\nSzigriszt's perspicuity index for Spanish. Results suggest a correlation\nbetween entropy and word diversity with quality of writing. Text genre also\ninfluences the resulting entropy and diversity of the text. Results suggest the\nplausibility of automated quality assessment of texts.\n",
        "published": "2014-01-28T03:48:01Z",
        "pdf_link": "http://arxiv.org/pdf/1401.7077v4"
    },
    {
        "id": "http://arxiv.org/abs/1402.0563v1",
        "title": "Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine\n  Translation",
        "summary": "  Although, Chinese and Spanish are two of the most spoken languages in the\nworld, not much research has been done in machine translation for this language\npair. This paper focuses on investigating the state-of-the-art of\nChinese-to-Spanish statistical machine translation (SMT), which nowadays is one\nof the most popular approaches to machine translation. For this purpose, we\nreport details of the available parallel corpus which are Basic Traveller\nExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we\nconduct experimental work with the largest of these three corpora to explore\nalternative SMT strategies by means of using a pivot language. Three\nalternatives are considered for pivoting: cascading, pseudo-corpus and\ntriangulation. As pivot language, we use either English, Arabic or French.\nResults show that, for a phrase-based SMT system, English is the best pivot\nlanguage between Chinese and Spanish. We propose a system output combination\nusing the pivot strategies which is capable of outperforming the direct\ntranslation strategy. The main objective of this work is motivating and\ninvolving the research community to work in this important pair of languages\ngiven their demographic impact.\n",
        "published": "2014-02-04T01:34:56Z",
        "pdf_link": "http://arxiv.org/pdf/1402.0563v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.0578v1",
        "title": "Natural Language Inference for Arabic Using Extended Tree Edit Distance\n  with Subtrees",
        "summary": "  Many natural language processing (NLP) applications require the computation\nof similarities between pairs of syntactic or semantic trees. Many researchers\nhave used tree edit distance for this task, but this technique suffers from the\ndrawback that it deals with single node operations only. We have extended the\nstandard tree edit distance algorithm to deal with subtree transformation\noperations as well as single nodes. The extended algorithm with subtree\noperations, TED+ST, is more effective and flexible than the standard algorithm,\nespecially for applications that pay attention to relations among nodes (e.g.\nin linguistic trees, deleting a modifier subtree should be cheaper than the sum\nof deleting its components individually). We describe the use of TED+ST for\nchecking entailment between two Arabic text snippets. The preliminary results\nof using TED+ST were encouraging when compared with two string-based approaches\nand with the standard algorithm.\n",
        "published": "2014-02-04T01:40:42Z",
        "pdf_link": "http://arxiv.org/pdf/1402.0578v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.0586v1",
        "title": "Topic Segmentation and Labeling in Asynchronous Conversations",
        "summary": "  Topic segmentation and labeling is often considered a prerequisite for\nhigher-level conversation analysis and has been shown to be useful in many\nNatural Language Processing (NLP) applications. We present two new corpora of\nemail and blog conversations annotated with topics, and evaluate annotator\nreliability for the segmentation and labeling tasks in these asynchronous\nconversations. We propose a complete computational framework for topic\nsegmentation and labeling in asynchronous conversations. Our approach extends\nstate-of-the-art methods by considering a fine-grained structure of an\nasynchronous conversation, along with other conversational features by applying\nrecent graph-based methods for NLP. For topic segmentation, we propose two\nnovel unsupervised models that exploit the fine-grained conversational\nstructure, and a novel graph-theoretic supervised model that combines lexical,\nconversational and topic features. For topic labeling, we propose two novel\n(unsupervised) random walk models that respectively capture conversation\nspecific clues from two different sources: the leading sentences and the\nfine-grained conversational structure. Empirical evaluation shows that the\nsegmentation and the labeling performed by our best models beat the\nstate-of-the-art, and are highly correlated with human annotations.\n",
        "published": "2014-02-04T01:43:35Z",
        "pdf_link": "http://arxiv.org/pdf/1402.0586v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.2561v1",
        "title": "The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance\n  a Bilingual Dictionary",
        "summary": "  Bilingual machine-readable dictionaries are knowledge resources useful in\nmany automatic tasks. However, compared to monolingual computational lexicons\nlike WordNet, bilingual dictionaries typically provide a lower amount of\nstructured information, such as lexical and semantic relations, and often do\nnot cover the entire range of possible translations for a word of interest. In\nthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the\nautomated disambiguation of ambiguous translations in the lexical entries of a\nbilingual machine-readable dictionary. The dictionary is represented as a\ngraph, and cyclic patterns are sought in the graph to assign an appropriate\nsense tag to each translation in a lexical entry. Further, we use the\nalgorithms output to improve the quality of the dictionary itself, by\nsuggesting accurate solutions to structural problems such as misalignments,\npartial alignments and missing entries. Finally, we successfully apply CQC to\nthe task of synonym extraction.\n",
        "published": "2014-01-18T21:08:40Z",
        "pdf_link": "http://arxiv.org/pdf/1402.2561v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.2796v1",
        "title": "PR2: A Language Independent Unsupervised Tool for Personality\n  Recognition from Text",
        "summary": "  We present PR2, a personality recognition system available online, that\nperforms instance-based classification of Big5 personality types from\nunstructured text, using language-independent features. It has been tested on\nEnglish and Italian, achieving performances up to f=.68.\n",
        "published": "2014-02-12T11:55:31Z",
        "pdf_link": "http://arxiv.org/pdf/1402.2796v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.3040v1",
        "title": "Event Structure of Transitive Verb: A MARVS perspective",
        "summary": "  Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of\nthe representation of verbal semantics that is based on Mandarin Chinese data\n(Huang et al. 2000). In the MARVS theory, there are two different types of\nmodules: Event Structure Modules and Role Modules. There are also two sets of\nattributes: Event-Internal Attributes and Role-Internal Attributes, which are\nlinked to the Event Structure Module and the Role Module, respectively. In this\nstudy, we focus on four transitive verbs as chi1(eat), wan2(play),\nhuan4(change) and shao1(burn) and explore their event structures by the MARVS\ntheory.\n",
        "published": "2014-02-13T06:44:24Z",
        "pdf_link": "http://arxiv.org/pdf/1402.3040v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.3371v1",
        "title": "An evaluative baseline for geo-semantic relatedness and similarity",
        "summary": "  In geographic information science and semantics, the computation of semantic\nsimilarity is widely recognised as key to supporting a vast number of tasks in\ninformation integration and retrieval. By contrast, the role of geo-semantic\nrelatedness has been largely ignored. In natural language processing, semantic\nrelatedness is often confused with the more specific semantic similarity. In\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\nsemantic fields, and we compare it with geo-semantic similarity. We then\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\nnew open dataset designed to evaluate computational measures of geo-semantic\nrelatedness and similarity. This dataset is larger than existing datasets of\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\nby 203 human subjects. GeReSiD is available online and can be used as an\nevaluation baseline to determine empirically to what degree a given\ncomputational model approximates geo-semantic relatedness and similarity.\n",
        "published": "2014-02-14T06:06:47Z",
        "pdf_link": "http://arxiv.org/pdf/1402.3371v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.3382v1",
        "title": "Machine Learning of Phonologically Conditioned Noun Declensions For\n  Tamil Morphological Generators",
        "summary": "  This paper presents machine learning solutions to a practical problem of\nNatural Language Generation (NLG), particularly the word formation in\nagglutinative languages like Tamil, in a supervised manner. The morphological\ngenerator is an important component of Natural Language Processing in\nArtificial Intelligence. It generates word forms given a root and affixes. The\nmorphophonemic changes like addition, deletion, alternation etc., occur when\ntwo or more morphemes or words joined together. The Sandhi rules should be\nexplicitly specified in the rule based morphological analyzers and generators.\nIn machine learning framework, these rules can be learned automatically by the\nsystem from the training samples and subsequently be applied for new inputs. In\nthis paper we proposed the machine learning models which learn the\nmorphophonemic rules for noun declensions from the given training data. These\nmodels are trained to learn sandhi rules using various learning algorithms and\nthe performance of those algorithms are presented. From this we conclude that\nmachine learning of morphological processing such as word form generation can\nbe successfully learned in a supervised manner, without explicit description of\nrules. The performance of Decision trees and Bayesian machine learning\nalgorithms on noun declensions are discussed.\n",
        "published": "2014-02-14T06:46:44Z",
        "pdf_link": "http://arxiv.org/pdf/1402.3382v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.4380v1",
        "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text\n  Classification",
        "summary": "  A Verbal Autopsy is the record of an interview about the circumstances of an\nuncertified death. In developing countries, if a death occurs away from health\nfacilities, a field-worker interviews a relative of the deceased about the\ncircumstances of the death; this Verbal Autopsy can be reviewed off-site. We\nreport on a comparative study of the processes involved in Text Classification\napplied to classifying Cause of Death: feature value representation; machine\nlearning classification algorithms; and feature reduction strategies in order\nto identify the suitable approaches applicable to the classification of Verbal\nAutopsy text. We demonstrate that normalised term frequency and the standard\nTFiDF achieve comparable performance across a number of classifiers. The\nresults also show Support Vector Machine is superior to other classification\nalgorithms employed in this research. Finally, we demonstrate the effectiveness\nof employing a \"locally-semi-supervised\" feature reduction strategy in order to\nincrease performance accuracy.\n",
        "published": "2014-02-18T16:02:05Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4380v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.4678v1",
        "title": "When Learners Surpass their Sources: Mathematical Modeling of Learning\n  from an Inconsistent Source",
        "summary": "  We present a new algorithm to model and investigate the learning process of a\nlearner mastering a set of grammatical rules from an inconsistent source. The\ncompelling interest of human language acquisition is that the learning succeeds\nin virtually every case, despite the fact that the input data are formally\ninadequate to explain the success of learning. Our model explains how a learner\ncan successfully learn from or even surpass its imperfect source without\npossessing any additional biases or constraints about the types of patterns\nthat exist in the language. We use the data collected by Singleton and Newport\n(2004) on the performance of a 7-year boy Simon, who mastered the American Sign\nLanguage (ASL) by learning it from his parents, both of whom were imperfect\nspeakers of ASL. We show that the algorithm possesses a frequency-boosting\nproperty, whereby the frequency of the most common form of the source is\nincreased by the learner. We also explain several key features of Simon's ASL.\n",
        "published": "2014-02-18T02:18:10Z",
        "pdf_link": "http://arxiv.org/pdf/1402.4678v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.6516v1",
        "title": "Modelling the Lexicon in Unsupervised Part of Speech Induction",
        "summary": "  Automatically inducing the syntactic part-of-speech categories for words in\ntext is a fundamental task in Computational Linguistics. While the performance\nof unsupervised tagging models has been slowly improving, current\nstate-of-the-art systems make the obviously incorrect assumption that all\ntokens of a given word type must share a single part-of-speech tag. This\none-tag-per-type heuristic counters the tendency of Hidden Markov Model based\ntaggers to over generate tags for a given word type. However, it is clearly\nincompatible with basic syntactic theory. In this paper we extend a\nstate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model\nof the lexicon. In doing so we are able to incorporate a soft bias towards\ninducing few tags per type. We develop a particle filter for drawing samples\nfrom the posterior of our model and present empirical results that show that\nour model is competitive with and faster than the state-of-the-art without\nmaking any unrealistic restrictions.\n",
        "published": "2014-02-26T12:37:04Z",
        "pdf_link": "http://arxiv.org/pdf/1402.6516v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.6880v1",
        "title": "It's distributions all the way down!: Second order changes in\n  statistical distributions also occur",
        "summary": "  The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et\nals) message on distributions; it largely examines the first-order effects of\nhow a single, signature distribution can predict population behaviour,\nneglecting second-order effects involving distributional shifts, either between\nsignature distributions or within a given signature distribution. Indeed,\nBentley et al. themselves under-emphasise the potential richness of the latter,\nwithin-distribution effects.\n",
        "published": "2014-02-27T12:12:11Z",
        "pdf_link": "http://arxiv.org/pdf/1402.6880v1"
    },
    {
        "id": "http://arxiv.org/abs/1402.7265v1",
        "title": "Semantics, Modelling, and the Problem of Representation of Meaning -- a\n  Brief Survey of Recent Literature",
        "summary": "  Over the past 50 years many have debated what representation should be used\nto capture the meaning of natural language utterances. Recently new needs of\nsuch representations have been raised in research. Here I survey some of the\ninteresting representations suggested to answer for these new needs.\n",
        "published": "2014-02-28T14:49:31Z",
        "pdf_link": "http://arxiv.org/pdf/1402.7265v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.0052v1",
        "title": "TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding\n  Initiative guidelines",
        "summary": "  This paper presents an attempt to customise the TEI (Text Encoding\nInitiative) guidelines in order to offer the possibility to incorporate TBX\n(TermBase eXchange) based terminological entries within any kind of TEI\ndocuments. After presenting the general historical, conceptual and technical\ncontexts, we describe the various design choices we had to take while creating\nthis customisation, which in turn have led to make various changes in the\nactual TBX serialisation. Keeping in mind the objective to provide the TEI\nguidelines with, again, an onomasiological model, we try to identify the best\ncomprise in maintaining both the isomorphism with the existing TBX Basic\nstandard and the characteristics of the TEI framework.\n",
        "published": "2014-03-01T06:46:11Z",
        "pdf_link": "http://arxiv.org/pdf/1403.0052v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.0531v1",
        "title": "We Tweet Like We Talk and Other Interesting Observations: An Analysis of\n  English Communication Modalities",
        "summary": "  Modalities of communication for human beings are gradually increasing in\nnumber with the advent of new forms of technology. Many human beings can\nreadily transition between these different forms of communication with little\nor no effort, which brings about the question: How similar are these different\ncommunication modalities? To understand technology$\\text{'}$s influence on\nEnglish communication, four different corpora were analyzed and compared:\nWriting from Books using the 1-grams database from the Google Books project,\nTwitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices\nrevealed that Talking has the most similarity when compared to the other modes\nof communication, while 1-grams were the least similar form of communication\nanalyzed. Based on the analysis of word usage, word usage frequency\ndistributions, and word class usage, among other things, Talking is also the\nmost similar to Twitter and IRC Chat. This suggests that communicating using\nTwitter and IRC Chat evolved from Talking rather than Writing. When we\ncommunicate online, even though we are writing, we do not Tweet or Chat how we\nwrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing\nwere clearly differentiable from our analysis with Twitter and Chat being much\nmore similar to Fiction than Nonfiction writing. These hypotheses were then\ntested using author and journalists Cory Doctorow. Mr. Doctorow$\\text{'}$s\nWriting, Twitter usage, and Talking were all found to have very similar\nvocabulary usage patterns as the amalgamized populations, as long as the\nwriting was Fiction. However, Mr. Doctorow$\\text{'}$s Nonfiction writing is\ndifferent from 1-grams and other collected Nonfiction writings. This data could\nperhaps be used to create more entertaining works of Nonfiction.\n",
        "published": "2014-03-03T19:27:23Z",
        "pdf_link": "http://arxiv.org/pdf/1403.0531v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.0801v2",
        "title": "Is getting the right answer just about choosing the right words? The\n  role of syntactically-informed features in short answer scoring",
        "summary": "  Developments in the educational landscape have spurred greater interest in\nthe problem of automatically scoring short answer questions. A recent shared\ntask on this topic revealed a fundamental divide in the modeling approaches\nthat have been applied to this problem, with the best-performing systems split\nbetween those that employ a knowledge engineering approach and those that\nalmost solely leverage lexical information (as opposed to higher-level\nsyntactic information) in assigning a score to a given response. This paper\naims to introduce the NLP community to the largest corpus currently available\nfor short-answer scoring, provide an overview of methods used in the shared\ntask using this data, and explore the extent to which more\nsyntactically-informed features can contribute to the short answer scoring task\nin a way that avoids the question-specific manual effort of the knowledge\nengineering approach.\n",
        "published": "2014-03-04T14:45:56Z",
        "pdf_link": "http://arxiv.org/pdf/1403.0801v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.2004v1",
        "title": "Natural Language Feature Selection via Cooccurrence",
        "summary": "  Specificity is important for extracting collocations, keyphrases, multi-word\nand index terms [Newman et al. 2012]. It is also useful for tagging, ontology\nconstruction [Ryu and Choi 2006], and automatic summarization of documents\n[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and\ninverse-document frequency (TF-IDF) are typically used to do this, but fail to\ntake advantage of the semantic relationships between terms [Church and Gale\n1995]. The result is that general idiomatic terms are mistaken for specific\nterms. We demonstrate use of relational data for estimation of term\nspecificity. The specificity of a term can be learned from its distribution of\nrelations with other terms. This technique is useful for identifying relevant\nwords or terms for other natural language processing tasks.\n",
        "published": "2014-03-08T20:10:37Z",
        "pdf_link": "http://arxiv.org/pdf/1403.2004v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.2124v1",
        "title": "Generating Music from Literature",
        "summary": "  We present a system, TransProse, that automatically generates musical pieces\nfrom text. TransProse uses known relations between elements of music such as\ntempo and scale, and the emotions they evoke. Further, it uses a novel\nmechanism to determine sequences of notes that capture the emotional activity\nin the text. The work has applications in information visualization, in\ncreating audio-visual e-books, and in developing music apps.\n",
        "published": "2014-03-10T01:41:09Z",
        "pdf_link": "http://arxiv.org/pdf/1403.2124v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.2837v1",
        "title": "HPS: a hierarchical Persian stemming method",
        "summary": "  In this paper, a novel hierarchical Persian stemming approach based on the\nPart-Of-Speech of the word in a sentence is presented. The implemented stemmer\nincludes hash tables and several deterministic finite automata in its different\nlevels of hierarchy for removing the prefixes and suffixes of the words. We had\ntwo intentions in using hash tables in our method. The first one is that the\nDFA don't support some special words, so hash table can partly solve the\naddressed problem. the second goal is to speed up the implemented stemmer with\nomitting the time that deterministic finite automata need. Because of the\nhierarchical organization, this method is fast and flexible enough. Our\nexperiments on test sets from Hamshahri collection and security news (istna.ir)\nshow that our method has the average accuracy of 95.37% which is even improved\nin using the method on a test set with common topics.\n",
        "published": "2014-03-12T08:08:49Z",
        "pdf_link": "http://arxiv.org/pdf/1403.2837v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.3351v1",
        "title": "Semantic Unification A sheaf theoretic approach to natural language",
        "summary": "  Language is contextual and sheaf theory provides a high level mathematical\nframework to model contextuality. We show how sheaf theory can model the\ncontextual nature of natural language and how gluing can be used to provide a\nglobal semantics for a discourse by putting together the local logical\nsemantics of each sentence within the discourse. We introduce a presheaf\nstructure corresponding to a basic form of Discourse Representation Structures.\nWithin this setting, we formulate a notion of semantic unification --- gluing\nmeanings of parts of a discourse into a coherent whole --- as a form of\nsheaf-theoretic gluing. We illustrate this idea with a number of examples where\nit can used to represent resolutions of anaphoric references. We also discuss\nmultivalued gluing, described using a distributions functor, which can be used\nto represent situations where multiple gluings are possible, and where we may\nneed to rank them using quantitative measures.\n  Dedicated to Jim Lambek on the occasion of his 90th birthday.\n",
        "published": "2014-03-13T18:20:10Z",
        "pdf_link": "http://arxiv.org/pdf/1403.3351v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.3668v2",
        "title": "Language Heedless of Logic - Philosophy Mindful of What? Failures of\n  Distributive and Absorption Laws",
        "summary": "  Much of philosophical logic and all of philosophy of language make empirical\nclaims about the vernacular natural language. They presume semantics under\nwhich `and' and `or' are related by the dually paired distributive and\nabsorption laws. However, at least one of each pair of laws fails in the\nvernacular. `Implicature'-based auxiliary theories associated with the\nprogramme of H.P. Grice do not prove remedial. Conceivable alternatives that\nmight replace the familiar logics as descriptive instruments are briefly noted:\n(i) substructural logics and (ii) meaning composition in linear algebras over\nthe reals, occasionally constrained by norms of classical logic. Alternative\n(ii) locates the problem in violations of one of the idempotent laws. Reasons\nfor a lack of curiosity about elementary and easily testable implications of\nthe received theory are considered. The concept of `reflective equilibrium' is\ncritically examined for its role in reconciling normative desiderata and\ndescriptive commitments.\n",
        "published": "2014-03-14T18:53:51Z",
        "pdf_link": "http://arxiv.org/pdf/1403.3668v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.4024v3",
        "title": "Measuring Global Similarity between Texts",
        "summary": "  We propose a new similarity measure between texts which, contrary to the\ncurrent state-of-the-art approaches, takes a global view of the texts to be\ncompared. We have implemented a tool to compute our textual distance and\nconducted experiments on several corpuses of texts. The experiments show that\nour methods can reliably identify different global types of texts.\n",
        "published": "2014-03-17T08:22:54Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4024v3"
    },
    {
        "id": "http://arxiv.org/abs/1403.4467v2",
        "title": "A hybrid formalism to parse Sign Languages",
        "summary": "  Sign Language (SL) linguistic is dependent on the expensive task of\nannotating. Some automation is already available for low-level information (eg.\nbody part tracking) and the lexical level has shown significant progresses. The\nsyntactic level lacks annotated corpora as well as complete and consistent\nmodels. This article presents a solution for the automatic annotation of SL\nsyntactic elements. It exposes a formalism able to represent both\nconstituency-based and dependency-based models. The first enable the\nrepresentation the structures one may want to annotate, the second aims at\nfulfilling the holes of the first. A parser is presented and used to conduct\ntwo experiments on the solution. One experiment is on a real corpus, the other\nis on a synthetic corpus.\n",
        "published": "2014-03-18T14:16:45Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4467v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.4473v1",
        "title": "Sign Language Gibberish for syntactic parsing evaluation",
        "summary": "  Sign Language (SL) automatic processing slowly progresses bottom-up. The\nfield has seen proposition to handle the video signal, to recognize and\nsynthesize sublexical and lexical units. It starts to see the development of\nsupra-lexical processing. But the recognition, at this level, lacks data. The\nsyntax of SL appears very specific as it uses massively the multiplicity of\narticulators and its access to the spatial dimensions. Therefore new parsing\ntechniques are developed. However these need to be evaluated. The shortage on\nreal data restrains the corpus-based models to small sizes. We propose here a\nsolution to produce data-sets for the evaluation of parsers on the specific\nproperties of SL. The article first describes the general model used to\ngenerates dependency grammars and the phrase generation from these lasts. It\nthen discusses the limits of approach. The solution shows to be of particular\ninterest to evaluate the scalability of the techniques on big models.\n",
        "published": "2014-03-18T14:31:51Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4473v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.4759v1",
        "title": "Spelling Error Trends and Patterns in Sindhi",
        "summary": "  Statistical error Correction technique is the most accurate and widely used\napproach today, but for a language like Sindhi which is a low resourced\nlanguage the trained corpora's are not available, so the statistical techniques\nare not possible at all. Instead a useful alternative would be to exploit\nvarious spelling error trends in Sindhi by using a Rule based approach. For\ndesigning such technique an essential prerequisite would be to study the\nvarious error patterns in a language. This pa per presents various studies of\nspelling error trends and their types in Sindhi Language. The research shows\nthat the error trends common to all languages are also encountered in Sindhi\nbut their do exist some error patters that are catered specifically to a Sindhi\nlanguage.\n",
        "published": "2014-03-19T10:45:29Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4759v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.4887v2",
        "title": "Using Entropy Estimates for DAG-Based Ontologies",
        "summary": "  Motivation: Entropy measurements on hierarchical structures have been used in\nmethods for information retrieval and natural language modeling. Here we\nexplore its application to semantic similarity. By finding shared ontology\nterms, semantic similarity can be established between annotated genes. A common\nprocedure for establishing semantic similarity is to calculate the\ndescriptiveness (information content) of ontology terms and use these values to\ndetermine the similarity of annotations. Most often information content is\ncalculated for an ontology term by analyzing its frequency in an annotation\ncorpus. The inherent problems in using these values to model functional\nsimilarity motivates our work. Summary: We present a novel calculation for\nestablishing the entropy of a DAG-based ontology, which can be used in an\nalternative method for establishing the information content of its terms. We\nalso compare our IC metric to two others using semantic and sequence\nsimilarity.\n",
        "published": "2014-03-19T17:29:24Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4887v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.4928v1",
        "title": "Clinical TempEval",
        "summary": "  We describe the Clinical TempEval task which is currently in preparation for\nthe SemEval-2015 evaluation exercise. This task involves identifying and\ndescribing events, times and the relations between them in clinical text. Six\ndiscrete subtasks are included, focusing on recognising mentions of times and\nevents, describing those mentions for both entity types, identifying the\nrelation between an event and the document creation time, and identifying\nnarrative container relations.\n",
        "published": "2014-03-19T19:59:49Z",
        "pdf_link": "http://arxiv.org/pdf/1403.4928v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.6381v1",
        "title": "An efficiency dependency parser using hybrid approach for tamil language",
        "summary": "  Natural language processing is a prompt research area across the country.\nParsing is one of the very crucial tool in language analysis system which aims\nto forecast the structural relationship among the words in a given sentence.\nMany researchers have already developed so many language tools but the accuracy\nis not meet out the human expectation level, thus the research is still exists.\nMachine translation is one of the major application area under Natural Language\nProcessing. While translation between one language to another language, the\nstructure identification of a sentence play a key role. This paper introduces\nthe hybrid way to solve the identification of relationship among the given\nwords in a sentence. In existing system is implemented using rule based\napproach, which is not suited in huge amount of data. The machine learning\napproaches is suitable for handle larger amount of data and also to get better\naccuracy via learning and training the system. The proposed approach takes a\nTamil sentence as an input and produce the result of a dependency relation as a\ntree like structure using hybrid approach. This proposed tool is very helpful\nfor researchers and act as an odd-on improve the quality of existing\napproaches.\n",
        "published": "2014-03-21T04:54:28Z",
        "pdf_link": "http://arxiv.org/pdf/1403.6381v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.6392v2",
        "title": "Implementation of an Automatic Sign Language Lexical Annotation\n  Framework based on Propositional Dynamic Logic",
        "summary": "  In this paper, we present the implementation of an automatic Sign Language\n(SL) sign annotation framework based on a formal logic, the Propositional\nDynamic Logic (PDL). Our system relies heavily on the use of a specific variant\nof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets\nus describe SL signs as formulae and corpora videos as labeled transition\nsystems (LTSs). Here, we intend to show how a generic annotation system can be\nconstructed upon these underlying theoretical principles, regardless of the\ntracking technologies available or the input format of corpora. With this in\nmind, we generated a development framework that adapts the system to specific\nuse cases. Furthermore, we present some results obtained by our application\nwhen adapted to one distinct case, 2D corpora analysis with pre-processed\ntracking information. We also present some insights on how such a technology\ncan be used to analyze 3D real-time data, captured with a depth device.\n",
        "published": "2014-03-25T15:36:36Z",
        "pdf_link": "http://arxiv.org/pdf/1403.6392v2"
    },
    {
        "id": "http://arxiv.org/abs/1403.6636v1",
        "title": "Sign Language Lexical Recognition With Propositional Dynamic Logic",
        "summary": "  This paper explores the use of Propositional Dynamic Logic (PDL) as a\nsuitable formal framework for describing Sign Language (SL), the language of\ndeaf people, in the context of natural language processing. SLs are visual,\ncomplete, standalone languages which are just as expressive as oral languages.\nSigns in SL usually correspond to sequences of highly specific body postures\ninterleaved with movements, which make reference to real world objects,\ncharacters or situations. Here we propose a formal representation of SL signs,\nthat will help us with the analysis of automatically-collected hand tracking\ndata from French Sign Language (FSL) video corpora. We further show how such a\nrepresentation could help us with the design of computer aided SL verification\ntools, which in turn would bring us closer to the development of an automatic\nrecognition system for these languages.\n",
        "published": "2014-03-26T11:47:37Z",
        "pdf_link": "http://arxiv.org/pdf/1403.6636v1"
    },
    {
        "id": "http://arxiv.org/abs/1403.7455v1",
        "title": "Hybrid Approach to English-Hindi Name Entity Transliteration",
        "summary": "  Machine translation (MT) research in Indian languages is still in its\ninfancy. Not much work has been done in proper transliteration of name entities\nin this domain. In this paper we address this issue. We have used English-Hindi\nlanguage pair for our experiments and have used a hybrid approach. At first we\nhave processed English words using a rule based approach which extracts\nindividual phonemes from the words and then we have applied statistical\napproach which converts the English into its equivalent Hindi phoneme and in\nturn the corresponding Hindi word. Through this approach we have attained\n83.40% accuracy.\n",
        "published": "2014-03-28T17:30:32Z",
        "pdf_link": "http://arxiv.org/pdf/1403.7455v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.1847v1",
        "title": "Evaluation and Ranking of Machine Translated Output in Hindi Language\n  using Precision and Recall Oriented Metrics",
        "summary": "  Evaluation plays a crucial role in development of Machine translation\nsystems. In order to judge the quality of an existing MT system i.e. if the\ntranslated output is of human translation quality or not, various automatic\nmetrics exist. We here present the implementation results of different metrics\nwhen used on Hindi language along with their comparisons, illustrating how\neffective are these metrics on languages like Hindi (free word order language).\n",
        "published": "2014-04-07T16:45:42Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1847v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.1872v1",
        "title": "Intgration des donnes d'un lexique syntaxique dans un analyseur\n  syntaxique probabiliste",
        "summary": "  This article reports the evaluation of the integration of data from a\nsyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic\nparser. We show that by changing the set of labels for verbs and predicational\nnouns, we can improve the performance on French of a non-lexicalized\nprobabilistic parser.\n",
        "published": "2014-04-07T18:12:08Z",
        "pdf_link": "http://arxiv.org/pdf/1404.1872v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.2071v1",
        "title": "Extracting a bilingual semantic grammar from FrameNet-annotated corpora",
        "summary": "  We present the creation of an English-Swedish FrameNet-based grammar in\nGrammatical Framework. The aim of this research is to make existing framenets\ncomputationally accessible for multilingual natural language applications via a\ncommon semantic grammar API, and to facilitate the porting of such grammar to\nother languages. In this paper, we describe the abstract syntax of the semantic\ngrammar while focusing on its automatic extraction possibilities. We have\nextracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley\nFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The\nabstract syntax defines 769 frame-specific valence patterns that cover 77.8%\nexamples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.\nAs a side result, we provide a unified method for comparing semantic and\nsyntactic valence patterns across framenets.\n",
        "published": "2014-04-08T10:08:22Z",
        "pdf_link": "http://arxiv.org/pdf/1404.2071v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.2188v1",
        "title": "A Convolutional Neural Network for Modelling Sentences",
        "summary": "  The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.\n",
        "published": "2014-04-08T15:46:44Z",
        "pdf_link": "http://arxiv.org/pdf/1404.2188v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.2878v1",
        "title": "Overview of Stemming Algorithms for Indian and Non-Indian Languages",
        "summary": "  Stemming is a pre-processing step in Text Mining applications as well as a\nvery common requirement of Natural Language processing functions. Stemming is\nthe process for reducing inflected words to their stem. The main purpose of\nstemming is to reduce different grammatical forms / word forms of a word like\nits noun, adjective, verb, adverb etc. to its root form. Stemming is widely\nuses in Information Retrieval system and reduces the size of index files. We\ncan say that the goal of stemming is to reduce inflectional forms and sometimes\nderivationally related forms of a word to a common base form. In this paper we\nhave discussed different stemming algorithm for non-Indian and Indian language,\nmethods of stemming, accuracy and errors.\n",
        "published": "2014-04-10T17:16:01Z",
        "pdf_link": "http://arxiv.org/pdf/1404.2878v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.3377v1",
        "title": "A Generalized Language Model as the Combination of Skipped n-grams and\n  Modified Kneser-Ney Smoothing",
        "summary": "  We introduce a novel approach for building language models based on a\nsystematic, recursive exploration of skip n-gram models which are interpolated\nusing modified Kneser-Ney smoothing. Our approach generalizes language models\nas it contains the classical interpolation with lower order models as a special\ncase. In this paper we motivate, formalize and present our approach. In an\nextensive empirical experiment over English text corpora we demonstrate that\nour generalized language models lead to a substantial reduction of perplexity\nbetween 3.1% and 12.7% in comparison to traditional language models using\nmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over\nthree other languages and a domain specific corpus where we observed consistent\nimprovements. Finally, we also show that the strength of our approach lies in\nits ability to cope in particular with sparse training data. Using a very small\ntraining data set of only 736 KB text we yield improvements of even 25.7%\nreduction of perplexity.\n",
        "published": "2014-04-13T12:39:41Z",
        "pdf_link": "http://arxiv.org/pdf/1404.3377v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.3759v1",
        "title": "Meta-evaluation of comparability metrics using parallel corpora",
        "summary": "  Metrics for measuring the comparability of corpora or texts need to be\ndeveloped and evaluated systematically. Applications based on a corpus, such as\ntraining Statistical MT systems in specialised narrow domains, require finding\na reasonable balance between the size of the corpus and its consistency, with\ncontrolled and benchmarked levels of comparability for any newly added\nsections. In this article we propose a method that can meta-evaluate\ncomparability metrics by calculating monolingual comparability scores\nseparately on the 'source' and 'target' sides of parallel corpora. The range of\nscores on the source side is then correlated (using Pearson's r coefficient)\nwith the range of 'target' scores; the higher the correlation - the more\nreliable is the metric. The intuition is that a good metric should yield the\nsame distance between different domains in different languages. Our method\ngives consistent results for the same metrics on different data sets, which\nindicates that it is reliable and can be used for metric comparison or for\noptimising settings of parametrised metrics.\n",
        "published": "2014-04-14T21:33:42Z",
        "pdf_link": "http://arxiv.org/pdf/1404.3759v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.3992v1",
        "title": "Assessing the Quality of MT Systems for Hindi to English Translation",
        "summary": "  Evaluation plays a vital role in checking the quality of MT output. It is\ndone either manually or automatically. Manual evaluation is very time consuming\nand subjective, hence use of automatic metrics is done most of the times. This\npaper evaluates the translation quality of different MT Engines for\nHindi-English (Hindi data is provided as input and English is obtained as\noutput) using various automatic metrics like BLEU, METEOR etc. Further the\ncomparison automatic evaluation results with Human ranking have also been\ngiven.\n",
        "published": "2014-04-15T17:13:26Z",
        "pdf_link": "http://arxiv.org/pdf/1404.3992v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.4314v1",
        "title": "An Empirical Comparison of Parsing Methods for Stanford Dependencies",
        "summary": "  Stanford typed dependencies are a widely desired representation of natural\nlanguage sentences, but parsing is one of the major computational bottlenecks\nin text analysis systems. In light of the evolving definition of the Stanford\ndependencies and developments in statistical dependency parsing algorithms,\nthis paper revisits the question of Cer et al. (2010): what is the tradeoff\nbetween accuracy and speed in obtaining Stanford dependencies in particular? We\nalso explore the effects of input representations on this tradeoff:\npart-of-speech tags, the novel use of an alternative dependency representation\nas input, and distributional representaions of words. We find that direct\ndependency parsing is a more viable solution than it was found to be in the\npast. An accompanying software release can be found at:\nhttp://www.ark.cs.cmu.edu/TBSD\n",
        "published": "2014-04-16T17:06:35Z",
        "pdf_link": "http://arxiv.org/pdf/1404.4314v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.4572v1",
        "title": "The First Parallel Multilingual Corpus of Persian: Toward a Persian\n  BLARK",
        "summary": "  In this article, we have introduced the first parallel corpus of Persian with\nmore than 10 other European languages. This article describes primary steps\ntoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,\nwe have proposed morphosyntactic specification of Persian based on\nEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article\nintroduces Persian Language, with emphasis on its orthography and\nmorphosyntactic features, then a new Part-of-Speech categorization and\northography for Persian in digital environments is proposed. Finally, the\ncorpus and related statistic will be analyzed.\n",
        "published": "2014-04-17T16:22:40Z",
        "pdf_link": "http://arxiv.org/pdf/1404.4572v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.4641v1",
        "title": "Multilingual Models for Compositional Distributed Semantics",
        "summary": "  We present a novel technique for learning semantic representations, which\nextends the distributional hypothesis to multilingual data and joint-space\nembeddings. Our models leverage parallel data and learn to strongly align the\nembeddings of semantically equivalent sentences, while maintaining sufficient\ndistance between those of dissimilar sentences. The models do not rely on word\nalignments or any syntactic information and are successfully applied to a\nnumber of diverse languages. We extend our approach to learn semantic\nrepresentations at the document level, too. We evaluate these models on two\ncross-lingual document classification tasks, outperforming the prior state of\nthe art. Through qualitative analysis and the study of pivoting effects we\ndemonstrate that our representations are semantically plausible and can capture\nsemantic relationships across languages without parallel data.\n",
        "published": "2014-04-17T20:18:03Z",
        "pdf_link": "http://arxiv.org/pdf/1404.4641v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.4714v1",
        "title": "Radical-Enhanced Chinese Character Embedding",
        "summary": "  We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks.\n",
        "published": "2014-04-18T07:48:02Z",
        "pdf_link": "http://arxiv.org/pdf/1404.4714v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.4740v1",
        "title": "Challenges in Persian Electronic Text Analysis",
        "summary": "  Farsi, also known as Persian, is the official language of Iran and Tajikistan\nand one of the two main languages spoken in Afghanistan. Farsi enjoys a unified\nArabic script as its writing system. In this paper we briefly introduce the\nwriting standards of Farsi and highlight problems one would face when analyzing\nFarsi electronic texts, especially during development of Farsi corpora\nregarding to transcription and encoding of Farsi e-texts. The pointes mentioned\nmay sounds easy but they are crucial when developing and processing written\ncorpora of Farsi.\n",
        "published": "2014-04-18T10:30:47Z",
        "pdf_link": "http://arxiv.org/pdf/1404.4740v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5278v1",
        "title": "The Frobenius anatomy of word meanings I: subject and object relative\n  pronouns",
        "summary": "  This paper develops a compositional vector-based semantics of subject and\nobject relative pronouns within a categorical framework. Frobenius algebras are\nused to formalise the operations required to model the semantics of relative\npronouns, including passing information between the relative clause and the\nmodified noun phrase, as well as copying, combining, and discarding parts of\nthe relative clause. We develop two instantiations of the abstract semantics,\none based on a truth-theoretic approach and one based on corpus statistics.\n",
        "published": "2014-04-21T19:31:48Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5278v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5357v1",
        "title": "Morphological Analysis of the Bishnupriya Manipuri Language using Finite\n  State Transducers",
        "summary": "  In this work we present a morphological analysis of Bishnupriya Manipuri\nlanguage, an Indo-Aryan language spoken in the north eastern India. As of now,\nthere is no computational work available for the language. Finite state\nmorphology is one of the successful approaches applied in a wide variety of\nlanguages over the year. Therefore we adapted the finite state approach to\nanalyse morphology of the Bishnupriya Manipuri language.\n",
        "published": "2014-04-22T00:17:26Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5357v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.5367v1",
        "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution",
        "summary": "  Most state-of-the-art approaches for named-entity recognition (NER) use semi\nsupervised information in the form of word clusters and lexicons. Recently\nneural network-based language models have been explored, as they as a byproduct\ngenerate highly informative vector representations for words, known as word\nembeddings. In this paper we present two contributions: a new form of learning\nword embeddings that can leverage information from relevant lexicons to improve\nthe representations, and the first system to use neural word embeddings to\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\nCoNLL 2003---significantly better than any previous system trained on public\ndata, and matching a system employing massive private industrial query-log\ndata.\n",
        "published": "2014-04-22T02:12:06Z",
        "pdf_link": "http://arxiv.org/pdf/1404.5367v1"
    },
    {
        "id": "http://arxiv.org/abs/1404.6312v2",
        "title": "Reconstructing Native Language Typology from Foreign Language Usage",
        "summary": "  Linguists and psychologists have long been studying cross-linguistic\ntransfer, the influence of native language properties on linguistic performance\nin a foreign language. In this work we provide empirical evidence for this\nprocess in the form of a strong correlation between language similarities\nderived from structural features in English as Second Language (ESL) texts and\nequivalent similarities obtained from the typological features of the native\nlanguages. We leverage this finding to recover native language typological\nsimilarity structure directly from ESL text, and perform prediction of\ntypological features in an unsupervised fashion with respect to the target\nlanguages. Our method achieves 72.2% accuracy on the typology prediction task,\na result that is highly competitive with equivalent methods that rely on\ntypological resources.\n",
        "published": "2014-04-25T04:10:57Z",
        "pdf_link": "http://arxiv.org/pdf/1404.6312v2"
    },
    {
        "id": "http://arxiv.org/abs/1404.7296v1",
        "title": "A Deep Architecture for Semantic Parsing",
        "summary": "  Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages.\n",
        "published": "2014-04-29T10:10:13Z",
        "pdf_link": "http://arxiv.org/pdf/1404.7296v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.1605v1",
        "title": "Les noms propres se traduisent-ils ? tude d'un corpus multilingue",
        "summary": "  In this paper, we tackle the problem of the translation of proper names. We\nintroduce our hypothesis according to which proper names can be translated more\noften than most people seem to think. Then, we describe the construction of a\nparallel multilingual corpus used to illustrate our point. We eventually\nevaluate both the advantages and limits of this corpus in our study.\n",
        "published": "2014-07-07T07:08:07Z",
        "pdf_link": "http://arxiv.org/pdf/1407.1605v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.1976v1",
        "title": "Inter-Rater Agreement Study on Readability Assessment in Bengali",
        "summary": "  An inter-rater agreement study is performed for readability assessment in\nBengali. A 1-7 rating scale was used to indicate different levels of\nreadability. We obtained moderate to fair agreement among seven independent\nannotators on 30 text passages written by four eminent Bengali authors. As a by\nproduct of our study, we obtained a readability-annotated ground truth dataset\nin Bengali. .\n",
        "published": "2014-07-08T07:35:16Z",
        "pdf_link": "http://arxiv.org/pdf/1407.1976v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.2019v1",
        "title": "Assamese-English Bilingual Machine Translation",
        "summary": "  Machine translation is the process of translating text from one language to\nanother. In this paper, Statistical Machine Translation is done on Assamese and\nEnglish language by taking their respective parallel corpus. A statistical\nphrase based translation toolkit Moses is used here. To develop the language\nmodel and to align the words we used two another tools IRSTLM, GIZA\nrespectively. BLEU score is used to check our translation system performance,\nhow good it is. A difference in BLEU scores is obtained while translating\nsentences from Assamese to English and vice-versa. Since Indian languages are\nmorphologically very rich hence translation is relatively harder from English\nto Assamese resulting in a low BLEU score. A statistical transliteration system\nis also introduced with our translation system to deal basically with proper\nnouns, OOV (out of vocabulary) words which are not present in our corpus.\n",
        "published": "2014-07-08T10:04:07Z",
        "pdf_link": "http://arxiv.org/pdf/1407.2019v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.2694v1",
        "title": "Quality Estimation Of Machine Translation Outputs Through Stemming",
        "summary": "  Machine Translation is the challenging problem for Indian languages. Every\nday we can see some machine translators being developed, but getting a high\nquality automatic translation is still a very distant dream . The correct\ntranslated sentence for Hindi language is rarely found. In this paper, we are\nemphasizing on English-Hindi language pair, so in order to preserve the correct\nMT output we present a ranking system, which employs some machine learning\ntechniques and morphological features. In ranking no human intervention is\nrequired. We have also validated our results by comparing it with human\nranking.\n",
        "published": "2014-07-10T05:26:14Z",
        "pdf_link": "http://arxiv.org/pdf/1407.2694v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.2918v1",
        "title": "A Survey of Named Entity Recognition in Assamese and other Indian\n  Languages",
        "summary": "  Named Entity Recognition is always important when dealing with major Natural\nLanguage Processing tasks such as information extraction, question-answering,\nmachine translation, document summarization etc so in this paper we put forward\na survey of Named Entities in Indian Languages with particular reference to\nAssamese. There are various rule-based and machine learning approaches\navailable for Named Entity Recognition. At the very first of the paper we give\nan idea of the available approaches for Named Entity Recognition and then we\ndiscuss about the related research in this field. Assamese like other Indian\nlanguages is agglutinative and suffers from lack of appropriate resources as\nNamed Entity Recognition requires large data sets, gazetteer list, dictionary\netc and some useful feature like capitalization as found in English cannot be\nfound in Assamese. Apart from this we also describe some of the issues faced in\nAssamese while doing Named Entity Recognition.\n",
        "published": "2014-07-09T13:59:27Z",
        "pdf_link": "http://arxiv.org/pdf/1407.2918v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.2989v1",
        "title": "Hidden Markov Model Based Part of Speech Tagger for Sinhala Language",
        "summary": "  In this paper we present a fundamental lexical semantics of Sinhala language\nand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala\nlanguage. In any Natural Language processing task, Part of Speech is a very\nvital topic, which involves analysing of the construction, behaviour and the\ndynamics of the language, which the knowledge could utilized in computational\nlinguistics analysis and automation applications. Though Sinhala is a\nmorphologically rich and agglutinative language, in which words are inflected\nwith various grammatical features, tagging is very essential for further\nanalysis of the language. Our research is based on statistical based approach,\nin which the tagging process is done by computing the tag sequence probability\nand the word-likelihood probability from the given corpus, where the linguistic\nknowledge is automatically extracted from the annotated corpus. The current\ntagger could reach more than 90% of accuracy for known words.\n",
        "published": "2014-07-10T23:57:54Z",
        "pdf_link": "http://arxiv.org/pdf/1407.2989v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.6853v1",
        "title": "Substitute Based SCODE Word Embeddings in Supervised NLP Tasks",
        "summary": "  We analyze a word embedding method in supervised tasks. It maps words on a\nsphere such that words co-occurring in similar contexts lie closely. The\nsimilarity of contexts is measured by the distribution of substitutes that can\nfill them. We compared word embeddings, including more recent representations,\nin Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine\nour framework in multilingual dependency parsing as well. The results show that\nthe proposed method achieves as good as or better results compared to the other\nword embeddings in the tasks we investigate. It achieves state-of-the-art\nresults in multilingual dependency parsing. Word embeddings in 7 languages are\navailable for public use.\n",
        "published": "2014-07-25T11:17:28Z",
        "pdf_link": "http://arxiv.org/pdf/1407.6853v1"
    },
    {
        "id": "http://arxiv.org/abs/1407.8215v1",
        "title": "Two-pass Discourse Segmentation with Pairing and Global Features",
        "summary": "  Previous attempts at RST-style discourse segmentation typically adopt\nfeatures centered on a single token to predict whether to insert a boundary\nbefore that token. In contrast, we develop a discourse segmenter utilizing a\nset of pairing features, which are centered on a pair of adjacent tokens in the\nsentence, by equally taking into account the information from both tokens.\nMoreover, we propose a novel set of global features, which encode\ncharacteristics of the segmentation as a whole, once we have an initial\nsegmentation. We show that both the pairing and global features are useful on\ntheir own, and their combination achieved an $F_1$ of 92.6% of identifying\nin-sentence discourse boundaries, which is a 17.8% error-rate reduction over\nthe state-of-the-art performance, approaching 95% of human performance. In\naddition, similar improvement is observed across different classification\nframeworks.\n",
        "published": "2014-07-30T21:00:25Z",
        "pdf_link": "http://arxiv.org/pdf/1407.8215v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.0782v1",
        "title": "Targetable Named Entity Recognition in Social Media",
        "summary": "  We present a novel approach for recognizing what we call targetable named\nentities; that is, named entities in a targeted set (e.g, movies, books, TV\nshows). Unlike many other NER systems that need to retrain their statistical\nmodels as new entities arrive, our approach does not require such retraining,\nwhich makes it more adaptable for types of entities that are frequently\nupdated. For this preliminary study, we focus on one entity type, movie title,\nusing data collected from Twitter. Our system is tested on two evaluation sets,\none including only entities corresponding to movies in our training set, and\nthe other excluding any of those entities. Our final model shows F1-scores of\n76.19% and 78.70% on these evaluation sets, which gives strong evidence that\nour approach is completely unbiased to any par- ticular set of entities found\nduring training.\n",
        "published": "2014-08-04T19:31:32Z",
        "pdf_link": "http://arxiv.org/pdf/1408.0782v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.1928v1",
        "title": "Microtask crowdsourcing for disease mention annotation in PubMed\n  abstracts",
        "summary": "  Identifying concepts and relationships in biomedical text enables knowledge\nto be applied in computational analyses. Many biological natural language\nprocess (BioNLP) projects attempt to address this challenge, but the state of\nthe art in BioNLP still leaves much room for improvement. Progress in BioNLP\nresearch depends on large, annotated corpora for evaluating information\nextraction systems and training machine learning models. Traditionally, such\ncorpora are created by small numbers of expert annotators often working over\nextended periods of time. Recent studies have shown that workers on microtask\ncrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in\naggregate, generate high-quality annotations of biomedical text. Here, we\ninvestigated the use of the AMT in capturing disease mentions in PubMed\nabstracts. We used the NCBI Disease corpus as a gold standard for refining and\nbenchmarking our crowdsourcing protocol. After several iterations, we arrived\nat a protocol that reproduced the annotations of the 593 documents in the\ntraining set of this gold standard with an overall F measure of 0.872\n(precision 0.862, recall 0.883). The output can also be tuned to optimize for\nprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when\nprecision = 0.436). Each document was examined by 15 workers, and their\nannotations were merged based on a simple voting method. In total 145 workers\ncombined to complete all 593 documents in the span of 1 week at a cost of $.06\nper abstract per worker. The quality of the annotations, as judged with the F\nmeasure, increases with the number of workers assigned to each task such that\nthe system can be tuned to balance cost against quality. These results\ndemonstrate that microtask crowdsourcing can be a valuable tool for generating\nwell-annotated corpora in BioNLP.\n",
        "published": "2014-08-08T17:49:02Z",
        "pdf_link": "http://arxiv.org/pdf/1408.1928v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.2359v2",
        "title": "Gap-weighted subsequences for automatic cognate identification and\n  phylogenetic inference",
        "summary": "  In this paper, we describe the problem of cognate identification and its\nrelation to phylogenetic inference. We introduce subsequence based features for\ndiscriminating cognates from non-cognates. We show that subsequence based\nfeatures perform better than the state-of-the-art string similarity measures\nfor the purpose of cognate identification. We use the cognate judgments for the\npurpose of phylogenetic inference and observe that these classifiers infer a\ntree which is close to the gold standard tree. The contribution of this paper\nis the use of subsequence features for cognate identification and to employ the\ncognate judgments for phylogenetic inference.\n",
        "published": "2014-08-11T09:32:39Z",
        "pdf_link": "http://arxiv.org/pdf/1408.2359v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.3153v2",
        "title": "Detection is the central problem in real-word spelling correction",
        "summary": "  Real-word spelling correction differs from non-word spelling correction in\nits aims and its challenges. Here we show that the central problem in real-word\nspelling correction is detection. Methods from non-word spelling correction,\nwhich focus instead on selection among candidate corrections, do not address\ndetection adequately, because detection is either assumed in advance or heavily\nconstrained. As we demonstrate in this paper, merely discriminating between the\nintended word and a random close variation of it within the context of a\nsentence is a task that can be performed with high accuracy using\nstraightforward models. Trigram models are sufficient in almost all cases. The\ndifficulty comes when every word in the sentence is a potential error, with a\nlarge set of possible candidate corrections. Despite their strengths, trigram\nmodels cannot reliably find true errors without introducing many more, at least\nnot when used in the obvious sequential way without added structure. The\ndetection task exposes weakness not visible in the selection task.\n",
        "published": "2014-08-13T22:09:23Z",
        "pdf_link": "http://arxiv.org/pdf/1408.3153v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.3456v1",
        "title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity\n  Estimation",
        "summary": "  We present SimLex-999, a gold standard resource for evaluating distributional\nsemantic models that improves on existing resources in several important ways.\nFirst, in contrast to gold standards such as WordSim-353 and MEN, it explicitly\nquantifies similarity rather than association or relatedness, so that pairs of\nentities that are associated but not actually similar [Freud, psychology] have\na low rating. We show that, via this focus on similarity, SimLex-999\nincentivizes the development of models with a different, and arguably wider\nrange of applications than those which reflect conceptual association. Second,\nSimLex-999 contains a range of concrete and abstract adjective, noun and verb\npairs, together with an independent rating of concreteness and (free)\nassociation strength for each pair. This diversity enables fine-grained\nanalyses of the performance of models on concepts of different types, and\nconsequently greater insight into how architectures can be improved. Further,\nunlike existing gold standard evaluations, for which automatic approaches have\nreached or surpassed the inter-annotator agreement ceiling, state-of-the-art\nmodels perform well below this ceiling on SimLex-999. There is therefore plenty\nof scope for SimLex-999 to quantify future improvements to distributional\nsemantic models, guiding the development of the next generation of\nrepresentation-learning architectures.\n",
        "published": "2014-08-15T01:59:29Z",
        "pdf_link": "http://arxiv.org/pdf/1408.3456v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.3731v2",
        "title": "Unsupervised Keyword Extraction from Polish Legal Texts",
        "summary": "  In this work, we present an application of the recently proposed unsupervised\nkeyword extraction algorithm RAKE to a corpus of Polish legal texts from the\nfield of public procurement. RAKE is essentially a language and domain\nindependent method. Its only language-specific input is a stoplist containing a\nset of non-content words. The performance of the method heavily depends on the\nchoice of such a stoplist, which should be domain adopted. Therefore, we\ncomplement RAKE algorithm with an automatic approach to selecting non-content\nwords, which is based on the statistical properties of term distribution.\n",
        "published": "2014-08-16T10:09:28Z",
        "pdf_link": "http://arxiv.org/pdf/1408.3731v2"
    },
    {
        "id": "http://arxiv.org/abs/1408.4753v1",
        "title": "Be Careful When Assuming the Obvious: Commentary on \"The placement of\n  the head that minimizes online memory: a complex systems approach\"",
        "summary": "  Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic\nand diachronic nature of word order based on the assumption that memory costs\nare a never decreasing function of distance and a few very general linguistic\nassumptions. However, even these minimal and seemingly obvious assumptions are\nnot as safe as they appear in light of recent typological and psycholinguistic\nevidence. The interaction of word order and memory has further depths to be\nexplored.\n",
        "published": "2014-08-20T18:40:13Z",
        "pdf_link": "http://arxiv.org/pdf/1408.4753v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.6179v1",
        "title": "Evaluating Neural Word Representations in Tensor-Based Compositional\n  Settings",
        "summary": "  We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks.\n",
        "published": "2014-08-26T16:28:21Z",
        "pdf_link": "http://arxiv.org/pdf/1408.6179v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.6181v1",
        "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning",
        "summary": "  This paper provides a method for improving tensor-based compositional\ndistributional models of meaning by the addition of an explicit disambiguation\nstep prior to composition. In contrast with previous research where this\nhypothesis has been successfully tested against relatively simple compositional\nmodels, in our work we use a robust model trained with linear regression. The\nresults we get in two experiments show the superiority of the prior\ndisambiguation method and suggest that the effectiveness of this approach is\nmodel-independent.\n",
        "published": "2014-08-26T16:43:30Z",
        "pdf_link": "http://arxiv.org/pdf/1408.6181v1"
    },
    {
        "id": "http://arxiv.org/abs/1408.6788v2",
        "title": "Strongly Incremental Repair Detection",
        "summary": "  We present STIR (STrongly Incremental Repair detection), a system that\ndetects speech repairs and edit terms on transcripts incrementally with minimal\nlatency. STIR uses information-theoretic measures from n-gram models as its\nprincipal decision features in a pipeline of classifiers detecting the\ndifferent stages of repairs. Results on the Switchboard disfluency tagged\ncorpus show utterance-final accuracy on a par with state-of-the-art incremental\nrepair detection methods, but with better incremental accuracy, faster\ntime-to-detection and less computational overhead. We evaluate its performance\nusing incremental metrics and propose new repair processing evaluation\nstandards.\n",
        "published": "2014-08-28T17:29:55Z",
        "pdf_link": "http://arxiv.org/pdf/1408.6788v2"
    },
    {
        "id": "http://arxiv.org/abs/1411.0588v1",
        "title": "On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using\n  GATE",
        "summary": "  In this article, we describe an approach for automatic detection of\nnoun-adjective agreement errors in Bulgarian texts by explaining the necessary\nsteps required to develop a simple Java-based language processing application.\nFor this purpose, we use the GATE language processing framework, which is\ncapable of analyzing texts in Bulgarian language and can be embedded in\nsoftware applications, accessed through a set of Java APIs. In our example\napplication we also demonstrate how to use the functionality of GATE to perform\nregular expressions over annotations for detecting agreement errors in simple\nnoun phrases formed by two words - attributive adjective and a noun, where the\nattributive adjective precedes the noun. The provided code samples can also be\nused as a starting point for implementing natural language processing\nfunctionalities in software applications related to language processing tasks\nlike detection, annotation and retrieval of word groups meeting a specific set\nof criteria.\n",
        "published": "2014-11-03T18:05:54Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0588v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.0778v1",
        "title": "Detecting Suicidal Ideation in Chinese Microblogs with Psychological\n  Lexicons",
        "summary": "  Suicide is among the leading causes of death in China. However, technical\napproaches toward preventing suicide are challenging and remaining under\ndevelopment. Recently, several actual suicidal cases were preceded by users who\nposted microblogs with suicidal ideation to Sina Weibo, a Chinese social media\nnetwork akin to Twitter. It would therefore be desirable to detect suicidal\nideations from microblogs in real-time, and immediately alert appropriate\nsupport groups, which may lead to successful prevention. In this paper, we\npropose a real-time suicidal ideation detection system deployed over Weibo,\nusing machine learning and known psychological techniques. Currently, we have\nidentified 53 known suicidal cases who posted suicide notes on Weibo prior to\ntheir deaths.We explore linguistic features of these known cases using a\npsychological lexicon dictionary, and train an effective suicidal Weibo post\ndetection model. 6714 tagged posts and several classifiers are used to verify\nthe model. By combining both machine learning and psychological knowledge, SVM\nclassifier has the best performance of different classifiers, yielding an\nF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.\n",
        "published": "2014-11-04T03:48:20Z",
        "pdf_link": "http://arxiv.org/pdf/1411.0778v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.2738v4",
        "title": "word2vec Parameter Learning Explained",
        "summary": "  The word2vec model and application by Mikolov et al. have attracted a great\namount of attention in recent two years. The vector representations of words\nlearned by word2vec models have been shown to carry semantic meanings and are\nuseful in various NLP tasks. As an increasing number of researchers would like\nto experiment with word2vec or similar techniques, I notice that there lacks a\nmaterial that comprehensively explains the parameter learning process of word\nembedding models in details, thus preventing researchers that are non-experts\nin neural networks from understanding the working mechanism of such models.\n  This note provides detailed derivations and explanations of the parameter\nupdate equations of the word2vec models, including the original continuous\nbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization\ntechniques, including hierarchical softmax and negative sampling. Intuitive\ninterpretations of the gradient equations are also provided alongside\nmathematical derivations.\n  In the appendix, a review on the basics of neuron networks and\nbackpropagation is provided. I also created an interactive demo, wevi, to\nfacilitate the intuitive understanding of the model.\n",
        "published": "2014-11-11T09:24:00Z",
        "pdf_link": "http://arxiv.org/pdf/1411.2738v4"
    },
    {
        "id": "http://arxiv.org/abs/1411.3146v1",
        "title": "Distributed Representations for Compositional Semantics",
        "summary": "  The mathematical representation of semantics is a key issue for Natural\nLanguage Processing (NLP). A lot of research has been devoted to finding ways\nof representing the semantics of individual words in vector spaces.\nDistributional approaches --- meaning distributed representations that exploit\nco-occurrence statistics of large corpora --- have proved popular and\nsuccessful across a number of tasks. However, natural language usually comes in\nstructures beyond the word level, with meaning arising not only from the\nindividual words but also the structure they are contained in at the phrasal or\nsentential level. Modelling the compositional process by which the meaning of\nan utterance arises from the meaning of its parts is an equally fundamental\ntask of NLP.\n  This dissertation explores methods for learning distributed semantic\nrepresentations and models for composing these into representations for larger\nlinguistic units. Our underlying hypothesis is that neural models are a\nsuitable vehicle for learning semantically rich representations and that such\nrepresentations in turn are suitable vehicles for solving important tasks in\nnatural language processing. The contribution of this thesis is a thorough\nevaluation of our hypothesis, as part of which we introduce several new\napproaches to representation learning and compositional semantics, as well as\nmultiple state-of-the-art models which apply distributed semantic\nrepresentations to various tasks in NLP.\n",
        "published": "2014-11-12T11:26:51Z",
        "pdf_link": "http://arxiv.org/pdf/1411.3146v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.3561v1",
        "title": "A Text to Speech (TTS) System with English to Punjabi Conversion",
        "summary": "  The paper aims to show how an application can be developed that converts the\nEnglish language into the Punjabi Language, and the same application can\nconvert the Text to Speech(TTS) i.e. pronounce the text. This application can\nbe really beneficial for those with special needs.\n",
        "published": "2014-11-13T14:44:00Z",
        "pdf_link": "http://arxiv.org/pdf/1411.3561v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.4166v4",
        "title": "Retrofitting Word Vectors to Semantic Lexicons",
        "summary": "  Vector space word representations are learned from distributional information\nof words in large corpora. Although such statistics are semantically\ninformative, they disregard the valuable information that is contained in\nsemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This\npaper proposes a method for refining vector space representations using\nrelational information from semantic lexicons by encouraging linked words to\nhave similar vector representations, and it makes no assumptions about how the\ninput vectors were constructed. Evaluated on a battery of standard lexical\nsemantic evaluation tasks in several languages, we obtain substantial\nimprovements starting with a variety of word vector models. Our refinement\nmethod outperforms prior techniques for incorporating semantic lexicons into\nthe word vector training algorithms.\n",
        "published": "2014-11-15T17:34:20Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4166v4"
    },
    {
        "id": "http://arxiv.org/abs/1411.4472v1",
        "title": "Opinion mining of text documents written in Macedonian language",
        "summary": "  The ability to extract public opinion from web portals such as review sites,\nsocial networks and blogs will enable companies and individuals to form a view,\nan attitude and make decisions without having to do lengthy and costly\nresearches and surveys. In this paper machine learning techniques are used for\ndetermining the polarity of forum posts on kajgana which are written in\nMacedonian language. The posts are classified as being positive, negative or\nneutral. We test different feature metrics and classifiers and provide detailed\nevaluation of their participation in improving the overall performance on a\nmanually generated dataset. By achieving 92% accuracy, we show that the\nperformance of systems for automated opinion mining is comparable to a human\nevaluator, thus making it a viable option for text data analysis. Finally, we\npresent a few statistics derived from the forum posts using the developed\nsystem.\n",
        "published": "2014-11-17T13:36:49Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4472v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.4614v1",
        "title": "Using graph transformation algorithms to generate natural language\n  equivalents of icons expressing medical concepts",
        "summary": "  A graphical language addresses the need to communicate medical information in\na synthetic way. Medical concepts are expressed by icons conveying fast visual\ninformation about patients' current state or about the known effects of drugs.\nIn order to increase the visual language's acceptance and usability, a natural\nlanguage generation interface is currently developed. In this context, this\npaper describes the use of an informatics method ---graph transformation--- to\nprepare data consisting of concepts in an OWL-DL ontology for use in a natural\nlanguage generation component. The OWL concept may be considered as a\nstar-shaped graph with a central node. The method transforms it into a graph\nrepresenting the deep semantic structure of a natural language phrase. This\nwork may be of future use in other contexts where ontology concepts have to be\nmapped to half-formalized natural language expressions.\n",
        "published": "2014-09-26T05:09:40Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4614v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.4960v1",
        "title": "Network Motifs Analysis of Croatian Literature",
        "summary": "  In this paper we analyse network motifs in the co-occurrence directed\nnetworks constructed from five different texts (four books and one portal) in\nthe Croatian language. After preparing the data and network construction, we\nperform the network motif analysis. We analyse the motif frequencies and\nZ-scores in the five networks. We present the triad significance profile for\nfive datasets. Furthermore, we compare our results with the existing results\nfor the linguistic networks. Firstly, we show that the triad significance\nprofile for the Croatian language is very similar with the other languages and\nall the networks belong to the same family of networks. However, there are\ncertain differences between the Croatian language and other analysed languages.\nWe conclude that this is due to the free word-order of the Croatian language.\n",
        "published": "2014-11-18T18:46:36Z",
        "pdf_link": "http://arxiv.org/pdf/1411.4960v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.5379v3",
        "title": "Type-Driven Incremental Semantic Parsing with Polymorphism",
        "summary": "  Semantic parsing has made significant progress, but most current semantic\nparsers are extremely slow (CKY-based) and rather primitive in representation.\nWe introduce three new techniques to tackle these problems. First, we design\nthe first linear-time incremental shift-reduce-style semantic parsing algorithm\nwhich is more efficient than conventional cubic-time bottom-up semantic\nparsers. Second, our parser, being type-driven instead of syntax-driven, uses\ntype-checking to decide the direction of reduction, which eliminates the need\nfor a syntactic grammar such as CCG. Third, to fully exploit the power of\ntype-driven semantic parsing beyond simple types (such as entities and truth\nvalues), we borrow from programming language theory the concepts of subtype\npolymorphism and parametric polymorphism to enrich the type system in order to\nbetter guide the parsing. Our system learns very accurate parses in GeoQuery,\nJobs and Atis domains.\n",
        "published": "2014-11-19T21:06:15Z",
        "pdf_link": "http://arxiv.org/pdf/1411.5379v3"
    },
    {
        "id": "http://arxiv.org/abs/1411.5796v1",
        "title": "Pre-processing of Domain Ontology Graph Generation System in Punjabi",
        "summary": "  This paper describes pre-processing phase of ontology graph generation system\nfrom Punjabi text documents of different domains. This research paper focuses\non pre-processing of Punjabi text documents. Pre-processing is structured\nrepresentation of the input text. Pre-processing of ontology graph generation\nincludes allowing input restrictions to the text, removal of special symbols\nand punctuation marks, removal of duplicate terms, removal of stop words,\nextract terms by matching input terms with dictionary and gazetteer lists\nterms.\n",
        "published": "2014-11-21T08:50:30Z",
        "pdf_link": "http://arxiv.org/pdf/1411.5796v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.7820v1",
        "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic\n  Models and Encyclopedic Knowledge",
        "summary": "  We present a method for coarse-grained cross-lingual alignment of comparable\ntexts: segments consisting of contiguous paragraphs that discuss the same theme\n(e.g. history, economy) are aligned based on induced multilingual topics. The\nmethod combines three ideas: a two-level LDA model that filters out words that\ndo not convey themes, an HMM that models the ordering of themes in the\ncollection of documents, and language-independent concept annotations to serve\nas a cross-language bridge and to strengthen the connection between paragraphs\nin the same segment through concept relations. The method is evaluated on\nEnglish and French data previously used for monolingual alignment. The results\nshow state-of-the-art performance in both monolingual and cross-lingual\nsettings.\n",
        "published": "2014-11-28T11:33:02Z",
        "pdf_link": "http://arxiv.org/pdf/1411.7820v1"
    },
    {
        "id": "http://arxiv.org/abs/1411.7942v2",
        "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs",
        "summary": "  The functional approach to compositional distributional semantics considers\ntransitive verbs to be linear maps that transform the distributional vectors\nrepresenting nouns into a vector representing a sentence. We conduct an initial\ninvestigation that uses a matrix consisting of the parameters of a logistic\nregression classifier trained on a plausibility task as a transitive verb\nfunction. We compare our method to a commonly used corpus-based method for\nconstructing a verb matrix and find that the plausibility training may be more\neffective for disambiguation tasks.\n",
        "published": "2014-11-28T16:57:34Z",
        "pdf_link": "http://arxiv.org/pdf/1411.7942v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.0751v1",
        "title": "Tiered Clustering to Improve Lexical Entailment",
        "summary": "  Many tasks in Natural Language Processing involve recognizing lexical\nentailment. Two different approaches to this problem have been proposed\nrecently that are quite different from each other. The first is an asymmetric\nsimilarity measure designed to give high scores when the contexts of the\nnarrower term in the entailment are a subset of those of the broader term. The\nsecond is a supervised approach where a classifier is learned to predict\nentailment given a concatenated latent vector representation of the word. Both\nof these approaches are vector space models that use a single context vector as\na representation of the word. In this work, I study the effects of clustering\nwords into senses and using these multiple context vectors to infer entailment\nusing extensions of these two algorithms. I find that this approach offers some\nimprovement to these entailment algorithms.\n",
        "published": "2014-12-02T00:53:35Z",
        "pdf_link": "http://arxiv.org/pdf/1412.0751v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.1215v1",
        "title": "Mary Astell's words in A Serious Proposal to the Ladies (part I), a\n  lexicographic inquiry with NooJ",
        "summary": "  In the following article we elected to study with NooJ the lexis of a 17 th\ncentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,\npart I, published in 1694. We first focused on the semantics to see how Astell\nbuilds her vindication of the female sex, which words she uses to sensitise\nwomen to their alienated condition and promote their education. Then we studied\nthe morphology of the lexemes (which is different from contemporary English)\nused by the author, thanks to the NooJ tools we have devised for this purpose.\nNooJ has great functionalities for lexicographic work. Its commands and graphs\nprove to be most efficient in the spotting of archaic words or variants in\nspelling. Introduction In our previous articles, we have studied the\nsingularities of 17 th century English within the framework of a diachronic\nanalysis thanks to syntactical and morphological graphs and thanks to the\ndictionaries we have compiled from a corpus that may be expanded overtime. Our\nearly work was based on a limited corpus of English travel literature to Greece\nin the 17 th century. This article deals with a late seventeenth century text\nwritten by a woman philosopher and essayist, Mary Astell (1666--1731),\nconsidered as one of the first English feminists. Astell wrote her essay at a\ntime in English history when women were \"the weaker vessel\" and their main\nbusiness in life was to charm and please men by their looks and submissiveness.\nIn this essay we will see how NooJ can help us analyse Astell's rhetoric (what\npoint of view does she adopt, does she speak in her own name, in the name of\nall women, what is her representation of men and women and their relationships\nin the text, what are the goals of education?). Then we will turn our attention\nto the morphology of words in the text and use NooJ commands and graphs to\ncarry out a lexicographic inquiry into Astell's lexemes.\n",
        "published": "2014-12-03T07:16:04Z",
        "pdf_link": "http://arxiv.org/pdf/1412.1215v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.1342v1",
        "title": "A perspective on the advancement of natural language processing tasks\n  via topological analysis of complex networks",
        "summary": "  Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).\n",
        "published": "2014-12-03T14:37:36Z",
        "pdf_link": "http://arxiv.org/pdf/1412.1342v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.1632v1",
        "title": "Deep Learning for Answer Sentence Selection",
        "summary": "  Answer sentence selection is the task of identifying sentences that contain\nthe answer to a given question. This is an important problem in its own right\nas well as in the larger context of open domain question answering. We propose\na novel approach to solving this task via means of distributed representations,\nand learn to match questions with answers by considering their semantic\nencoding. This contrasts prior work on this task, which typically relies on\nclassifiers with large numbers of hand-crafted syntactic and semantic features\nand various external resources. Our approach does not require any feature\nengineering nor does it involve specialist linguistic data, making this model\neasily applicable to a wide range of domains and languages. Experimental\nresults on a standard benchmark dataset from TREC demonstrate that---despite\nits simplicity---our model matches state of the art performance on the answer\nsentence selection task.\n",
        "published": "2014-12-04T11:53:02Z",
        "pdf_link": "http://arxiv.org/pdf/1412.1632v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.1820v2",
        "title": "Context-Dependent Fine-Grained Entity Type Tagging",
        "summary": "  Entity type tagging is the task of assigning category labels to each mention\nof an entity in a document. While standard systems focus on a small set of\ntypes, recent work (Ling and Weld, 2012) suggests that using a large\nfine-grained label set can lead to dramatic improvements in downstream tasks.\nIn the absence of labeled training data, existing fine-grained tagging systems\nobtain examples automatically, using resolved entities and their types\nextracted from a knowledge base. However, since the appropriate type often\ndepends on context (e.g. Washington could be tagged either as city or\ngovernment), this procedure can result in spurious labels, leading to poorer\ngeneralization. We propose the task of context-dependent fine type tagging,\nwhere the set of acceptable labels for a mention is restricted to only those\ndeducible from the local context (e.g. sentence or document). We introduce new\nresources for this task: 12,017 mentions annotated with their context-dependent\nfine types, and we provide baseline experimental results on this data.\n",
        "published": "2014-12-03T23:26:33Z",
        "pdf_link": "http://arxiv.org/pdf/1412.1820v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.2007v2",
        "title": "On Using Very Large Target Vocabulary for Neural Machine Translation",
        "summary": "  Neural machine translation, a recently proposed approach to machine\ntranslation based purely on neural networks, has shown promising results\ncompared to the existing approaches such as phrase-based statistical machine\ntranslation. Despite its recent success, neural machine translation has its\nlimitation in handling a larger vocabulary, as training complexity as well as\ndecoding complexity increase proportionally to the number of target words. In\nthis paper, we propose a method that allows us to use a very large target\nvocabulary without increasing training complexity, based on importance\nsampling. We show that decoding can be efficiently done even with the model\nhaving a very large target vocabulary by selecting only a small subset of the\nwhole target vocabulary. The models trained by the proposed approach are\nempirically found to outperform the baseline models with a small vocabulary as\nwell as the LSTM-based neural machine translation models. Furthermore, when we\nuse the ensemble of a few models with very large target vocabularies, we\nachieve the state-of-the-art translation performance (measured by BLEU) on the\nEnglish->German translation and almost as high performance as state-of-the-art\nEnglish->French translation system.\n",
        "published": "2014-12-05T14:26:27Z",
        "pdf_link": "http://arxiv.org/pdf/1412.2007v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.2197v3",
        "title": "Practice in Synonym Extraction at Large Scale",
        "summary": "  Synonym extraction is an important task in natural language processing and\noften used as a submodule in query expansion, question answering and other\napplications. Automatic synonym extractor is highly preferred for large scale\napplications. Previous studies in synonym extraction are most limited to small\nscale datasets. In this paper, we build a large dataset with 3.4 million\nsynonym/non-synonym pairs to capture the challenges in real world scenarios. We\nproposed (1) a new cost function to accommodate the unbalanced learning\nproblem, and (2) a feature learning based deep neural network to model the\ncomplicated relationships in synonym pairs. We compare several different\napproaches based on SVMs and neural networks, and find out a novel feature\nlearning based neural network outperforms the methods with hand-assigned\nfeatures. Specifically, the best performance of our model surpasses the SVM\nbaseline with a significant 97\\% relative improvement.\n",
        "published": "2014-12-06T04:40:18Z",
        "pdf_link": "http://arxiv.org/pdf/1412.2197v3"
    },
    {
        "id": "http://arxiv.org/abs/1412.2378v1",
        "title": "Learning Word Representations from Relational Graphs",
        "summary": "  Attributes of words and relations between two words are central to numerous\ntasks in Artificial Intelligence such as knowledge representation, similarity\nmeasurement, and analogy detection. Often when two words share one or more\nattributes in common, they are connected by some semantic relations. On the\nother hand, if there are numerous semantic relations between two words, we can\nexpect some of the attributes of one of the words to be inherited by the other.\nMotivated by this close connection between attributes and relations, given a\nrelational graph in which words are inter- connected via numerous semantic\nrelations, we propose a method to learn a latent representation for the\nindividual words. The proposed method considers not only the co-occurrences of\nwords as done by existing approaches for word representation learning, but also\nthe semantic relations in which two words co-occur. To evaluate the accuracy of\nthe word representations learnt using the proposed method, we use the learnt\nword representations to solve semantic word analogy problems. Our experimental\nresults show that it is possible to learn better word representations by using\nsemantic semantics between words.\n",
        "published": "2014-12-07T17:49:53Z",
        "pdf_link": "http://arxiv.org/pdf/1412.2378v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.2442v1",
        "title": "Rediscovering the Alphabet - On the Innate Universal Grammar",
        "summary": "  Universal Grammar (UG) theory has been one of the most important research\ntopics in linguistics since introduced five decades ago. UG specifies the\nrestricted set of languages learnable by human brain, and thus, many\nresearchers believe in its biological roots. Numerous empirical studies of\nneurobiological and cognitive functions of the human brain, and of many natural\nlanguages, have been conducted to unveil some aspects of UG. This, however,\nresulted in different and sometimes contradicting theories that do not indicate\na universally unique grammar. In this research, we tackle the UG problem from\nan entirely different perspective. We search for the Unique Universal Grammar\n(UUG) that facilitates communication and knowledge transfer, the sole purpose\nof a language. We formulate this UG and show that it is unique, intrinsic, and\ncosmic, rather than humanistic. Initial analysis on a widespread natural\nlanguage already showed some positive results.\n",
        "published": "2014-12-08T04:14:05Z",
        "pdf_link": "http://arxiv.org/pdf/1412.2442v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.3336v2",
        "title": "Statistical Patterns in Written Language",
        "summary": "  Quantitative linguistics has been allowed, in the last few decades, within\nthe admittedly blurry boundaries of the field of complex systems. A growing\nhost of applied mathematicians and statistical physicists devote their efforts\nto disclose regularities, correlations, patterns, and structural properties of\nlanguage streams, using techniques borrowed from statistics and information\ntheory. Overall, results can still be categorized as modest, but the prospects\nare promising: medium- and long-range features in the organization of human\nlanguage -which are beyond the scope of traditional linguistics- have already\nemerged from this kind of analysis and continue to be reported, contributing a\nnew perspective to our understanding of this most complex communication system.\nThis short book is intended to review some of these recent contributions.\n",
        "published": "2014-12-10T15:22:08Z",
        "pdf_link": "http://arxiv.org/pdf/1412.3336v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.4021v5",
        "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules\n  for Part-of-Speech Tagging",
        "summary": "  In this paper, we propose a new approach to construct a system of\ntransformation rules for the Part-of-Speech (POS) tagging task. Our approach is\nbased on an incremental knowledge acquisition method where rules are stored in\nan exception structure and new rules are only added to correct the errors of\nexisting rules; thus allowing systematic control of the interaction between the\nrules. Experimental results on 13 languages show that our approach is fast in\nterms of training time and tagging speed. Furthermore, our approach obtains\nvery competitive accuracy in comparison to state-of-the-art POS and\nmorphological taggers.\n",
        "published": "2014-12-12T15:26:43Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4021v5"
    },
    {
        "id": "http://arxiv.org/abs/1412.4369v3",
        "title": "Incorporating Both Distributional and Relational Semantics in Word\n  Representations",
        "summary": "  We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases.\n",
        "published": "2014-12-14T15:18:18Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4369v3"
    },
    {
        "id": "http://arxiv.org/abs/1412.4682v1",
        "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on\n  Plutchik's Wheel",
        "summary": "  We study sentiment analysis beyond the typical granularity of polarity and\ninstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an\nextension to the Rule-Based Emission Model algorithm to deduce such emotions\nfrom human-written messages. We evaluate our approach on two different datasets\nand compare its performance with the current state-of-the-art techniques for\nemotion detection, including a recursive auto-encoder. The results of the\nexperimental study suggest that RBEM-Emo is a promising approach advancing the\ncurrent state-of-the-art in emotion detection.\n",
        "published": "2014-12-15T17:20:47Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4682v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.4930v2",
        "title": "Rehabilitation of Count-based Models for Word Vector Representations",
        "summary": "  Recent works on word representations mostly rely on predictive models.\nDistributed word representations (aka word embeddings) are trained to optimally\npredict the contexts in which the corresponding words tend to appear. Such\nmodels have succeeded in capturing word similarties as well as semantic and\nsyntactic regularities. Instead, we aim at reviving interest in a model based\non counts. We present a systematic study of the use of the Hellinger distance\nto extract semantic representations from the word co-occurence statistics of\nlarge text corpora. We show that this distance gives good performance on word\nsimilarity and analogy tasks, with a proper type and size of context, and a\ndimensionality reduction based on a stochastic low-rank approximation. Besides\nbeing both simple and intuitive, this method also provides an encoding function\nwhich can be used to infer unseen words or phrases. This becomes a clear\nadvantage compared to predictive models which must train these new words.\n",
        "published": "2014-12-16T09:43:56Z",
        "pdf_link": "http://arxiv.org/pdf/1412.4930v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.5212v1",
        "title": "Application of Topic Models to Judgments from Public Procurement Domain",
        "summary": "  In this work, automatic analysis of themes contained in a large corpora of\njudgments from public procurement domain is performed. The employed technique\nis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,\nto use LDA in conjunction with recently developed method of unsupervised\nkeyword extraction. Such an approach improves the interpretability of the\nautomatically obtained topics and allows for better computational performance.\nThe described analysis illustrates a potential of the method in detecting\nrecurring themes and discovering temporal trends in lodged contract appeals.\nThese results may be in future applied to improve information retrieval from\nrepositories of legal texts or as auxiliary material for legal analyses carried\nout by human experts.\n",
        "published": "2014-12-16T22:00:52Z",
        "pdf_link": "http://arxiv.org/pdf/1412.5212v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.5477v1",
        "title": "Computational Model to Generate Case-Inflected Forms of Masculine Nouns\n  for Word Search in Sanskrit E-Text",
        "summary": "  The problem of word search in Sanskrit is inseparable from complexities that\ninclude those caused by euphonic conjunctions and case-inflections. The\ncase-inflectional forms of a noun normally number 24 owing to the fact that in\nSanskrit there are eight cases and three numbers-singular, dual and plural. The\ntraditional method of generating these inflectional forms is rather elaborate\nowing to the fact that there are differences in the forms generated between\neven very similar words and there are subtle nuances involved. Further, it\nwould be a cumbersome exercise to generate and search for 24 forms of a word\nduring a word search in a large text, using the currently available\ncase-inflectional form generators. This study presents a new approach to\ngenerating case-inflectional forms that is simpler to compute. Further, an\noptimized model that is sufficient for generating only those word forms that\nare required in a word search and is more than 80% efficient compared to the\ncomplete case-inflectional forms generator, is presented in this study for the\nfirst time.\n",
        "published": "2014-12-17T16:56:43Z",
        "pdf_link": "http://arxiv.org/pdf/1412.5477v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.5836v3",
        "title": "Incorporating Both Distributional and Relational Semantics in Word\n  Representations",
        "summary": "  We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases.\n",
        "published": "2014-12-18T12:30:55Z",
        "pdf_link": "http://arxiv.org/pdf/1412.5836v3"
    },
    {
        "id": "http://arxiv.org/abs/1412.6045v2",
        "title": "A Simple and Efficient Method To Generate Word Sense Representations",
        "summary": "  Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner.\n",
        "published": "2014-12-18T20:14:10Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6045v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.6264v1",
        "title": "Supertagging: Introduction, learning, and application",
        "summary": "  Supertagging is an approach originally developed by Bangalore and Joshi\n(1999) to improve the parsing efficiency. In the beginning, the scholars used\nsmall training datasets and somewhat na\\\"ive smoothing techniques to learn the\nprobability distributions of supertags. Since its inception, the applicability\nof Supertags has been explored for TAG (tree-adjoining grammar) formalism as\nwell as other related yet, different formalisms such as CCG. This article will\ntry to summarize the various chapters, relevant to statistical parsing, from\nthe most recent edited book volume (Bangalore and Joshi, 2010). The chapters\nwere selected so as to blend the learning of supertags, its integration into\nfull-scale parsing, and in semantic parsing.\n",
        "published": "2014-12-19T09:53:57Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6264v1"
    },
    {
        "id": "http://arxiv.org/abs/1412.6277v2",
        "title": "N-gram-Based Low-Dimensional Representation for Document Classification",
        "summary": "  The bag-of-words (BOW) model is the common approach for classifying\ndocuments, where words are used as feature for training a classifier. This\ngenerally involves a huge number of features. Some techniques, such as Latent\nSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been\ndesigned to summarize documents in a lower dimension with the least semantic\ninformation loss. Some semantic information is nevertheless always lost, since\nonly words are considered. Instead, we aim at using information coming from\nn-grams to overcome this limitation, while remaining in a low-dimension space.\nMany approaches, such as the Skip-gram model, provide good word vector\nrepresentations very quickly. We propose to average these representations to\nobtain representations of n-grams. All n-grams are thus embedded in a same\nsemantic space. A K-means clustering can then group them into semantic\nconcepts. The number of features is therefore dramatically reduced and\ndocuments can be represented as bag of semantic concepts. We show that this\nmodel outperforms LSA and LDA on a sentiment classification task, and yields\nsimilar results than a traditional BOW-model with far less features.\n",
        "published": "2014-12-19T10:29:33Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6277v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.6334v4",
        "title": "Leveraging Monolingual Data for Crosslingual Compositional Word\n  Representations",
        "summary": "  In this work, we present a novel neural network based architecture for\ninducing compositional crosslingual word representations. Unlike previously\nproposed methods, our method fulfills the following three criteria; it\nconstrains the word-level representations to be compositional, it is capable of\nleveraging both bilingual and monolingual data, and it is scalable to large\nvocabularies and large quantities of data. The key component of our approach is\nwhat we refer to as a monolingual inclusion criterion, that exploits the\nobservation that phrases are more closely semantically related to their\nsub-phrases than to other randomly sampled phrases. We evaluate our method on a\nwell-established crosslingual document classification task and achieve results\nthat are either comparable, or greatly improve upon previous state-of-the-art\nmethods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for\nthe English to German and German to English sub-tasks respectively. The former\nadvances the state of the art by 0.9% points of accuracy, the latter is an\nabsolute improvement upon the previous state of the art by 7.7% points of\naccuracy and an improvement of 33.0% in error reduction.\n",
        "published": "2014-12-19T13:23:35Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6334v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.6448v4",
        "title": "Embedding Word Similarity with Neural Machine Translation",
        "summary": "  Neural language models learn word representations, or embeddings, that\ncapture rich linguistic and conceptual information. Here we investigate the\nembeddings learned by neural machine translation models, a recently-developed\nclass of neural language model. We show that embeddings from translation models\noutperform those learned by monolingual models at tasks that require knowledge\nof both conceptual similarity and lexical-syntactic role. We further show that\nthese effects hold when translating from both English to French and English to\nGerman, and argue that the desirable properties of translation embeddings\nshould emerge largely independently of the source and target languages.\nFinally, we apply a new method for training neural translation models with very\nlarge vocabularies, and show that this vocabulary expansion algorithm results\nin minimal degradation of embedding quality. Our embedding spaces can be\nqueried in an online demo and downloaded from our web page. Overall, our\nanalyses indicate that translation-based embeddings should be used in\napplications that require concepts to be organised according to similarity\nand/or lexical function, while monolingual embeddings are better suited to\nmodelling (nonspecific) inter-word relatedness.\n",
        "published": "2014-12-19T17:22:03Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6448v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.6575v4",
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge\n  Bases",
        "summary": "  We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning.\n",
        "published": "2014-12-20T01:37:16Z",
        "pdf_link": "http://arxiv.org/pdf/1412.6575v4"
    },
    {
        "id": "http://arxiv.org/abs/1412.7119v3",
        "title": "Pragmatic Neural Language Modelling in Machine Translation",
        "summary": "  This paper presents an in-depth investigation on integrating neural language\nmodels in translation systems. Scaling neural language models is a difficult\ntask, but crucial for real-world applications. This paper evaluates the impact\non end-to-end MT quality of both new and existing scaling techniques. We show\nwhen explicitly normalising neural models is necessary and what optimisation\ntricks one should use in such scenarios. We also focus on scalable training\nalgorithms and investigate noise contrastive estimation and diagonal contexts\nas sources for further speed improvements. We explore the trade-offs between\nneural models and back-off n-gram models and find that neural models make\nstrong candidates for natural language applications in memory constrained\nenvironments, yet still lag behind traditional models in raw translation\nquality. We conclude with a set of recommendations one should follow to build a\nscalable neural language model for MT.\n",
        "published": "2014-12-22T20:08:06Z",
        "pdf_link": "http://arxiv.org/pdf/1412.7119v3"
    },
    {
        "id": "http://arxiv.org/abs/1412.7415v2",
        "title": "A prototype Malayalam to Sign Language Automatic Translator",
        "summary": "  Sign language, which is a medium of communication for deaf people, uses\nmanual communication and body language to convey meaning, as opposed to using\nsound. This paper presents a prototype Malayalam text to sign language\ntranslation system. The proposed system takes Malayalam text as input and\ngenerates corresponding Sign Language. Output animation is rendered using a\ncomputer generated model. This system will help to disseminate information to\nthe deaf people in public utility places like railways, banks, hospitals etc.\nThis will also act as an educational tool in learning Sign Language.\n",
        "published": "2014-12-23T15:51:41Z",
        "pdf_link": "http://arxiv.org/pdf/1412.7415v2"
    },
    {
        "id": "http://arxiv.org/abs/1412.8010v1",
        "title": "Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary",
        "summary": "  SentiWordNet is an important lexical resource supporting sentiment analysis\nin opinion mining applications. In this paper, we propose a novel approach to\nconstruct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated\nfrom WordNet in which each synset has numerical scores to indicate its opinion\npolarities. Many previous studies obtained these scores by applying a machine\nlearning method to WordNet. However, Vietnamese WordNet is not available\nunfortunately by the time of this paper. Therefore, we propose a method to\nconstruct VSWN from a Vietnamese dictionary, not from WordNet. We show the\neffectiveness of the proposed method by generating a VSWN with 39,561 synsets\nautomatically. The method is experimentally tested with 266 synsets with aspect\nof positivity and negativity. It attains a competitive result compared with\nEnglish SentiWordNet that is 0.066 and 0.052 differences for positivity and\nnegativity sets respectively.\n",
        "published": "2014-12-27T01:54:15Z",
        "pdf_link": "http://arxiv.org/pdf/1412.8010v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.01245v1",
        "title": "Authorship recognition via fluctuation analysis of network topology and\n  word intermittency",
        "summary": "  Statistical methods have been widely employed in many practical natural\nlanguage processing applications. More specifically, complex networks concepts\nand methods from dynamical systems theory have been successfully applied to\nrecognize stylistic patterns in written texts. Despite the large amount of\nstudies devoted to represent texts with physical models, only a few studies\nhave assessed the relevance of attributes derived from the analysis of\nstylistic fluctuations. Because fluctuations represent a pivotal factor for\ncharacterizing a myriad of real systems, this study focused on the analysis of\nthe properties of stylistic fluctuations in texts via topological analysis of\ncomplex networks and intermittency measurements. The results showed that\ndifferent authors display distinct fluctuation patterns. In particular, it was\nfound that it is possible to identify the authorship of books using the\nintermittency of specific words. Taken together, the results described here\nsuggest that the patterns found in stylistic fluctuations could be used to\nanalyze other related complex systems. Furthermore, the discovery of novel\npatterns related to textual stylistic fluctuations indicates that these\npatterns could be useful to improve the state of the art of many\nstylistic-based natural language processing tasks.\n",
        "published": "2015-02-04T16:12:45Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01245v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.01271v2",
        "title": "INRIASAC: Simple Hypernym Extraction Methods",
        "summary": "  Given a set of terms from a given domain, how can we structure them into a\ntaxonomy without manual intervention? This is the task 17 of SemEval 2015. Here\nwe present our simple taxonomy structuring techniques which, despite their\nsimplicity, ranked first in this 2015 benchmark. We use large quantities of\ntext (English Wikipedia) and simple heuristics such as term overlap and\ndocument and sentence co-occurrence to produce hypernym lists. We describe\nthese techniques and pre-sent an initial evaluation of results.\n",
        "published": "2015-02-04T17:53:01Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01271v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.01446v1",
        "title": "Beyond Word-based Language Model in Statistical Machine Translation",
        "summary": "  Language model is one of the most important modules in statistical machine\ntranslation and currently the word-based language model dominants this\ncommunity. However, many translation models (e.g. phrase-based models) generate\nthe target language sentences by rendering and compositing the phrases rather\nthan the words. Thus, it is much more reasonable to model dependency between\nphrases, but few research work succeed in solving this problem. In this paper,\nwe tackle this problem by designing a novel phrase-based language model which\nattempts to solve three key sub-problems: 1, how to define a phrase in language\nmodel; 2, how to determine the phrase boundary in the large-scale monolingual\ndata in order to enlarge the training set; 3, how to alleviate the data\nsparsity problem due to the huge vocabulary size of phrases. By carefully\nhandling these issues, the extensive experiments on Chinese-to-English\ntranslation show that our phrase-based language model can significantly improve\nthe translation quality by up to +1.47 absolute BLEU score.\n",
        "published": "2015-02-05T07:42:18Z",
        "pdf_link": "http://arxiv.org/pdf/1502.01446v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.02655v1",
        "title": "An investigation into language complexity of World-of-Warcraft\n  game-external texts",
        "summary": "  We present a language complexity analysis of World of Warcraft (WoW)\ncommunity texts, which we compare to texts from a general corpus of web\nEnglish. Results from several complexity types are presented, including lexical\ndiversity, density, readability and syntactic complexity. The language of WoW\ntexts is found to be comparable to the general corpus on some complexity\nmeasures, yet more specialized on other measures. Our findings can be used by\neducators willing to include game-related activities into school curricula.\n",
        "published": "2015-02-07T21:59:21Z",
        "pdf_link": "http://arxiv.org/pdf/1502.02655v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.03671v2",
        "title": "Phrase-based Image Captioning",
        "summary": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\nin two popular datasets for the task: Flickr30k and the recently proposed\nMicrosoft COCO.\n",
        "published": "2015-02-12T14:17:15Z",
        "pdf_link": "http://arxiv.org/pdf/1502.03671v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.03752v1",
        "title": "A new hybrid metric for verifying parallel corpora of Arabic-English",
        "summary": "  This paper discusses a new metric that has been applied to verify the quality\nin translation between sentence pairs in parallel corpora of Arabic-English.\nThis metric combines two techniques, one based on sentence length and the other\nbased on compression code length. Experiments on sample test parallel\nArabic-English corpora indicate the combination of these two techniques\nimproves accuracy of the identification of satisfactory and unsatisfactory\nsentence pairs compared to sentence length and compression code length alone.\nThe new method proposed in this research is effective at filtering noise and\nreducing mis-translations resulting in greatly improved quality.\n",
        "published": "2015-02-12T17:49:45Z",
        "pdf_link": "http://arxiv.org/pdf/1502.03752v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.04174v1",
        "title": "Probabilistic Models for High-Order Projective Dependency Parsing",
        "summary": "  This paper presents generalized probabilistic models for high-order\nprojective dependency parsing and an algorithmic framework for learning these\nstatistical models involving dependency trees. Partition functions and\nmarginals for high-order dependency trees can be computed efficiently, by\nadapting our algorithms which extend the inside-outside algorithm to\nhigher-order cases. To show the effectiveness of our algorithms, we perform\nexperiments on three languages---English, Chinese and Czech, using maximum\nconditional likelihood estimation for model training and L-BFGS for parameter\nestimation. Our methods achieve competitive performance for English, and\noutperform all previously reported dependency parsers for Chinese and Czech.\n",
        "published": "2015-02-14T06:47:34Z",
        "pdf_link": "http://arxiv.org/pdf/1502.04174v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.04938v2",
        "title": "A Survey of Word Reordering in Statistical Machine Translation:\n  Computational Models and Language Phenomena",
        "summary": "  Word reordering is one of the most difficult aspects of statistical machine\ntranslation (SMT), and an important factor of its quality and efficiency.\nDespite the vast amount of research published to date, the interest of the\ncommunity in this problem has not decreased, and no single method appears to be\nstrongly dominant across language pairs. Instead, the choice of the optimal\napproach for a new translation task still seems to be mostly driven by\nempirical trials. To orientate the reader in this vast and complex research\narea, we present a comprehensive survey of word reordering viewed as a\nstatistical modeling challenge and as a natural language phenomenon. The survey\ndescribes in detail how word reordering is modeled within different\nstring-based and tree-based SMT frameworks and as a stand-alone task, including\nsystematic overviews of the literature in advanced reordering modeling. We then\nquestion why some approaches are more successful than others in different\nlanguage pairs. We argue that, besides measuring the amount of reordering, it\nis important to understand which kinds of reordering occur in a given language\npair. To this end, we conduct a qualitative analysis of word reordering\nphenomena in a diverse sample of language pairs, based on a large collection of\nlinguistic knowledge. Empirical results in the SMT literature are shown to\nsupport the hypothesis that a few linguistic facts can be very useful to\nanticipate the reordering characteristics of a language pair and to select the\nSMT framework that best suits them.\n",
        "published": "2015-02-17T15:59:09Z",
        "pdf_link": "http://arxiv.org/pdf/1502.04938v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.07038v1",
        "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing",
        "summary": "  We develop novel first- and second-order features for dependency parsing\nbased on the Google Syntactic Ngrams corpus, a collection of subtree counts of\nparsed sentences from scanned books. We also extend previous work on surface\n$n$-gram features from Web1T to the Google Books corpus and from first-order to\nsecond-order, comparing and analysing performance over newswire and web\ntreebanks.\n  Surface and syntactic $n$-grams both produce substantial and complementary\ngains in parsing accuracy across domains. Our best system combines the two\nfeature sets, achieving up to 0.8% absolute UAS improvements on newswire and\n1.4% on web text.\n",
        "published": "2015-02-25T03:27:38Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07038v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.07257v2",
        "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram",
        "summary": "  Recently proposed Skip-gram model is a powerful method for learning\nhigh-dimensional word representations that capture rich semantic relationships\nbetween words. However, Skip-gram as well as most prior work on learning word\nrepresentations does not take into account word ambiguity and maintain only\nsingle representation per word. Although a number of Skip-gram modifications\nwere proposed to overcome this limitation and learn multi-prototype word\nrepresentations, they either require a known number of word meanings or learn\nthem using greedy heuristic approaches. In this paper we propose the Adaptive\nSkip-gram model which is a nonparametric Bayesian extension of Skip-gram\ncapable to automatically learn the required number of representations for all\nwords at desired semantic resolution. We derive efficient online variational\nlearning algorithm for the model and empirically demonstrate its efficiency on\nword-sense induction task.\n",
        "published": "2015-02-25T17:15:56Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07257v2"
    },
    {
        "id": "http://arxiv.org/abs/1502.07504v1",
        "title": "Rational Kernels for Arabic Stemming and Text Classification",
        "summary": "  In this paper, we address the problems of Arabic Text Classification and\nstemming using Transducers and Rational Kernels. We introduce a new stemming\ntechnique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns\nare modelled using transducers and stemming is done without depending on any\ndictionary. Using transducers for stemming, documents are transformed into\nfinite state transducers. This document representation allows us to use and\nexplore rational kernels as a framework for Arabic Text Classification.\nStemming experiments are conducted on three word collections and classification\nexperiments are done on the Saudi Press Agency dataset. Results show that our\napproach, when compared with other approaches, is promising specially in terms\nof Accuracy, Recall and F1.\n",
        "published": "2015-02-26T11:09:59Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07504v1"
    },
    {
        "id": "http://arxiv.org/abs/1502.07920v1",
        "title": "Local Translation Prediction with Global Sentence Representation",
        "summary": "  Statistical machine translation models have made great progress in improving\nthe translation quality. However, the existing models predict the target\ntranslation with only the source- and target-side local context information. In\npractice, distinguishing good translations from bad ones does not only depend\non the local features, but also rely on the global sentence-level information.\nIn this paper, we explore the source-side global sentence-level features for\ntarget-side local translation prediction. We propose a novel\nbilingually-constrained chunk-based convolutional neural network to learn\nsentence semantic representations. With the sentence-level feature\nrepresentation, we further design a feed-forward neural network to better\npredict translations using both local and global information. The large-scale\nexperiments show that our method can obtain substantial improvements in\ntranslation quality over the strong baseline: the hierarchical phrase-based\ntranslation model augmented with the neural network joint model.\n",
        "published": "2015-02-27T14:55:15Z",
        "pdf_link": "http://arxiv.org/pdf/1502.07920v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.00030v1",
        "title": "Parsing as Reduction",
        "summary": "  We reduce phrase-representation parsing to dependency parsing. Our reduction\nis grounded on a new intermediate representation, \"head-ordered dependency\ntrees\", shown to be isomorphic to constituent trees. By encoding order\ninformation in the dependency labels, we show that any off-the-shelf, trainable\ndependency parser can be used to produce constituents. When this parser is\nnon-projective, we can perform discontinuous parsing in a very natural manner.\nDespite the simplicity of our approach, experiments show that the resulting\nparsers are on par with strong baselines, such as the Berkeley parser for\nEnglish and the best single system in the SPMRL-2014 shared task. Results are\nparticularly striking for discontinuous parsing of German, where we surpass the\ncurrent state of the art by a wide margin.\n",
        "published": "2015-02-27T22:52:37Z",
        "pdf_link": "http://arxiv.org/pdf/1503.00030v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.00095v3",
        "title": "Task-Oriented Learning of Word Embeddings for Semantic Relation\n  Classification",
        "summary": "  We present a novel learning method for word embeddings designed for relation\nclassification. Our word embeddings are trained by predicting words between\nnoun pairs using lexical relation-specific features on a large unlabeled\ncorpus. This allows us to explicitly incorporate relation-specific information\ninto the word embeddings. The learned word embeddings are then used to\nconstruct feature vectors for a relation classification model. On a\nwell-established semantic relation classification task, our method\nsignificantly outperforms a baseline based on a previously introduced word\nembedding method, and compares favorably to previous state-of-the-art models\nthat use syntactic information or manually constructed external resources.\n",
        "published": "2015-02-28T07:59:59Z",
        "pdf_link": "http://arxiv.org/pdf/1503.00095v3"
    },
    {
        "id": "http://arxiv.org/abs/1503.00168v1",
        "title": "The NLP Engine: A Universal Turing Machine for NLP",
        "summary": "  It is commonly accepted that machine translation is a more complex task than\npart of speech tagging. But how much more complex? In this paper we make an\nattempt to develop a general framework and methodology for computing the\ninformational and/or processing complexity of NLP applications and tasks. We\ndefine a universal framework akin to a Turning Machine that attempts to fit\n(most) NLP tasks into one paradigm. We calculate the complexities of various\nNLP tasks using measures of Shannon Entropy, and compare `simple' ones such as\npart of speech tagging to `complex' ones such as machine translation. This\npaper provides a first, though far from perfect, attempt to quantify NLP tasks\nunder a uniform paradigm. We point out current deficiencies and suggest some\navenues for fruitful research.\n",
        "published": "2015-02-28T19:46:50Z",
        "pdf_link": "http://arxiv.org/pdf/1503.00168v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.01655v2",
        "title": "Studying the Wikipedia Hyperlink Graph for Relatedness and\n  Disambiguation",
        "summary": "  Hyperlinks and other relations in Wikipedia are a extraordinary resource\nwhich is still not fully understood. In this paper we study the different types\nof links in Wikipedia, and contrast the use of the full graph with respect to\njust direct links. We apply a well-known random walk algorithm on two tasks,\nword relatedness and named-entity disambiguation. We show that using the full\ngraph is more effective than just direct links by a large margin, that\nnon-reciprocal links harm performance, and that there is no benefit from\ncategories and infoboxes, with coherent results on both tasks. We set new\nstate-of-the-art figures for systems based on Wikipedia links, comparable to\nsystems exploiting several information sources and/or supervised machine\nlearning. Our approach is open source, with instruction to reproduce results,\nand amenable to be integrated with complementary text-based methods.\n",
        "published": "2015-03-05T15:08:21Z",
        "pdf_link": "http://arxiv.org/pdf/1503.01655v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.02335v1",
        "title": "An Unsupervised Method for Uncovering Morphological Chains",
        "summary": "  Most state-of-the-art systems today produce morphological analysis based only\non orthographic patterns. In contrast, we propose a model for unsupervised\nmorphological analysis that integrates orthographic and semantic views of\nwords. We model word formation in terms of morphological chains, from base\nwords to the observed words, breaking the chains into parent-child relations.\nWe use log-linear models with morpheme and word-level features to predict\npossible parents, including their modifications, for each word. The limited set\nof candidate parents for each word render contrastive estimation feasible. Our\nmodel consistently matches or outperforms five state-of-the-art systems on\nArabic, English and Turkish.\n",
        "published": "2015-03-08T22:18:30Z",
        "pdf_link": "http://arxiv.org/pdf/1503.02335v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.03535v2",
        "title": "On Using Monolingual Corpora in Neural Machine Translation",
        "summary": "  Recent work on end-to-end neural network-based architectures for machine\ntranslation has shown promising results for En-Fr and En-De translation.\nArguably, one of the major factors behind this success has been the\navailability of high quality parallel corpora. In this work, we investigate how\nto leverage abundant monolingual corpora for neural machine translation.\nCompared to a phrase-based and hierarchical baseline, we obtain up to $1.96$\nBLEU improvement on the low-resource language pair Turkish-English, and $1.59$\nBLEU on the focused domain task of Chinese-English chat messages. While our\nmethod was initially targeted toward such tasks with less parallel data, we\nshow that it also extends to high resource languages such as Cs-En and De-En\nwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural\nmachine translation baselines, respectively.\n",
        "published": "2015-03-11T23:50:04Z",
        "pdf_link": "http://arxiv.org/pdf/1503.03535v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.03989v1",
        "title": "An implementation of Apertium based Assamese morphological analyzer",
        "summary": "  Morphological Analysis is an important branch of linguistics for any Natural\nLanguage Processing Technology. Morphology studies the word structure and\nformation of word of a language. In current scenario of NLP research,\nmorphological analysis techniques have become more popular day by day. For\nprocessing any language, morphology of the word should be first analyzed.\nAssamese language contains very complex morphological structure. In our work we\nhave used Apertium based Finite-State-Transducers for developing morphological\nanalyzer for Assamese Language with some limited domain and we get 72.7%\naccuracy\n",
        "published": "2015-03-13T09:03:21Z",
        "pdf_link": "http://arxiv.org/pdf/1503.03989v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.05034v2",
        "title": "$gen$CNN: A Convolutional Architecture for Word Sequence Prediction",
        "summary": "  We propose a novel convolutional architecture, named $gen$CNN, for word\nsequence prediction. Different from previous work on neural network-based\nlanguage modeling and generation (e.g., RNN or LSTM), we choose not to greedily\nsummarize the history of words as a fixed length vector. Instead, we use a\nconvolutional neural network to predict the next word with the history of words\nof variable length. Also different from the existing feedforward networks for\nlanguage modeling, our model can effectively fuse the local correlation and\nglobal correlation in the word sequence, with a convolution-gating strategy\nspecifically designed for the task. We argue that our model can give adequate\nrepresentation of the history, and therefore can naturally exploit both the\nshort and long range dependencies. Our model is fast, easy to train, and\nreadily parallelized. Our extensive experiments on text generation and $n$-best\nre-ranking in machine translation show that $gen$CNN outperforms the\nstate-of-the-arts with big margins.\n",
        "published": "2015-03-17T13:26:08Z",
        "pdf_link": "http://arxiv.org/pdf/1503.05034v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.05123v1",
        "title": "Prediction Using Note Text: Synthetic Feature Creation with word2vec",
        "summary": "  word2vec affords a simple yet powerful approach of extracting quantitative\nvariables from unstructured textual data. Over half of healthcare data is\nunstructured and therefore hard to model without involved expertise in data\nengineering and natural language processing. word2vec can serve as a bridge to\nquickly gather intelligence from such data sources.\n  In this study, we ran 650 megabytes of unstructured, medical chart notes from\nthe Providence Health & Services electronic medical record through word2vec. We\nused two different approaches in creating predictive variables and tested them\non the risk of readmission for patients with COPD (Chronic Obstructive Lung\nDisease). As a comparative benchmark, we ran the same test using the LACE risk\nmodel (a single score based on length of stay, acuity, comorbid conditions, and\nemergency department visits).\n  Using only free text and mathematical might, we found word2vec comparable to\nLACE in predicting the risk of readmission of COPD patients.\n",
        "published": "2015-03-17T17:04:27Z",
        "pdf_link": "http://arxiv.org/pdf/1503.05123v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.05626v1",
        "title": "Phrase database Approach to structural and semantic disambiguation in\n  English-Korean Machine Translation",
        "summary": "  In machine translation it is common phenomenon that machine-readable\ndictionaries and standard parsing rules are not enough to ensure accuracy in\nparsing and translating English phrases into Korean language, which is revealed\nin misleading translation results due to consequent structural and semantic\nambiguities. This paper aims to suggest a solution to structural and semantic\nambiguities due to the idiomaticity and non-grammaticalness of phrases commonly\nused in English language by applying bilingual phrase database in\nEnglish-Korean Machine Translation (EKMT). This paper firstly clarifies what\nthe phrase unit in EKMT is based on the definition of the English phrase,\nsecondly clarifies what kind of language unit can be the target of the phrase\ndatabase for EKMT, thirdly suggests a way to build the phrase database by\npresenting the format of the phrase database with examples, and finally\ndiscusses briefly the method to apply this bilingual phrase database to the\nEKMT for structural and semantic disambiguation.\n",
        "published": "2015-03-19T01:37:40Z",
        "pdf_link": "http://arxiv.org/pdf/1503.05626v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.05907v1",
        "title": "Syntagma Lexical Database",
        "summary": "  This paper discusses the structure of Syntagma's Lexical Database (focused on\nItalian). The basic database consists in four tables. Table Forms contains word\ninflections, used by the POS-tagger for the identification of input-words.\nForms is related to Lemma. Table Lemma stores all kinds of grammatical features\nof words, word-level semantic data and restrictions. In the table Meanings\nmeaning-related data are stored: definition, examples, domain, and semantic\ninformation. Table Valency contains the argument structure of each meaning,\nwith syntactic and semantic features for each argument. The extended version of\nSLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of\nother languages.\n",
        "published": "2015-03-19T19:45:24Z",
        "pdf_link": "http://arxiv.org/pdf/1503.05907v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.06151v1",
        "title": "On measuring linguistic intelligence",
        "summary": "  This work addresses the problem of measuring how many languages a person\n\"effectively\" speaks given that some of the languages are close to each other.\nIn other words, to assign a meaningful number to her language portfolio.\nIntuition says that someone who speaks fluently Spanish and Portuguese is\nlinguistically less proficient compared to someone who speaks fluently Spanish\nand Chinese since it takes more effort for a native Spanish speaker to learn\nChinese than Portuguese. As the number of languages grows and their proficiency\nlevels vary, it gets even more complicated to assign a score to a language\nportfolio. In this article we propose such a measure (\"linguistic quotient\" -\nLQ) that can account for these effects.\n  We define the properties that such a measure should have. They are based on\nthe idea of coherent risk measures from the mathematical finance. Having laid\ndown the foundation, we propose one such a measure together with the algorithm\nthat works on languages classification tree as input.\n  The algorithm together with the input is available online at lingvometer.com\n",
        "published": "2015-03-20T16:41:05Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06151v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.06450v3",
        "title": "Multilingual Open Relation Extraction Using Cross-lingual Projection",
        "summary": "  Open domain relation extraction systems identify relation and argument\nphrases in a sentence without relying on any underlying schema. However,\ncurrent state-of-the-art relation extraction systems are available only for\nEnglish because of their heavy reliance on linguistic tools such as\npart-of-speech taggers and dependency parsers. We present a cross-lingual\nannotation projection method for language independent relation extraction. We\nevaluate our method on a manually annotated test set and present results on\nthree typologically different languages. We release these manual annotations\nand extracted relations in 61 languages from Wikipedia.\n",
        "published": "2015-03-22T18:05:08Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06450v3"
    },
    {
        "id": "http://arxiv.org/abs/1503.06733v2",
        "title": "Yara Parser: A Fast and Accurate Dependency Parser",
        "summary": "  Dependency parsers are among the most crucial tools in natural language\nprocessing as they have many important applications in downstream tasks such as\ninformation retrieval, machine translation and knowledge acquisition. We\nintroduce the Yara Parser, a fast and accurate open-source dependency parser\nbased on the arc-eager algorithm and beam search. It achieves an unlabeled\naccuracy of 93.32 on the standard WSJ test set which ranks it among the top\ndependency parsers. At its fastest, Yara can parse about 4000 sentences per\nsecond when in greedy mode (1 beam). When optimizing for accuracy (using 64\nbeams and Brown cluster features), Yara can parse 45 sentences per second. The\nparser can be trained on any syntactic dependency treebank and different\noptions are provided in order to make it more flexible and tunable for specific\ntasks. It is released with the Apache version 2.0 license and can be used for\nboth commercial and academic purposes. The parser can be found at\nhttps://github.com/yahoo/YaraParser.\n",
        "published": "2015-03-23T17:20:54Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06733v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.06760v1",
        "title": "Unsupervised POS Induction with Word Embeddings",
        "summary": "  Unsupervised word embeddings have been shown to be valuable as features in\nsupervised learning problems; however, their role in unsupervised problems has\nbeen less thoroughly explored. In this paper, we show that embeddings can\nlikewise add value to the problem of unsupervised POS induction. In two\nrepresentative models of POS induction, we replace multinomial distributions\nover the vocabulary with multivariate Gaussian distributions over word\nembeddings and observe consistent improvements in eight languages. We also\nanalyze the effect of various choices while inducing word embeddings on\n\"downstream\" POS induction results.\n",
        "published": "2015-03-23T18:32:56Z",
        "pdf_link": "http://arxiv.org/pdf/1503.06760v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.07283v1",
        "title": "Morphological Analyzer and Generator for Russian and Ukrainian Languages",
        "summary": "  pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian\nlanguages. It uses large efficiently encoded lexi- cons built from OpenCorpora\nand LanguageTool data. A set of linguistically motivated rules is developed to\nenable morphological analysis and generation of out-of-vocabulary words\nobserved in real-world documents. For Russian pymorphy2 provides\nstate-of-the-arts morphological analysis quality. The analyzer is implemented\nin Python programming language with optional C++ extensions. Emphasis is put on\nease of use, documentation and extensibility. The package is distributed under\na permissive open-source license, encouraging its use in both academic and\ncommercial setting.\n",
        "published": "2015-03-25T05:28:50Z",
        "pdf_link": "http://arxiv.org/pdf/1503.07283v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.07613v1",
        "title": "Unsupervised authorship attribution",
        "summary": "  We describe a technique for attributing parts of a written text to a set of\nunknown authors. Nothing is assumed to be known a priori about the writing\nstyles of potential authors. We use multiple independent clusterings of an\ninput text to identify parts that are similar and dissimilar to one another. We\ndescribe algorithms necessary to combine the multiple clusterings into a\nmeaningful output. We show results of the application of the technique on texts\nhaving multiple writing styles.\n",
        "published": "2015-03-26T04:02:26Z",
        "pdf_link": "http://arxiv.org/pdf/1503.07613v1"
    },
    {
        "id": "http://arxiv.org/abs/1503.08167v2",
        "title": "Normalization of Non-Standard Words in Croatian Texts",
        "summary": "  This paper presents text normalization which is an integral part of any\ntext-to-speech synthesis system. Text normalization is a set of methods with a\ntask to write non-standard words, like numbers, dates, times, abbreviations,\nacronyms and the most common symbols, in their full expanded form are\npresented. The whole taxonomy for classification of non-standard words in\nCroatian language together with rule-based normalization methods combined with\na lookup dictionary are proposed. Achieved token rate for normalization of\nCroatian texts is 95%, where 80% of expanded words are in correct morphological\nform.\n",
        "published": "2015-03-27T17:57:00Z",
        "pdf_link": "http://arxiv.org/pdf/1503.08167v2"
    },
    {
        "id": "http://arxiv.org/abs/1503.09144v1",
        "title": "Towards Using Machine Translation Techniques to Induce Multilingual\n  Lexica of Discourse Markers",
        "summary": "  Discourse markers are universal linguistic events subject to language\nvariation. Although an extensive literature has already reported language\nspecific traits of these events, little has been said on their cross-language\nbehavior and on building an inventory of multilingual lexica of discourse\nmarkers. This work describes new methods and approaches for the description,\nclassification, and annotation of discourse markers in the specific domain of\nthe Europarl corpus. The study of discourse markers in the context of\ntranslation is crucial due to the idiomatic nature of these structures.\nMultilingual lexica together with the functional analysis of such structures\nare useful tools for the hard task of translating discourse markers into\npossible equivalents from one language to another. Using Daniel Marcu's\nvalidated discourse markers for English, extracted from the Brown Corpus, our\npurpose is to build multilingual lexica of discourse markers for other\nlanguages, based on machine translation techniques. The major assumption in\nthis study is that the usage of a discourse marker is independent of the\nlanguage, i.e., the rhetorical function of a discourse marker in a sentence in\none language is equivalent to the rhetorical function of the same discourse\nmarker in another language.\n",
        "published": "2015-03-31T17:56:07Z",
        "pdf_link": "http://arxiv.org/pdf/1503.09144v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.00133v1",
        "title": "Prior Polarity Lexical Resources for the Italian Language",
        "summary": "  In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for\nItalian Natural language Applications) a manually annotated prior polarity\nlexical resource for Italian natural language applications in the field of\nopinion mining and sentiment induction. The resource consists in two different\nsets, an Italian dictionary of more than 277.000 words tagged with their prior\npolarity value, and a set of polarity modifiers, containing more than 200\nwords, which can be used in combination with non neutral terms of the\ndictionary in order to induce the sentiment of Italian compound terms. To the\nbest of our knowledge this is the first prior polarity manually annotated\nresource which has been developed for the Italian natural language.\n",
        "published": "2015-07-01T07:29:12Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00133v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.00639v1",
        "title": "Simple, Fast Semantic Parsing with a Tensor Kernel",
        "summary": "  We describe a simple approach to semantic parsing based on a tensor product\nkernel. We extract two feature vectors: one for the query and one for each\ncandidate logical form. We then train a classifier using the tensor product of\nthe two vectors. Using very simple features for both, our system achieves an\naverage F1 score of 40.1% on the WebQuestions dataset. This is comparable to\nmore complex systems but is simpler to implement and runs faster.\n",
        "published": "2015-07-02T15:58:25Z",
        "pdf_link": "http://arxiv.org/pdf/1507.00639v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.01127v1",
        "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and\n  Lexemes",
        "summary": "  We present \\textit{AutoExtend}, a system to learn embeddings for synsets and\nlexemes. It is flexible in that it can take any word embeddings as input and\ndoes not need an additional training corpus. The synset/lexeme embeddings\nobtained live in the same vector space as the word embeddings. A sparse tensor\nformalization guarantees efficiency and parallelizability. We use WordNet as a\nlexical resource, but AutoExtend can be easily applied to other resources like\nFreebase. AutoExtend achieves state-of-the-art performance on word similarity\nand word sense disambiguation tasks.\n",
        "published": "2015-07-04T16:59:30Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01127v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.01529v1",
        "title": "Correspondence Factor Analysis of Big Data Sets: A Case Study of 30\n  Million Words; and Contrasting Analytics using Apache Solr and Correspondence\n  Analysis in R",
        "summary": "  We consider a large number of text data sets. These are cooking recipes. Term\ndistribution and other distributional properties of the data are investigated.\nOur aim is to look at various analytical approaches which allow for mining of\ninformation on both high and low detail scales. Metric space embedding is\nfundamental to our interest in the semantic properties of this data. We\nconsider the projection of all data into analyses of aggregated versions of the\ndata. We contrast that with projection of aggregated versions of the data into\nanalyses of all the data. Analogously for the term set, we look at analysis of\nselected terms. We also look at inherent term associations such as between\nsingular and plural. In addition to our use of Correspondence Analysis in R,\nfor latent semantic space mapping, we also use Apache Solr. Setting up the Solr\nserver and carrying out querying is described. A further novelty is that\nquerying is supported in Solr based on the principal factor plane mapping of\nall the data. This uses a bounding box query, based on factor projections.\n",
        "published": "2015-07-06T16:32:52Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01529v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.01636v1",
        "title": "Reflections on Sentiment/Opinion Analysis",
        "summary": "  In this paper, we described possible directions for deeper understanding,\nhelping bridge the gap between psychology / cognitive science and computational\napproaches in sentiment/opinion analysis literature. We focus on the opinion\nholder's underlying needs and their resultant goals, which, in a utilitarian\nmodel of sentiment, provides the basis for explaining the reason a sentiment\nvalence is held. While these thoughts are still immature, scattered,\nunstructured, and even imaginary, we believe that these perspectives might\nsuggest fruitful avenues for various kinds of future work.\n",
        "published": "2015-07-06T22:25:55Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01636v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.01701v1",
        "title": "A Survey and Classification of Controlled Natural Languages",
        "summary": "  What is here called controlled natural language (CNL) has traditionally been\ngiven many different names. Especially during the last four decades, a wide\nvariety of such languages have been designed. They are applied to improve\ncommunication among humans, to improve translation, or to provide natural and\nintuitive representations for formal notations. Despite the apparent\ndifferences, it seems sensible to put all these languages under the same\numbrella. To bring order to the variety of languages, a general classification\nscheme is presented here. A comprehensive survey of existing English-based CNLs\nis given, listing and describing 100 languages from 1930 until today.\nClassification of these languages reveals that they form a single scattered\ncloud filling the conceptual space between natural languages such as English on\nthe one end and formal languages such as propositional logic on the other. The\ngoal of this article is to provide a common terminology and a common model for\nCNL, to contribute to the understanding of their general nature, to provide a\nstarting point for researchers interested in the area, and to help developers\nto make design decisions.\n",
        "published": "2015-07-07T08:23:31Z",
        "pdf_link": "http://arxiv.org/pdf/1507.01701v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.02012v1",
        "title": "Hindi to English Transfer Based Machine Translation System",
        "summary": "  In large societies like India there is a huge demand to convert one human\nlanguage into another. Lots of work has been done in this area. Many transfer\nbased MTS have developed for English to other languages, as MANTRA CDAC Pune,\nMATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is a\nlittle work done for Hindi to other languages. Currently we are working on it.\nIn this paper we focus on designing a system, that translate the document from\nHindi to English by using transfer based approach. This system takes an input\ntext check its structure through parsing. Reordering rules are used to generate\nthe text in target language. It is better than Corpus Based MTS because Corpus\nBased MTS require large amount of word aligned data for translation that is not\navailable for many languages while Transfer Based MTS requires only knowledge\nof both the languages(source language and target language) to make transfer\nrules. We get correct translation for simple assertive sentences and almost\ncorrect for complex and compound sentences.\n",
        "published": "2015-07-08T03:50:47Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02012v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.02045v2",
        "title": "What Your Username Says About You",
        "summary": "  Usernames are ubiquitous on the Internet, and they are often suggestive of\nuser demographics. This work looks at the degree to which gender and language\ncan be inferred from a username alone by making use of unsupervised morphology\ninduction to decompose usernames into sub-units. Experimental results on the\ntwo tasks demonstrate the effectiveness of the proposed morphological features\ncompared to a character n-gram baseline.\n",
        "published": "2015-07-08T06:52:50Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02045v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.02062v1",
        "title": "Multi-Document Summarization via Discriminative Summary Reranking",
        "summary": "  Existing multi-document summarization systems usually rely on a specific\nsummarization model (i.e., a summarization method with a specific parameter\nsetting) to extract summaries for different document sets with different\ntopics. However, according to our quantitative analysis, none of the existing\nsummarization models can always produce high-quality summaries for different\ndocument sets, and even a summarization model with good overall performance may\nproduce low-quality summaries for some document sets. On the contrary, a\nbaseline summarization model may produce high-quality summaries for some\ndocument sets. Based on the above observations, we treat the summaries produced\nby different summarization models as candidate summaries, and then explore\ndiscriminative reranking techniques to identify high-quality summaries from the\ncandidates for difference document sets. We propose to extract a set of\ncandidate summaries for each document set based on an ILP framework, and then\nleverage Ranking SVM for summary reranking. Various useful features have been\ndeveloped for the reranking process, including word-level features,\nsentence-level features and summary-level features. Evaluation results on the\nbenchmark DUC datasets validate the efficacy and robustness of our proposed\napproach.\n",
        "published": "2015-07-08T08:26:23Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02062v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.02145v2",
        "title": "Learning to Mine Chinese Coordinate Terms Using the Web",
        "summary": "  Coordinate relation refers to the relation between instances of a concept and\nthe relation between the directly hyponyms of a concept. In this paper, we\nfocus on the task of extracting terms which are coordinate with a user given\nseed term in Chinese, and grouping the terms which belong to different concepts\nif the seed term has several meanings. We propose a semi-supervised method that\nintegrates manually defined linguistic patterns and automatically learned\nsemi-structural patterns to extract coordinate terms in Chinese from web search\nresults. In addition, terms are grouped into different concepts based on their\nco-occurring terms and contexts. We further calculate the saliency scores of\nextracted terms and rank them accordingly. Experimental results demonstrate\nthat our proposed method generates results with high quality and wide coverage.\n",
        "published": "2015-07-08T13:27:43Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02145v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.02628v1",
        "title": "FAQ-based Question Answering via Word Alignment",
        "summary": "  In this paper, we propose a novel word-alignment-based method to solve the\nFAQ-based question answering task. First, we employ a neural network model to\ncalculate question similarity, where the word alignment between two questions\nis used for extracting features. Second, we design a bootstrap-based feature\nextraction method to extract a small set of effective lexical features. Third,\nwe propose a learning-to-rank algorithm to train parameters more suitable for\nthe ranking tasks. Experimental results, conducted on three languages (English,\nSpanish and Japanese), demonstrate that the question similarity model is more\neffective than baseline systems, the sparse features bring 5% improvements on\ntop-1 accuracy, and the learning-to-rank algorithm works significantly better\nthan the traditional method. We further evaluate our method on the answer\nsentence selection task. Our method outperforms all the previous systems on the\nstandard TREC data set.\n",
        "published": "2015-07-09T18:11:03Z",
        "pdf_link": "http://arxiv.org/pdf/1507.02628v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03223v1",
        "title": "Classifier-Based Text Simplification for Improved Machine Translation",
        "summary": "  Machine Translation is one of the research fields of Computational\nLinguistics. The objective of many MT Researchers is to develop an MT System\nthat produce good quality and high accuracy output translations and which also\ncovers maximum language pairs. As internet and Globalization is increasing day\nby day, we need a way that improves the quality of translation. For this\nreason, we have developed a Classifier based Text Simplification Model for\nEnglish-Hindi Machine Translation Systems. We have used support vector machines\nand Na\\\"ive Bayes Classifier to develop this model. We have also evaluated the\nperformance of these classifiers.\n",
        "published": "2015-07-12T12:14:19Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03223v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03462v1",
        "title": "Supervised Hierarchical Classification for Student Answer Scoring",
        "summary": "  This paper describes a hierarchical system that predicts one label at a time\nfor automated student response analysis. For the task, we build a\nclassification binary tree that delays more easily confused labels to later\nstages using hierarchical processes. In particular, the paper describes how the\nhierarchical classifier has been built and how the classification task has been\nbroken down into binary subtasks. It finally discusses the motivations and\nfundamentals of such an approach.\n",
        "published": "2015-07-13T14:00:22Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03462v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03471v1",
        "title": "Incremental LSTM-based Dialog State Tracker",
        "summary": "  A dialog state tracker is an important component in modern spoken dialog\nsystems. We present an incremental dialog state tracker, based on LSTM\nnetworks. It directly uses automatic speech recognition hypotheses to track the\nstate. We also present the key non-standard aspects of the model that bring its\nperformance close to the state-of-the-art and experimentally analyze their\ncontribution: including the ASR confidence scores, abstracting scarcely\nrepresented values, including transcriptions in the training data, and model\naveraging.\n",
        "published": "2015-07-13T14:27:16Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03471v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.03934v2",
        "title": "Recurrent Polynomial Network for Dialogue State Tracking",
        "summary": "  Dialogue state tracking (DST) is a process to estimate the distribution of\nthe dialogue states as a dialogue progresses. Recent studies on constrained\nMarkov Bayesian polynomial (CMBP) framework take the first step towards\nbridging the gap between rule-based and statistical approaches for DST. In this\npaper, the gap is further bridged by a novel framework -- recurrent polynomial\nnetwork (RPN). RPN's unique structure enables the framework to have all the\nadvantages of CMBP including efficiency, portability and interpretability.\nAdditionally, RPN achieves more properties of statistical approaches than CMBP.\nRPN was evaluated on the data corpora of the second and the third Dialog State\nTracking Challenge (DSTC-2/3). Experiments showed that RPN can significantly\noutperform both traditional rule-based approaches and statistical approaches\nwith similar feature set. Compared with the state-of-the-art statistical DST\napproaches with a lot richer features, RPN is also competitive.\n",
        "published": "2015-07-14T17:18:30Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03934v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.04214v1",
        "title": "Associative Measures and Multi-word Unit Extraction in Turkish",
        "summary": "  Associative measures are \"mathematical formulas determining the strength of\nassociation between two or more words based on their occurrences and\ncooccurrences in a text corpus\" (Pecina, 2010, p. 138). The purpose of this\npaper is to test the 12 associative measures that Text-NSP (Banerjee &\nPedersen, 2003) contains on a 10-million-word subcorpus of Turkish National\nCorpus (TNC) (Aksan et.al., 2012). A statistical comparison of those measures\nis out of the scope of the study, and the measures will be evaluated according\nto the linguistic relevance of the rankings they provide. The focus of the\nstudy is basically on optimizing the corpus data, before applying the measures\nand then, evaluating the rankings produced by these measures as a whole, not on\nthe linguistic relevance of individual n-grams. The findings include\nintra-linguistically relevant associative measures for a comma delimited,\nsentence splitted, lower-cased, well-balanced, representative, 10-million-word\ncorpus of Turkish.\n",
        "published": "2015-07-15T13:40:28Z",
        "pdf_link": "http://arxiv.org/pdf/1507.04214v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.05523v1",
        "title": "How to Generate a Good Word Embedding?",
        "summary": "  We analyze three critical components of word embedding training: the model,\nthe corpus, and the training parameters. We systematize existing\nneural-network-based word embedding algorithms and compare them using the same\ncorpus. We evaluate each word embedding in three ways: analyzing its semantic\nproperties, using it as a feature for supervised tasks and using it to\ninitialize neural networks. We also provide several simple guidelines for\ntraining word embeddings. First, we discover that corpus domain is more\nimportant than corpus size. We recommend choosing a corpus in a suitable domain\nfor the desired task, after that, using a larger corpus yields better results.\nSecond, we find that faster models provide sufficient performance in most\ncases, and more complex models can be used if the training corpus is\nsufficiently large. Third, the early stopping metric for iterating should rely\non the development set of the desired task rather than the validation loss of\ntraining embedding.\n",
        "published": "2015-07-20T15:07:53Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05523v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.05630v1",
        "title": "Notes About a More Aware Dependency Parser",
        "summary": "  In this paper I explain the reasons that led me to research and conceive a\nnovel technology for dependency parsing, mixing together the strengths of\ndata-driven transition-based and constraint-based approaches. In particular I\nhighlight the problem to infer the reliability of the results of a data-driven\ntransition-based parser, which is extremely important for high-level processes\nthat expect to use correct parsing results. I then briefly introduce a number\nof notes about a new parser model I'm working on, capable to proceed with the\nanalysis in a \"more aware\" way, with a more \"robust\" concept of robustness.\n",
        "published": "2015-07-20T20:01:44Z",
        "pdf_link": "http://arxiv.org/pdf/1507.05630v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.06073v2",
        "title": "Discriminative Segmental Cascades for Feature-Rich Phone Recognition",
        "summary": "  Discriminative segmental models, such as segmental conditional random fields\n(SCRFs) and segmental structured support vector machines (SSVMs), have had\nsuccess in speech recognition via both lattice rescoring and first-pass\ndecoding. However, such models suffer from slow decoding, hampering the use of\ncomputationally expensive features, such as segment neural networks or other\nhigh-order features. A typical solution is to use approximate decoding, either\nby beam pruning in a single pass or by beam pruning to generate a lattice\nfollowed by a second pass. In this work, we study discriminative segmental\nmodels trained with a hinge loss (i.e., segmental structured SVMs). We show\nthat beam search is not suitable for learning rescoring models in this\napproach, though it gives good approximate decoding performance when the model\nis already well-trained. Instead, we consider an approach inspired by\nstructured prediction cascades, which use max-marginal pruning to generate\nlattices. We obtain a high-accuracy phonetic recognition system with several\nexpensive feature types: a segment neural network, a second-order language\nmodel, and second-order phone boundary features.\n",
        "published": "2015-07-22T06:54:09Z",
        "pdf_link": "http://arxiv.org/pdf/1507.06073v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.07636v1",
        "title": "Reasoning about Linguistic Regularities in Word Embeddings using Matrix\n  Manifolds",
        "summary": "  Recent work has explored methods for learning continuous vector space word\nrepresentations reflecting the underlying semantics of words. Simple vector\nspace arithmetic using cosine distances has been shown to capture certain types\nof analogies, such as reasoning about plurals from singulars, past tense from\npresent tense, etc. In this paper, we introduce a new approach to capture\nanalogies in continuous word representations, based on modeling not just\nindividual word vectors, but rather the subspaces spanned by groups of words.\nWe exploit the property that the set of subspaces in n-dimensional Euclidean\nspace form a curved manifold space called the Grassmannian, a quotient subgroup\nof the Lie group of rotations in n- dimensions. Based on this mathematical\nmodel, we develop a modified cosine distance model based on geodesic kernels\nthat captures relation-specific distances across word categories. Our\nexperiments on analogy tasks show that our approach performs significantly\nbetter than the previous approaches for the given task.\n",
        "published": "2015-07-28T03:51:43Z",
        "pdf_link": "http://arxiv.org/pdf/1507.07636v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.07826v1",
        "title": "Classifying informative and imaginative prose using complex networks",
        "summary": "  Statistical methods have been widely employed in recent years to grasp many\nlanguage properties. The application of such techniques have allowed an\nimprovement of several linguistic applications, which encompasses machine\ntranslation, automatic summarization and document classification. In the\nlatter, many approaches have emphasized the semantical content of texts, as it\nis the case of bag-of-word language models. This approach has certainly yielded\nreasonable performance. However, some potential features such as the structural\norganization of texts have been used only on a few studies. In this context, we\nprobe how features derived from textual structure analysis can be effectively\nemployed in a classification task. More specifically, we performed a supervised\nclassification aiming at discriminating informative from imaginative documents.\nUsing a networked model that describes the local topological/dynamical\nproperties of function words, we achieved an accuracy rate of up to 95%, which\nis much higher than similar networked approaches. A systematic analysis of\nfeature relevance revealed that symmetry and accessibility measurements are\namong the most prominent network measurements. Our results suggest that these\nmeasurements could be used in related language applications, as they play a\ncomplementary role in characterizing texts.\n",
        "published": "2015-07-28T15:59:39Z",
        "pdf_link": "http://arxiv.org/pdf/1507.07826v1"
    },
    {
        "id": "http://arxiv.org/abs/1507.08449v2",
        "title": "One model, two languages: training bilingual parsers with harmonized\n  treebanks",
        "summary": "  We introduce an approach to train lexicalized parsers using bilingual corpora\nobtained by merging harmonized treebanks of different languages, producing\nparsers that can analyze sentences in either of the learned languages, or even\nsentences that mix both. We test the approach on the Universal Dependency\nTreebanks, training with MaltParser and MaltOptimizer. The results show that\nthese bilingual parsers are more than competitive, as most combinations not\nonly preserve accuracy, but some even achieve significant improvements over the\ncorresponding monolingual parsers. Preliminary experiments also show the\napproach to be promising on texts with code-switching and when more languages\nare added.\n",
        "published": "2015-07-30T10:53:11Z",
        "pdf_link": "http://arxiv.org/pdf/1507.08449v2"
    },
    {
        "id": "http://arxiv.org/abs/1507.08452v3",
        "title": "Unsupervised Sentence Simplification Using Deep Semantics",
        "summary": "  We present a novel approach to sentence simplification which departs from\nprevious work in two main ways. First, it requires neither hand written rules\nnor a training corpus of aligned standard and simplified sentences. Second,\nsentence splitting operates on deep semantic structure. We show (i) that the\nunsupervised framework we propose is competitive with four state-of-the-art\nsupervised systems and (ii) that our semantic based approach allows for a\nprincipled and effective handling of sentence splitting.\n",
        "published": "2015-07-30T11:05:01Z",
        "pdf_link": "http://arxiv.org/pdf/1507.08452v3"
    },
    {
        "id": "http://arxiv.org/abs/1507.08539v1",
        "title": "Multilayer Network of Language: a Unified Framework for Structural\n  Analysis of Linguistic Subsystems",
        "summary": "  Recently, the focus of complex networks research has shifted from the\nanalysis of isolated properties of a system toward a more realistic modeling of\nmultiple phenomena - multilayer networks. Motivated by the prosperity of\nmultilayer approach in social, transport or trade systems, we propose the\nintroduction of multilayer networks for language. The multilayer network of\nlanguage is a unified framework for modeling linguistic subsystems and their\nstructural properties enabling the exploration of their mutual interactions.\nVarious aspects of natural language systems can be represented as complex\nnetworks, whose vertices depict linguistic units, while links model their\nrelations. The multilayer network of language is defined by three aspects: the\nnetwork construction principle, the linguistic subsystem and the language of\ninterest. More precisely, we construct a word-level (syntax, co-occurrence and\nits shuffled counterpart) and a subword level (syllables and graphemes) network\nlayers, from five variations of original text (in the modeled language). The\nobtained results suggest that there are substantial differences between the\nnetworks structures of different language subsystems, which are hidden during\nthe exploration of an isolated layer. The word-level layers share structural\nproperties regardless of the language (e.g. Croatian or English), while the\nsyllabic subword level expresses more language dependent structural properties.\nThe preserved weighted overlap quantifies the similarity of word-level layers\nin weighted and directed networks. Moreover, the analysis of motifs reveals a\nclose topological structure of the syntactic and syllabic layers for both\nlanguages. The findings corroborate that the multilayer network framework is a\npowerful, consistent and systematic approach to model several linguistic\nsubsystems simultaneously and hence to provide a more unified view on language.\n",
        "published": "2015-07-30T15:11:00Z",
        "pdf_link": "http://arxiv.org/pdf/1507.08539v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.00106v5",
        "title": "Separated by an Un-common Language: Towards Judgment Language Informed\n  Vector Space Modeling",
        "summary": "  A common evaluation practice in the vector space models (VSMs) literature is\nto measure the models' ability to predict human judgments about lexical\nsemantic relations between word pairs. Most existing evaluation sets, however,\nconsist of scores collected for English word pairs only, ignoring the potential\nimpact of the judgment language in which word pairs are presented on the human\nscores. In this paper we translate two prominent evaluation sets, wordsim353\n(association) and SimLex999 (similarity), from English to Italian, German and\nRussian and collect scores for each dataset from crowdworkers fluent in its\nlanguage. Our analysis reveals that human judgments are strongly impacted by\nthe judgment language. Moreover, we show that the predictions of monolingual\nVSMs do not necessarily best correlate with human judgments made with the\nlanguage used for model training, suggesting that models and humans are\naffected differently by the language they use when making semantic judgments.\nFinally, we show that in a large number of setups, multilingual VSM combination\nresults in improved correlations with human judgments, suggesting that\nmultilingualism may partially compensate for the judgment language effect on\nhuman judgments.\n",
        "published": "2015-08-01T10:24:27Z",
        "pdf_link": "http://arxiv.org/pdf/1508.00106v5"
    },
    {
        "id": "http://arxiv.org/abs/1508.00305v1",
        "title": "Compositional Semantic Parsing on Semi-Structured Tables",
        "summary": "  Two important aspects of semantic parsing for question answering are the\nbreadth of the knowledge source and the depth of logical compositionality.\nWhile existing work trades off one aspect for another, this paper\nsimultaneously makes progress on both fronts through a new task: answering\ncomplex questions on semi-structured tables using question-answer pairs as\nsupervision. The central challenge arises from two compounding factors: the\nbroader domain results in an open-ended set of relations, and the deeper\ncompositionality results in a combinatorial explosion in the space of logical\nforms. We propose a logical-form driven parsing algorithm guided by strong\ntyping constraints and show that it obtains significant improvements over\nnatural baselines. For evaluation, we created a new dataset of 22,033 complex\nquestions on Wikipedia tables, which is made publicly available.\n",
        "published": "2015-08-03T02:53:01Z",
        "pdf_link": "http://arxiv.org/pdf/1508.00305v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.00657v2",
        "title": "Improved Transition-Based Parsing by Modeling Characters instead of\n  Words with LSTMs",
        "summary": "  We present extensions to a continuous-state dependency parsing method that\nmakes it applicable to morphologically rich languages. Starting with a\nhigh-performance transition-based parser that uses long short-term memory\n(LSTM) recurrent neural networks to learn representations of the parser state,\nwe replace lookup-based word representations with representations constructed\nfrom the orthographic representations of the words, also using LSTMs. This\nallows statistical sharing across word forms that are similar on the surface.\nExperiments for morphologically rich languages show that the parsing model\nbenefits from incorporating the character-based encodings of words.\n",
        "published": "2015-08-04T04:36:36Z",
        "pdf_link": "http://arxiv.org/pdf/1508.00657v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.01346v1",
        "title": "Word sense disambiguation: a survey",
        "summary": "  In this paper, we made a survey on Word Sense Disambiguation (WSD). Near\nabout in all major languages around the world, research in WSD has been\nconducted upto different extents. In this paper, we have gone through a survey\nregarding the different approaches adopted in different research works, the\nState of the Art in the performance in this domain, recent works in different\nIndian languages and finally a survey in Bengali language. We have made a\nsurvey on different competitions in this field and the bench mark results,\nobtained from those competitions.\n",
        "published": "2015-08-06T10:15:51Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01346v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01349v1",
        "title": "Automatic classification of bengali sentences based on sense definitions\n  present in bengali wordnet",
        "summary": "  Based on the sense definition of words available in the Bengali WordNet, an\nattempt is made to classify the Bengali sentences automatically into different\ngroups in accordance with their underlying senses. The input sentences are\ncollected from 50 different categories of the Bengali text corpus developed in\nthe TDIL project of the Govt. of India, while information about the different\nsenses of particular ambiguous lexical item is collected from Bengali WordNet.\nIn an experimental basis we have used Naive Bayes probabilistic model as a\nuseful classifier of sentences. We have applied the algorithm over 1747\nsentences that contain a particular Bengali lexical item which, because of its\nambiguous nature, is able to trigger different senses that render sentences in\ndifferent meanings. In our experiment we have achieved around 84% accurate\nresult on the sense classification over the total input sentences. We have\nanalyzed those residual sentences that did not comply with our experiment and\ndid affect the results to note that in many cases, wrong syntactic structures\nand less semantic information are the main hurdles in semantic classification\nof sentences. The applicational relevance of this study is attested in\nautomatic text classification, machine learning, information extraction, and\nword sense disambiguation.\n",
        "published": "2015-08-06T10:26:40Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01349v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01476v1",
        "title": "Hyponymy extraction of domain ontology concept based on ccrfs and\n  hierarchy clustering",
        "summary": "  Concept hierarchy is the backbone of ontology, and the concept hierarchy\nacquisition has been a hot topic in the field of ontology learning. this paper\nproposes a hyponymy extraction method of domain ontology concept based on\ncascaded conditional random field(CCRFs) and hierarchy clustering. It takes\nfree text as extracting object, adopts CCRFs identifying the domain concepts.\nFirst the low layer of CCRFs is used to identify simple domain concept, then\nthe results are sent to the high layer, in which the nesting concepts are\nrecognized. Next we adopt hierarchy clustering to identify the hyponymy\nrelation between domain ontology concepts. The experimental results demonstrate\nthe proposed method is efficient.\n",
        "published": "2015-08-06T18:02:54Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01476v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01718v1",
        "title": "Study of Phonemes Confusions in Hierarchical Automatic Phoneme\n  Recognition System",
        "summary": "  In this paper, we have analyzed the impact of confusions on the robustness of\nphoneme recognitions system. The confusions are detected at the pronunciation\nand the confusions matrices of the phoneme recognizer. The confusions show that\nsome similarities between phonemes at the pronunciation affect significantly\nthe recognition rates. This paper proposes to understand those confusions in\norder to improve the performance of the phoneme recognition system by isolating\nthe problematic phonemes. Confusion analysis leads to build a new hierarchical\nrecognizer using new phoneme distribution and the information from the\nconfusion matrices. This new hierarchical phoneme recognition system shows\nsignificant improvements of the recognition rates on TIMIT database.\n",
        "published": "2015-08-07T15:06:13Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01718v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01745v2",
        "title": "Semantically Conditioned LSTM-based Natural Language Generation for\n  Spoken Dialogue Systems",
        "summary": "  Natural language generation (NLG) is a critical component of spoken dialogue\nand it has a significant impact both on usability and perceived quality. Most\nNLG systems in common use employ rules and heuristics and tend to generate\nrigid and stylised responses without the natural variation of human language.\nThey are also not easily scaled to systems covering multiple domains and\nlanguages. This paper presents a statistical language generator based on a\nsemantically controlled Long Short-term Memory (LSTM) structure. The LSTM\ngenerator can learn from unaligned data by jointly optimising sentence planning\nand surface realisation using a simple cross entropy training criterion, and\nlanguage variation can be easily achieved by sampling from output candidates.\nWith fewer heuristics, an objective evaluation in two differing test domains\nshowed the proposed method improved performance compared to previous methods.\nHuman judges scored the LSTM system higher on informativeness and naturalness\nand overall preferred it to the other systems.\n",
        "published": "2015-08-07T16:16:44Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01745v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.01755v1",
        "title": "Stochastic Language Generation in Dialogue using Recurrent Neural\n  Networks with Convolutional Sentence Reranking",
        "summary": "  The natural language generation (NLG) component of a spoken dialogue system\n(SDS) usually needs a substantial amount of handcrafting or a well-labeled\ndataset to be trained on. These limitations add significantly to development\ncosts and make cross-domain, multi-lingual dialogue systems intractable.\nMoreover, human languages are context-aware. The most natural response should\nbe directly learned from data rather than depending on predefined syntaxes or\nrules. This paper presents a statistical language generator based on a joint\nrecurrent and convolutional neural network structure which can be trained on\ndialogue act-utterance pairs without any semantic alignments or predefined\ngrammar trees. Objective metrics suggest that this new model outperforms\nprevious methods under the same experimental conditions. Results of an\nevaluation by human judges indicate that it produces not only high quality but\nlinguistically varied utterances which are preferred compared to n-gram and\nrule-based systems.\n",
        "published": "2015-08-07T16:34:11Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01755v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01991v1",
        "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
        "summary": "  In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.\n",
        "published": "2015-08-09T06:32:47Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01991v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.01996v2",
        "title": "An Automatic Machine Translation Evaluation Metric Based on Dependency\n  Parsing Model",
        "summary": "  Most of the syntax-based metrics obtain the similarity by comparing the\nsub-structures extracted from the trees of hypothesis and reference. These\nsub-structures are defined by human and can't express all the information in\nthe trees because of the limited length of sub-structures. In addition, the\noverlapped parts between these sub-structures are computed repeatedly. To avoid\nthese problems, we propose a novel automatic evaluation metric based on\ndependency parsing model, with no need to define sub-structures by human.\nFirst, we train a dependency parsing model by the reference dependency tree.\nThen we generate the hypothesis dependency tree and the corresponding\nprobability by the dependency parsing model. The quality of the hypothesis can\nbe judged by this probability. In order to obtain the lexicon similarity, we\nalso introduce the unigram F-score to the new metric. Experiment results show\nthat the new metric gets the state-of-the-art performance on system level, and\nis comparable with METEOR on sentence level.\n",
        "published": "2015-08-09T07:55:51Z",
        "pdf_link": "http://arxiv.org/pdf/1508.01996v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.02060v1",
        "title": "Egyptian Dialect Stopword List Generation from Social Network Data",
        "summary": "  This paper proposes a methodology for generating a stopword list from online\nsocial network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is\nto investigate the effect of removingED stopwords on the Sentiment Analysis\n(SA) task. The stopwords lists generated before were on Modern Standard Arabic\n(MSA) which is not the common language used in OSN. We have generated a\nstopword list of Egyptian dialect to be used with the OSN corpora. We compare\nthe efficiency of text classification when using the generated list along with\npreviously generated lists of MSA and combining the Egyptian dialect list with\nthe MSA list. The text classification was performed using Na\\\"ive Bayes and\nDecision Tree classifiers and two feature selection approaches, unigram and\nbigram. The experiments show that removing ED stopwords give better performance\nthan using lists of MSA stopwords only.\n",
        "published": "2015-04-13T15:56:00Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02060v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.02096v2",
        "title": "Finding Function in Form: Compositional Character Models for Open\n  Vocabulary Word Representation",
        "summary": "  We introduce a model for constructing vector representations of words by\ncomposing characters using bidirectional LSTMs. Relative to traditional word\nrepresentation models that have independent vectors for each word type, our\nmodel requires only a single vector per character type and a fixed set of\nparameters for the compositional model. Despite the compactness of this model\nand, more importantly, the arbitrary nature of the form-function relationship\nin language, our \"composed\" word representations yield state-of-the-art results\nin language modeling and part-of-speech tagging. Benefits over traditional\nbaselines are particularly pronounced in morphologically rich languages (e.g.,\nTurkish).\n",
        "published": "2015-08-09T23:41:38Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02096v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.02142v1",
        "title": "Feature-based Decipherment for Large Vocabulary Machine Translation",
        "summary": "  Orthographic similarities across languages provide a strong signal for\nprobabilistic decipherment, especially for closely related language pairs. The\nexisting decipherment models, however, are not well-suited for exploiting these\northographic similarities. We propose a log-linear model with latent variables\nthat incorporates orthographic similarity features. Maximum likelihood training\nis computationally expensive for the proposed log-linear model. To address this\nchallenge, we perform approximate inference via MCMC sampling and contrastive\ndivergence. Our results show that the proposed log-linear model with\ncontrastive divergence scales to large vocabularies and outperforms the\nexisting generative decipherment models by exploiting the orthographic\nfeatures.\n",
        "published": "2015-08-10T07:02:49Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02142v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.02225v2",
        "title": "Improve the Evaluation of Fluency Using Entropy for Machine Translation\n  Evaluation Metrics",
        "summary": "  The widely-used automatic evaluation metrics cannot adequately reflect the\nfluency of the translations. The n-gram-based metrics, like BLEU, limit the\nmaximum length of matched fragments to n and cannot catch the matched fragments\nlonger than n, so they can only reflect the fluency indirectly. METEOR, which\nis not limited by n-gram, uses the number of matched chunks but it does not\nconsider the length of each chunk. In this paper, we propose an entropy-based\nmethod, which can sufficiently reflect the fluency of translations through the\ndistribution of matched words. This method can easily combine with the\nwidely-used automatic evaluation metrics to improve the evaluation of fluency.\nExperiments show that the correlations of BLEU and METEOR are improved on\nsentence level after combining with the entropy-based method on WMT 2010 and\nWMT 2012.\n",
        "published": "2015-08-10T12:46:52Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02225v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.02285v1",
        "title": "Adapting Phrase-based Machine Translation to Normalise Medical Terms in\n  Social Media Messages",
        "summary": "  Previous studies have shown that health reports in social media, such as\nDailyStrength and Twitter, have potential for monitoring health conditions\n(e.g. adverse drug reactions, infectious diseases) in particular communities.\nHowever, in order for a machine to understand and make inferences on these\nhealth conditions, the ability to recognise when laymen's terms refer to a\nparticular medical concept (i.e.\\ text normalisation) is required. To achieve\nthis, we propose to adapt an existing phrase-based machine translation (MT)\ntechnique and a vector representation of words to map between a social media\nphrase and a medical concept. We evaluate our proposed approach using a\ncollection of phrases from tweets related to adverse drug reactions. Our\nexperimental results show that the combination of a phrase-based MT technique\nand the similarity between word vector representations outperforms the\nbaselines that apply only either of them by up to 55%.\n",
        "published": "2015-08-10T15:29:22Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02285v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.02297v1",
        "title": "Measuring Word Significance using Distributed Representations of Words",
        "summary": "  Distributed representations of words as real-valued vectors in a relatively\nlow-dimensional space aim at extracting syntactic and semantic features from\nlarge text corpora. A recently introduced neural network, named word2vec\n(Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic\ninformation in the direction of the word vectors. In this brief report, it is\nproposed to use the length of the vectors, together with the term frequency, as\nmeasure of word significance in a corpus. Experimental evidence using a\ndomain-specific corpus of abstracts is presented to support this proposal. A\nuseful visualization technique for text corpora emerges, where words are mapped\nonto a two-dimensional plane and automatically ranked by significance.\n",
        "published": "2015-08-10T15:52:49Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02297v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.02445v1",
        "title": "Removing Biases from Trainable MT Metrics by Using Self-Training",
        "summary": "  Most trainable machine translation (MT) metrics train their weights on human\njudgments of state-of-the-art MT systems outputs. This makes trainable metrics\nbiases in many ways. One of them is preferring longer translations. These\nbiased metrics when used for tuning are evaluating different types of\ntranslations -- n-best lists of translations with very diverse quality. Systems\ntuned with these metrics tend to produce overly long translations that are\npreferred by the metric but not by humans. This is usually solved by manually\ntweaking metric's weights to equally value recall and precision. Our solution\nis more general: (1) it does not address only the recall bias but also all\nother biases that might be present in the data and (2) it does not require any\nknowledge of the types of features used which is useful in cases when manual\ntuning of metric's weights is not possible. This is accomplished by\nself-training on unlabeled n-best lists by using metric that was initially\ntrained on standard human judgments. One way of looking at this is as domain\nadaptation from the domain of state-of-the-art MT translations to diverse\nn-best list translations.\n",
        "published": "2015-08-10T22:24:36Z",
        "pdf_link": "http://arxiv.org/pdf/1508.02445v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.03040v7",
        "title": "Syntax Evolution: Problems and Recursion",
        "summary": "  To investigate the evolution of syntax, we need to ascertain the evolutionary\nr\\^ole of syntax and, before that, the very nature of syntax. Here, we will\nassume that syntax is computing. And then, since we are computationally Turing\ncomplete, we meet an evolutionary anomaly, the anomaly of sytax: we are\nsyntactically too competent for syntax. Assuming that problem solving is\ncomputing, and realizing that the evolutionary advantage of Turing completeness\nis full problem solving and not syntactic proficiency, we explain the anomaly\nof syntax by postulating that syntax and problem solving co-evolved in humans\ntowards Turing completeness. Examining the requirements that full problem\nsolving impose on language, we find firstly that semantics is not sufficient\nand that syntax is necessary to represent problems. Our final conclusion is\nthat full problem solving requires a functional semantics on an infinite\ntree-structured syntax. Besides these results, the introduction of Turing\ncompleteness and problem solving to explain the evolution of syntax should help\nus to fit the evolution of language within the evolution of cognition, giving\nus some new clues to understand the elusive relation between language and\nthinking.\n",
        "published": "2015-08-12T09:04:01Z",
        "pdf_link": "http://arxiv.org/pdf/1508.03040v7"
    },
    {
        "id": "http://arxiv.org/abs/1508.04025v5",
        "title": "Effective Approaches to Attention-based Neural Machine Translation",
        "summary": "  An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.\n",
        "published": "2015-08-17T13:43:19Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04025v5"
    },
    {
        "id": "http://arxiv.org/abs/1508.04257v2",
        "title": "Learning Meta-Embeddings by Using Ensembles of Embedding Sets",
        "summary": "  Word embeddings -- distributed representations of words -- in deep learning\nare beneficial for many tasks in natural language processing (NLP). However,\ndifferent embedding sets vary greatly in quality and characteristics of the\ncaptured semantics. Instead of relying on a more advanced algorithm for\nembedding learning, this paper proposes an ensemble approach of combining\ndifferent public embedding sets with the aim of learning meta-embeddings.\nExperiments on word similarity and analogy tasks and on part-of-speech tagging\nshow better performance of meta-embeddings compared to individual embedding\nsets. One advantage of meta-embeddings is the increased vocabulary coverage. We\nwill release our meta-embeddings publicly.\n",
        "published": "2015-08-18T09:29:22Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04257v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.04271v1",
        "title": "Probabilistic Modelling of Morphologically Rich Languages",
        "summary": "  This thesis investigates how the sub-structure of words can be accounted for\nin probabilistic models of language. Such models play an important role in\nnatural language processing tasks such as translation or speech recognition,\nbut often rely on the simplistic assumption that words are opaque symbols. This\nassumption does not fit morphologically complex language well, where words can\nhave rich internal structure and sub-word elements are shared across distinct\nword forms.\n  Our approach is to encode basic notions of morphology into the assumptions of\nthree different types of language models, with the intention that leveraging\nshared sub-word structure can improve model performance and help overcome data\nsparsity that arises from morphological processes.\n  In the context of n-gram language modelling, we formulate a new Bayesian\nmodel that relies on the decomposition of compound words to attain better\nsmoothing, and we develop a new distributed language model that learns vector\nrepresentations of morphemes and leverages them to link together\nmorphologically related words. In both cases, we show that accounting for word\nsub-structure improves the models' intrinsic performance and provides benefits\nwhen applied to other tasks, including machine translation.\n  We then shift the focus beyond the modelling of word sequences and consider\nmodels that automatically learn what the sub-word elements of a given language\nare, given an unannotated list of words. We formulate a novel model that can\nlearn discontiguous morphemes in addition to the more conventional contiguous\nmorphemes that most previous models are limited to. This approach is\ndemonstrated on Semitic languages, and we find that modelling discontiguous\nsub-word structures leads to improvements in the task of segmenting words into\ntheir contiguous morphemes.\n",
        "published": "2015-08-18T10:29:10Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04271v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.04515v1",
        "title": "Exploring Metaphorical Senses and Word Representations for Identifying\n  Metonyms",
        "summary": "  A metonym is a word with a figurative meaning, similar to a metaphor. Because\nmetonyms are closely related to metaphors, we apply features that are used\nsuccessfully for metaphor recognition to the task of detecting metonyms. On the\nACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system\nachieved 86.45% accuracy on the location metonyms. Our code can be found on\nGitHub.\n",
        "published": "2015-08-19T03:26:05Z",
        "pdf_link": "http://arxiv.org/pdf/1508.04515v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.05051v1",
        "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models",
        "summary": "  Neural networks have been shown to improve performance across a range of\nnatural-language tasks. However, designing and training them can be\ncomplicated. Frequently, researchers resort to repeated experimentation to pick\noptimal settings. In this paper, we address the issue of choosing the correct\nnumber of units in hidden layers. We introduce a method for automatically\nadjusting network size by pruning out hidden units through $\\ell_{\\infty,1}$\nand $\\ell_{2,1}$ regularization. We apply this method to language modeling and\ndemonstrate its ability to correctly choose the number of hidden units while\nmaintaining perplexity. We also include these models in a machine translation\ndecoder and show that these smaller neural models maintain the significant\nimprovements of their unpruned versions.\n",
        "published": "2015-08-20T17:21:50Z",
        "pdf_link": "http://arxiv.org/pdf/1508.05051v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.05154v2",
        "title": "Posterior calibration and exploratory analysis for natural language\n  processing models",
        "summary": "  Many models in natural language processing define probabilistic distributions\nover linguistic structures. We argue that (1) the quality of a model' s\nposterior distribution can and should be directly evaluated, as to whether\nprobabilities correspond to empirical frequencies, and (2) NLP uncertainty can\nbe projected not only to pipeline components, but also to exploratory data\nanalysis, telling a user when to trust and not trust the NLP analysis. We\npresent a method to analyze calibration, and apply it to compare the\nmiscalibration of several commonly used models. We also contribute a\ncoreference sampling algorithm that can create confidence intervals for a\npolitical event extraction task.\n",
        "published": "2015-08-21T00:25:51Z",
        "pdf_link": "http://arxiv.org/pdf/1508.05154v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.05326v1",
        "title": "A large annotated corpus for learning natural language inference",
        "summary": "  Understanding entailment and contradiction is fundamental to understanding\nnatural language, and inference about entailment and contradiction is a\nvaluable testing ground for the development of semantic representations.\nHowever, machine learning research in this area has been dramatically limited\nby the lack of large-scale resources. To address this, we introduce the\nStanford Natural Language Inference corpus, a new, freely available collection\nof labeled sentence pairs, written by humans doing a novel grounded task based\non image captioning. At 570K pairs, it is two orders of magnitude larger than\nall other resources of its type. This increase in scale allows lexicalized\nclassifiers to outperform some sophisticated existing entailment models, and it\nallows a neural network-based model to perform competitively on natural\nlanguage inference benchmarks for the first time.\n",
        "published": "2015-08-21T16:17:01Z",
        "pdf_link": "http://arxiv.org/pdf/1508.05326v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.06044v1",
        "title": "Visualizing NLP annotations for Crowdsourcing",
        "summary": "  Visualizing NLP annotation is useful for the collection of training data for\nthe statistical NLP approaches. Existing toolkits either provide limited visual\naid, or introduce comprehensive operators to realize sophisticated linguistic\nrules. Workers must be well trained to use them. Their audience thus can hardly\nbe scaled to large amounts of non-expert crowdsourced workers. In this paper,\nwe present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to\nannotate two general categories of NLP problems: clustering and parsing.\nWorkers can finish the tasks with simplified operators in an interactive\ninterface, and fix errors conveniently. User studies show our toolkit is very\nfriendly to NLP non-experts, and allow them to produce high quality labels for\nseveral sophisticated problems. We release our source code and toolkit to spur\nfuture research.\n",
        "published": "2015-08-25T06:34:00Z",
        "pdf_link": "http://arxiv.org/pdf/1508.06044v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.06491v2",
        "title": "Alignment-based compositional semantics for instruction following",
        "summary": "  This paper describes an alignment-based model for interpreting natural\nlanguage instructions in context. We approach instruction following as a search\nover plans, scoring sequences of actions conditioned on structured observations\nof text and the environment. By explicitly modeling both the low-level\ncompositional structure of individual actions and the high-level structure of\nfull plans, we are able to learn both grounded representations of sentence\nmeaning and pragmatic constraints on interpretation. To demonstrate the model's\nflexibility, we apply it to a diverse set of benchmark tasks. On every task, we\noutperform strong task-specific baselines, and achieve several new\nstate-of-the-art results.\n",
        "published": "2015-08-26T13:44:54Z",
        "pdf_link": "http://arxiv.org/pdf/1508.06491v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.06669v1",
        "title": "Component-Enhanced Chinese Character Embeddings",
        "summary": "  Distributed word representations are very useful for capturing semantic\ninformation and have been successfully applied in a variety of NLP tasks,\nespecially on English. In this work, we innovatively develop two\ncomponent-enhanced Chinese character embedding models and their bigram\nextensions. Distinguished from English word embeddings, our models explore the\ncompositions of Chinese characters, which often serve as semantic indictors\ninherently. The evaluations on both word similarity and text classification\ndemonstrate the effectiveness of our models.\n",
        "published": "2015-08-26T21:25:25Z",
        "pdf_link": "http://arxiv.org/pdf/1508.06669v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.07544v2",
        "title": "Computational Sociolinguistics: A Survey",
        "summary": "  Language is a social phenomenon and variation is inherent to its social\nnature. Recently, there has been a surge of interest within the computational\nlinguistics (CL) community in the social dimension of language. In this article\nwe present a survey of the emerging field of \"Computational Sociolinguistics\"\nthat reflects this increased interest. We aim to provide a comprehensive\noverview of CL research on sociolinguistic themes, featuring topics such as the\nrelation between language and social identity, language use in social\ninteraction and multilingual communication. Moreover, we demonstrate the\npotential for synergy between the research communities involved, by showing how\nthe large-scale data-driven methods that are widely used in CL can complement\nexisting sociolinguistic studies, and how sociolinguistics can inform and\nchallenge the methods and assumptions employed in CL studies. We hope to convey\nthe possible benefits of a closer collaboration between the two communities and\nconclude with a discussion of open challenges.\n",
        "published": "2015-08-30T08:39:02Z",
        "pdf_link": "http://arxiv.org/pdf/1508.07544v2"
    },
    {
        "id": "http://arxiv.org/abs/1508.07555v1",
        "title": "An Event Network for Exploring Open Information",
        "summary": "  In this paper, an event network is presented for exploring open information,\nwhere linguistic units about an event are organized for analysing. The process\nis divided into three steps: document event detection, event network\nconstruction and event network analysis. First, by implementing event detection\nor tracking, documents are retrospectively (or on-line) organized into document\nevents. Secondly, for each of the document event, linguistic units are\nextracted and combined into event networks. Thirdly, various analytic methods\nare proposed for event network analysis. In our application methodologies are\npresented for exploring open information.\n",
        "published": "2015-08-30T11:22:38Z",
        "pdf_link": "http://arxiv.org/pdf/1508.07555v1"
    },
    {
        "id": "http://arxiv.org/abs/1508.07909v5",
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "summary": "  Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary. In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively.\n",
        "published": "2015-08-31T16:37:31Z",
        "pdf_link": "http://arxiv.org/pdf/1508.07909v5"
    },
    {
        "id": "http://arxiv.org/abs/1509.00705v1",
        "title": "Analysis of Communication Pattern with Scammers in Enron Corpus",
        "summary": "  This paper is an exploratory analysis into fraud detection taking Enron email\ncorpus as the case study. The paper posits conclusions like strict servitude\nand unquestionable faith among employees as breeding grounds for sham among\nhigher executives. We also try to infer on the nature of communication between\nfraudulent employees and between non- fraudulent-fraudulent employees\n",
        "published": "2015-09-02T13:54:57Z",
        "pdf_link": "http://arxiv.org/pdf/1509.00705v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.00963v1",
        "title": "On TimeML-Compliant Temporal Expression Extraction in Turkish",
        "summary": "  It is commonly acknowledged that temporal expression extractors are important\ncomponents of larger natural language processing systems like information\nretrieval and question answering systems. Extraction and normalization of\ntemporal expressions in Turkish has not been given attention so far except the\nextraction of some date and time expressions within the course of named entity\nrecognition. As TimeML is the current standard of temporal expression and event\nannotation in natural language texts, in this paper, we present an analysis of\ntemporal expressions in Turkish based on the related TimeML classification\n(i.e., date, time, duration, and set expressions). We have created a lexicon\nfor Turkish temporal expressions and devised considerably wide-coverage\npatterns using the lexical classes as the building blocks. We believe that the\nproposed patterns, together with convenient normalization rules, can be readily\nused by prospective temporal expression extraction tools for Turkish.\n",
        "published": "2015-09-03T07:23:10Z",
        "pdf_link": "http://arxiv.org/pdf/1509.00963v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.01007v3",
        "title": "Encoding Prior Knowledge with Eigenword Embeddings",
        "summary": "  Canonical correlation analysis (CCA) is a method for reducing the dimension\nof data represented using two views. It has been previously used to derive word\nembeddings, where one view indicates a word, and the other view indicates its\ncontext. We describe a way to incorporate prior knowledge into CCA, give a\ntheoretical justification for it, and test it by deriving word embeddings and\nevaluating them on a myriad of datasets.\n",
        "published": "2015-09-03T09:39:36Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01007v3"
    },
    {
        "id": "http://arxiv.org/abs/1509.01310v1",
        "title": "The influence of Chunking on Dependency Crossing and Distance",
        "summary": "  This paper hypothesizes that chunking plays important role in reducing\ndependency distance and dependency crossings. Computer simulations, when\ncompared with natural languages,show that chunking reduces mean dependency\ndistance (MDD) of a linear sequence of nodes (constrained by continuity or\nprojectivity) to that of natural languages. More interestingly, chunking alone\nbrings about less dependency crossings as well, though having failed to reduce\nthem, to such rarity as found in human languages. These results suggest that\nchunking may play a vital role in the minimization of dependency distance, and\na somewhat contributing role in the rarity of dependency crossing. In addition,\nthe results point to a possibility that the rarity of dependency crossings is\nnot a mere side-effect of minimization of dependency distance, but a linguistic\nphenomenon with its own motivations.\n",
        "published": "2015-09-03T23:57:46Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01310v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.01692v4",
        "title": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility\n  of Vector Differences for Lexical Relation Learning",
        "summary": "  Recent work on word embeddings has shown that simple vector subtraction over\npre-trained embeddings is surprisingly effective at capturing different lexical\nrelations, despite lacking explicit supervision. Prior work has evaluated this\nintriguing result using a word analogy prediction formulation and hand-selected\nrelations, but the generality of the finding over a broader range of lexical\nrelation types and different learning settings has not been evaluated. In this\npaper, we carry out such an evaluation in two learning settings: (1) spectral\nclustering to induce word relations, and (2) supervised learning to classify\nvector differences into relation types. We find that word embeddings capture a\nsurprising amount of information, and that, under suitable supervised training,\nvector subtraction generalises well to a broad range of relations, including\nover unseen lexical items.\n",
        "published": "2015-09-05T11:23:44Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01692v4"
    },
    {
        "id": "http://arxiv.org/abs/1509.01722v2",
        "title": "A commentary on \"The now-or-never bottleneck: a fundamental constraint\n  on language\", by Christiansen and Chater (2016)",
        "summary": "  In a recent article, Christiansen and Chater (2016) present a fundamental\nconstraint on language, i.e. a now-or-never bottleneck that arises from our\nfleeting memory, and explore its implications, e.g., chunk-and-pass processing,\noutlining a framework that promises to unify different areas of research. Here\nwe explore additional support for this constraint and suggest further\nconnections from quantitative linguistics and information theory.\n",
        "published": "2015-09-05T17:52:16Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01722v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.01899v2",
        "title": "Integrate Document Ranking Information into Confidence Measure\n  Calculation for Spoken Term Detection",
        "summary": "  This paper proposes an algorithm to improve the calculation of confidence\nmeasure for spoken term detection (STD). Given an input query term, the\nalgorithm first calculates a measurement named document ranking weight for each\ndocument in the speech database to reflect its relevance with the query term by\nsumming all the confidence measures of the hypothesized term occurrences in\nthis document. The confidence measure of each term occurrence is then\nre-estimated through linear interpolation with the calculated document ranking\nweight to improve its reliability by integrating document-level information.\nExperiments are conducted on three standard STD tasks for Tamil, Vietnamese and\nEnglish respectively. The experimental results all demonstrate that the\nproposed algorithm achieves consistent improvements over the state-of-the-art\nmethod for confidence measure calculation. Furthermore, this algorithm is still\neffective even if a high accuracy speech recognizer is not available, which\nmakes it applicable for the languages with limited speech resources.\n",
        "published": "2015-09-07T04:40:14Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01899v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.01938v1",
        "title": "Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical\n  Machine Translation",
        "summary": "  Statistical machine translation for dialectal Arabic is characterized by a\nlack of data since data acquisition involves the transcription and translation\nof spoken language. In this study we develop techniques for extracting parallel\ndata for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain\ncorpora in different dialects of Arabic or in Modern Standard Arabic. We\ncompare two different data selection strategies (cross-entropy based and\nsubmodular selection) and demonstrate that a very small but highly targeted\namount of found data can improve the performance of a baseline machine\ntranslation system. We furthermore report on preliminary experiments on using\nautomatically translated speech data as additional training data.\n",
        "published": "2015-09-07T07:54:17Z",
        "pdf_link": "http://arxiv.org/pdf/1509.01938v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02208v1",
        "title": "Unsupervised Discovery of Linguistic Structure Including Two-level\n  Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization",
        "summary": "  Techniques for unsupervised discovery of acoustic patterns are getting\nincreasingly attractive, because huge quantities of speech data are becoming\navailable but manual annotations remain hard to acquire. In this paper, we\npropose an approach for unsupervised discovery of linguistic structure for the\ntarget spoken language given raw speech data. This linguistic structure\nincludes two-level (subword-like and word-like) acoustic patterns, the lexicon\nof word-like patterns in terms of subword-like patterns and the N-gram language\nmodel based on word-like patterns. All patterns, models, and parameters can be\nautomatically learned from the unlabelled speech corpus. This is achieved by an\ninitialization step followed by three cascaded stages for acoustic, linguistic,\nand lexical iterative optimization. The lexicon of word-like patterns defines\nallowed consecutive sequence of HMMs for subword-like patterns. In each\niteration, model training and decoding produces updated labels from which the\nlexicon and HMMs can be further updated. In this way, model parameters and\ndecoded labels are respectively optimized in each iteration, and the knowledge\nabout the linguistic structure is learned gradually layer after layer. The\nproposed approach was tested in preliminary experiments on a corpus of Mandarin\nbroadcast news, including a task of spoken term detection with performance\ncompared to a parallel test using models trained in a supervised way. Results\nshow that the proposed system not only yields reasonable performance on its\nown, but is also complimentary to existing large vocabulary ASR systems.\n",
        "published": "2015-09-07T22:23:01Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02208v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02213v1",
        "title": "Unsupervised Spoken Term Detection with Spoken Queries by Multi-level\n  Acoustic Patterns with Varying Model Granularity",
        "summary": "  This paper presents a new approach for unsupervised Spoken Term Detection\nwith spoken queries using multiple sets of acoustic patterns automatically\ndiscovered from the target corpus. The different pattern HMM\nconfigurations(number of states per model, number of distinct models, number of\nGaussians per state)form a three-dimensional model granularity space. Different\nsets of acoustic patterns automatically discovered on different points properly\ndistributed over this three-dimensional space are complementary to one another,\nthus can jointly capture the characteristics of the spoken terms. By\nrepresenting the spoken content and spoken query as sequences of acoustic\npatterns, a series of approaches for matching the pattern index sequences while\nconsidering the signal variations are developed. In this way, not only the\non-line computation load can be reduced, but the signal distributions caused by\ndifferent speakers and acoustic conditions can be reasonably taken care of. The\nresults indicate that this approach significantly outperformed the unsupervised\nfeature-based DTW baseline by 16.16\\% in mean average precision on the TIMIT\ncorpus.\n",
        "published": "2015-09-07T22:40:31Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02213v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02217v1",
        "title": "Enhancing Automatically Discovered Multi-level Acoustic Patterns\n  Considering Context Consistency With Applications in Spoken Term Detection",
        "summary": "  This paper presents a novel approach for enhancing the multiple sets of\nacoustic patterns automatically discovered from a given corpus. In a previous\nwork it was proposed that different HMM configurations (number of states per\nmodel, number of distinct models) for the acoustic patterns form a\ntwo-dimensional space. Multiple sets of acoustic patterns automatically\ndiscovered with the HMM configurations properly located on different points\nover this two-dimensional space were shown to be complementary to one another,\njointly capturing the characteristics of the given corpus. By representing the\ngiven corpus as sequences of acoustic patterns on different HMM sets, the\npattern indices in these sequences can be relabeled considering the context\nconsistency across the different sequences. Good improvements were observed in\npreliminary experiments of pattern spoken term detection (STD) performed on\nboth TIMIT and Mandarin Broadcast News with such enhanced patterns.\n",
        "published": "2015-09-07T22:56:49Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02217v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.02301v3",
        "title": "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking",
        "summary": "  Many fundamental problems in natural language processing rely on determining\nwhat entities appear in a given text. Commonly referenced as entity linking,\nthis step is a fundamental component of many NLP tasks such as text\nunderstanding, automatic summarization, semantic search or machine translation.\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\ndistribution of entities contribute to the complexity of this problem.\n  We here propose a probabilistic approach that makes use of an effective\ngraphical model to perform collective entity disambiguation. Input mentions\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\ndocument by combining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their surrounding context. The\nmodel is based on simple sufficient statistics extracted from data, thus\nrelying on few parameters to be learned.\n  Our method does not require extensive feature engineering, nor an expensive\ntraining procedure. We use loopy belief propagation to perform approximate\ninference. The low complexity of our model makes this step sufficiently fast\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\nrange of benchmark datasets, showing that it matches, and in many cases\noutperforms, existing state-of-the-art methods.\n",
        "published": "2015-09-08T09:43:13Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02301v3"
    },
    {
        "id": "http://arxiv.org/abs/1509.02412v1",
        "title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for\n  Acoustic Modelling in Speech Recognition",
        "summary": "  Speech recognition systems are often highly domain dependent, a fact widely\nreported in the literature. However the concept of domain is complex and not\nbound to clear criteria. Hence it is often not evident if data should be\nconsidered to be out-of-domain. While both acoustic and language models can be\ndomain specific, work in this paper concentrates on acoustic modelling. We\npresent a novel method to perform unsupervised discovery of domains using\nLatent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is\nassumed to exist in the data, whereby each audio segment can be considered to\nbe a weighted mixture of domain properties. The classification of audio\nsegments into domains allows the creation of domain specific acoustic models\nfor automatic speech recognition. Experiments are conducted on a dataset of\ndiverse speech data covering speech from radio and TV broadcasts, telephone\nconversations, meetings, lectures and read speech, with a joint training set of\n60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to\nLDA based domains was shown to yield relative Word Error Rate (WER)\nimprovements of up to 16% relative, compared to pooled training, and up to 10%,\ncompared with models adapted with human-labelled prior domain knowledge.\n",
        "published": "2015-09-08T15:29:23Z",
        "pdf_link": "http://arxiv.org/pdf/1509.02412v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.03208v1",
        "title": "Towards Understanding Egyptian Arabic Dialogues",
        "summary": "  Labelling of user's utterances to understanding his attends which called\nDialogue Act (DA) classification, it is considered the key player for dialogue\nlanguage understanding layer in automatic dialogue systems. In this paper, we\nproposed a novel approach to user's utterances labeling for Egyptian\nspontaneous dialogues and Instant Messages using Machine Learning (ML) approach\nwithout relying on any special lexicons, cues, or rules. Due to the lack of\nEgyptian dialect dialogue corpus, the system evaluated by multi-genre corpus\nincludes 4725 utterances for three domains, which are collected and annotated\nmanually from Egyptian call-centers. The system achieves F1 scores of 70. 36%\noverall domains.\n",
        "published": "2015-07-14T02:47:40Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03208v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.03488v2",
        "title": "Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality -\n  Revisiting the Issue of Meaning Grounded in Syntax",
        "summary": "  We revisit Levin's theory about the correspondence of verb meaning and syntax\nand infer semantic classes from a large syntactic classification of more than\n600 German verbs taking clausal and non-finite arguments. Grasping the meaning\ncomponents of Levin-classes is known to be hard. We address this challenge by\nsetting up a multi-perspective semantic characterization of the inferred\nclasses. To this end, we link the inferred classes and their English\ntranslation to independently constructed semantic classes in three different\nlexicons - the German wordnet GermaNet, VerbNet and FrameNet - and perform a\ndetailed analysis and evaluation of the resulting German-English classification\n(available at www.ukp.tu-darmstadt.de/modality-verbclasses/).\n",
        "published": "2015-09-11T13:05:15Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03488v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.03611v2",
        "title": "A Parallel Corpus of Translationese",
        "summary": "  We describe a set of bilingual English--French and English--German parallel\ncorpora in which the direction of translation is accurately and reliably\nannotated. The corpora are diverse, consisting of parliamentary proceedings,\nliterary works, transcriptions of TED talks and political commentary. They will\nbe instrumental for research of translationese and its applications to (human\nand machine) translation; specifically, they can be used for the task of\ntranslationese identification, a research direction that enjoys a growing\ninterest in recent years. To validate the quality and reliability of the\ncorpora, we replicated previous results of supervised and unsupervised\nidentification of translationese, and further extended the experiments to\nadditional datasets and languages.\n",
        "published": "2015-09-11T19:07:49Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03611v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.03739v1",
        "title": "Improving distant supervision using inference learning",
        "summary": "  Distant supervision is a widely applied approach to automatic training of\nrelation extraction systems and has the advantage that it can generate large\namounts of labelled data with minimal effort. However, this data may contain\nerrors and consequently systems trained using distant supervision tend not to\nperform as well as those based on manually labelled data. This work proposes a\nnovel method for detecting potential false negative training examples using a\nknowledge inference method. Results show that our approach improves the\nperformance of relation extraction systems trained using distantly supervised\ndata.\n",
        "published": "2015-09-12T12:59:05Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03739v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.03870v1",
        "title": "The USFD Spoken Language Translation System for IWSLT 2014",
        "summary": "  The University of Sheffield (USFD) participated in the International Workshop\nfor Spoken Language Translation (IWSLT) in 2014. In this paper, we will\nintroduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is\nachieved by two multi-pass deep neural network systems with adaptation and\nrescoring techniques. Machine translation (MT) is achieved by a phrase-based\nsystem. The USFD primary system incorporates state-of-the-art ASR and MT\ntechniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French\nand English-to-German speech-to-text translation task with the IWSLT 2014 data.\nThe USFD contrastive systems explore the integration of ASR and MT by using a\nquality estimation system to rescore the ASR outputs, optimising towards better\ntranslation. This gives a further 0.54 and 0.26 BLEU improvement respectively\non the IWSLT 2012 and 2014 evaluation data.\n",
        "published": "2015-09-13T16:58:41Z",
        "pdf_link": "http://arxiv.org/pdf/1509.03870v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.04385v1",
        "title": "Kannada named entity recognition and classification (nerc) based on\n  multinomial nave bayes (mnb) classifier",
        "summary": "  Named Entity Recognition and Classification (NERC) is a process of\nidentification of proper nouns in the text and classification of those nouns\ninto certain predefined categories like person name, location, organization,\ndate, and time etc. NERC in Kannada is an essential and challenging task. The\naim of this work is to develop a novel model for NERC, based on Multinomial\nNa\\\"ive Bayes (MNB) Classifier. The Methodology adopted in this paper is based\non feature extraction of training corpus, by using term frequency, inverse\ndocument frequency and fitting them to a tf-idf-vectorizer. The paper discusses\nthe various issues in developing the proposed model. The details of\nimplementation and performance evaluation are discussed. The experiments are\nconducted on a training corpus of size 95,170 tokens and test corpus of 5,000\ntokens. It is observed that the model works with Precision, Recall and\nF1-measure of 83%, 79% and 81% respectively.\n",
        "published": "2015-09-12T06:07:16Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04385v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.04393v1",
        "title": "Dependency length minimization: Puzzles and Promises",
        "summary": "  In the recent issue of PNAS, Futrell et al. claims that their study of 37\nlanguages gives the first large scale cross-language evidence for Dependency\nLength Minimization, which is an overstatement that ignores similar previous\nresearches. In addition,this study seems to pay no attention to factors like\nthe uniformity of genres,which weakens the validity of the argument that DLM is\nuniversal. Another problem is that this study sets the baseline random language\nas projective, which fails to truly uncover the difference between natural\nlanguage and random language, since projectivity is an important feature of\nmany natural languages. Finally, the paper contends an \"apparent relationship\nbetween head finality and dependency length\" despite the lack of an explicit\nstatistical comparison, which renders this conclusion rather hasty and\nimproper.\n",
        "published": "2015-09-15T04:29:50Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04393v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.04473v1",
        "title": "Splitting Compounds by Semantic Analogy",
        "summary": "  Compounding is a highly productive word-formation process in some languages\nthat is often problematic for natural language processing applications. In this\npaper, we investigate whether distributional semantics in the form of word\nembeddings can enable a deeper, i.e., more knowledge-rich, processing of\ncompounds than the standard string-based methods. We present an unsupervised\napproach that exploits regularities in the semantic vector space (based on\nanalogies such as \"bookshop is to shop as bookshelf is to shelf\") to produce\ncompound analyses of high quality. A subsequent compound splitting algorithm\nbased on these analyses is highly effective, particularly for ambiguous\ncompounds. German to English machine translation experiments show that this\nsemantic analogy-based compound splitter leads to better translations than a\ncommonly used frequency-based method.\n",
        "published": "2015-09-15T10:03:35Z",
        "pdf_link": "http://arxiv.org/pdf/1509.04473v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.05488v7",
        "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding",
        "summary": "  Recently, knowledge graph embedding, which projects symbolic entities and\nrelations into continuous vector space, has become a new, hot topic in\nartificial intelligence. This paper addresses a new issue of multiple relation\nsemantics that a relation may have multiple meanings revealed by the entity\npairs associated with the corresponding triples, and proposes a novel Gaussian\nmixture model for embedding, TransG. The new model can discover latent\nsemantics for a relation and leverage a mixture of relation component vectors\nfor embedding a fact triple. To the best of our knowledge, this is the first\ngenerative model for knowledge graph embedding, which is able to deal with\nmultiple relation semantics. Extensive experiments show that the proposed model\nachieves substantial improvements against the state-of-the-art baselines.\n",
        "published": "2015-09-18T02:30:17Z",
        "pdf_link": "http://arxiv.org/pdf/1509.05488v7"
    },
    {
        "id": "http://arxiv.org/abs/1509.05490v2",
        "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding",
        "summary": "  Knowledge representation is a major topic in AI, and many studies attempt to\nrepresent entities and relations of knowledge base in a continuous vector\nspace. Among these attempts, translation-based methods build entity and\nrelation vectors by minimizing the translation loss from a head entity to a\ntail one. In spite of the success of these methods, translation-based methods\nalso suffer from the oversimplified loss metric, and are not competitive enough\nto model various and complex entities/relations in knowledge bases. To address\nthis issue, we propose \\textbf{TransA}, an adaptive metric approach for\nembedding, utilizing the metric learning ideas to provide a more flexible\nembedding method. Experiments are conducted on the benchmark datasets and our\nproposed method makes significant and consistent improvements over the\nstate-of-the-art baselines.\n",
        "published": "2015-09-18T02:40:07Z",
        "pdf_link": "http://arxiv.org/pdf/1509.05490v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.05517v1",
        "title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium\n  Free/Open-Source Machine Translation Platform",
        "summary": "  This paper describes a free/open-source implementation of the light\nsliding-window (LSW) part-of-speech tagger for the Apertium free/open-source\nmachine translation platform. Firstly, the mechanism and training process of\nthe tagger are reviewed, and a new method for incorporating linguistic rules is\nproposed. Secondly, experiments are conducted to compare the performances of\nthe tagger under different window settings, with or without Apertium-style\n\"forbid\" rules, with or without Constraint Grammar, and also with respect to\nthe traditional HMM tagger in Apertium.\n",
        "published": "2015-09-18T06:56:38Z",
        "pdf_link": "http://arxiv.org/pdf/1509.05517v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.06053v1",
        "title": "Early text classification: a Naive solution",
        "summary": "  Text classification is a widely studied problem, and it can be considered\nsolved for some domains and under certain circumstances. There are scenarios,\nhowever, that have received little or no attention at all, despite its\nrelevance and applicability. One of such scenarios is early text\nclassification, where one needs to know the category of a document by using\npartial information only. A document is processed as a sequence of terms, and\nthe goal is to devise a method that can make predictions as fast as possible.\nThe importance of this variant of the text classification problem is evident in\ndomains like sexual predator detection, where one wants to identify an offender\nas early as possible. This paper analyzes the suitability of the standard naive\nBayes classifier for approaching this problem. Specifically, we assess its\nperformance when classifying documents after seeing an increasingly number of\nterms. A simple modification to the standard naive Bayes implementation allows\nus to make predictions with partial information. To the best of our knowledge\nnaive Bayes has not been used for this purpose before. Throughout an extensive\nexperimental evaluation we show the effectiveness of the classifier for early\ntext classification. What is more, we show that this simple solution is very\ncompetitive when compared with state of the art methodologies that are more\nelaborated. We foresee our work will pave the way for the development of more\neffective early text classification techniques based in the naive Bayes\nformulation.\n",
        "published": "2015-09-20T21:01:51Z",
        "pdf_link": "http://arxiv.org/pdf/1509.06053v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.06928v2",
        "title": "Automatic Dialect Detection in Arabic Broadcast Speech",
        "summary": "  We investigate different approaches for dialect identification in Arabic\nbroadcast speech, using phonetic, lexical features obtained from a speech\nrecognition system, and acoustic features using the i-vector framework. We\nstudied both generative and discriminate classifiers, and we combined these\nfeatures using a multi-class Support Vector Machine (SVM). We validated our\nresults on an Arabic/English language identification task, with an accuracy of\n100%. We used these features in a binary classifier to discriminate between\nModern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We\nfurther report results using the proposed method to discriminate between the\nfive most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine,\nNorth African, and MSA, with an accuracy of 52%. We discuss dialect\nidentification errors in the context of dialect code-switching between\nDialectal Arabic and MSA, and compare the error pattern between manually\nlabeled data, and the output from our classifier. We also release the train and\ntest data as standard corpus for dialect identification.\n",
        "published": "2015-09-23T11:41:10Z",
        "pdf_link": "http://arxiv.org/pdf/1509.06928v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.06937v1",
        "title": "Fully automatic multi-language translation with a catalogue of phrases -\n  successful employment for the Swiss avalanche bulletin",
        "summary": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nBecause this catalogue of phrases is limited to a small sublanguage, the system\nis able to automatically translate such sentences from German into the target\nlanguages French, Italian and English without subsequent proofreading or\ncorrection. Having been operational for two winter seasons, we assess here the\nquality of the produced texts based on two different surveys where participants\nrated texts from real avalanche bulletins from both origins, the catalogue of\nphrases versus manually written and translated texts. With a mean recognition\nrate of 55%, users can hardly distinguish between thetwo types of texts, and\ngive very similar ratings with respect to their language quality. Overall, the\noutput from the catalogue system can be considered virtually equivalent to a\ntext written by avalanche forecasters and then manually translated by\nprofessional translators. Furthermore, forecasters declared that all relevant\nsituations were captured by the system with sufficient accuracy. Forecaster's\nworking load did not change with the introduction of the catalogue: the extra\ntime to find matching sentences is compensated by the fact that they no longer\nneed to double-check manually translated texts. The reduction of daily\ntranslation costs is expected to offset the initial development costs within a\nfew years.\n",
        "published": "2015-09-23T12:09:07Z",
        "pdf_link": "http://arxiv.org/pdf/1509.06937v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.07308v2",
        "title": "Bilingual Distributed Word Representations from Document-Aligned\n  Comparable Data",
        "summary": "  We propose a new model for learning bilingual word representations from\nnon-parallel document-aligned data. Following the recent advances in word\nrepresentation learning, our model learns dense real-valued word vectors, that\nis, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which\nheavily relied on parallel sentence-aligned corpora and/or readily available\ntranslation resources such as dictionaries, the article reveals that BWEs may\nbe learned solely on the basis of document-aligned comparable data without any\nadditional lexical resources nor syntactic information. We present a comparison\nof our approach with previous state-of-the-art models for learning bilingual\nword representations from comparable data that rely on the framework of\nmultilingual probabilistic topic modeling (MuPTM), as well as with\ndistributional local context-counting models. We demonstrate the utility of the\ninduced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2)\nsuggesting word translations in context for polysemous words. Our simple yet\neffective BWE-based models significantly outperform the MuPTM-based and\ncontext-counting representation models from comparable data as well as prior\nBWE-based models, and acquire the best reported results on both tasks for all\nthree tested language pairs.\n",
        "published": "2015-09-24T11:00:04Z",
        "pdf_link": "http://arxiv.org/pdf/1509.07308v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.07513v1",
        "title": "Description of the Odin Event Extraction Framework and Rule Language",
        "summary": "  This document describes the Odin framework, which is a domain-independent\nplatform for developing rule-based event extraction models. Odin aims to be\npowerful (the rule language allows the modeling of complex syntactic\nstructures) and robust (to recover from syntactic parsing errors, syntactic\npatterns can be freely mixed with surface, token-based patterns), while\nremaining simple (some domain grammars can be up and running in minutes), and\nfast (Odin processes over 100 sentences/second in a real-world domain with over\n200 rules). Here we include a thorough definition of the Odin rule language,\ntogether with a description of the Odin API in the Scala language, which allows\none to apply these rules to arbitrary texts.\n",
        "published": "2015-09-24T20:10:27Z",
        "pdf_link": "http://arxiv.org/pdf/1509.07513v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.07612v1",
        "title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications\n  for General Purpose Realtime Sentiment Analysis",
        "summary": "  State of the art benchmarks for Twitter Sentiment Analysis do not consider\nthe fact that for more than half of the tweets from the public stream a\ndistinct sentiment cannot be chosen. This paper provides a new perspective on\nTwitter Sentiment Analysis by highlighting the necessity of explicitly\nincorporating uncertainty. Moreover, a dataset of high quality to evaluate\nsolutions for this new problem is introduced and made publicly available.\n",
        "published": "2015-09-25T07:55:26Z",
        "pdf_link": "http://arxiv.org/pdf/1509.07612v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.07761v2",
        "title": "Sentiment of Emojis",
        "summary": "  There is a new generation of emoticons, called emojis, that is increasingly\nbeing used in mobile communications and social media. In the past two years,\nover ten billion emojis were used on Twitter. Emojis are Unicode graphic\nsymbols, used as a shorthand to express concepts and ideas. In contrast to the\nsmall number of well-known emoticons that carry clear emotional contents, there\nare hundreds of emojis. But what are their emotional contents? We provide the\nfirst emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a\nsentiment map of the 751 most frequently used emojis. The sentiment of the\nemojis is computed from the sentiment of the tweets in which they occur. We\nengaged 83 human annotators to label over 1.6 million tweets in 13 European\nlanguages by the sentiment polarity (negative, neutral, or positive). About 4%\nof the annotated tweets contain emojis. The sentiment analysis of the emojis\nallows us to draw several interesting conclusions. It turns out that most of\nthe emojis are positive, especially the most popular ones. The sentiment\ndistribution of the tweets with and without emojis is significantly different.\nThe inter-annotator agreement on the tweets with emojis is higher. Emojis tend\nto occur at the end of the tweets, and their sentiment polarity increases with\nthe distance. We observe no significant differences in the emoji rankings\nbetween the 13 languages and the Emoji Sentiment Ranking. Consequently, we\npropose our Emoji Sentiment Ranking as a European language-independent resource\nfor automated sentiment analysis. Finally, the paper provides a formalization\nof sentiment and a novel visualization in the form of a sentiment bar.\n",
        "published": "2015-09-25T15:41:13Z",
        "pdf_link": "http://arxiv.org/pdf/1509.07761v2"
    },
    {
        "id": "http://arxiv.org/abs/1509.08842v1",
        "title": "Automatically Segmenting Oral History Transcripts",
        "summary": "  Dividing oral histories into topically coherent segments can make them more\naccessible online. People regularly make judgments about where coherent\nsegments can be extracted from oral histories. But making these judgments can\nbe taxing, so automated assistance is potentially attractive to speed the task\nof extracting segments from open-ended interviews. When different people are\nasked to extract coherent segments from the same oral histories, they often do\nnot agree about precisely where such segments begin and end. This low agreement\nmakes the evaluation of algorithmic segmenters challenging, but there is reason\nto believe that for segmenting oral history transcripts, some approaches are\nmore promising than others. The BayesSeg algorithm performs slightly better\nthan TextTiling, while TextTiling does not perform significantly better than a\nuniform segmentation. BayesSeg might be used to suggest boundaries to someone\nsegmenting oral histories, but this segmentation task needs to be better\ndefined.\n",
        "published": "2015-09-29T16:46:52Z",
        "pdf_link": "http://arxiv.org/pdf/1509.08842v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.08874v1",
        "title": "Polish - English Speech Statistical Machine Translation Systems for the\n  IWSLT 2014",
        "summary": "  This research explores effects of various training settings between Polish\nand English Statistical Machine Translation systems for spoken language.\nVarious elements of the TED parallel text corpora for the IWSLT 2014 evaluation\ncampaign were used as the basis for training of language models, and for\ndevelopment, tuning and testing of the translation system as well as Wikipedia\nbased comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics\nwere used to evaluate the effects of data preparations on translation results.\nOur experiments included systems, which use lemma and morphological information\non Polish words. We also conducted a deep analysis of provided Polish data as\npreparatory work for the automatic data correction and cleaning phase.\n",
        "published": "2015-09-29T18:17:22Z",
        "pdf_link": "http://arxiv.org/pdf/1509.08874v1"
    },
    {
        "id": "http://arxiv.org/abs/1509.09121v2",
        "title": "The \"handedness\" of language: Directional symmetry breaking of sign\n  usage in words",
        "summary": "  Language, which allows complex ideas to be communicated through symbolic\nsequences, is a characteristic feature of our species and manifested in a\nmultitude of forms. Using large written corpora for many different languages\nand scripts, we show that the occurrence probability distributions of signs at\nthe left and right ends of words have a distinct heterogeneous nature.\nCharacterizing this asymmetry using quantitative inequality measures, viz.\ninformation entropy and the Gini index, we show that the beginning of a word is\nless restrictive in sign usage than the end. This property is not simply\nattributable to the use of common affixes as it is seen even when only word\nroots are considered. We use the existence of this asymmetry to infer the\ndirection of writing in undeciphered inscriptions that agrees with the\narchaeological evidence. Unlike traditional investigations of phonotactic\nconstraints which focus on language-specific patterns, our study reveals a\nproperty valid across languages and writing systems. As both language and\nwriting are unique aspects of our species, this universal signature may reflect\nan innate feature of the human cognitive phenomenon.\n",
        "published": "2015-09-30T10:59:24Z",
        "pdf_link": "http://arxiv.org/pdf/1509.09121v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.00215v1",
        "title": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network\n  with Word Embedding",
        "summary": "  Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has\nbeen shown to be very effective for modeling and predicting sequential data,\ne.g. speech utterances or handwritten documents. In this study, we propose to\nuse BLSTM-RNN for a unified tagging solution that can be applied to various\ntagging tasks including part-of-speech tagging, chunking and named entity\nrecognition. Instead of exploiting specific features carefully optimized for\neach task, our solution only uses one set of task-independent features and\ninternal representations learnt from unlabeled text for all tasks.Requiring no\ntask specific knowledge or sophisticated feature engineering, our approach gets\nnearly state-of-the-art performance in all these three tagging tasks.\n",
        "published": "2015-11-01T07:59:48Z",
        "pdf_link": "http://arxiv.org/pdf/1511.00215v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.01574v2",
        "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix\n  Language Model",
        "summary": "  We describe Sparse Non-negative Matrix (SNM) language model estimation using\nmultinomial loss on held-out data.\n  Being able to train on held-out data is important in practical situations\nwhere the training data is usually mismatched from the held-out/test data. It\nis also less constrained than the previous training algorithm using\nleave-one-out on training data: it allows the use of richer meta-features in\nthe adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing\nwhich would be difficult to deal with correctly in leave-one-out training.\n  In experiments on the one billion words language modeling benchmark, we are\nable to slightly improve on our previous results which use a different loss\nfunction, and employ leave-one-out training on a subset of the main training\nset. Surprisingly, an adjustment model with meta-features that discard all\nlexical information can perform as well as lexicalized meta-features. We find\nthat fairly small amounts of held-out data (on the order of 30-70 thousand\nwords) are sufficient for training the adjustment model.\n  In a real-life scenario where the training data is a mix of data sources that\nare imbalanced in size, and of different degrees of relevance to the held-out\nand test data, taking into account the data source for a given skip-/n-gram\nfeature and combining them for best performance on held-out/test data improves\nover skip-/n-gram SNM models trained on pooled data by about 8% in the SMT\nsetup, or as much as 15% in the ASR/IME setup.\n  The ability to mix various data sources based on how relevant they are to a\nmismatched held-out set is probably the most attractive feature of the new\nestimation method for SNM LM.\n",
        "published": "2015-11-05T01:45:29Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01574v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.01665v1",
        "title": "An Empirical Study on Sentiment Classification of Chinese Review using\n  Word Embedding",
        "summary": "  In this article, how word embeddings can be used as features in Chinese\nsentiment classification is presented. Firstly, a Chinese opinion corpus is\nbuilt with a million comments from hotel review websites. Then the word\nembeddings which represent each comment are used as input in different machine\nlearning methods for sentiment classification, including SVM, Logistic\nRegression, Convolutional Neural Network (CNN) and ensemble methods. These\nmethods get better performance compared with N-gram models using Naive Bayes\n(NB) and Maximum Entropy (ME). Finally, a combination of machine learning\nmethods is proposed which presents an outstanding performance in precision,\nrecall and F1 score. After selecting the most useful methods to construct the\ncombinational model and testing over the corpus, the final F1 score is 0.920.\n",
        "published": "2015-11-05T09:25:21Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01665v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.01666v1",
        "title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping",
        "summary": "  The development of plot or story in novels is reflected in the content and\nthe words used. The flow of sentiments, which is one aspect of writing style,\ncan be quantified by analyzing the flow of words. This study explores literary\nworks as signals in word embedding space and tries to compare writing styles of\npopular classic novels using dynamic time warping.\n",
        "published": "2015-11-05T09:25:41Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01666v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.01756v2",
        "title": "\"Pale as death\" or \"ple comme la mort\" : Frozen similes used as\n  literary clichs",
        "summary": "  The present study is focused on the automatic identification and description\nof frozen similes in British and French novels written between the 19 th\ncentury and the beginning of the 20 th century. Two main patterns of frozen\nsimiles were considered: adjectival ground + simile marker + nominal vehicle\n(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.\nsleep like a top). All potential similes and their components were first\nextracted using a rule-based algorithm. Then, frozen similes were identified\nbased on reference lists of existing similes and semantic distance between the\ntenor and the vehicle. The results obtained tend to confirm the fact that\nfrozen similes are not used haphazardly in literary texts. In addition,\ncontrary to how they are often presented, frozen similes often go beyond the\nground or the eventuality and the vehicle to also include the tenor.\n",
        "published": "2015-11-05T14:20:01Z",
        "pdf_link": "http://arxiv.org/pdf/1511.01756v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.02014v2",
        "title": "Population size predicts lexical diversity, but so does the mean sea\n  level - why it is important to correctly account for the structure of\n  temporal data",
        "summary": "  In order to demonstrate why it is important to correctly account for the\n(serial dependent) structure of temporal data, we document an apparently\nspectacular relationship between population size and lexical diversity: for\nfive out of seven investigated languages, there is a strong relationship\nbetween population size and lexical diversity of the primary language in this\ncountry. We show that this relationship is the result of a misspecified model\nthat does not consider the temporal aspect of the data by presenting a similar\nbut nonsensical relationship between the global annual mean sea level and\nlexical diversity. Given the fact that in the recent past, several studies were\npublished that present surprising links between different economic, cultural,\npolitical and (socio-)demographical variables on the one hand and cultural or\nlinguistic characteristics on the other hand, but seem to suffer from exactly\nthis problem, we explain the cause of the misspecification and show that it has\nprofound consequences. We demonstrate how simple transformation of the time\nseries can often solve problems of this type and argue that the evaluation of\nthe plausibility of a relationship is important in this context. We hope that\nour paper will help both researchers and reviewers to understand why it is\nimportant to use special models for the analysis of data with a natural\ntemporal ordering.\n",
        "published": "2015-11-06T09:39:24Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02014v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.02117v1",
        "title": "Introducing SKYSET - a Quintuple Approach for Improving Instructions",
        "summary": "  A new approach called SKYSET (Synthetic Knowledge Yield Social Entities\nTranslation) is proposed to validate completeness and to reduce ambiguity from\nwritten instructional documentation. SKYSET utilizes a quintuple set of\nstandardized categories, which differs from traditional approaches that\ntypically use triples. The SKYSET System defines the categories required to\nform a standard template for representing information that is portable across\ndifferent domains. It provides a standardized framework that enables sentences\nfrom written instructions to be translated into sets of category typed entities\non a table or database. The SKYSET entities contain conceptual units or phrases\nthat represent information from the original source documentation. SKYSET\nenables information concatenation where multiple documents from different\ndomains can be translated and combined into a single common filterable and\nsearchable table of entities.\n",
        "published": "2015-11-06T15:36:54Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02117v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.02301v4",
        "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory\n  Representations",
        "summary": "  We introduce a new test of how well language models capture meaning in\nchildren's books. Unlike standard language modelling benchmarks, it\ndistinguishes the task of predicting syntactic function words from that of\npredicting lower-frequency words, which carry greater semantic content. We\ncompare a range of state-of-the-art models, each with a different way of\nencoding what has been previously read. We show that models which store\nexplicit representations of long-term contexts outperform state-of-the-art\nneural language models at predicting semantic content words, although this\nadvantage is not observed for syntactic function words. Interestingly, we find\nthat the amount of text encoded in a single memory representation is highly\ninfluential to the performance: there is a sweet-spot, not too big and not too\nsmall, between single words and full sentences that allows the most meaningful\ninformation in a text to be effectively retained and recalled. Further, the\nattention over such window-based memories can be trained effectively through\nself-supervision. We then assess the generality of this principle by applying\nit to the CNN QA benchmark, which involves identifying named entities in\nparaphrased summaries of news articles, and achieve state-of-the-art\nperformance.\n",
        "published": "2015-11-07T04:36:20Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02301v4"
    },
    {
        "id": "http://arxiv.org/abs/1511.02435v1",
        "title": "A Chinese POS Decision Method Using Korean Translation Information",
        "summary": "  In this paper we propose a method that imitates a translation expert using\nthe Korean translation information and analyse the performance. Korean is good\nat tagging than Chinese, so we can use this property in Chinese POS tagging.\n",
        "published": "2015-11-08T03:44:26Z",
        "pdf_link": "http://arxiv.org/pdf/1511.02435v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.03053v1",
        "title": "Investigating the stylistic relevance of adjective and verb simile\n  markers",
        "summary": "  Similes play an important role in literary texts not only as rhetorical\ndevices and as figures of speech but also because of their evocative power,\ntheir aptness for description and the relative ease with which they can be\ncombined with other figures of speech (Israel et al. 2004). Detecting all types\nof simile constructions in a particular text therefore seems crucial when\nanalysing the style of an author. Few research studies however have been\ndedicated to the study of less prominent simile markers in fictional prose and\ntheir relevance for stylistic studies. The present paper studies the frequency\nof adjective and verb simile markers in a corpus of British and French novels\nin order to determine which ones are really informative and worth including in\na stylistic analysis. Furthermore, are those adjectives and verb simile markers\nused differently in both languages?\n",
        "published": "2015-11-10T10:33:47Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03053v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.03088v1",
        "title": "USFD: Twitter NER with Drift Compensation and Linked Data",
        "summary": "  This paper describes a pilot NER system for Twitter, comprising the USFD\nsystem entry to the W-NUT 2015 NER shared task. The goal is to correctly label\nentities in a tweet dataset, using an inventory of ten types. We employ\nstructured learning, drawing on gazetteers taken from Linked Data, and on\nunsupervised clustering features, and attempting to compensate for stylistic\nand topic drift - a key challenge in social media text. Our result is\ncompetitive; we provide an analysis of the components of our methodology, and\nan examination of the target dataset in the context of this task.\n",
        "published": "2015-11-10T12:34:47Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03088v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.03729v2",
        "title": "Larger-Context Language Modelling",
        "summary": "  In this work, we propose a novel method to incorporate corpus-level discourse\ninformation into language modelling. We call this larger-context language\nmodel. We introduce a late fusion approach to a recurrent language model based\non long short-term memory units (LSTM), which helps the LSTM unit keep\nintra-sentence dependencies and inter-sentence dependencies separate from each\nother. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),\nwe demon- strate that the proposed model improves perplexity significantly. In\nthe experi- ments, we evaluate the proposed approach while varying the number\nof context sentences and observe that the proposed late fusion is superior to\nthe usual way of incorporating additional inputs to the LSTM. By analyzing the\ntrained larger- context language model, we discover that content words,\nincluding nouns, adjec- tives and verbs, benefit most from an increasing number\nof context sentences. This analysis suggests that larger-context language model\nimproves the unconditional language model by capturing the theme of a document\nbetter and more easily.\n",
        "published": "2015-11-11T23:24:29Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03729v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.03924v1",
        "title": "A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural\n  Language",
        "summary": "  Berkeley FrameNet is a lexico-semantic resource for English based on the\ntheory of frame semantics. It has been exploited in a range of natural language\nprocessing applications and has inspired the development of framenets for many\nlanguages. We present a methodological approach to the extraction and\ngeneration of a computational multilingual FrameNet-based grammar and lexicon.\nThe approach leverages FrameNet-annotated corpora to automatically extract a\nset of cross-lingual semantico-syntactic valence patterns. Based on data from\nBerkeley FrameNet and Swedish FrameNet, the proposed approach has been\nimplemented in Grammatical Framework (GF), a categorial grammar formalism\nspecialized for multilingual grammars. The implementation of the grammar and\nlexicon is supported by the design of FrameNet, providing a frame semantic\nabstraction layer, an interlingual semantic API (application programming\ninterface), over the interlingual syntactic API already provided by GF Resource\nGrammar Library. The evaluation of the acquired grammar and lexicon shows the\nfeasibility of the approach. Additionally, we illustrate how the FrameNet-based\ngrammar and lexicon are exploited in two distinct multilingual controlled\nnatural language applications. The produced resources are available under an\nopen source license.\n",
        "published": "2015-11-12T15:23:37Z",
        "pdf_link": "http://arxiv.org/pdf/1511.03924v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.04586v1",
        "title": "Character-based Neural Machine Translation",
        "summary": "  We introduce a neural machine translation model that views the input and\noutput sentences as sequences of characters rather than words. Since word-level\ninformation provides a crucial source of bias, our input model composes\nrepresentations of character sequences into representations of words (as\ndetermined by whitespace boundaries), and then these are translated using a\njoint attention/translation model. In the target language, the translation is\nmodeled as a sequence of word vectors, but each word is generated one character\nat a time, conditional on the previous character generations in each word. As\nthe representation and generation of words is performed at the character level,\nour model is capable of interpreting and generating unseen word forms. A\nsecondary benefit of this approach is that it alleviates much of the challenges\nassociated with preprocessing/tokenization of the source and target languages.\nWe show that our model can achieve translation results that are on par with\nconventional word-based models.\n",
        "published": "2015-11-14T17:36:43Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04586v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.04623v2",
        "title": "Learning to Represent Words in Context with Multilingual Supervision",
        "summary": "  We present a neural network architecture based on bidirectional LSTMs to\ncompute representations of words in the sentential contexts. These\ncontext-sensitive word representations are suitable for, e.g., distinguishing\ndifferent word senses and other context-modulated variations in meaning. To\nlearn the parameters of our model, we use cross-lingual supervision,\nhypothesizing that a good representation of a word in context will be one that\nis sufficient for selecting the correct translation into a second language. We\nevaluate the quality of our representations as features in three downstream\ntasks: prediction of semantic supersenses (which assign nouns and verbs into a\nfew dozen semantic classes), low resource machine translation, and a lexical\nsubstitution task, and obtain state-of-the-art results on all of these.\n",
        "published": "2015-11-14T21:36:38Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04623v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.04661v1",
        "title": "A System for Extracting Sentiment from Large-Scale Arabic Social Data",
        "summary": "  Social media data in Arabic language is becoming more and more abundant. It\nis a consensus that valuable information lies in social media data. Mining this\ndata and making the process easier are gaining momentum in the industries. This\npaper describes an enterprise system we developed for extracting sentiment from\nlarge volumes of social data in Arabic dialects. First, we give an overview of\nthe Big Data system for information extraction from multilingual social data\nfrom a variety of sources. Then, we focus on the Arabic sentiment analysis\ncapability that was built on top of the system including normalizing written\nArabic dialects, building sentiment lexicons, sentiment classification, and\nperformance evaluation. Lastly, we demonstrate the value of enriching sentiment\nresults with user profiles in understanding sentiments of a specific user\ngroup.\n",
        "published": "2015-11-15T05:53:13Z",
        "pdf_link": "http://arxiv.org/pdf/1511.04661v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.05076v1",
        "title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media\n  Archives for Deep Neural Network Adaptation",
        "summary": "  This paper presents a new method for the discovery of latent domains in\ndiverse speech data, for the use of adaptation of Deep Neural Networks (DNNs)\nfor Automatic Speech Recognition. Our work focuses on transcription of\nmulti-genre broadcast media, which is often only categorised broadly in terms\nof high level genres such as sports, news, documentary, etc. However, in terms\nof acoustic modelling these categories are coarse. Instead, it is expected that\na mixture of latent domains can better represent the complex and diverse\nbehaviours within a TV show, and therefore lead to better and more robust\nperformance. We propose a new method, whereby these latent domains are\ndiscovered with Latent Dirichlet Allocation, in an unsupervised manner. These\nare used to adapt DNNs using the Unique Binary Code (UBIC) representation for\nthe LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more\nthan 2,000 shows for training and 47 shows for testing, show that the use of\nLDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline\nhybrid DNN models.\n",
        "published": "2015-11-16T18:25:33Z",
        "pdf_link": "http://arxiv.org/pdf/1511.05076v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.05389v4",
        "title": "Learning to retrieve out-of-vocabulary words in speech recognition",
        "summary": "  Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech\nrecognition systems used to process diachronic audio data. To help recovery of\nthe PNs missed by the system, relevant OOV PNs can be retrieved out of the many\nOOVs by exploiting semantic context of the spoken content. In this paper, we\npropose two neural network models targeted to retrieve OOV PNs relevant to an\naudio document: (a) Document level Continuous Bag of Words (D-CBOW), (b)\nDocument level Continuous Bag of Weighted Words (D-CBOW2). Both these models\ntake document words as input and learn with an objective to maximise the\nretrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new\napproach in which the input embedding layer is augmented with a context anchor\nlayer. This layer learns to assign importance to input words and has the\nability to capture (task specific) key-words in a bag-of-word neural network\nmodel. With experiments on French broadcast news videos we show that these two\nmodels outperform the baseline methods based on raw embeddings from LDA,\nSkip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives\nfaster convergence during training.\n",
        "published": "2015-11-17T13:18:07Z",
        "pdf_link": "http://arxiv.org/pdf/1511.05389v4"
    },
    {
        "id": "http://arxiv.org/abs/1511.06246v1",
        "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes",
        "summary": "  Recently, word representation has been increasingly focused on for its\nexcellent properties in representing the word semantics. Previous works mainly\nsuffer from the problem of polysemy phenomenon. To address this problem, most\nof previous models represent words as multiple distributed vectors. However, it\ncannot reflect the rich relations between words by representing words as points\nin the embedded space. In this paper, we propose the Gaussian mixture skip-gram\n(GMSG) model to learn the Gaussian mixture embeddings for words based on\nskip-gram framework. Each word can be regarded as a gaussian mixture\ndistribution in the embedded space, and each gaussian component represents a\nword sense. Since the number of senses varies from word to word, we further\npropose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense\nnumber of words during training. Experiments on four benchmarks show the\neffectiveness of our proposed model.\n",
        "published": "2015-11-19T16:46:49Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06246v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.06312v1",
        "title": "Good, Better, Best: Choosing Word Embedding Context",
        "summary": "  We propose two methods of learning vector representations of words and\nphrases that each combine sentence context with structural features extracted\nfrom dependency trees. Using several variations of neural network classifier,\nwe show that these combined methods lead to improved performance when used as\ninput features for supervised term-matching.\n",
        "published": "2015-11-19T19:13:58Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06312v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.06426v4",
        "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering",
        "summary": "  Question answering tasks have shown remarkable progress with distributed\nvector representation. In this paper, we investigate the recently proposed\nFacebook bAbI tasks which consist of twenty different categories of questions\nthat require complex reasoning. Because the previous work on bAbI are all\nend-to-end models, errors could come from either an imperfect understanding of\nsemantics or in certain steps of the reasoning. For clearer analysis, we\npropose two vector space models inspired by Tensor Product Representation (TPR)\nto perform knowledge encoding and logical reasoning based on common-sense\ninference. They together achieve near-perfect accuracy on all categories\nincluding positional reasoning and path finding that have proved difficult for\nmost of the previous approaches. We hypothesize that the difficulties in these\ncategories are due to the multi-relations in contrast to uni-relational\ncharacteristic of other categories. Our exploration sheds light on designing\nmore sophisticated dataset and moving one step toward integrating transparent\nand interpretable formalism of TPR into existing learning paradigms.\n",
        "published": "2015-11-19T22:30:10Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06426v4"
    },
    {
        "id": "http://arxiv.org/abs/1511.06591v1",
        "title": "Polysemy in Controlled Natural Language Texts",
        "summary": "  Computational semantics and logic-based controlled natural languages (CNL) do\nnot address systematically the word sense disambiguation problem of content\nwords, i.e., they tend to interpret only some functional words that are crucial\nfor construction of discourse representation structures. We show that\nmicro-ontologies and multi-word units allow integration of the rich and\npolysemous multi-domain background knowledge into CNL thus providing\ninterpretation for the content words. The proposed approach is demonstrated by\nextending the Attempto Controlled English (ACE) with polysemous and procedural\nconstructs resulting in a more natural CNL named PAO covering narrative\nmulti-domain texts.\n",
        "published": "2015-11-20T13:41:31Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06591v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.06709v4",
        "title": "Improving Neural Machine Translation Models with Monolingual Data",
        "summary": "  Neural Machine Translation (NMT) has obtained state-of-the art performance\nfor several language pairs, while only using parallel data for training.\nTarget-side monolingual data plays an important role in boosting fluency for\nphrase-based statistical machine translation, and we investigate the use of\nmonolingual data for NMT. In contrast to previous work, which combines NMT\nmodels with separately trained language models, we note that encoder-decoder\nNMT architectures already have the capacity to learn the same information as a\nlanguage model, and we explore strategies to train with monolingual data\nwithout changing the neural network architecture. By pairing monolingual\ntraining data with an automatic back-translation, we can treat it as additional\nparallel training data, and we obtain substantial improvements on the WMT 15\ntask English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task\nTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We\nalso show that fine-tuning on in-domain monolingual and parallel data gives\nsubstantial improvements for the IWSLT 15 task English->German.\n",
        "published": "2015-11-20T17:58:37Z",
        "pdf_link": "http://arxiv.org/pdf/1511.06709v4"
    },
    {
        "id": "http://arxiv.org/abs/1511.07788v1",
        "title": "Spoken Language Translation for Polish",
        "summary": "  Spoken language translation (SLT) is becoming more important in the\nincreasingly globalized world, both from a social and economic point of view.\nIt is one of the major challenges for automatic speech recognition (ASR) and\nmachine translation (MT), driving intense research activities in these areas.\nWhile past research in SLT, due to technology limitations, dealt mostly with\nspeech recorded under controlled conditions, today's major challenge is the\ntranslation of spoken language as it can be found in real life. Considered\napplication scenarios range from portable translators for tourists, lectures\nand presentations translation, to broadcast news and shows with live\ncaptioning. We would like to present PJIIT's experiences in the SLT gained from\nthe Eu-Bridge 7th framework project and the U-Star consortium activities for\nthe Polish/English language pair. Presented research concentrates on ASR\nadaptation for Polish (state-of-the-art acoustic models: DBN-BLSTM training,\nKaldi: LDA+MLLT+SAT+MMI), language modeling for ASR & MT (text normalization,\nRNN-based LMs, n-gram model domain interpolation) and statistical translation\ntechniques (hierarchical models, factored translation models, automatic casing\nand punctuation, comparable and bilingual corpora preparation). While results\nfor the well-defined domains (phrases for travelers, parliament speeches,\nmedical documentation, movie subtitling) are very encouraging, less defined\ndomains (presentation, lectures) still form a challenge. Our progress in the\nIWSLT TED task (MT only) will be presented, as well as current progress in the\nPolish ASR.\n",
        "published": "2015-11-24T16:28:16Z",
        "pdf_link": "http://arxiv.org/pdf/1511.07788v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.08411v1",
        "title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological\n  Similarity",
        "summary": "  Text segmentation (TS) aims at dividing long text into coherent segments\nwhich reflect the subtopic structure of the text. It is beneficial to many\nnatural language processing tasks, such as Information Retrieval (IR) and\ndocument summarisation. Current approaches to text segmentation are similar in\nthat they all use word-frequency metrics to measure the similarity between two\nregions of text, so that a document is segmented based on the lexical cohesion\nbetween its words. Various NLP tasks are now moving towards the semantic web\nand ontologies, such as ontology-based IR systems, to capture the\nconceptualizations associated with user needs and contents. Text segmentation\nbased on lexical cohesion between words is hence not sufficient anymore for\nsuch tasks. This paper proposes OntoSeg, a novel approach to text segmentation\nbased on the ontological similarity between text blocks. The proposed method\nuses ontological similarity to explore conceptual relations between text\nsegments and a Hierarchical Agglomerative Clustering (HAC) algorithm to\nrepresent the text as a tree-like hierarchy that is conceptually structured.\nThe rich structure of the created tree further allows the segmentation of text\nin a linear fashion at various levels of granularity. The proposed method was\nevaluated on a wellknown dataset, and the results show that using ontological\nsimilarity in text segmentation is very promising. Also we enhance the proposed\nmethod by combining ontological similarity with lexical similarity and the\nresults show an enhancement of the segmentation quality.\n",
        "published": "2015-11-26T15:10:18Z",
        "pdf_link": "http://arxiv.org/pdf/1511.08411v1"
    },
    {
        "id": "http://arxiv.org/abs/1511.08629v2",
        "title": "Category Enhanced Word Embedding",
        "summary": "  Distributed word representations have been demonstrated to be effective in\ncapturing semantic and syntactic regularities. Unsupervised representation\nlearning from large unlabeled corpora can learn similar representations for\nthose words that present similar co-occurrence statistics. Besides local\noccurrence statistics, global topical information is also important knowledge\nthat may help discriminate a word from another. In this paper, we incorporate\ncategory information of documents in the learning of word representations and\nto learn the proposed models in a document-wise manner. Our models outperform\nseveral state-of-the-art models in word analogy and word similarity tasks.\nMoreover, we evaluate the learned word vectors on sentiment analysis and text\nclassification tasks, which shows the superiority of our learned word vectors.\nWe also learn high-quality category embeddings that reflect topical meanings.\n",
        "published": "2015-11-27T11:38:57Z",
        "pdf_link": "http://arxiv.org/pdf/1511.08629v2"
    },
    {
        "id": "http://arxiv.org/abs/1511.08630v2",
        "title": "A C-LSTM Neural Network for Text Classification",
        "summary": "  Neural network models have been demonstrated to be capable of achieving\nremarkable performance in sentence and document modeling. Convolutional neural\nnetwork (CNN) and recurrent neural network (RNN) are two mainstream\narchitectures for such modeling tasks, which adopt totally different ways of\nunderstanding natural languages. In this work, we combine the strengths of both\narchitectures and propose a novel and unified model called C-LSTM for sentence\nrepresentation and text classification. C-LSTM utilizes CNN to extract a\nsequence of higher-level phrase representations, and are fed into a long\nshort-term memory recurrent neural network (LSTM) to obtain the sentence\nrepresentation. C-LSTM is able to capture both local features of phrases as\nwell as global and temporal sentence semantics. We evaluate the proposed\narchitecture on sentiment classification and question classification tasks. The\nexperimental results show that the C-LSTM outperforms both CNN and LSTM and can\nachieve excellent performance on these tasks.\n",
        "published": "2015-11-27T11:44:17Z",
        "pdf_link": "http://arxiv.org/pdf/1511.08630v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.00103v2",
        "title": "Multilingual Language Processing From Bytes",
        "summary": "  We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads\ntext as bytes and outputs span annotations of the form [start, length, label]\nwhere start positions, lengths, and labels are separate entries in our\nvocabulary. Because we operate directly on unicode bytes rather than\nlanguage-specific words or characters, we can analyze text in many languages\nwith a single model. Due to the small vocabulary size, these multilingual\nmodels are very compact, but produce results similar to or better than the\nstate-of- the-art in Part-of-Speech tagging and Named Entity Recognition that\nuse only the provided training datasets (no external data sources). Our models\nare learning \"from scratch\" in that they do not rely on any elements of the\nstandard pipeline in Natural Language Processing (including tokenization), and\nthus can run in standalone fashion on raw text.\n",
        "published": "2015-12-01T00:23:44Z",
        "pdf_link": "http://arxiv.org/pdf/1512.00103v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.00170v1",
        "title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT",
        "summary": "  Pivot language is employed as a way to solve the data sparseness problem in\nmachine translation, especially when the data for a particular language pair\ndoes not exist. The combination of source-to-pivot and pivot-to-target\ntranslation models can induce a new translation model through the pivot\nlanguage. However, the errors in two models may compound as noise, and still,\nthe combined model may suffer from a serious phrase sparsity problem. In this\npaper, we directly employ the word lexical model in IBM models as an additional\nresource to augment pivot phrase table. In addition, we also propose a phrase\ntable pruning method which takes into account both of the source and target\nphrasal coverage. Experimental result shows that our pruning method\nsignificantly outperforms the conventional one, which only considers source\nside phrasal coverage. Furthermore, by including the entries in the lexicon\nmodel, the phrase coverage increased, and we achieved improved results in\nChinese-to-Japanese translation using English as pivot language.\n",
        "published": "2015-12-01T08:10:49Z",
        "pdf_link": "http://arxiv.org/pdf/1512.00170v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.00531v5",
        "title": "Benchmarking sentiment analysis methods for large-scale texts: A case\n  for using continuum-scored words and word shift graphs",
        "summary": "  The emergence and global adoption of social media has rendered possible the\nreal-time estimation of population-scale sentiment, bearing profound\nimplications for our understanding of human behavior. Given the growing\nassortment of sentiment measuring instruments, comparisons between them are\nevidently required. Here, we perform detailed tests of 6 dictionary-based\nmethods applied to 4 different corpora, and briefly examine a further 20\nmethods. We show that a dictionary-based method will only perform both reliably\nand meaningfully if (1) the dictionary covers a sufficiently large enough\nportion of a given text's lexicon when weighted by word usage frequency; and\n(2) words are scored on a continuous scale.\n",
        "published": "2015-12-02T00:34:51Z",
        "pdf_link": "http://arxiv.org/pdf/1512.00531v5"
    },
    {
        "id": "http://arxiv.org/abs/1512.00728v1",
        "title": "Annotating Character Relationships in Literary Texts",
        "summary": "  We present a dataset of manually annotated relationships between characters\nin literary texts, in order to support the training and evaluation of automatic\nmethods for relation type prediction in this domain (Makazhanov et al., 2014;\nKokkinakis, 2013) and the broader computational analysis of literary character\n(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and\nGurevych, 2015). In this work, we solicit annotations from workers on Amazon\nMechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_\non four dimensions of interest: for a given pair of characters, we collect\njudgments as to the coarse-grained category (professional, social, familial),\nfine-grained category (friend, lover, parent, rival, employer), and affinity\n(positive, negative, neutral) that describes their primary relationship in a\ntext. We do not assume that this relationship is static; we also collect\njudgments as to whether it changes at any point in the course of the text.\n",
        "published": "2015-12-02T15:09:31Z",
        "pdf_link": "http://arxiv.org/pdf/1512.00728v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01100v2",
        "title": "Effective LSTMs for Target-Dependent Sentiment Classification",
        "summary": "  Target-dependent sentiment classification remains a challenge: modeling the\nsemantic relatedness of a target with its context words in a sentence.\nDifferent context words have different influences on determining the sentiment\npolarity of a sentence towards the target. Therefore, it is desirable to\nintegrate the connections between target word and context words when building a\nlearning system. In this paper, we develop two target dependent long short-term\nmemory (LSTM) models, where target information is automatically taken into\naccount. We evaluate our methods on a benchmark dataset from Twitter. Empirical\nresults show that modeling sentence representation with standard LSTM does not\nperform well. Incorporating target information into LSTM can significantly\nboost the classification accuracy. The target-dependent LSTM models achieve\nstate-of-the-art performances without using syntactic parser or external\nsentiment lexicons.\n",
        "published": "2015-12-03T14:54:39Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01100v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.01337v4",
        "title": "Neural Generative Question Answering",
        "summary": "  This paper presents an end-to-end neural network model, named Neural\nGenerative Question Answering (GENQA), that can generate answers to simple\nfactoid questions, based on the facts in a knowledge-base. More specifically,\nthe model is built on the encoder-decoder framework for sequence-to-sequence\nlearning, while equipped with the ability to enquire the knowledge-base, and is\ntrained on a corpus of question-answer pairs, with their associated triples in\nthe knowledge-base. Empirical study shows the proposed model can effectively\ndeal with the variations of questions and answers, and generate right and\nnatural answers by referring to the facts in the knowledge-base. The experiment\non question answering demonstrates that the proposed model can outperform an\nembedding-based QA model as well as a neural dialogue model trained on the same\ndata.\n",
        "published": "2015-12-04T08:31:02Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01337v4"
    },
    {
        "id": "http://arxiv.org/abs/1512.01409v1",
        "title": "What Makes it Difficult to Understand a Scientific Literature?",
        "summary": "  In the artificial intelligence area, one of the ultimate goals is to make\ncomputers understand human language and offer assistance. In order to achieve\nthis ideal, researchers of computer science have put forward a lot of models\nand algorithms attempting at enabling the machine to analyze and process human\nnatural language on different levels of semantics. Although recent progress in\nthis field offers much hope, we still have to ask whether current research can\nprovide assistance that people really desire in reading and comprehension. To\nthis end, we conducted a reading comprehension test on two scientific papers\nwhich are written in different styles. We use the semantic link models to\nanalyze the understanding obstacles that people will face in the process of\nreading and figure out what makes it difficult for human to understand a\nscientific literature. Through such analysis, we summarized some\ncharacteristics and problems which are reflected by people with different\nlevels of knowledge on the comprehension of difficult science and technology\nliterature, which can be modeled in semantic link network. We believe that\nthese characteristics and problems will help us re-examine the existing machine\nmodels and are helpful in the designing of new one.\n",
        "published": "2015-12-04T14:01:32Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01409v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.01768v1",
        "title": "Want Answers? A Reddit Inspired Study on How to Pose Questions",
        "summary": "  Questions form an integral part of our everyday communication, both offline\nand online. Getting responses to our questions from others is fundamental to\nsatisfying our information need and in extending our knowledge boundaries. A\nquestion may be represented using various factors such as social, syntactic,\nsemantic, etc. We hypothesize that these factors contribute with varying\ndegrees towards getting responses from others for a given question. We perform\na thorough empirical study to measure effects of these factors using a novel\nquestion and answer dataset from the website Reddit.com. To the best of our\nknowledge, this is the first such analysis of its kind on this important topic.\nWe also use a sparse nonnegative matrix factorization technique to\nautomatically induce interpretable semantic factors from the question dataset.\nWe also document various patterns on response prediction we observe during our\nanalysis in the data. For instance, we found that preference-probing questions\nare scantily answered. Our method is robust to capture such latent response\nfactors. We hope to make our code and datasets publicly available upon\npublication of the paper.\n",
        "published": "2015-12-06T10:31:12Z",
        "pdf_link": "http://arxiv.org/pdf/1512.01768v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.02433v3",
        "title": "Minimum Risk Training for Neural Machine Translation",
        "summary": "  We propose minimum risk training for end-to-end neural machine translation.\nUnlike conventional maximum likelihood estimation, minimum risk training is\ncapable of optimizing model parameters directly with respect to arbitrary\nevaluation metrics, which are not necessarily differentiable. Experiments show\nthat our approach achieves significant improvements over maximum likelihood\nestimation on a state-of-the-art neural machine translation system across\nvarious languages pairs. Transparent to architectures, our approach can be\napplied to more neural networks and potentially benefit more NLP tasks.\n",
        "published": "2015-12-08T12:42:00Z",
        "pdf_link": "http://arxiv.org/pdf/1512.02433v3"
    },
    {
        "id": "http://arxiv.org/abs/1512.02595v1",
        "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
        "summary": "  We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages.\nBecause it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages. Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale.\n",
        "published": "2015-12-08T19:13:50Z",
        "pdf_link": "http://arxiv.org/pdf/1512.02595v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.03465v3",
        "title": "Mined Semantic Analysis: A New Concept Space Model for Semantic\n  Representation of Textual Data",
        "summary": "  Mined Semantic Analysis (MSA) is a novel concept space model which employs\nunsupervised learning to generate semantic representations of text. MSA\nrepresents textual structures (terms, phrases, documents) as a Bag of Concepts\n(BoC) where concepts are derived from concept rich encyclopedic corpora.\nTraditional concept space models exploit only target corpus content to\nconstruct the concept space. MSA, alternatively, uncovers implicit relations\nbetween concepts by mining for their associations (e.g., mining Wikipedia's\n\"See also\" link graph). We evaluate MSA's performance on benchmark datasets for\nmeasuring semantic relatedness of words and sentences. Empirical results show\ncompetitive performance of MSA compared to prior state-of-the-art methods.\nAdditionally, we introduce the first analytical study to examine statistical\nsignificance of results reported by different semantic relatedness methods. Our\nstudy shows that, the nuances of results across top performing methods could be\nstatistically insignificant. The study positions MSA as one of state-of-the-art\nmethods for measuring semantic relatedness, besides the inherent\ninterpretability and simplicity of the generated semantic representation.\n",
        "published": "2015-12-10T22:15:10Z",
        "pdf_link": "http://arxiv.org/pdf/1512.03465v3"
    },
    {
        "id": "http://arxiv.org/abs/1512.03950v1",
        "title": "A Hidden Markov Model Based System for Entity Extraction from Social\n  Media English Text at FIRE 2015",
        "summary": "  This paper presents the experiments carried out by us at Jadavpur University\nas part of the participation in FIRE 2015 task: Entity Extraction from Social\nMedia Text - Indian Languages (ESM-IL). The tool that we have developed for the\ntask is based on Trigram Hidden Markov Model that utilizes information like\ngazetteer list, POS tag and some other word level features to enhance the\nobservation probabilities of the known tokens as well as unknown tokens. We\nsubmitted runs for English only. A statistical HMM (Hidden Markov Models) based\nmodel has been used to implement our system. The system has been trained and\ntested on the datasets released for FIRE 2015 task: Entity Extraction from\nSocial Media Text - Indian Languages (ESM-IL). Our system is the best performer\nfor English language and it obtains precision, recall and F-measures of 61.96,\n39.46 and 48.21 respectively.\n",
        "published": "2015-12-12T18:57:11Z",
        "pdf_link": "http://arxiv.org/pdf/1512.03950v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.04650v2",
        "title": "Agreement-based Joint Training for Bidirectional Attention-based Neural\n  Machine Translation",
        "summary": "  The attentional mechanism has proven to be effective in improving end-to-end\nneural machine translation. However, due to the intricate structural divergence\nbetween natural languages, unidirectional attention-based models might only\ncapture partial aspects of attentional regularities. We propose agreement-based\njoint training for bidirectional attention-based end-to-end neural machine\ntranslation. Instead of training source-to-target and target-to-source\ntranslation models independently,our approach encourages the two complementary\nmodels to agree on word alignment matrices on the same training data.\nExperiments on Chinese-English and English-French translation tasks show that\nagreement-based joint training significantly improves both alignment and\ntranslation quality over independent training.\n",
        "published": "2015-12-15T04:55:06Z",
        "pdf_link": "http://arxiv.org/pdf/1512.04650v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.05030v3",
        "title": "Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised\n  Learning",
        "summary": "  Morpho-syntactic lexicons provide information about the morphological and\nsyntactic roles of words in a language. Such lexicons are not available for all\nlanguages and even when available, their coverage can be limited. We present a\ngraph-based semi-supervised learning method that uses the morphological,\nsyntactic and semantic relations between words to automatically construct wide\ncoverage lexicons from small seed sets. Our method is language-independent, and\nwe show that we can expand a 1000 word seed lexicon to more than 100 times its\nsize with high quality for 11 languages. In addition, the automatically created\nlexicons provide features that improve performance in two downstream tasks:\nmorphological tagging and dependency parsing.\n",
        "published": "2015-12-16T02:27:14Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05030v3"
    },
    {
        "id": "http://arxiv.org/abs/1512.05193v4",
        "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling\n  Sentence Pairs",
        "summary": "  How to model a pair of sentences is a critical issue in many NLP tasks such\nas answer selection (AS), paraphrase identification (PI) and textual entailment\n(TE). Most prior work (i) deals with one individual task by fine-tuning a\nspecific system; (ii) models each sentence's representation separately, rarely\nconsidering the impact of the other sentence; or (iii) relies fully on manually\ndesigned, task-specific linguistic features. This work presents a general\nAttention Based Convolutional Neural Network (ABCNN) for modeling a pair of\nsentences. We make three contributions. (i) ABCNN can be applied to a wide\nvariety of tasks that require modeling of sentence pairs. (ii) We propose three\nattention schemes that integrate mutual influence between sentences into CNN;\nthus, the representation of each sentence takes into consideration its\ncounterpart. These interdependent sentence pair representations are more\npowerful than isolated sentence representations. (iii) ABCNN achieves\nstate-of-the-art performance on AS, PI and TE tasks.\n",
        "published": "2015-12-16T14:55:17Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05193v4"
    },
    {
        "id": "http://arxiv.org/abs/1512.05670v2",
        "title": "Towards automating the generation of derivative nouns in Sanskrit by\n  simulating Panini",
        "summary": "  About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with\ngeneration of derivative nouns, making it one of the largest topical sections\nin Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76.\nThis section is a systematic arrangement of rules that enumerates various\naffixes that are used in the derivation under specific semantic relations. We\npropose a system that automates the process of generation of derivative nouns\nas per the rules in Astadhyayi. The proposed system follows a completely object\noriented approach, that models each rule as a class of its own and then groups\nthem as rule groups. The rule groups are decided on the basis of selective\ngrouping of rules by virtue of anuvrtti. The grouping of rules results in an\ninheritance network of rules which is a directed acyclic graph. Every rule\ngroup has a head rule and the head rule notifies all the direct member rules of\nthe group about the environment which contains all the details about data\nentities, participating in the derivation process. The system implements this\nmechanism using multilevel inheritance and observer design patterns. The system\nfocuses not only on generation of the desired final form, but also on the\ncorrectness of sequence of rules applied to make sure that the derivation has\ntaken place in strict adherence to Astadhyayi. The proposed system's design\nallows to incorporate various conflict resolution methods mentioned in\nauthentic texts and hence the effectiveness of those rules can be validated\nwith the results from the system. We also present cases where we have checked\nthe applicability of the system with the rules which are not specifically\napplicable to derivation of derivative nouns, in order to see the effectiveness\nof the proposed schema as a generic system for modeling Astadhyayi.\n",
        "published": "2015-12-17T16:55:57Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05670v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.05919v2",
        "title": "A Planning based Framework for Essay Generation",
        "summary": "  Generating an article automatically with computer program is a challenging\ntask in artificial intelligence and natural language processing. In this paper,\nwe target at essay generation, which takes as input a topic word in mind and\ngenerates an organized article under the theme of the topic. We follow the idea\nof text planning \\cite{Reiter1997} and develop an essay generation framework.\nThe framework consists of three components, including topic understanding,\nsentence extraction and sentence reordering. For each component, we studied\nseveral statistical algorithms and empirically compared between them in terms\nof qualitative or quantitative analysis. Although we run experiments on Chinese\ncorpus, the method is language independent and can be easily adapted to other\nlanguage. We lay out the remaining challenges and suggest avenues for future\nresearch.\n",
        "published": "2015-12-18T12:10:42Z",
        "pdf_link": "http://arxiv.org/pdf/1512.05919v2"
    },
    {
        "id": "http://arxiv.org/abs/1512.06110v3",
        "title": "Morphological Inflection Generation Using Character Sequence to Sequence\n  Learning",
        "summary": "  Morphological inflection generation is the task of generating the inflected\nform of a given lemma corresponding to a particular linguistic transformation.\nWe model the problem of inflection generation as a character sequence to\nsequence learning problem and present a variant of the neural encoder-decoder\nmodel for solving it. Our model is language independent and can be trained in\nboth supervised and semi-supervised settings. We evaluate our system on seven\ndatasets of morphologically rich languages and achieve either better or\ncomparable results to existing state-of-the-art models of inflection\ngeneration.\n",
        "published": "2015-12-18T20:48:26Z",
        "pdf_link": "http://arxiv.org/pdf/1512.06110v3"
    },
    {
        "id": "http://arxiv.org/abs/1512.06643v1",
        "title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast\n  Media",
        "summary": "  We describe the University of Sheffield system for participation in the 2015\nMulti-Genre Broadcast (MGB) challenge task of transcribing multi-genre\nbroadcast shows. Transcription was one of four tasks proposed in the MGB\nchallenge, with the aim of advancing the state of the art of automatic speech\nrecognition, speaker diarisation and automatic alignment of subtitles for\nbroadcast media. Four topics are investigated in this work: Data selection\ntechniques for training with unreliable data, automatic speech segmentation of\nbroadcast media shows, acoustic modelling and adaptation in highly variable\nenvironments, and language modelling of multi-genre shows. The final system\noperates in multiple passes, using an initial unadapted decoding stage to\nrefine segmentation, followed by three adapted passes: a hybrid DNN pass with\ninput features normalised by speaker-based cepstral normalisation, another\nhybrid stage with input features normalised by speaker feature-MLLR\ntransformations, and finally a bottleneck-based tandem stage with noise and\nspeaker factorisation. The combination of these three system outputs provides a\nfinal error rate of 27.5% on the official development set, consisting of 47\nmulti-genre shows.\n",
        "published": "2015-12-21T14:31:31Z",
        "pdf_link": "http://arxiv.org/pdf/1512.06643v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.08066v1",
        "title": "The Improvement of Negative Sentences Translation in English-to-Korean\n  Machine Translation",
        "summary": "  This paper describes the algorithm for translating English negative sentences\ninto Korean in English-Korean Machine Translation (EKMT). The proposed\nalgorithm is based on the comparative study of English and Korean negative\nsentences. The earlier translation software cannot translate English negative\nsentences into accurate Korean equivalents. We established a new algorithm for\nthe negative sentence translation and evaluated it.\n",
        "published": "2015-12-26T01:52:03Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08066v1"
    },
    {
        "id": "http://arxiv.org/abs/1512.08183v5",
        "title": "Learning Document Embeddings by Predicting N-grams for Sentiment\n  Classification of Long Movie Reviews",
        "summary": "  Despite the loss of semantic information, bag-of-ngram based methods still\nachieve state-of-the-art results for tasks such as sentiment classification of\nlong movie reviews. Many document embeddings methods have been proposed to\ncapture semantics, but they still can't outperform bag-of-ngram based methods\non this task. In this paper, we modify the architecture of the recently\nproposed Paragraph Vector, allowing it to learn document vectors by predicting\nnot only words, but n-gram features as well. Our model is able to capture both\nsemantics and word order in documents while keeping the expressive power of\nlearned vectors. Experimental results on IMDB movie review dataset shows that\nour model outperforms previous deep learning models and bag-of-ngram based\nmodels due to the above advantages. More robust results are also obtained when\nour model is combined with other models. The source code of our model will be\nalso published together with this paper.\n",
        "published": "2015-12-27T08:12:53Z",
        "pdf_link": "http://arxiv.org/pdf/1512.08183v5"
    },
    {
        "id": "http://arxiv.org/abs/1601.00087v3",
        "title": "Sentiment/Subjectivity Analysis Survey for Languages other than English",
        "summary": "  Subjective and sentiment analysis have gained considerable attention\nrecently. Most of the resources and systems built so far are done for English.\nThe need for designing systems for other languages is increasing. This paper\nsurveys different ways used for building systems for subjective and sentiment\nanalysis for languages other than English. There are three different types of\nsystems used for building these systems. The first (and the best) one is the\nlanguage specific systems. The second type of systems involves reusing or\ntransferring sentiment resources from English to the target language. The third\ntype of methods is based on using language independent methods. The paper\npresents a separate section devoted to Arabic sentiment analysis.\n",
        "published": "2016-01-01T15:42:27Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00087v3"
    },
    {
        "id": "http://arxiv.org/abs/1601.00248v2",
        "title": "Contrastive Entropy: A new evaluation metric for unnormalized language\n  models",
        "summary": "  Perplexity (per word) is the most widely used metric for evaluating language\nmodels. Despite this, there has been no dearth of criticism for this metric.\nMost of these criticisms center around lack of correlation with extrinsic\nmetrics like word error rate (WER), dependence upon shared vocabulary for model\ncomparison and unsuitability for unnormalized language model evaluation. In\nthis paper, we address the last problem and propose a new discriminative\nentropy based intrinsic metric that works for both traditional word level\nmodels and unnormalized language models like sentence level models. We also\npropose a discriminatively trained sentence level interpretation of recurrent\nneural network based language model (RNN) as an example of unnormalized\nsentence level model. We demonstrate that for word level models, contrastive\nentropy shows a strong correlation with perplexity. We also observe that when\ntrained at lower distortion levels, sentence level RNN considerably outperforms\ntraditional RNNs on this new metric.\n",
        "published": "2016-01-03T05:47:42Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00248v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.00620v1",
        "title": "Distant IE by Bootstrapping Using Lists and Document Structure",
        "summary": "  Distant labeling for information extraction (IE) suffers from noisy training\ndata. We describe a way of reducing the noise associated with distant IE by\nidentifying coupling constraints between potential instance labels. As one\nexample of coupling, items in a list are likely to have the same label. A\nsecond example of coupling comes from analysis of document structure: in some\ncorpora, sections can be identified such that items in the same section are\nlikely to have the same label. Such sections do not exist in all corpora, but\nwe show that augmenting a large corpus with coupling constraints from even a\nsmall, well-structured corpus can improve performance substantially, doubling\nF1 on one task.\n",
        "published": "2016-01-04T19:46:00Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00620v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.00710v1",
        "title": "Multi-Source Neural Translation",
        "summary": "  We build a multi-source machine translation model and train it to maximize\nthe probability of a target English string given French and German sources.\nUsing the neural encoder-decoder framework, we explore several combination\nmethods and report up to +4.8 Bleu increases on top of a very strong\nattention-based neural translation model.\n",
        "published": "2016-01-05T00:49:22Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00710v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.00893v2",
        "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings",
        "summary": "  We provide the first extensive evaluation of how using different types of\ncontext to learn skip-gram word embeddings affects performance on a wide range\nof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic\ntasks tend to exhibit a clear preference to particular types of contexts and\nhigher dimensionality, more careful tuning is required for finding the optimal\nsettings for most of the extrinsic tasks that we considered. Furthermore, for\nthese extrinsic tasks, we find that once the benefit from increasing the\nembedding dimensionality is mostly exhausted, simple concatenation of word\nembeddings, learned with different context types, can yield further performance\ngains. As an additional contribution, we propose a new variant of the skip-gram\nmodel that learns word embeddings from weighted contexts of substitute words.\n",
        "published": "2016-01-05T16:28:42Z",
        "pdf_link": "http://arxiv.org/pdf/1601.00893v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.01085v1",
        "title": "Incorporating Structural Alignment Biases into an Attentional Neural\n  Translation Model",
        "summary": "  Neural encoder-decoder models of machine translation have achieved impressive\nresults, rivalling traditional translation models. However their modelling\nformulation is overly simplistic, and omits several key inductive biases built\ninto traditional models. In this paper we extend the attentional neural\ntranslation model to include structural biases from word based alignment\nmodels, including positional bias, Markov conditioning, fertility and agreement\nover translation directions. We show improvements over a baseline attentional\nmodel and standard phrase-based model over several language pairs, evaluating\non difficult languages in a low resource setting.\n",
        "published": "2016-01-06T06:03:17Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01085v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.01195v1",
        "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON\n  2015",
        "summary": "  This paper discusses the experiments carried out by us at Jadavpur University\nas part of the participation in ICON 2015 task: POS Tagging for Code-mixed\nIndian Social Media Text. The tool that we have developed for the task is based\non Trigram Hidden Markov Model that utilizes information from dictionary as\nwell as some other word level features to enhance the observation probabilities\nof the known tokens as well as unknown tokens. We submitted runs for\nBengali-English, Hindi-English and Tamil-English Language pairs. Our system has\nbeen trained and tested on the datasets released for ICON 2015 shared task: POS\nTagging For Code-mixed Indian Social Media Text. In constrained mode, our\nsystem obtains average overall accuracy (averaged over all three language\npairs) of 75.60% which is very close to other participating two systems (76.79%\nfor IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In\nunconstrained mode, our system obtains average overall accuracy of 70.65% which\nis also close to the system (72.85% for AMRITA_CEN) which obtains the highest\naverage overall accuracy.\n",
        "published": "2016-01-06T14:40:38Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01195v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.01272v2",
        "title": "Recurrent Memory Networks for Language Modeling",
        "summary": "  Recurrent Neural Networks (RNN) have obtained excellent result in many\nnatural language processing (NLP) tasks. However, understanding and\ninterpreting the source of this success remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN), a novel RNN architecture, that not only\namplifies the power of RNN but also facilitates our understanding of its\ninternal functioning and allows us to discover underlying patterns in data. We\ndemonstrate the power of RMN on language modeling and sentence completion\ntasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and English dataset. Additionally we\nperform in-depth analysis of various linguistic dimensions that RMN captures.\nOn Sentence Completion Challenge, for which it is essential to capture sentence\ncoherence, our RMN obtains 69.2% accuracy, surpassing the previous\nstate-of-the-art by a large margin.\n",
        "published": "2016-01-06T18:44:07Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01272v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.01280v2",
        "title": "Language to Logical Form with Neural Attention",
        "summary": "  Semantic parsing aims at mapping natural language to machine interpretable\nmeaning representations. Traditional approaches rely on high-quality lexicons,\nmanually-built templates, and linguistic features which are either domain- or\nrepresentation-specific. In this paper we present a general method based on an\nattention-enhanced encoder-decoder model. We encode input utterances into\nvector representations, and generate their logical forms by conditioning the\noutput sequences or trees on the encoding vectors. Experimental results on four\ndatasets show that our approach performs competitively without using\nhand-engineered features and is easy to adapt across domains and meaning\nrepresentations.\n",
        "published": "2016-01-06T19:13:12Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01280v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.01343v4",
        "title": "Joint Learning of the Embedding of Words and Entities for Named Entity\n  Disambiguation",
        "summary": "  Named Entity Disambiguation (NED) refers to the task of resolving multiple\nnamed entity mentions in a document to their correct references in a knowledge\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\nspecifically designed for NED. The proposed method jointly maps words and\nentities into the same continuous vector space. We extend the skip-gram model\nby using two models. The KB graph model learns the relatedness of entities\nusing the link structure of the KB, whereas the anchor context model aims to\nalign vectors such that similar words and entities occur close to one another\nin the vector space by leveraging KB anchors and their context words. By\ncombining contexts based on the proposed embedding with standard NED features,\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\nand 85.2% on the TAC 2010 dataset.\n",
        "published": "2016-01-06T22:19:20Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01343v4"
    },
    {
        "id": "http://arxiv.org/abs/1601.01530v4",
        "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic\n  Slot Filling",
        "summary": "  Recurrent Neural Network (RNN) and one of its specific architectures, Long\nShort-Term Memory (LSTM), have been widely used for sequence labeling. In this\npaper, we first enhance LSTM-based sequence labeling to explicitly model label\ndependencies. Then we propose another enhancement to incorporate the global\ninformation spanning over the whole input sequence. The latter proposed method,\nencoder-labeler LSTM, first encodes the whole input sequence into a fixed\nlength vector with the encoder LSTM, and then uses this encoded vector as the\ninitial state of another LSTM for sequence labeling. Combining these methods,\nwe can predict the label sequence with considering label dependencies and\ninformation of whole input sequence. In the experiments of a slot filling task,\nwhich is an essential component of natural language understanding, with using\nthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.\n",
        "published": "2016-01-07T13:32:31Z",
        "pdf_link": "http://arxiv.org/pdf/1601.01530v4"
    },
    {
        "id": "http://arxiv.org/abs/1601.02166v1",
        "title": "Empirical Gaussian priors for cross-lingual transfer learning",
        "summary": "  Sequence model learning algorithms typically maximize log-likelihood minus\nthe norm of the model (or minimize Hamming loss + norm). In cross-lingual\npart-of-speech (POS) tagging, our target language training data consists of\nsequences of sentences with word-by-word labels projected from translations in\n$k$ languages for which we have labeled data, via word alignments. Our training\ndata is therefore very noisy, and if Rademacher complexity is high, learning\nalgorithms are prone to overfit. Norm-based regularization assumes a constant\nwidth and zero mean prior. We instead propose to use the $k$ source language\nmodels to estimate the parameters of a Gaussian prior for learning new POS\ntaggers. This leads to significantly better performance in multi-source\ntransfer set-ups. We also present a drop-out version that injects (empirical)\nGaussian noise during online learning. Finally, we note that using empirical\nGaussian priors leads to much lower Rademacher complexity, and is superior to\noptimally weighted model interpolation.\n",
        "published": "2016-01-09T23:34:05Z",
        "pdf_link": "http://arxiv.org/pdf/1601.02166v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.02403v5",
        "title": "Argumentation Mining in User-Generated Web Discourse",
        "summary": "  The goal of argumentation mining, an evolving research field in computational\nlinguistics, is to design methods capable of analyzing people's argumentation.\nIn this article, we go beyond the state of the art in several ways. (i) We deal\nwith actual Web data and take up the challenges given by the variety of\nregisters, multiple domains, and unrestricted noisy user-generated Web\ndiscourse. (ii) We bridge the gap between normative argumentation theories and\nargumentation phenomena encountered in actual data by adapting an argumentation\nmodel tested in an extensive annotation study. (iii) We create a new gold\nstandard corpus (90k tokens in 340 documents) and experiment with several\nmachine learning methods to identify argument components. We offer the data,\nsource codes, and annotation guidelines to the community under free licenses.\nOur findings show that argumentation mining in user-generated Web discourse is\na feasible but challenging task.\n",
        "published": "2016-01-11T11:28:49Z",
        "pdf_link": "http://arxiv.org/pdf/1601.02403v5"
    },
    {
        "id": "http://arxiv.org/abs/1601.02431v1",
        "title": "The Effects of Age, Gender and Region on Non-standard Linguistic\n  Variation in Online Social Networks",
        "summary": "  We present a corpus-based analysis of the effects of age, gender and region\nof origin on the production of both \"netspeak\" or \"chatspeak\" features and\nregional speech features in Flemish Dutch posts that were collected from a\nBelgian online social network platform. The present study shows that combining\nquantitative and qualitative approaches is essential for understanding\nnon-standard linguistic variation in a CMC corpus. It also presents a\nmethodology that enables the systematic study of this variation by including\nall non-standard words in the corpus. The analyses resulted in a convincing\nillustration of the Adolescent Peak Principle. In addition, our approach\nrevealed an intriguing correlation between the use of regional speech features\nand chatspeak features.\n",
        "published": "2016-01-11T13:02:59Z",
        "pdf_link": "http://arxiv.org/pdf/1601.02431v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.02502v1",
        "title": "Trans-gram, Fast Cross-lingual Word-embeddings",
        "summary": "  We introduce Trans-gram, a simple and computationally-efficient method to\nsimultaneously learn and align wordembeddings for a variety of languages, using\nonly monolingual data and a smaller set of sentence-aligned data. We use our\nnew method to compute aligned wordembeddings for twenty-one languages using\nEnglish as a pivot language. We show that some linguistic features are aligned\nacross languages for which we do not have aligned data, even though those\nproperties do not exist in the pivot language. We also achieve state of the art\nresults on standard cross-lingual text classification and word translation\ntasks.\n",
        "published": "2016-01-11T16:12:32Z",
        "pdf_link": "http://arxiv.org/pdf/1601.02502v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.02553v2",
        "title": "Environmental Noise Embeddings for Robust Speech Recognition",
        "summary": "  We propose a novel deep neural network architecture for speech recognition\nthat explicitly employs knowledge of the background environmental noise within\na deep neural network acoustic model. A deep neural network is used to predict\nthe acoustic environment in which the system in being used. The discriminative\nembedding generated at the bottleneck layer of this network is then\nconcatenated with traditional acoustic features as input to a deep neural\nnetwork acoustic model. Through a series of experiments on Resource Management,\nCHiME-3 task, and Aurora4, we show that the proposed approach significantly\nimproves speech recognition accuracy in noisy and highly reverberant\nenvironments, outperforming multi-condition training, noise-aware training,\ni-vector framework, and multi-task learning on both in-domain noise and unseen\nnoise.\n",
        "published": "2016-01-11T18:38:18Z",
        "pdf_link": "http://arxiv.org/pdf/1601.02553v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.03288v1",
        "title": "Predicting the Effectiveness of Self-Training: Application to Sentiment\n  Classification",
        "summary": "  The goal of this paper is to investigate the connection between the\nperformance gain that can be obtained by selftraining and the similarity\nbetween the corpora used in this approach. Self-training is a semi-supervised\ntechnique designed to increase the performance of machine learning algorithms\nby automatically classifying instances of a task and adding these as additional\ntraining material to the same classifier. In the context of language processing\ntasks, this training material is mostly an (annotated) corpus. Unfortunately\nself-training does not always lead to a performance increase and whether it\nwill is largely unpredictable. We show that the similarity between corpora can\nbe used to identify those setups for which self-training can be beneficial. We\nconsider this research as a step in the process of developing a classifier that\nis able to adapt itself to each new test corpus that it is presented with.\n",
        "published": "2016-01-13T15:55:36Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03288v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.03313v2",
        "title": "Political Speech Generation",
        "summary": "  In this report we present a system that can generate political speeches for a\ndesired political party. Furthermore, the system allows to specify whether a\nspeech should hold a supportive or opposing opinion. The system relies on a\ncombination of several state-of-the-art NLP methods which are discussed in this\nreport. These include n-grams, Justeson & Katz POS tag filter, recurrent neural\nnetworks, and latent Dirichlet allocation. Sequences of words are generated\nbased on probabilities obtained from two underlying models: A language model\ntakes care of the grammatical correctness while a topic model aims for textual\nconsistency. Both models were trained on the Convote dataset which contains\ntranscripts from US congressional floor debates. Furthermore, we present a\nmanual and an automated approach to evaluate the quality of generated speeches.\nIn an experimental evaluation generated speeches have shown very high quality\nin terms of grammatical correctness and sentence transitions.\n",
        "published": "2016-01-13T16:58:05Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03313v2"
    },
    {
        "id": "http://arxiv.org/abs/1601.03317v3",
        "title": "Implicit Distortion and Fertility Models for Attention-based\n  Encoder-Decoder NMT Model",
        "summary": "  Neural machine translation has shown very promising results lately. Most NMT\nmodels follow the encoder-decoder framework. To make encoder-decoder models\nmore flexible, attention mechanism was introduced to machine translation and\nalso other tasks like speech recognition and image captioning. We observe that\nthe quality of translation by attention-based encoder-decoder can be\nsignificantly damaged when the alignment is incorrect. We attribute these\nproblems to the lack of distortion and fertility models. Aiming to resolve\nthese problems, we propose new variations of attention-based encoder-decoder\nand compare them with other models on machine translation. Our proposed method\nachieved an improvement of 2 BLEU points over the original attention-based\nencoder-decoder.\n",
        "published": "2016-01-13T17:14:01Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03317v3"
    },
    {
        "id": "http://arxiv.org/abs/1601.03348v1",
        "title": "EvoGrader: an online formative assessment tool for automatically\n  evaluating written evolutionary explanations",
        "summary": "  EvoGrader is a free, online, on-demand formative assessment service designed\nfor use in undergraduate biology classrooms. EvoGrader's web portal is powered\nby Amazon's Elastic Cloud and run with LightSIDE Lab's open-source\nmachine-learning tools. The EvoGrader web portal allows biology instructors to\nupload a response file (.csv) containing unlimited numbers of evolutionary\nexplanations written in response to 86 different ACORNS (Assessing COntextual\nReasoning about Natural Selection) instrument items. The system automatically\nanalyzes the responses and provides detailed information about the scientific\nand naive concepts contained within each student's response, as well as overall\nstudent (and sample) reasoning model types. Graphs and visual models provided\nby EvoGrader summarize class-level responses; downloadable files of raw scores\n(in .csv format) are also provided for more detailed analyses. Although the\ncomputational machinery that EvoGrader employs is complex, using the system is\neasy. Users only need to know how to use spreadsheets to organize student\nresponses, upload files to the web, and use a web browser. A series of\nexperiments using new samples of 2,200 written evolutionary explanations\ndemonstrate that EvoGrader scores are comparable to those of trained human\nraters, although EvoGrader scoring takes 99% less time and is free. EvoGrader\nwill be of interest to biology instructors teaching large classes who seek to\nemphasize scientific practices such as generating scientific explanations, and\nto teach crosscutting ideas such as evolution and natural selection. The\nsoftware architecture of EvoGrader is described as it may serve as a template\nfor developing machine-learning portals for other core concepts within biology\nand across other disciplines.\n",
        "published": "2016-01-13T18:59:06Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03348v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.03650v4",
        "title": "Smoothing parameter estimation framework for IBM word alignment models",
        "summary": "  IBM models are very important word alignment models in Machine Translation.\nFollowing the Maximum Likelihood Estimation principle to estimate their\nparameters, the models will easily overfit the training data when the data are\nsparse. While smoothing is a very popular solution in Language Model, there\nstill lacks studies on smoothing for word alignment. In this paper, we propose\na framework which generalizes the notable work Moore [2004] of applying\nadditive smoothing to word alignment models. The framework allows developers to\ncustomize the smoothing amount for each pair of word. The added amount will be\nscaled appropriately by a common factor which reflects how much the framework\ntrusts the adding strategy according to the performance on data. We also\ncarefully examine various performance criteria and propose a smoothened version\nof the error count, which generally gives the best result.\n",
        "published": "2016-01-14T16:30:09Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03650v4"
    },
    {
        "id": "http://arxiv.org/abs/1601.03783v1",
        "title": "Towards Turkish ASR: Anatomy of a rule-based Turkish g2p",
        "summary": "  This paper describes the architecture and implementation of a rule-based\ngrapheme to phoneme converter for Turkish. The system accepts surface form as\ninput, outputs SAMPA mapping of the all parallel pronounciations according to\nthe morphological analysis together with stress positions. The system has been\nimplemented in Python\n",
        "published": "2016-01-15T00:09:52Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03783v1"
    },
    {
        "id": "http://arxiv.org/abs/1601.03916v3",
        "title": "Multimodal Pivots for Image Caption Translation",
        "summary": "  We present an approach to improve statistical machine translation of image\ndescriptions by multimodal pivots defined in visual space. The key idea is to\nperform image retrieval over a database of images that are captioned in the\ntarget language, and use the captions of the most similar images for\ncrosslingual reranking of translation outputs. Our approach does not depend on\nthe availability of large amounts of in-domain parallel data, but only relies\non available large datasets of monolingually captioned images, and on\nstate-of-the-art convolutional neural networks to compute image similarities.\nOur experimental evaluation shows improvements of 1 BLEU point over strong\nbaselines.\n",
        "published": "2016-01-15T13:42:04Z",
        "pdf_link": "http://arxiv.org/pdf/1601.03916v3"
    },
    {
        "id": "http://arxiv.org/abs/1601.04012v1",
        "title": "Detecting and Extracting Events from Text Documents",
        "summary": "  Events of various kinds are mentioned and discussed in text documents,\nwhether they are books, news articles, blogs or microblog feeds. The paper\nstarts by giving an overview of how events are treated in linguistics and\nphilosophy. We follow this discussion by surveying how events and associated\ninformation are handled in computationally. In particular, we look at how\ntextual documents can be mined to extract events and ancillary information.\nThese days, it is mostly through the application of various machine learning\ntechniques. We also discuss applications of event detection and extraction\nsystems, particularly in summarization, in the medical domain and in the\ncontext of Twitter posts. We end the paper with a discussion of challenges and\nfuture directions.\n",
        "published": "2016-01-15T17:33:39Z",
        "pdf_link": "http://arxiv.org/pdf/1601.04012v1"
    }
]